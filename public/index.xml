<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Econometrics and Free Software</title>
    <link>/</link>
    <description>Recent content on Econometrics and Free Software</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Feb 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to draw a map of arbitrary contiguous regions, or visualizing the spread of COVID-19 in the Greater Region</title>
      <link>/blog/2021-02-06-echarts_map/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/2021-02-06-echarts_map/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://happy-newton-bf63ad.netlify.app/&#34;&gt;
&lt;img src=&#34;/img/covidGrandeRegion.gif&#34; title = &#34;Click to go to visualisations&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I was able to blog during the year 2020 without mentioning the ongoing pandemic once. It’s not that I made
any conscious effort not to talk about it, but I did not really want to do something that had already
been done a 1000 times. This changed this year, when I wanted to look at the spread of
COVID-19, not only in the Grand-Duchy of Luxembourg, the country I live in, but also among our
neighbours. You see, the Grand-Duchy of Luxembourg is like an island, but instead of being surrounded
by water, it’s surrounded by Belgians, Germans and Frenchmen. Many of them commute every day to Luxembourg
to work, and even though they technically don’t live inside the country, many aspects of their
lives happen inside Luxembourguish borders. Their children might even come to school here, and sometimes
they live so close by the border, that they can catch Luxembourguish public transportation in their
towns. 200k commuters from Belgium, Germany and France work here every day. That’s half our
workforce! So that’s why I thought that it would make sense to look at the spread of the disease
at the level of the so-called &lt;em&gt;Greater Region&lt;/em&gt;. This &lt;em&gt;Greater Region&lt;/em&gt; is made up of the Grand-Duchy
of Luxembourg, the Provinces of Liège and Luxembourg in Belgium (hence why I keep writing the
&lt;em&gt;Grand-Duchy of&lt;/em&gt; Luxembourg to refer to the country, and the &lt;em&gt;Province of Luxembourg&lt;/em&gt; to refer
to the Belgian province of the same name), and two German &lt;em&gt;Länders&lt;/em&gt;, the Saarland and
the Rhineland-Palatinate. Confused? Welcome to Europe, where supranational institutions
literally have to have a page entitled &lt;a href=&#34;https://www.coe.int/en/web/about-us/do-not-get-confused&#34;&gt;Do not get confused&lt;/a&gt;
so that citizens don’t get lost (we still do).&lt;/p&gt;
&lt;p&gt;So the Greater Region is not a state, but facilitates collaboration between the regions comprising
it. To me, technically a citizen of the Greater Region, it feels like there was a want to &lt;strong&gt;peacefully&lt;/strong&gt; correct
for the randomness of history, where German-speaking regions ended up in both France and Belgium,
and where Belgium and Luxembourg, well, somehow became independent countries.&lt;/p&gt;
&lt;p&gt;Anyways, what I wanted to do was to first of all get the COVID-19 daily cases data for each of these
regions. I did that, and even created a package called &lt;code&gt;{covidGrandeRegion}&lt;/code&gt; hosted
&lt;a href=&#34;https://github.com/b-rodrigues/covidGrandeRegion&#34;&gt;here&lt;/a&gt; that makes it very easy to download the
latest data for the Greater Region. I will write another blog post about it, I have something
in mind that I wanted to try for some time, and this was the first step.
Then I thought that adding a function that would create a map could also be nice. And this is
where the technical aspect of this blog post starts.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problems-to-map-the-greater-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problems to map the Greater Region&lt;/h2&gt;
&lt;p&gt;So how do you draw a map for an arbitrary landmass like the Greater Region? I wanted to draw the
maps using &lt;code&gt;{echarts4r}&lt;/code&gt;, and there’s a very easy &lt;a href=&#34;https://echarts4r.john-coene.com/articles/make-geo-json.html&#34;&gt;guide you can read&lt;/a&gt;.
If you want to draw a map for one, or several, countries, this guide is all you need. But I wanted
a map with only parts of France, Belgium and Germany. The only complete country was Luxembourg.
So the first problem was how to get only parts of a country. The second problem, is that I had
daily covid cases for the lowest administrative levels for France (which are &lt;em&gt;Départements&lt;/em&gt;),
Belgium (the &lt;em&gt;Provinces&lt;/em&gt;) and Germany (&lt;em&gt;Land-&lt;/em&gt; and &lt;em&gt;Stadtkreise&lt;/em&gt;). But for the Grand-Duchy of Luxembourg,
there’s only data at the level of the country. So this would be another problem. How to draw a map
with unequal levels of precision?
One final problem: the names of the administrative divisions in my covid datasets are not the same
than the ones that get downloaded if you follow the guide I linked before. So I had to rename
them as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-solutions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The solutions&lt;/h2&gt;
&lt;p&gt;Let’s first start by following the guide, so loading the packages, and getting the maps I need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(echarts4r)
library(sp)
library(raster)
library(geojsonio)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;france_dep &amp;lt;- getData(&amp;quot;GADM&amp;quot;, country = &amp;quot;FRANCE&amp;quot;, level = 2)

ger_kreise &amp;lt;- getData(&amp;quot;GADM&amp;quot;, country = &amp;quot;GERMANY&amp;quot;, level = 2)

be_province &amp;lt;- getData(&amp;quot;GADM&amp;quot;, country = &amp;quot;BELGIUM&amp;quot;, level = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above lines of code load the required packages, and download the maps for France, Belgium
and Germany with the required administrative level I need. I’ll leave Luxembourg for last.&lt;/p&gt;
&lt;p&gt;Let’s take a look at what type of object we’re dealing with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(france_dep)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;SpatialPolygonsDataFrame&amp;quot;
## attr(,&amp;quot;package&amp;quot;)
## [1] &amp;quot;sp&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So it seems to be something like a data frame, but probably more complex. Looking for some help
online, I saw that you can coerce it to a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.data.frame(be_province)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    GID_0  NAME_0   GID_1     NAME_1 NL_NAME_1     GID_2          NAME_2
## 1    BEL Belgium BEL.1_1  Bruxelles      &amp;lt;NA&amp;gt; BEL.1.1_1       Bruxelles
## 2    BEL Belgium BEL.2_1 Vlaanderen      &amp;lt;NA&amp;gt; BEL.2.1_1       Antwerpen
## 3    BEL Belgium BEL.2_1 Vlaanderen      &amp;lt;NA&amp;gt; BEL.2.2_1         Limburg
## 4    BEL Belgium BEL.2_1 Vlaanderen      &amp;lt;NA&amp;gt; BEL.2.3_1 Oost-Vlaanderen
## 5    BEL Belgium BEL.2_1 Vlaanderen      &amp;lt;NA&amp;gt; BEL.2.4_1  Vlaams Brabant
## 6    BEL Belgium BEL.2_1 Vlaanderen      &amp;lt;NA&amp;gt; BEL.2.5_1 West-Vlaanderen
## 7    BEL Belgium BEL.3_1   Wallonie      &amp;lt;NA&amp;gt; BEL.3.1_1  Brabant Wallon
## 8    BEL Belgium BEL.3_1   Wallonie      &amp;lt;NA&amp;gt; BEL.3.2_1         Hainaut
## 9    BEL Belgium BEL.3_1   Wallonie      &amp;lt;NA&amp;gt; BEL.3.3_1           Liège
## 10   BEL Belgium BEL.3_1   Wallonie      &amp;lt;NA&amp;gt; BEL.3.4_1      Luxembourg
## 11   BEL Belgium BEL.3_1   Wallonie      &amp;lt;NA&amp;gt; BEL.3.5_1           Namur
##                                                                                                             VARNAME_2
## 1  Brussel Hoofstadt|Brusselse Hoofdstedelijke Gewest|Brüssel|Bruxelas|Région de Bruxelles-Capitale|Brussels|Bruselas
## 2                                                                            Amberes|Antuérpia|Antwerp|Anvers|Anversa
## 3                                                                                                   Limbourg|Limburgo
## 4                   Flandres Oriental|Fiandra Orientale|Flandes Oriental|Flandre orientale|East Flanders|Ost Flandern
## 5                                                 Brabant Flamand|Brabante Flamenco|Brabante Flamengo|Flemish Brabant
## 6           Fiandra Occidentale|Flandes Occidental|Flandre occidentale|Flandres Ocidental|West Flandern|West Flanders
## 7                                                                                       Waals Brabant|Walloon Brabant
## 8                                                                                                 Henegouwen|Hennegau
## 9                                                                                            Luik|Liegi|Lieja|Lüttich
## 10                                                                                   Lussemburgo|Luxemburg|Luxemburgo
## 11                                                                                                              Namen
##    NL_NAME_2                                TYPE_2      ENGTYPE_2 CC_2 HASC_2
## 1       &amp;lt;NA&amp;gt; Hoofdstedelijk Gewest|Région Capitale Capital Region &amp;lt;NA&amp;gt;  BE.BU
## 2       &amp;lt;NA&amp;gt;                             Provincie       Province &amp;lt;NA&amp;gt;  BE.AN
## 3       &amp;lt;NA&amp;gt;                             Provincie       Province &amp;lt;NA&amp;gt;  BE.LI
## 4       &amp;lt;NA&amp;gt;                             Provincie       Province &amp;lt;NA&amp;gt;  BE.OV
## 5       &amp;lt;NA&amp;gt;                             Provincie       Province &amp;lt;NA&amp;gt;  BE.VB
## 6       &amp;lt;NA&amp;gt;                             Provincie       Province &amp;lt;NA&amp;gt;  BE.WV
## 7       &amp;lt;NA&amp;gt;                              Province      Provincie &amp;lt;NA&amp;gt;  BE.BW
## 8       &amp;lt;NA&amp;gt;                              Province      Provincie &amp;lt;NA&amp;gt;  BE.HT
## 9       &amp;lt;NA&amp;gt;                              Province      Provincie &amp;lt;NA&amp;gt;  BE.LG
## 10      &amp;lt;NA&amp;gt;                              Province      Provincie &amp;lt;NA&amp;gt;  BE.LX
## 11      &amp;lt;NA&amp;gt;                              Province      Provincie &amp;lt;NA&amp;gt;  BE.NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re not going to convert them to data frames however; but this is an interesting clue; these &lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt;
objects share common methods with data frames. What this means is that we can use the usual,
base R way of manipulating these objects.&lt;/p&gt;
&lt;p&gt;So to get only the French &lt;em&gt;départements&lt;/em&gt; I need, I can slice them like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lorraine &amp;lt;- france_dep[`%in%`(france_dep$NAME_2, c(&amp;quot;Meurthe-et-Moselle&amp;quot;, &amp;quot;Meuse&amp;quot;, &amp;quot;Moselle&amp;quot;, &amp;quot;Vosges&amp;quot;)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Same for the German &lt;em&gt;kreise&lt;/em&gt;, here I select the &lt;em&gt;Länder&lt;/em&gt; which are a higher administrative division
than the Kreise, which makes it faster (so I don’t need to type all the 40+ Kreise):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ger_kreise &amp;lt;- ger_kreise[`%in%`(ger_kreise$NAME_1, c(&amp;quot;Rheinland-Pfalz&amp;quot;, &amp;quot;Saarland&amp;quot;)),]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For Germany, many Kreise had a name which was different than on my covid data, so I had to
rename them. So here again, the base R way of doing things works:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ger_kreise$NAME_2[ger_kreise$NAME_2 == &amp;quot;Eifelkreis Bitburg-Prüm&amp;quot;]  &amp;lt;- &amp;quot;Bitburg-Prüm&amp;quot;
ger_kreise$NAME_2[ger_kreise$NAME_2 == &amp;quot;St. Wendel&amp;quot;]  &amp;lt;- &amp;quot;Sankt Wendel&amp;quot;
ger_kreise$NAME_2[ger_kreise$NAME_2 == &amp;quot;Altenkirchen (Westerwald)&amp;quot;]  &amp;lt;- &amp;quot;Altenkirchen&amp;quot;
ger_kreise$NAME_2[ger_kreise$NAME_2 == &amp;quot;Neustadt an der Weinstraße&amp;quot;]  &amp;lt;- &amp;quot;Neustadt a.d.Weinstraße&amp;quot;
ger_kreise$NAME_2[ger_kreise$NAME_2 == &amp;quot;Landau in der Pfalz&amp;quot;]  &amp;lt;- &amp;quot;Landau i.d.Pfalz&amp;quot;
ger_kreise$NAME_2[ger_kreise$NAME_2 == &amp;quot;Ludwigshafen am Rhein&amp;quot;]  &amp;lt;- &amp;quot;Ludwigshafen&amp;quot;
ger_kreise$NAME_2[ger_kreise$NAME_2 == &amp;quot;Frankenthal (Pfalz)&amp;quot;]  &amp;lt;- &amp;quot;Frankenthal&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I do the same for Belgium, and rename their province of Luxembourg, which was simply called
“Luxembourg”, to “Province de Luxembourg”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;be_wallonia &amp;lt;- be_province[be_province$NAME_1 == &amp;quot;Wallonie&amp;quot;, ]
be_wallonia$NAME_2[be_wallonia$NAME_2 == &amp;quot;Luxembourg&amp;quot;]  &amp;lt;- &amp;quot;Province de Luxembourg&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I rename the province because the Grand-Duchy of Luxembourg is also only called “Luxembourg” in the
data, and this would cause issues when mapping.&lt;/p&gt;
&lt;p&gt;Now, comes Luxembourg. As I’ve written above, I only have data at the level of the country, so
I download the country map:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lu_map_0 &amp;lt;- getData(&amp;quot;GADM&amp;quot;, country = &amp;quot;LUXEMBOURG&amp;quot;, level = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s also see how it looks like as a data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.data.frame(lu_map_0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   GID_0     NAME_0
## 1   LUX Luxembourg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike the previous &lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt;s, there are much less columns and this will cause
an issue. Indeed, in order to have a single &lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt; object to draw my map,
I will need to combine them. This will be very easy, by simple using the &lt;code&gt;rbind()&lt;/code&gt; function.
Again, simply using base R functions. However, this only works if the data frames have the same
columns. Another issue, is that I will be using the names of the regions which are in the &lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt;s’
column called &lt;code&gt;NAME_2&lt;/code&gt;, but for Luxembourg, the name of the region (in this case the whole country)
is in the column called &lt;code&gt;NAME_0&lt;/code&gt;. So I need to add this columns to the &lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt;
object for Luxembourg:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lu_map_0$GID_1 &amp;lt;- NA
lu_map_0$NAME_1 &amp;lt;- NA
lu_map_0$NL_NAME_1 &amp;lt;- NA
lu_map_0$GID_2 &amp;lt;- NA
lu_map_0$NAME_2 &amp;lt;- &amp;quot;Luxembourg&amp;quot;
lu_map_0$VARNAME_2 &amp;lt;- NA
lu_map_0$NL_NAME_2 &amp;lt;- NA
lu_map_0$TYPE_2 &amp;lt;- NA
lu_map_0$ENGTYPE_2 &amp;lt;- NA
lu_map_0$CC_2 &amp;lt;- NA
lu_map_0$HASC_2 &amp;lt;- NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aaaand… that’s it! Wasn’t that hard, but a bit convoluted nonetheless. Now I can bind all
the &lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt; objects in one and use that for mapping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grande_region &amp;lt;- do.call(rbind, list(lorraine, ger_kreise, be_wallonia, lu_map_0))

as.data.frame(grande_region)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     GID_0     NAME_0    GID_1          NAME_1 NL_NAME_1       GID_2
## 76    FRA     France  FRA.6_1       Grand Est      &amp;lt;NA&amp;gt;   FRA.6.7_1
## 77    FRA     France  FRA.6_1       Grand Est      &amp;lt;NA&amp;gt;   FRA.6.8_1
## 78    FRA     France  FRA.6_1       Grand Est      &amp;lt;NA&amp;gt;   FRA.6.9_1
## 70    FRA     France  FRA.6_1       Grand Est      &amp;lt;NA&amp;gt;  FRA.6.10_1
## 99    DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.1_1
## 110   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.2_1
## 121   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.3_1
## 129   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.4_1
## 130   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.5_1
## 131   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.6_1
## 132   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.7_1
## 133   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.8_1
## 134   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt;  DEU.11.9_1
## 100   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.10_1
## 101   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.11_1
## 102   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.12_1
## 104   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.14_1
## 103   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.13_1
## 105   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.15_1
## 106   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.16_1
## 107   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.17_1
## 108   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.18_1
## 111   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.20_1
## 109   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.19_1
## 112   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.21_1
## 113   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.22_1
## 114   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.23_1
## 115   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.24_1
## 116   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.25_1
## 117   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.26_1
## 118   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.27_1
## 119   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.28_1
## 120   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.29_1
## 122   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.30_1
## 124   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.32_1
## 123   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.31_1
## 125   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.33_1
## 126   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.34_1
## 127   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.35_1
## 128   DEU    Germany DEU.11_1 Rheinland-Pfalz      &amp;lt;NA&amp;gt; DEU.11.36_1
## 135   DEU    Germany DEU.12_1        Saarland      &amp;lt;NA&amp;gt;  DEU.12.1_1
## 136   DEU    Germany DEU.12_1        Saarland      &amp;lt;NA&amp;gt;  DEU.12.2_1
## 137   DEU    Germany DEU.12_1        Saarland      &amp;lt;NA&amp;gt;  DEU.12.3_1
## 138   DEU    Germany DEU.12_1        Saarland      &amp;lt;NA&amp;gt;  DEU.12.4_1
## 139   DEU    Germany DEU.12_1        Saarland      &amp;lt;NA&amp;gt;  DEU.12.5_1
## 140   DEU    Germany DEU.12_1        Saarland      &amp;lt;NA&amp;gt;  DEU.12.6_1
## 7     BEL    Belgium  BEL.3_1        Wallonie      &amp;lt;NA&amp;gt;   BEL.3.1_1
## 8     BEL    Belgium  BEL.3_1        Wallonie      &amp;lt;NA&amp;gt;   BEL.3.2_1
## 9     BEL    Belgium  BEL.3_1        Wallonie      &amp;lt;NA&amp;gt;   BEL.3.3_1
## 10    BEL    Belgium  BEL.3_1        Wallonie      &amp;lt;NA&amp;gt;   BEL.3.4_1
## 11    BEL    Belgium  BEL.3_1        Wallonie      &amp;lt;NA&amp;gt;   BEL.3.5_1
## 1     LUX Luxembourg     &amp;lt;NA&amp;gt;            &amp;lt;NA&amp;gt;      &amp;lt;NA&amp;gt;        &amp;lt;NA&amp;gt;
##                                NAME_2                        VARNAME_2
## 76                 Meurthe-et-Moselle                             &amp;lt;NA&amp;gt;
## 77                              Meuse                             &amp;lt;NA&amp;gt;
## 78                            Moselle                       Lothringen
## 70                             Vosges                             &amp;lt;NA&amp;gt;
## 99                          Ahrweiler                             &amp;lt;NA&amp;gt;
## 110                      Altenkirchen                             &amp;lt;NA&amp;gt;
## 121                       Alzey-Worms                             &amp;lt;NA&amp;gt;
## 129                      Bad Dürkheim                             &amp;lt;NA&amp;gt;
## 130                     Bad Kreuznach                             &amp;lt;NA&amp;gt;
## 131               Bernkastel-Wittlich                             &amp;lt;NA&amp;gt;
## 132                        Birkenfeld                             &amp;lt;NA&amp;gt;
## 133                       Cochem-Zell                             &amp;lt;NA&amp;gt;
## 134                  Donnersbergkreis                             &amp;lt;NA&amp;gt;
## 100                      Bitburg-Prüm                             &amp;lt;NA&amp;gt;
## 101                       Frankenthal                             &amp;lt;NA&amp;gt;
## 102                       Germersheim                             &amp;lt;NA&amp;gt;
## 104                    Kaiserslautern                             &amp;lt;NA&amp;gt;
## 103 Kaiserslautern (Kreisfreie Stadt)                             &amp;lt;NA&amp;gt;
## 105                           Koblenz                             &amp;lt;NA&amp;gt;
## 106                             Kusel                             &amp;lt;NA&amp;gt;
## 107                  Landau i.d.Pfalz                             &amp;lt;NA&amp;gt;
## 108                      Ludwigshafen                             &amp;lt;NA&amp;gt;
## 111                             Mainz                             &amp;lt;NA&amp;gt;
## 109                      Mainz-Bingen                             &amp;lt;NA&amp;gt;
## 112                     Mayen-Koblenz                             &amp;lt;NA&amp;gt;
## 113           Neustadt a.d.Weinstraße                             &amp;lt;NA&amp;gt;
## 114                           Neuwied                             &amp;lt;NA&amp;gt;
## 115                         Pirmasens                             &amp;lt;NA&amp;gt;
## 116              Rhein-Hunsrück-Kreis                             &amp;lt;NA&amp;gt;
## 117                  Rhein-Lahn-Kreis                             &amp;lt;NA&amp;gt;
## 118                 Rhein-Pfalz-Kreis                             &amp;lt;NA&amp;gt;
## 119                            Speyer                             &amp;lt;NA&amp;gt;
## 120               Südliche Weinstraße                             &amp;lt;NA&amp;gt;
## 122                      Südwestpfalz                             &amp;lt;NA&amp;gt;
## 124                             Trier                             &amp;lt;NA&amp;gt;
## 123                    Trier-Saarburg                             &amp;lt;NA&amp;gt;
## 125                       Vulkaneifel                             &amp;lt;NA&amp;gt;
## 126                   Westerwaldkreis                             &amp;lt;NA&amp;gt;
## 127                             Worms                             &amp;lt;NA&amp;gt;
## 128                       Zweibrücken                             &amp;lt;NA&amp;gt;
## 135                     Merzig-Wadern                             &amp;lt;NA&amp;gt;
## 136                       Neunkirchen                             &amp;lt;NA&amp;gt;
## 137       Regionalverband Saarbrücken                             &amp;lt;NA&amp;gt;
## 138                         Saarlouis                             &amp;lt;NA&amp;gt;
## 139                   Saarpfalz-Kreis                             &amp;lt;NA&amp;gt;
## 140                      Sankt Wendel                             &amp;lt;NA&amp;gt;
## 7                      Brabant Wallon    Waals Brabant|Walloon Brabant
## 8                             Hainaut              Henegouwen|Hennegau
## 9                               Liège         Luik|Liegi|Lieja|Lüttich
## 10             Province de Luxembourg Lussemburgo|Luxemburg|Luxemburgo
## 11                              Namur                            Namen
## 1                          Luxembourg                             &amp;lt;NA&amp;gt;
##     NL_NAME_2           TYPE_2  ENGTYPE_2  CC_2   HASC_2
## 76       &amp;lt;NA&amp;gt;      Département Department    54    FR.MM
## 77       &amp;lt;NA&amp;gt;      Département Department    55    FR.MS
## 78       &amp;lt;NA&amp;gt;      Département Department    57    FR.MO
## 70       &amp;lt;NA&amp;gt;      Département Department    88    FR.VG
## 99       &amp;lt;NA&amp;gt;        Landkreis   District 07131 DE.RP.AR
## 110      &amp;lt;NA&amp;gt;        Landkreis   District 07132 DE.RP.AT
## 121      &amp;lt;NA&amp;gt;        Landkreis   District 07331 DE.RP.AW
## 129      &amp;lt;NA&amp;gt;        Landkreis   District 07332 DE.RP.BD
## 130      &amp;lt;NA&amp;gt;        Landkreis   District 07133 DE.RP.BK
## 131      &amp;lt;NA&amp;gt;        Landkreis   District 07231 DE.RP.BW
## 132      &amp;lt;NA&amp;gt;        Landkreis   District 07134 DE.RP.BR
## 133      &amp;lt;NA&amp;gt;        Landkreis   District 07135 DE.RP.CZ
## 134      &amp;lt;NA&amp;gt;        Landkreis   District 07333 DE.RP.DN
## 100      &amp;lt;NA&amp;gt;        Landkreis   District 07232 DE.RP.EB
## 101      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07311 DE.RP.FA
## 102      &amp;lt;NA&amp;gt;        Landkreis   District 07334 DE.RP.GR
## 104      &amp;lt;NA&amp;gt;        Landkreis   District 07335 DE.RP.KL
## 103      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07312 DE.RP.KL
## 105      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07111 DE.RP.KO
## 106      &amp;lt;NA&amp;gt;        Landkreis   District 07336 DE.RP.KU
## 107      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07313 DE.RP.LP
## 108      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07314 DE.RP.LR
## 111      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07315 DE.RP.MI
## 109      &amp;lt;NA&amp;gt;        Landkreis   District 07339 DE.RP.MB
## 112      &amp;lt;NA&amp;gt;        Landkreis   District 07137 DE.RP.MK
## 113      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07316 DE.RP.NW
## 114      &amp;lt;NA&amp;gt;        Landkreis   District 07138 DE.RP.NU
## 115      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07317 DE.RP.PR
## 116      &amp;lt;NA&amp;gt;        Landkreis   District 07140 DE.RP.RH
## 117      &amp;lt;NA&amp;gt;        Landkreis   District 07141 DE.RP.RN
## 118      &amp;lt;NA&amp;gt;        Landkreis   District 07338 DE.RP.RZ
## 119      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07318 DE.RP.SE
## 120      &amp;lt;NA&amp;gt;        Landkreis   District 07337 DE.RP.SW
## 122      &amp;lt;NA&amp;gt;        Landkreis   District 07340 DE.RP.SD
## 124      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07211 DE.RP.TI
## 123      &amp;lt;NA&amp;gt;        Landkreis   District 07235 DE.RP.TS
## 125      &amp;lt;NA&amp;gt;        Landkreis   District 07233 DE.RP.VL
## 126      &amp;lt;NA&amp;gt;        Landkreis   District 07143 DE.RP.WS
## 127      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07319 DE.RP.WR
## 128      &amp;lt;NA&amp;gt; Kreisfreie Stadt   District 07320 DE.RP.ZE
## 135      &amp;lt;NA&amp;gt;        Landkreis   District 10042 DE.SL.MW
## 136      &amp;lt;NA&amp;gt;        Landkreis   District 10043 DE.SL.NU
## 137      &amp;lt;NA&amp;gt;        Landkreis   District 10041 DE.SL.SB
## 138      &amp;lt;NA&amp;gt;        Landkreis   District 10044 DE.SL.SA
## 139      &amp;lt;NA&amp;gt;        Landkreis   District 10045 DE.SL.SP
## 140      &amp;lt;NA&amp;gt;        Landkreis   District 10046 DE.SL.SW
## 7        &amp;lt;NA&amp;gt;         Province  Provincie  &amp;lt;NA&amp;gt;    BE.BW
## 8        &amp;lt;NA&amp;gt;         Province  Provincie  &amp;lt;NA&amp;gt;    BE.HT
## 9        &amp;lt;NA&amp;gt;         Province  Provincie  &amp;lt;NA&amp;gt;    BE.LG
## 10       &amp;lt;NA&amp;gt;         Province  Provincie  &amp;lt;NA&amp;gt;    BE.LX
## 11       &amp;lt;NA&amp;gt;         Province  Provincie  &amp;lt;NA&amp;gt;    BE.NA
## 1        &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;       &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;     &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now I can continue following the tutorial from the &lt;code&gt;{echarts4r}&lt;/code&gt; website, by converting this
&lt;code&gt;SpatialPolygonsDataFrame&lt;/code&gt; object for the Greater Region into a geojson file which can now be
used to draw maps! You can take a look at the final result &lt;a href=&#34;https://happy-newton-bf63ad.netlify.app/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I don’t post the code to draw the map here, because it would require some more tinkering by
joining the COVID data. But you can find my raw script &lt;a href=&#34;https://github.com/b-rodrigues/covidGrandeRegion/blob/master/data-raw/maps.R&#34;&gt;here&lt;/a&gt;
(lines 51 to 61) or you could also take a look at the &lt;code&gt;draw_map()&lt;/code&gt; function from the package
I made, which you can find &lt;a href=&#34;https://github.com/b-rodrigues/covidGrandeRegion/blob/master/R/draw_map.R&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I really like the end result, &lt;code&gt;{echarts4r}&lt;/code&gt; is really a fantastic package!
Stay tuned part 2 of the project, which will deal with machine learning.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.
You can also watch my videos on &lt;a href=&#34;https://www.youtube.com/c/BrunoRodrigues1988/&#34;&gt;youtube&lt;/a&gt;.
So much content for you to consoom!&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A year in review</title>
      <link>/blog/2020-12-30-year_review/</link>
      <pubDate>Wed, 30 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-12-30-year_review/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;
&lt;img src=&#34;/img/2020_review.png&#34; title = &#34;It wasn&#39;t the worst year ever, but it was quite crap.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script src=&#34;https://www.sciencemag.org/news/2018/11/why-536-was-worst-year-be-alive&#34;&gt;&lt;/script&gt;
&lt;p&gt;This blog post just contains the links I mention in my video that you can watch &lt;a href=&#34;https://youtu.be/Z5xNALiILzg&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I mention the following books, packages, and people in my video:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://echarts4r.john-coene.com/index.html&#34;&gt;echarts4r&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wlandau.github.io/targets/&#34;&gt;targets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/easystats&#34;&gt;easystats&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hhsievertsen.github.io/applied_econ_with_r/#Welcome&#34;&gt;Applied Economics with R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/coolbutuseless?tab=repositories&#34;&gt;coolbutuseless&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCKxHtRdtFEPihEjTtjG8Y8w/featured&#34;&gt;Data Science ZJ&lt;/a&gt; and &lt;a href=&#34;https://diskframe.com/&#34;&gt;disk.frame&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34;&gt;Statistical Rethinking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://thinkr-open.github.io/golem/&#34;&gt;Golem&lt;/a&gt; and &lt;a href=&#34;https://engineering-shiny.org/&#34;&gt;Engineering production-grade shiny apps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://moderndive.com/&#34;&gt;ModernDive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many others created and shared amazing content during the year, so sorry I could not mention
everyone!&lt;/p&gt;
&lt;p&gt;Happy new year to all and thank you for the ongoing support!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.
You can also watch my videos on &lt;a href=&#34;https://www.youtube.com/c/BrunoRodrigues1988/&#34;&gt;youtube&lt;/a&gt;.
So much content for you to consoom!&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>(Half) Lies, (half) truths and (half) statistics</title>
      <link>/blog/2020-12-12-ethics_statistics/</link>
      <pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-12-12-ethics_statistics/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3-lasso/d3-lasso.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/ggiraphjs/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/ggiraphjs/ggiraphjs.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/girafe-binding/girafe.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;
&lt;img src=&#34;/img/simpson.gif&#34; title = &#34;Sometimes, simple does not mean trivial, but many confuse the two.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
&lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;Note: if you’re reading this and images are not showing, visit the original post on my blog. The
blog post contains interactive plots which help in understanding the point I’m making.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I’ve recently come across this graph (on Twitter) from the Economist:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/economist_safe_vaccines.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can read the article &lt;a href=&#34;https://archive.is/EcdNZ&#34;&gt;here&lt;/a&gt; (archived for posterity). There are many
things wrong with this chart.
First of all, the economist is fitting a linear regression to some data points, and does not provide
anything else to the reader, namely the regression coefficients, their standard errors, and the R².
I know already that some readers will disagree, thinking something along the lines of
“But Bruno, come on, this is only to show that’s there a negative correlation between GDP per
capita and trust in vaccines! The readers don’t need to be bothered with these complex concepts.
This is just an illustration, and it’s good enough.”&lt;/p&gt;
&lt;p&gt;WRONG.&lt;/p&gt;
&lt;p&gt;Look, I’m all for good enough. That’s very likely going to be my epitaph. But sometimes, you can’t
simplify things so much that they’re not only misleading, but lies. In this case here, the
relationship between GDP per capita and trust in vaccines, if there is any, is probably highly nonlinear,
and very difficult to pinpoint with any degree of accuracy.
But before going further, let’s get the data and replicate the graph. I’ll be adding the equation
of the regression line as well as the R² to the plot. I won’t comment my code, since the point of this blog
post is not to teach you how to do it, but of course, you’re very welcome to reproduce the analysis.&lt;/p&gt;
&lt;p&gt;You can download the data &lt;a href=&#34;https://wellcome.org/reports/wellcome-global-monitor/2018/appendix-country-level-data&#34;&gt;here&lt;/a&gt;,
under “Full dataset for this chart”. You can also grab a csv version &lt;a href=&#34;https://gist.githubusercontent.com/b-rodrigues/388f6309a462c9ccbdf00f32ac9055cb/raw/92962f08f9e23b9a8586045291795f4ab21ad053/wgm2018.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Click to see the code
&lt;/summary&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggiraph)

dataset &amp;lt;- data.table::fread(&amp;quot;https://gist.githubusercontent.com/b-rodrigues/388f6309a462c9ccbdf00f32ac9055cb/raw/92962f08f9e23b9a8586045291795f4ab21ad053/wgm2018.csv&amp;quot;)

dataset &amp;lt;- dataset %&amp;gt;%
  filter(grepl(&amp;quot;(GDP per capita)|(Q25)&amp;quot;, question_statistic)) %&amp;gt;%
  mutate(response_type = ifelse(response_type == &amp;quot;&amp;quot;, &amp;quot;GDP per capita&amp;quot;, response_type)) %&amp;gt;%
  filter(grepl(&amp;quot;(National Total)|(GDP)&amp;quot;, response_type)) %&amp;gt;%
  mutate(response_type = str_remove(response_type, &amp;quot;National Total: &amp;quot;)) %&amp;gt;%
  select(country_name, response = response_type, value = result_percent) %&amp;gt;%
  mutate(gdp_per_capita = ifelse(grepl(&amp;quot;GDP&amp;quot;, response), value, NA)) %&amp;gt;% 
  fill(gdp_per_capita, .direction = &amp;quot;down&amp;quot;) %&amp;gt;%
  filter(!grepl(&amp;quot;GDP&amp;quot;, response))  %&amp;gt;%
  mutate(gdp_per_capita = as.numeric(gdp_per_capita),
         value = as.numeric(value),
         l_gdp = log(gdp_per_capita))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data &amp;lt;- dataset %&amp;gt;%
  mutate(agree = ifelse(grepl(&amp;quot; agree$&amp;quot;, response), &amp;quot;safe&amp;quot;, &amp;quot;not_safe&amp;quot;)) %&amp;gt;%  
  group_by(country_name, l_gdp, agree) %&amp;gt;% 
  summarise(value = sum(value)) %&amp;gt;%
  filter(agree == &amp;quot;safe&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;country_name&amp;#39;, &amp;#39;l_gdp&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lin_mod &amp;lt;- lm(value ~ l_gdp, data = plot_data)

lin_mod_coefs &amp;lt;- coefficients(lin_mod)
lin_mod_se &amp;lt;- sqrt(diag(vcov(lin_mod)))

regression_line_result &amp;lt;- paste0(&amp;quot;value = &amp;quot;,
       round(lin_mod_coefs[1], 2),
       &amp;quot;[&amp;quot;,
       round(lin_mod_se[1], 2),
       &amp;quot;]&amp;quot;,
       round(lin_mod_coefs[2], 2),
       &amp;quot;[&amp;quot;,
       round(lin_mod_se[2], 2),
       &amp;quot;]&amp;quot;,
       &amp;quot;*l_gdp&amp;quot;,
       &amp;quot;,\n R2 = &amp;quot;,
       round(summary(lin_mod)$r.squared, 2))

my_plot &amp;lt;- plot_data %&amp;gt;%
  ggplot(aes(y = value, x = l_gdp)) +
  geom_point_interactive(aes(tooltip = country_name), colour = &amp;quot;orange&amp;quot;) +
  #geom_point() +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE) +
  #ggrepel::geom_label_repel(aes(label = country_name)) +
  geom_text(y = 35, x = 8,
            label = regression_line_result,
            colour = &amp;quot;white&amp;quot;,
            size = 3) +
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;If you look at the code above, you’ll see that I’m doing a bunch of stuff to reproduce the graph.
Let’s take a look at it (you can mouse over the points to see the country names over the labels):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;girafe(ggobj = my_plot, width_svg = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;girafe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;?xml version=\&#34;1.0\&#34; encoding=\&#34;UTF-8\&#34;?&gt;\n&lt;svg xmlns=\&#34;http://www.w3.org/2000/svg\&#34; xmlns:xlink=\&#34;http://www.w3.org/1999/xlink\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621\&#34; viewBox=\&#34;0 0 576.00 360.00\&#34;&gt;\n  &lt;g&gt;\n    &lt;defs&gt;\n      &lt;clipPath id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_1\&#34;&gt;\n        &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34;/&gt;\n      &lt;\/clipPath&gt;\n    &lt;\/defs&gt;\n    &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_1\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_1)\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.75\&#34; stroke=\&#34;#FFFFFF\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34;/&gt;\n    &lt;defs&gt;\n      &lt;clipPath id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_2\&#34;&gt;\n        &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34;/&gt;\n      &lt;\/clipPath&gt;\n    &lt;\/defs&gt;\n    &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_2\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_2)\&#34; fill=\&#34;#272B30\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#000000\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34;/&gt;\n    &lt;defs&gt;\n      &lt;clipPath id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3\&#34;&gt;\n        &lt;rect x=\&#34;40.25\&#34; y=\&#34;5.48\&#34; width=\&#34;530.27\&#34; height=\&#34;322.82\&#34;/&gt;\n      &lt;\/clipPath&gt;\n    &lt;\/defs&gt;\n    &lt;polyline points=\&#34;40.25,309.31 570.52,309.31\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_3\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,222.99 570.52,222.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_4\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,136.68 570.52,136.68\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_5\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,50.36 570.52,50.36\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_6\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;54.78,328.30 54.78,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_7\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;152.14,328.30 152.14,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_8\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;249.50,328.30 249.50,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_9\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;346.86,328.30 346.86,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_10\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;444.22,328.30 444.22,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_11\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;541.58,328.30 541.58,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_12\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,266.15 570.52,266.15\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_13\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,179.84 570.52,179.84\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_14\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,93.52 570.52,93.52\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_15\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,7.21 570.52,7.21\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_16\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;103.46,328.30 103.46,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_17\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;200.82,328.30 200.82,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_18\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;298.18,328.30 298.18,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_19\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;395.54,328.30 395.54,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_20\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;492.90,328.30 492.90,5.48\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_21\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;circle cx=\&#34;160.60\&#34; cy=\&#34;33.10\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_22\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Afghanistan\&#34;/&gt;\n    &lt;circle cx=\&#34;343.78\&#34; cy=\&#34;166.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_23\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Albania\&#34;/&gt;\n    &lt;circle cx=\&#34;359.81\&#34; cy=\&#34;166.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_24\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Algeria\&#34;/&gt;\n    &lt;circle cx=\&#34;389.89\&#34; cy=\&#34;58.99\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_25\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Argentina\&#34;/&gt;\n    &lt;circle cx=\&#34;315.17\&#34; cy=\&#34;257.52\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_26\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Armenia\&#34;/&gt;\n    &lt;circle cx=\&#34;472.31\&#34; cy=\&#34;80.57\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_27\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Australia\&#34;/&gt;\n    &lt;circle cx=\&#34;479.91\&#34; cy=\&#34;201.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_28\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Austria\&#34;/&gt;\n    &lt;circle cx=\&#34;372.57\&#34; cy=\&#34;166.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_29\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Azerbaijan\&#34;/&gt;\n    &lt;circle cx=\&#34;226.20\&#34; cy=\&#34;20.15\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_30\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Bangladesh\&#34;/&gt;\n    &lt;circle cx=\&#34;380.31\&#34; cy=\&#34;283.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_31\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Belarus\&#34;/&gt;\n    &lt;circle cx=\&#34;471.05\&#34; cy=\&#34;171.20\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_32\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Belgium\&#34;/&gt;\n    &lt;circle cx=\&#34;174.37\&#34; cy=\&#34;84.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_33\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Benin\&#34;/&gt;\n    &lt;circle cx=\&#34;291.42\&#34; cy=\&#34;115.10\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_34\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Bolivia\&#34;/&gt;\n    &lt;circle cx=\&#34;345.01\&#34; cy=\&#34;136.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_35\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Bosnia Herzegovina\&#34;/&gt;\n    &lt;circle cx=\&#34;370.25\&#34; cy=\&#34;97.84\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_36\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Botswana\&#34;/&gt;\n    &lt;circle cx=\&#34;361.22\&#34; cy=\&#34;93.52\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_37\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Brazil\&#34;/&gt;\n    &lt;circle cx=\&#34;390.65\&#34; cy=\&#34;266.15\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_38\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Bulgaria\&#34;/&gt;\n    &lt;circle cx=\&#34;155.01\&#34; cy=\&#34;145.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_39\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Burkina Faso\&#34;/&gt;\n    &lt;circle cx=\&#34;64.36\&#34; cy=\&#34;63.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_40\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Burundi\&#34;/&gt;\n    &lt;circle cx=\&#34;229.67\&#34; cy=\&#34;63.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_41\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Cambodia\&#34;/&gt;\n    &lt;circle cx=\&#34;222.24\&#34; cy=\&#34;106.47\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_42\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Cameroon\&#34;/&gt;\n    &lt;circle cx=\&#34;468.72\&#34; cy=\&#34;110.78\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_43\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Canada\&#34;/&gt;\n    &lt;circle cx=\&#34;159.06\&#34; cy=\&#34;89.20\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_44\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Chad\&#34;/&gt;\n    &lt;circle cx=\&#34;406.44\&#34; cy=\&#34;119.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_45\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Chile\&#34;/&gt;\n    &lt;circle cx=\&#34;369.21\&#34; cy=\&#34;123.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_46\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;China\&#34;/&gt;\n    &lt;circle cx=\&#34;354.65\&#34; cy=\&#34;84.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_47\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Colombia\&#34;/&gt;\n    &lt;circle cx=\&#34;192.80\&#34; cy=\&#34;54.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_48\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Comoros\&#34;/&gt;\n    &lt;circle cx=\&#34;259.43\&#34; cy=\&#34;115.10\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_49\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Congo, Rep.\&#34;/&gt;\n    &lt;circle cx=\&#34;370.74\&#34; cy=\&#34;67.63\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_50\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Costa Rica\&#34;/&gt;\n    &lt;circle cx=\&#34;412.76\&#34; cy=\&#34;192.78\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_51\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Croatia\&#34;/&gt;\n    &lt;circle cx=\&#34;439.24\&#34; cy=\&#34;171.20\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_52\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Cyprus\&#34;/&gt;\n    &lt;circle cx=\&#34;444.25\&#34; cy=\&#34;162.57\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_53\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Czech Republic\&#34;/&gt;\n    &lt;circle cx=\&#34;477.97\&#34; cy=\&#34;140.99\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_54\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Denmark\&#34;/&gt;\n    &lt;circle cx=\&#34;364.60\&#34; cy=\&#34;54.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_55\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Dominican Republic\&#34;/&gt;\n    &lt;circle cx=\&#34;333.00\&#34; cy=\&#34;67.63\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_56\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ecuador\&#34;/&gt;\n    &lt;circle cx=\&#34;332.97\&#34; cy=\&#34;20.15\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_57\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Egypt\&#34;/&gt;\n    &lt;circle cx=\&#34;297.01\&#34; cy=\&#34;102.15\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_58\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;El Salvador\&#34;/&gt;\n    &lt;circle cx=\&#34;431.12\&#34; cy=\&#34;218.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_59\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Estonia\&#34;/&gt;\n    &lt;circle cx=\&#34;431.12\&#34; cy=\&#34;110.78\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_60\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Eswatini\&#34;/&gt;\n    &lt;circle cx=\&#34;156.93\&#34; cy=\&#34;24.47\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_61\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ethiopia\&#34;/&gt;\n    &lt;circle cx=\&#34;464.81\&#34; cy=\&#34;123.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_62\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Finland\&#34;/&gt;\n    &lt;circle cx=\&#34;460.33\&#34; cy=\&#34;235.94\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_63\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;France\&#34;/&gt;\n    &lt;circle cx=\&#34;376.29\&#34; cy=\&#34;166.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_64\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Gabon\&#34;/&gt;\n    &lt;circle cx=\&#34;325.09\&#34; cy=\&#34;136.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_65\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Georgia\&#34;/&gt;\n    &lt;circle cx=\&#34;476.59\&#34; cy=\&#34;149.63\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_66\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Germany\&#34;/&gt;\n    &lt;circle cx=\&#34;240.75\&#34; cy=\&#34;58.99\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_67\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ghana\&#34;/&gt;\n    &lt;circle cx=\&#34;417.51\&#34; cy=\&#34;97.84\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_68\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Greece\&#34;/&gt;\n    &lt;circle cx=\&#34;298.75\&#34; cy=\&#34;63.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_69\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Guatemala\&#34;/&gt;\n    &lt;circle cx=\&#34;173.10\&#34; cy=\&#34;97.84\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_70\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Guinea\&#34;/&gt;\n    &lt;circle cx=\&#34;152.51\&#34; cy=\&#34;132.36\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_71\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Haiti\&#34;/&gt;\n    &lt;circle cx=\&#34;250.91\&#34; cy=\&#34;54.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_72\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Honduras\&#34;/&gt;\n    &lt;circle cx=\&#34;419.28\&#34; cy=\&#34;106.47\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_73\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Hungary\&#34;/&gt;\n    &lt;circle cx=\&#34;481.31\&#34; cy=\&#34;175.52\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_74\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Iceland\&#34;/&gt;\n    &lt;circle cx=\&#34;284.75\&#34; cy=\&#34;28.78\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_75\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;India\&#34;/&gt;\n    &lt;circle cx=\&#34;338.68\&#34; cy=\&#34;80.57\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_76\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Indonesia\&#34;/&gt;\n    &lt;circle cx=\&#34;390.15\&#34; cy=\&#34;140.99\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_77\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Iran\&#34;/&gt;\n    &lt;circle cx=\&#34;369.74\&#34; cy=\&#34;50.36\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_78\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Iraq\&#34;/&gt;\n    &lt;circle cx=\&#34;515.67\&#34; cy=\&#34;119.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_79\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ireland\&#34;/&gt;\n    &lt;circle cx=\&#34;449.30\&#34; cy=\&#34;115.10\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_80\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Israel\&#34;/&gt;\n    &lt;circle cx=\&#34;452.22\&#34; cy=\&#34;110.78\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_81\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Italy\&#34;/&gt;\n    &lt;circle cx=\&#34;227.89\&#34; cy=\&#34;106.47\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_82\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ivory Coast\&#34;/&gt;\n    &lt;circle cx=\&#34;461.30\&#34; cy=\&#34;292.04\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_83\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Japan\&#34;/&gt;\n    &lt;circle cx=\&#34;310.05\&#34; cy=\&#34;41.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_84\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Jordan\&#34;/&gt;\n    &lt;circle cx=\&#34;413.30\&#34; cy=\&#34;240.26\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_85\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kazakhstan\&#34;/&gt;\n    &lt;circle cx=\&#34;210.29\&#34; cy=\&#34;54.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_86\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kenya\&#34;/&gt;\n    &lt;circle cx=\&#34;210.29\&#34; cy=\&#34;132.36\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_87\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kosovo\&#34;/&gt;\n    &lt;circle cx=\&#34;510.78\&#34; cy=\&#34;67.63\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_88\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kuwait\&#34;/&gt;\n    &lt;circle cx=\&#34;222.53\&#34; cy=\&#34;145.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_89\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kyrgyzstan\&#34;/&gt;\n    &lt;circle cx=\&#34;284.26\&#34; cy=\&#34;41.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_90\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Laos\&#34;/&gt;\n    &lt;circle cx=\&#34;419.59\&#34; cy=\&#34;244.57\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_91\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Latvia\&#34;/&gt;\n    &lt;circle cx=\&#34;354.71\&#34; cy=\&#34;76.26\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_92\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Lebanon\&#34;/&gt;\n    &lt;circle cx=\&#34;118.71\&#34; cy=\&#34;24.47\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_93\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Liberia\&#34;/&gt;\n    &lt;circle cx=\&#34;384.33\&#34; cy=\&#34;119.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_94\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Libya\&#34;/&gt;\n    &lt;circle cx=\&#34;434.89\&#34; cy=\&#34;214.36\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_95\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Lithuania\&#34;/&gt;\n    &lt;circle cx=\&#34;546.42\&#34; cy=\&#34;132.36\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_96\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Luxembourg\&#34;/&gt;\n    &lt;circle cx=\&#34;360.00\&#34; cy=\&#34;205.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_97\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Macedonia\&#34;/&gt;\n    &lt;circle cx=\&#34;137.46\&#34; cy=\&#34;71.94\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_98\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Madagascar\&#34;/&gt;\n    &lt;circle cx=\&#34;112.41\&#34; cy=\&#34;41.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_99\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Malawi\&#34;/&gt;\n    &lt;circle cx=\&#34;423.81\&#34; cy=\&#34;46.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_100\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Malaysia\&#34;/&gt;\n    &lt;circle cx=\&#34;171.84\&#34; cy=\&#34;153.94\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_101\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mali\&#34;/&gt;\n    &lt;circle cx=\&#34;456.11\&#34; cy=\&#34;97.84\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_102\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Malta\&#34;/&gt;\n    &lt;circle cx=\&#34;228.22\&#34; cy=\&#34;119.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_103\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mauritania\&#34;/&gt;\n    &lt;circle cx=\&#34;396.78\&#34; cy=\&#34;166.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_104\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mauritius\&#34;/&gt;\n    &lt;circle cx=\&#34;377.35\&#34; cy=\&#34;54.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_105\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mexico\&#34;/&gt;\n    &lt;circle cx=\&#34;263.89\&#34; cy=\&#34;248.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_106\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Moldova\&#34;/&gt;\n    &lt;circle cx=\&#34;343.59\&#34; cy=\&#34;119.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_107\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mongolia\&#34;/&gt;\n    &lt;circle cx=\&#34;382.94\&#34; cy=\&#34;244.57\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_108\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Montenegro\&#34;/&gt;\n    &lt;circle cx=\&#34;299.55\&#34; cy=\&#34;93.52\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_109\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Morocco\&#34;/&gt;\n    &lt;circle cx=\&#34;116.02\&#34; cy=\&#34;41.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_110\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mozambique\&#34;/&gt;\n    &lt;circle cx=\&#34;271.50\&#34; cy=\&#34;46.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_111\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Myanmar\&#34;/&gt;\n    &lt;circle cx=\&#34;322.93\&#34; cy=\&#34;80.57\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_112\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Namibia\&#34;/&gt;\n    &lt;circle cx=\&#34;191.06\&#34; cy=\&#34;41.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_113\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Nepal\&#34;/&gt;\n    &lt;circle cx=\&#34;480.11\&#34; cy=\&#34;149.63\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_114\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Netherlands\&#34;/&gt;\n    &lt;circle cx=\&#34;456.29\&#34; cy=\&#34;128.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_115\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;New Zealand\&#34;/&gt;\n    &lt;circle cx=\&#34;266.33\&#34; cy=\&#34;41.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_116\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Nicaragua\&#34;/&gt;\n    &lt;circle cx=\&#34;96.08\&#34; cy=\&#34;63.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_117\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Niger\&#34;/&gt;\n    &lt;circle cx=\&#34;266.87\&#34; cy=\&#34;46.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_118\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Nigeria\&#34;/&gt;\n    &lt;circle cx=\&#34;266.87\&#34; cy=\&#34;58.99\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_119\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Northern Cyprus\&#34;/&gt;\n    &lt;circle cx=\&#34;495.37\&#34; cy=\&#34;80.57\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_120\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Norway\&#34;/&gt;\n    &lt;circle cx=\&#34;260.94\&#34; cy=\&#34;63.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_121\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Pakistan\&#34;/&gt;\n    &lt;circle cx=\&#34;248.92\&#34; cy=\&#34;46.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_122\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Palestine\&#34;/&gt;\n    &lt;circle cx=\&#34;405.78\&#34; cy=\&#34;63.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_123\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Panama\&#34;/&gt;\n    &lt;circle cx=\&#34;344.81\&#34; cy=\&#34;102.15\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_124\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Paraguay\&#34;/&gt;\n    &lt;circle cx=\&#34;347.40\&#34; cy=\&#34;136.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_125\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Peru\&#34;/&gt;\n    &lt;circle cx=\&#34;301.02\&#34; cy=\&#34;97.84\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_126\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Philippines\&#34;/&gt;\n    &lt;circle cx=\&#34;422.73\&#34; cy=\&#34;102.15\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_127\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Poland\&#34;/&gt;\n    &lt;circle cx=\&#34;430.90\&#34; cy=\&#34;89.20\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_128\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Portugal\&#34;/&gt;\n    &lt;circle cx=\&#34;414.12\&#34; cy=\&#34;158.26\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_129\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Romania\&#34;/&gt;\n    &lt;circle cx=\&#34;409.92\&#34; cy=\&#34;248.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_130\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Russia\&#34;/&gt;\n    &lt;circle cx=\&#34;163.85\&#34; cy=\&#34;33.10\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_131\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Rwanda\&#34;/&gt;\n    &lt;circle cx=\&#34;482.45\&#34; cy=\&#34;71.94\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_132\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Saudi Arabia\&#34;/&gt;\n    &lt;circle cx=\&#34;215.05\&#34; cy=\&#34;123.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_133\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Senegal\&#34;/&gt;\n    &lt;circle cx=\&#34;360.88\&#34; cy=\&#34;153.94\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_134\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Serbia\&#34;/&gt;\n    &lt;circle cx=\&#34;135.70\&#34; cy=\&#34;37.42\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_135\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Sierra Leone\&#34;/&gt;\n    &lt;circle cx=\&#34;536.72\&#34; cy=\&#34;110.78\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_136\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Singapore\&#34;/&gt;\n    &lt;circle cx=\&#34;430.73\&#34; cy=\&#34;123.73\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_137\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Slovakia\&#34;/&gt;\n    &lt;circle cx=\&#34;440.26\&#34; cy=\&#34;136.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_138\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Slovenia\&#34;/&gt;\n    &lt;circle cx=\&#34;347.86\&#34; cy=\&#34;84.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_139\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;South Africa\&#34;/&gt;\n    &lt;circle cx=\&#34;449.49\&#34; cy=\&#34;227.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_140\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;South Korea\&#34;/&gt;\n    &lt;circle cx=\&#34;448.63\&#34; cy=\&#34;97.84\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_141\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Spain\&#34;/&gt;\n    &lt;circle cx=\&#34;342.96\&#34; cy=\&#34;63.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_142\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Sri Lanka\&#34;/&gt;\n    &lt;circle cx=\&#34;475.76\&#34; cy=\&#34;140.99\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_143\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Sweden\&#34;/&gt;\n    &lt;circle cx=\&#34;500.47\&#34; cy=\&#34;210.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_144\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Switzerland\&#34;/&gt;\n    &lt;circle cx=\&#34;207.57\&#34; cy=\&#34;46.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_145\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Tajikistan\&#34;/&gt;\n    &lt;circle cx=\&#34;199.67\&#34; cy=\&#34;24.47\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_146\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Tanzania\&#34;/&gt;\n    &lt;circle cx=\&#34;375.19\&#34; cy=\&#34;37.42\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_147\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Thailand\&#34;/&gt;\n    &lt;circle cx=\&#34;145.88\&#34; cy=\&#34;63.31\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_148\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;The Gambia\&#34;/&gt;\n    &lt;circle cx=\&#34;143.82\&#34; cy=\&#34;218.68\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_149\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Togo\&#34;/&gt;\n    &lt;circle cx=\&#34;335.68\&#34; cy=\&#34;106.47\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_150\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Tunisia\&#34;/&gt;\n    &lt;circle cx=\&#34;413.61\&#34; cy=\&#34;84.89\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_151\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Turkey\&#34;/&gt;\n    &lt;circle cx=\&#34;375.85\&#34; cy=\&#34;67.63\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_152\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Turkmenistan\&#34;/&gt;\n    &lt;circle cx=\&#34;513.36\&#34; cy=\&#34;80.57\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_153\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;UAE\&#34;/&gt;\n    &lt;circle cx=\&#34;155.12\&#34; cy=\&#34;67.63\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_154\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Uganda\&#34;/&gt;\n    &lt;circle cx=\&#34;461.28\&#34; cy=\&#34;115.10\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_155\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;UK\&#34;/&gt;\n    &lt;circle cx=\&#34;304.73\&#34; cy=\&#34;313.62\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_156\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ukraine\&#34;/&gt;\n    &lt;circle cx=\&#34;397.88\&#34; cy=\&#34;128.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_157\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Uruguay\&#34;/&gt;\n    &lt;circle cx=\&#34;492.34\&#34; cy=\&#34;128.05\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_158\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;USA\&#34;/&gt;\n    &lt;circle cx=\&#34;282.04\&#34; cy=\&#34;37.42\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_159\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Uzbekistan\&#34;/&gt;\n    &lt;circle cx=\&#34;373.92\&#34; cy=\&#34;37.42\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_160\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Venezuela\&#34;/&gt;\n    &lt;circle cx=\&#34;280.76\&#34; cy=\&#34;132.36\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_161\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Vietnam\&#34;/&gt;\n    &lt;circle cx=\&#34;187.54\&#34; cy=\&#34;76.26\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_162\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Yemen\&#34;/&gt;\n    &lt;circle cx=\&#34;230.03\&#34; cy=\&#34;93.52\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_163\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Zambia\&#34;/&gt;\n    &lt;circle cx=\&#34;180.87\&#34; cy=\&#34;71.94\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_164\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;#FFA500\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#FFA500\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Zimbabwe\&#34;/&gt;\n    &lt;polyline points=\&#34;64.36,53.13 70.46,54.47 76.56,55.80 82.66,57.14 88.77,58.48 94.87,59.82 100.97,61.16 107.07,62.50 113.17,63.84 119.28,65.18 125.38,66.52 131.48,67.86 137.58,69.20 143.68,70.54 149.79,71.88 155.89,73.22 161.99,74.56 168.09,75.90 174.19,77.24 180.30,78.58 186.40,79.92 192.50,81.26 198.60,82.60 204.70,83.93 210.81,85.27 216.91,86.61 223.01,87.95 229.11,89.29 235.21,90.63 241.32,91.97 247.42,93.31 253.52,94.65 259.62,95.99 265.72,97.33 271.83,98.67 277.93,100.01 284.03,101.35 290.13,102.69 296.23,104.03 302.34,105.37 308.44,106.71 314.54,108.05 320.64,109.39 326.74,110.73 332.85,112.07 338.95,113.40 345.05,114.74 351.15,116.08 357.25,117.42 363.36,118.76 369.46,120.10 375.56,121.44 381.66,122.78 387.76,124.12 393.87,125.46 399.97,126.80 406.07,128.14 412.17,129.48 418.27,130.82 424.38,132.16 430.48,133.50 436.58,134.84 442.68,136.18 448.79,137.52 454.89,138.86 460.99,140.20 467.09,141.53 473.19,142.87 479.30,144.21 485.40,145.55 491.50,146.89 497.60,148.23 503.70,149.57 509.81,150.91 515.91,152.25 522.01,153.59 528.11,154.93 534.21,156.27 540.32,157.61 546.42,158.95\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_165\&#34; clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;2.13396\&#34; stroke=\&#34;#3366FF\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_166\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_167\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_168\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_169\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_170\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_171\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_172\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_173\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_174\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_175\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_176\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_177\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_178\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_179\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_180\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_181\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_182\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_183\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_184\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_185\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_186\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_187\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_188\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_189\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_190\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_191\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_192\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_193\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_194\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_195\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_196\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_197\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_198\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_199\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_200\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_201\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_202\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_203\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_204\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_205\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_206\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_207\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_208\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_209\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_210\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_211\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_212\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_213\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_214\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_215\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_216\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_217\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_218\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_219\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_220\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_221\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_222\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_223\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_224\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_225\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_226\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_227\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_228\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_229\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_230\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_231\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_232\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_233\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_234\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_235\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_236\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_237\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_238\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_239\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_240\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_241\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_242\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_243\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_244\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_245\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_246\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_247\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_248\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_249\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_250\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_251\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_252\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_253\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_254\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_255\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_256\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_257\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_258\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_259\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_260\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_261\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_262\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_263\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_264\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_265\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_266\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_267\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_268\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_269\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_270\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_271\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_272\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_273\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_274\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_275\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_276\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_277\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_278\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_279\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_280\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_281\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_282\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_283\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_284\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_285\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_286\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_287\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_288\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_289\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_290\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_291\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_292\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_293\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_294\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_295\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_296\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_297\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_298\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_299\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_300\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_301\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_302\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_303\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_304\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_305\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_306\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_307\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_308\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_309\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_310\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_311\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_312\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_313\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_314\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_315\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_316\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_317\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_318\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_319\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_320\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_321\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_322\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_323\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_324\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_325\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_326\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_327\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_328\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_329\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_330\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_331\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_332\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_333\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_334\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_335\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_336\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_337\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_338\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_339\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_340\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_341\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_342\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_343\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_344\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_345\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_346\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_347\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_348\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_349\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_350\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_351\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_352\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_353\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_354\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_355\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_356\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_357\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_358\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_359\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_360\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_361\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_362\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_363\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_364\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_365\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_366\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_367\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_368\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_369\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_370\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_371\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_372\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_373\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_374\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_375\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_376\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_377\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_378\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_379\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_380\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_381\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_382\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_383\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_384\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_385\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_386\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_387\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_388\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_389\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_390\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_391\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_392\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_393\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_394\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_395\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_396\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_397\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_398\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_399\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_400\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_401\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_402\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_403\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_404\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_405\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_406\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_407\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_408\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_409\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_410\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_411\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_412\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_413\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_414\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_415\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_416\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_417\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_418\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_419\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_420\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_421\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_422\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_423\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_424\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_425\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_426\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_427\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_428\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_429\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_430\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_431\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_432\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_433\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_434\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_435\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_436\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_437\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_438\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_439\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_440\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_441\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_442\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_443\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_444\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_445\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_446\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_447\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_448\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_449\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_450\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_451\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;119.01\&#34; y=\&#34;284.69\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_452\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value = 122.04[9.3]-4.95[0.98]*l_gdp,&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_3)\&#34;&gt;\n      &lt;text x=\&#34;177.99\&#34; y=\&#34;296.99\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_453\&#34; font-size=\&#34;6.40pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt; R2 = 0.15&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;defs&gt;\n      &lt;clipPath id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4\&#34;&gt;\n        &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34;/&gt;\n      &lt;\/clipPath&gt;\n    &lt;\/defs&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;24.13\&#34; y=\&#34;269.36\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_454\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;40&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;24.13\&#34; y=\&#34;183.04\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_455\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;60&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;24.13\&#34; y=\&#34;96.73\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_456\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;80&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;18.53\&#34; y=\&#34;10.41\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_457\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;100&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;100.66\&#34; y=\&#34;339.64\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_458\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;7&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;198.02\&#34; y=\&#34;339.64\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_459\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;8&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;295.38\&#34; y=\&#34;339.64\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_460\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;9&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;389.94\&#34; y=\&#34;339.64\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_461\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;10&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;487.30\&#34; y=\&#34;339.64\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_462\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;11&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;290.64\&#34; y=\&#34;352.23\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_463\&#34; font-size=\&#34;8.25pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;l_gdp&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_cl_4)\&#34;&gt;\n      &lt;text transform=\&#34;translate(13.50,181.91) rotate(-90)\&#34; id=\&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621_el_464\&#34; font-size=\&#34;8.25pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value&lt;\/text&gt;\n    &lt;\/g&gt;\n  &lt;\/g&gt;\n&lt;\/svg&gt;\n&#34;,&#34;js&#34;:null,&#34;uid&#34;:&#34;svg_453fcb5d-815e-47b7-98f5-9b9c70ef5621&#34;,&#34;ratio&#34;:1.6,&#34;settings&#34;:{&#34;tooltip&#34;:{&#34;css&#34;:&#34; .tooltip_SVGID_ { padding:5px;background:black;color:white;border-radius:2px 2px 2px 2px ; position:absolute;pointer-events:none;z-index:999;}\n&#34;,&#34;offx&#34;:10,&#34;offy&#34;:0,&#34;use_cursor_pos&#34;:true,&#34;opacity&#34;:0.9,&#34;usefill&#34;:false,&#34;usestroke&#34;:false,&#34;delay&#34;:{&#34;over&#34;:200,&#34;out&#34;:500}},&#34;hover&#34;:{&#34;css&#34;:&#34; .hover_SVGID_ { fill:orange;stroke:gray; }\n&#34;},&#34;hoverkey&#34;:{&#34;css&#34;:&#34; .hover_key_SVGID_ { stroke:red; }\n&#34;},&#34;hovertheme&#34;:{&#34;css&#34;:&#34; .hover_theme_SVGID_ { fill:green; }\n&#34;},&#34;zoom&#34;:{&#34;min&#34;:1,&#34;max&#34;:1},&#34;capture&#34;:{&#34;css&#34;:&#34; .selected_SVGID_ { fill:red;stroke:gray; }\n&#34;,&#34;type&#34;:&#34;multiple&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturekey&#34;:{&#34;css&#34;:&#34; .selected_key_SVGID_ { stroke:gray; }\n&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturetheme&#34;:{&#34;css&#34;:&#34; .selected_theme_SVGID_ { stroke:gray; }\n&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;toolbar&#34;:{&#34;position&#34;:&#34;topright&#34;,&#34;saveaspng&#34;:true},&#34;sizing&#34;:{&#34;rescale&#34;:true,&#34;width&#34;:1}}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;So what’s actually going on? &lt;code&gt;value&lt;/code&gt; is the percentage of people, in a country, that believe vaccines
are safe. &lt;code&gt;l_gdp&lt;/code&gt; is the logarithm of GDP per capita in that same country. Looking at this,
many people will conclude that the richer the country, the less people trust vaccines. This is
the story the Economist is telling its readers. This is a simple explanation, and it’s backed by
numbers and stats, so it must be correct. Right?&lt;/p&gt;
&lt;p&gt;WRONG.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the regression equation (standard errors in square brackets):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{value} = 122.04[9.3] - 4.95[0.98] * \text{l_gdp}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Both coefficients are significant at the usual 5% level (the intercept is interesting though, as
it implies a value greater than 100 for very low levels of log of GDP). This gives comfort to the
person believing the basic story.&lt;/p&gt;
&lt;p&gt;But take a look at the R². It’s 0.15. That means that the linear regression will be able to predict
up to 15% of the variance in the dependent variable using the log of GDP per capita as a predictor.
That already should sound all sorts of alarms in your head (if that scatter plot that looks almost
like random noise didn’t already). However, I’m not done yet.&lt;/p&gt;
&lt;p&gt;What if you wanted to do something a little bit more elaborate? For
instance, let’s say that you’d like to see if infant mortality plays a role? After all, you could
argue that in very poor countries, where people seem to trust vaccines very much, infant mortality
is very high. Vaccinating your kid seems like a no-brainer when the alternative is almost
certain death from any of the many diseases afflicting children (don’t get me wrong here, vaccinating
children against deadly diseases is a no-brainer anywhere on the planet).
Maybe people in wealthier countries don’t ascribe low infant mortality to vaccines, but to other
things such as access to clean water, good infrastructure etc, and thus tend to downplay the role of
vaccines. Who knows. But let’s dig deeper and get some more data.&lt;/p&gt;
&lt;p&gt;For this I’m using another data set that gives the infant mortality rate in 2018 for most of the
countries from the original analysis. I got that data from the Worldbank, and you can easily
download the csv from &lt;a href=&#34;https://gist.github.com/b-rodrigues/33f64ce6910e6ec4df9d586eacf335c2&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below, I’m downloading the data and joining that to my original dataset. Then I’m computing a rank
based on the median infant mortality rate. Countries that have an infant mortality rate below
the median are classified as “low infant mortality rate” countries and countries that have an
infant mortality rate above the median infant mortality rate are classified as
“high infant mortality rate” countries. I then redo the same plot as before, but I’m computing
one regression line per group of countries.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Click to see the code
&lt;/summary&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;infant_mortality_rate &amp;lt;- data.table::fread(&amp;quot;https://gist.githubusercontent.com/b-rodrigues/33f64ce6910e6ec4df9d586eacf335c2/raw/01df8977edd3924a3687f783e7e5a134d5f3fd87/infant_mortality_rate_2018.csv&amp;quot;) %&amp;gt;%
  janitor::clean_names() %&amp;gt;%
  select(country_name, imr = x2018_yr2018)

plot_data_simpson &amp;lt;- plot_data %&amp;gt;%
  ungroup() %&amp;gt;%  
  left_join(infant_mortality_rate) %&amp;gt;%
  mutate(imr = as.numeric(imr)) %&amp;gt;%  
  filter(!is.na(imr)) %&amp;gt;%  
  mutate(rank = ntile(imr, n = 2))  %&amp;gt;%
  mutate(rank = ifelse(rank == 2,
                       &amp;quot;High infant mortality rate&amp;quot;,
                       &amp;quot;Low infant mortality rate&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;country_name&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plot &amp;lt;- plot_data_simpson %&amp;gt;%
  ggplot(aes(y = value, x = l_gdp)) +
  geom_point_interactive(aes(tooltip = country_name, colour = rank)) +
  geom_smooth(aes(group = rank), method = &amp;quot;lm&amp;quot;) +
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;girafe(ggobj = my_plot, width_svg = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;girafe html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;html&#34;:&#34;&lt;?xml version=\&#34;1.0\&#34; encoding=\&#34;UTF-8\&#34;?&gt;\n&lt;svg xmlns=\&#34;http://www.w3.org/2000/svg\&#34; xmlns:xlink=\&#34;http://www.w3.org/1999/xlink\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b\&#34; viewBox=\&#34;0 0 576.00 360.00\&#34;&gt;\n  &lt;g&gt;\n    &lt;defs&gt;\n      &lt;clipPath id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_1\&#34;&gt;\n        &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34;/&gt;\n      &lt;\/clipPath&gt;\n    &lt;\/defs&gt;\n    &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_1\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_1)\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.75\&#34; stroke=\&#34;#FFFFFF\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34;/&gt;\n    &lt;defs&gt;\n      &lt;clipPath id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_2\&#34;&gt;\n        &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34;/&gt;\n      &lt;\/clipPath&gt;\n    &lt;\/defs&gt;\n    &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_2\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_2)\&#34; fill=\&#34;#272B30\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#000000\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34;/&gt;\n    &lt;defs&gt;\n      &lt;clipPath id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3\&#34;&gt;\n        &lt;rect x=\&#34;40.25\&#34; y=\&#34;5.48\&#34; width=\&#34;530.27\&#34; height=\&#34;283.62\&#34;/&gt;\n      &lt;\/clipPath&gt;\n    &lt;\/defs&gt;\n    &lt;polyline points=\&#34;40.25,272.42 570.52,272.42\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_3\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,196.58 570.52,196.58\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_4\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,120.75 570.52,120.75\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_5\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,44.91 570.52,44.91\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_6\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;54.78,289.10 54.78,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_7\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;152.14,289.10 152.14,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_8\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;249.50,289.10 249.50,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_9\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;346.86,289.10 346.86,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_10\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;444.22,289.10 444.22,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_11\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;541.58,289.10 541.58,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_12\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;0.533489\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,234.50 570.52,234.50\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_13\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,158.66 570.52,158.66\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_14\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,82.83 570.52,82.83\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_15\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;40.25,7.00 570.52,7.00\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_16\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;103.46,289.10 103.46,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_17\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;200.82,289.10 200.82,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_18\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;298.18,289.10 298.18,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_19\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;395.54,289.10 395.54,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_20\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polyline points=\&#34;492.90,289.10 492.90,5.48\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_21\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;1.06698\&#34; stroke=\&#34;#425D65\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;circle cx=\&#34;160.60\&#34; cy=\&#34;29.75\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_22\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Afghanistan\&#34;/&gt;\n    &lt;circle cx=\&#34;343.78\&#34; cy=\&#34;147.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_23\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Albania\&#34;/&gt;\n    &lt;circle cx=\&#34;359.81\&#34; cy=\&#34;147.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_24\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Algeria\&#34;/&gt;\n    &lt;circle cx=\&#34;389.89\&#34; cy=\&#34;52.50\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_25\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Argentina\&#34;/&gt;\n    &lt;circle cx=\&#34;315.17\&#34; cy=\&#34;226.91\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_26\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Armenia\&#34;/&gt;\n    &lt;circle cx=\&#34;472.31\&#34; cy=\&#34;71.46\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_27\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Australia\&#34;/&gt;\n    &lt;circle cx=\&#34;479.91\&#34; cy=\&#34;177.62\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_28\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Austria\&#34;/&gt;\n    &lt;circle cx=\&#34;372.57\&#34; cy=\&#34;147.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_29\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Azerbaijan\&#34;/&gt;\n    &lt;circle cx=\&#34;226.20\&#34; cy=\&#34;18.37\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_30\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Bangladesh\&#34;/&gt;\n    &lt;circle cx=\&#34;380.31\&#34; cy=\&#34;249.66\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_31\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Belarus\&#34;/&gt;\n    &lt;circle cx=\&#34;471.05\&#34; cy=\&#34;151.08\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_32\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Belgium\&#34;/&gt;\n    &lt;circle cx=\&#34;174.37\&#34; cy=\&#34;75.25\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_33\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Benin\&#34;/&gt;\n    &lt;circle cx=\&#34;291.42\&#34; cy=\&#34;101.79\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_34\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Bolivia\&#34;/&gt;\n    &lt;circle cx=\&#34;345.01\&#34; cy=\&#34;120.75\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_35\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Bosnia Herzegovina\&#34;/&gt;\n    &lt;circle cx=\&#34;370.25\&#34; cy=\&#34;86.62\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_36\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Botswana\&#34;/&gt;\n    &lt;circle cx=\&#34;361.22\&#34; cy=\&#34;82.83\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_37\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Brazil\&#34;/&gt;\n    &lt;circle cx=\&#34;390.65\&#34; cy=\&#34;234.50\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_38\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Bulgaria\&#34;/&gt;\n    &lt;circle cx=\&#34;155.01\&#34; cy=\&#34;128.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_39\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Burkina Faso\&#34;/&gt;\n    &lt;circle cx=\&#34;64.36\&#34; cy=\&#34;56.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_40\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Burundi\&#34;/&gt;\n    &lt;circle cx=\&#34;229.67\&#34; cy=\&#34;56.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_41\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Cambodia\&#34;/&gt;\n    &lt;circle cx=\&#34;222.24\&#34; cy=\&#34;94.21\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_42\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Cameroon\&#34;/&gt;\n    &lt;circle cx=\&#34;468.72\&#34; cy=\&#34;98.00\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_43\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Canada\&#34;/&gt;\n    &lt;circle cx=\&#34;159.06\&#34; cy=\&#34;79.04\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_44\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Chad\&#34;/&gt;\n    &lt;circle cx=\&#34;406.44\&#34; cy=\&#34;105.58\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_45\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Chile\&#34;/&gt;\n    &lt;circle cx=\&#34;369.21\&#34; cy=\&#34;109.37\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_46\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;China\&#34;/&gt;\n    &lt;circle cx=\&#34;354.65\&#34; cy=\&#34;75.25\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_47\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Colombia\&#34;/&gt;\n    &lt;circle cx=\&#34;192.80\&#34; cy=\&#34;48.70\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_48\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Comoros\&#34;/&gt;\n    &lt;circle cx=\&#34;259.43\&#34; cy=\&#34;101.79\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_49\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Congo, Rep.\&#34;/&gt;\n    &lt;circle cx=\&#34;370.74\&#34; cy=\&#34;60.08\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_50\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Costa Rica\&#34;/&gt;\n    &lt;circle cx=\&#34;412.76\&#34; cy=\&#34;170.04\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_51\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Croatia\&#34;/&gt;\n    &lt;circle cx=\&#34;439.24\&#34; cy=\&#34;151.08\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_52\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Cyprus\&#34;/&gt;\n    &lt;circle cx=\&#34;444.25\&#34; cy=\&#34;143.50\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_53\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Czech Republic\&#34;/&gt;\n    &lt;circle cx=\&#34;477.97\&#34; cy=\&#34;124.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_54\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Denmark\&#34;/&gt;\n    &lt;circle cx=\&#34;364.60\&#34; cy=\&#34;48.70\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_55\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Dominican Republic\&#34;/&gt;\n    &lt;circle cx=\&#34;333.00\&#34; cy=\&#34;60.08\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_56\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ecuador\&#34;/&gt;\n    &lt;circle cx=\&#34;332.97\&#34; cy=\&#34;18.37\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_57\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Egypt\&#34;/&gt;\n    &lt;circle cx=\&#34;297.01\&#34; cy=\&#34;90.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_58\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;El Salvador\&#34;/&gt;\n    &lt;circle cx=\&#34;431.12\&#34; cy=\&#34;192.79\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_59\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Estonia\&#34;/&gt;\n    &lt;circle cx=\&#34;431.12\&#34; cy=\&#34;98.00\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_60\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Eswatini\&#34;/&gt;\n    &lt;circle cx=\&#34;156.93\&#34; cy=\&#34;22.16\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_61\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ethiopia\&#34;/&gt;\n    &lt;circle cx=\&#34;464.81\&#34; cy=\&#34;109.37\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_62\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Finland\&#34;/&gt;\n    &lt;circle cx=\&#34;460.33\&#34; cy=\&#34;207.96\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_63\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;France\&#34;/&gt;\n    &lt;circle cx=\&#34;376.29\&#34; cy=\&#34;147.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_64\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Gabon\&#34;/&gt;\n    &lt;circle cx=\&#34;325.09\&#34; cy=\&#34;120.75\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_65\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Georgia\&#34;/&gt;\n    &lt;circle cx=\&#34;476.59\&#34; cy=\&#34;132.12\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_66\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Germany\&#34;/&gt;\n    &lt;circle cx=\&#34;240.75\&#34; cy=\&#34;52.50\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_67\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ghana\&#34;/&gt;\n    &lt;circle cx=\&#34;417.51\&#34; cy=\&#34;86.62\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_68\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Greece\&#34;/&gt;\n    &lt;circle cx=\&#34;298.75\&#34; cy=\&#34;56.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_69\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Guatemala\&#34;/&gt;\n    &lt;circle cx=\&#34;173.10\&#34; cy=\&#34;86.62\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_70\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Guinea\&#34;/&gt;\n    &lt;circle cx=\&#34;152.51\&#34; cy=\&#34;116.96\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_71\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Haiti\&#34;/&gt;\n    &lt;circle cx=\&#34;250.91\&#34; cy=\&#34;48.70\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_72\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Honduras\&#34;/&gt;\n    &lt;circle cx=\&#34;419.28\&#34; cy=\&#34;94.21\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_73\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Hungary\&#34;/&gt;\n    &lt;circle cx=\&#34;481.31\&#34; cy=\&#34;154.87\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_74\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Iceland\&#34;/&gt;\n    &lt;circle cx=\&#34;284.75\&#34; cy=\&#34;25.95\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_75\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;India\&#34;/&gt;\n    &lt;circle cx=\&#34;338.68\&#34; cy=\&#34;71.46\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_76\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Indonesia\&#34;/&gt;\n    &lt;circle cx=\&#34;390.15\&#34; cy=\&#34;124.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_77\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Iran\&#34;/&gt;\n    &lt;circle cx=\&#34;369.74\&#34; cy=\&#34;44.91\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_78\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Iraq\&#34;/&gt;\n    &lt;circle cx=\&#34;515.67\&#34; cy=\&#34;105.58\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_79\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ireland\&#34;/&gt;\n    &lt;circle cx=\&#34;449.30\&#34; cy=\&#34;101.79\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_80\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Israel\&#34;/&gt;\n    &lt;circle cx=\&#34;452.22\&#34; cy=\&#34;98.00\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_81\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Italy\&#34;/&gt;\n    &lt;circle cx=\&#34;227.89\&#34; cy=\&#34;94.21\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_82\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ivory Coast\&#34;/&gt;\n    &lt;circle cx=\&#34;461.30\&#34; cy=\&#34;257.25\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_83\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Japan\&#34;/&gt;\n    &lt;circle cx=\&#34;310.05\&#34; cy=\&#34;37.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_84\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Jordan\&#34;/&gt;\n    &lt;circle cx=\&#34;413.30\&#34; cy=\&#34;211.75\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_85\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kazakhstan\&#34;/&gt;\n    &lt;circle cx=\&#34;210.29\&#34; cy=\&#34;48.70\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_86\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kenya\&#34;/&gt;\n    &lt;circle cx=\&#34;510.78\&#34; cy=\&#34;60.08\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_87\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kuwait\&#34;/&gt;\n    &lt;circle cx=\&#34;222.53\&#34; cy=\&#34;128.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_88\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Kyrgyzstan\&#34;/&gt;\n    &lt;circle cx=\&#34;284.26\&#34; cy=\&#34;37.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_89\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Laos\&#34;/&gt;\n    &lt;circle cx=\&#34;419.59\&#34; cy=\&#34;215.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_90\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Latvia\&#34;/&gt;\n    &lt;circle cx=\&#34;354.71\&#34; cy=\&#34;67.66\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_91\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Lebanon\&#34;/&gt;\n    &lt;circle cx=\&#34;118.71\&#34; cy=\&#34;22.16\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_92\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Liberia\&#34;/&gt;\n    &lt;circle cx=\&#34;384.33\&#34; cy=\&#34;105.58\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_93\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Libya\&#34;/&gt;\n    &lt;circle cx=\&#34;434.89\&#34; cy=\&#34;189.00\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_94\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Lithuania\&#34;/&gt;\n    &lt;circle cx=\&#34;546.42\&#34; cy=\&#34;116.96\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_95\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Luxembourg\&#34;/&gt;\n    &lt;circle cx=\&#34;360.00\&#34; cy=\&#34;181.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_96\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Macedonia\&#34;/&gt;\n    &lt;circle cx=\&#34;137.46\&#34; cy=\&#34;63.87\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_97\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Madagascar\&#34;/&gt;\n    &lt;circle cx=\&#34;112.41\&#34; cy=\&#34;37.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_98\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Malawi\&#34;/&gt;\n    &lt;circle cx=\&#34;423.81\&#34; cy=\&#34;41.12\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_99\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Malaysia\&#34;/&gt;\n    &lt;circle cx=\&#34;171.84\&#34; cy=\&#34;135.91\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_100\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mali\&#34;/&gt;\n    &lt;circle cx=\&#34;456.11\&#34; cy=\&#34;86.62\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_101\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Malta\&#34;/&gt;\n    &lt;circle cx=\&#34;228.22\&#34; cy=\&#34;105.58\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_102\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mauritania\&#34;/&gt;\n    &lt;circle cx=\&#34;396.78\&#34; cy=\&#34;147.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_103\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mauritius\&#34;/&gt;\n    &lt;circle cx=\&#34;377.35\&#34; cy=\&#34;48.70\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_104\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mexico\&#34;/&gt;\n    &lt;circle cx=\&#34;263.89\&#34; cy=\&#34;219.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_105\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Moldova\&#34;/&gt;\n    &lt;circle cx=\&#34;343.59\&#34; cy=\&#34;105.58\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_106\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mongolia\&#34;/&gt;\n    &lt;circle cx=\&#34;382.94\&#34; cy=\&#34;215.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_107\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Montenegro\&#34;/&gt;\n    &lt;circle cx=\&#34;299.55\&#34; cy=\&#34;82.83\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_108\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Morocco\&#34;/&gt;\n    &lt;circle cx=\&#34;116.02\&#34; cy=\&#34;37.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_109\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Mozambique\&#34;/&gt;\n    &lt;circle cx=\&#34;271.50\&#34; cy=\&#34;41.12\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_110\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Myanmar\&#34;/&gt;\n    &lt;circle cx=\&#34;322.93\&#34; cy=\&#34;71.46\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_111\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Namibia\&#34;/&gt;\n    &lt;circle cx=\&#34;191.06\&#34; cy=\&#34;37.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_112\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Nepal\&#34;/&gt;\n    &lt;circle cx=\&#34;480.11\&#34; cy=\&#34;132.12\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_113\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Netherlands\&#34;/&gt;\n    &lt;circle cx=\&#34;456.29\&#34; cy=\&#34;113.16\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_114\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;New Zealand\&#34;/&gt;\n    &lt;circle cx=\&#34;266.33\&#34; cy=\&#34;37.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_115\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Nicaragua\&#34;/&gt;\n    &lt;circle cx=\&#34;96.08\&#34; cy=\&#34;56.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_116\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Niger\&#34;/&gt;\n    &lt;circle cx=\&#34;266.87\&#34; cy=\&#34;41.12\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_117\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Nigeria\&#34;/&gt;\n    &lt;circle cx=\&#34;495.37\&#34; cy=\&#34;71.46\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_118\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Norway\&#34;/&gt;\n    &lt;circle cx=\&#34;260.94\&#34; cy=\&#34;56.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_119\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Pakistan\&#34;/&gt;\n    &lt;circle cx=\&#34;248.92\&#34; cy=\&#34;41.12\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_120\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Palestine\&#34;/&gt;\n    &lt;circle cx=\&#34;405.78\&#34; cy=\&#34;56.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_121\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Panama\&#34;/&gt;\n    &lt;circle cx=\&#34;344.81\&#34; cy=\&#34;90.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_122\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Paraguay\&#34;/&gt;\n    &lt;circle cx=\&#34;347.40\&#34; cy=\&#34;120.75\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_123\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Peru\&#34;/&gt;\n    &lt;circle cx=\&#34;301.02\&#34; cy=\&#34;86.62\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_124\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Philippines\&#34;/&gt;\n    &lt;circle cx=\&#34;422.73\&#34; cy=\&#34;90.41\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_125\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Poland\&#34;/&gt;\n    &lt;circle cx=\&#34;430.90\&#34; cy=\&#34;79.04\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_126\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Portugal\&#34;/&gt;\n    &lt;circle cx=\&#34;414.12\&#34; cy=\&#34;139.71\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_127\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Romania\&#34;/&gt;\n    &lt;circle cx=\&#34;409.92\&#34; cy=\&#34;219.33\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_128\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Russia\&#34;/&gt;\n    &lt;circle cx=\&#34;163.85\&#34; cy=\&#34;29.75\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_129\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Rwanda\&#34;/&gt;\n    &lt;circle cx=\&#34;482.45\&#34; cy=\&#34;63.87\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_130\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Saudi Arabia\&#34;/&gt;\n    &lt;circle cx=\&#34;215.05\&#34; cy=\&#34;109.37\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_131\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Senegal\&#34;/&gt;\n    &lt;circle cx=\&#34;360.88\&#34; cy=\&#34;135.91\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_132\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Serbia\&#34;/&gt;\n    &lt;circle cx=\&#34;135.70\&#34; cy=\&#34;33.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_133\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Sierra Leone\&#34;/&gt;\n    &lt;circle cx=\&#34;536.72\&#34; cy=\&#34;98.00\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_134\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Singapore\&#34;/&gt;\n    &lt;circle cx=\&#34;430.73\&#34; cy=\&#34;109.37\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_135\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Slovakia\&#34;/&gt;\n    &lt;circle cx=\&#34;440.26\&#34; cy=\&#34;120.75\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_136\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Slovenia\&#34;/&gt;\n    &lt;circle cx=\&#34;347.86\&#34; cy=\&#34;75.25\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_137\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;South Africa\&#34;/&gt;\n    &lt;circle cx=\&#34;449.49\&#34; cy=\&#34;200.37\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_138\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;South Korea\&#34;/&gt;\n    &lt;circle cx=\&#34;448.63\&#34; cy=\&#34;86.62\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_139\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Spain\&#34;/&gt;\n    &lt;circle cx=\&#34;342.96\&#34; cy=\&#34;56.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_140\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Sri Lanka\&#34;/&gt;\n    &lt;circle cx=\&#34;475.76\&#34; cy=\&#34;124.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_141\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Sweden\&#34;/&gt;\n    &lt;circle cx=\&#34;500.47\&#34; cy=\&#34;185.21\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_142\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Switzerland\&#34;/&gt;\n    &lt;circle cx=\&#34;207.57\&#34; cy=\&#34;41.12\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_143\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Tajikistan\&#34;/&gt;\n    &lt;circle cx=\&#34;199.67\&#34; cy=\&#34;22.16\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_144\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Tanzania\&#34;/&gt;\n    &lt;circle cx=\&#34;375.19\&#34; cy=\&#34;33.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_145\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Thailand\&#34;/&gt;\n    &lt;circle cx=\&#34;145.88\&#34; cy=\&#34;56.29\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_146\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;The Gambia\&#34;/&gt;\n    &lt;circle cx=\&#34;143.82\&#34; cy=\&#34;192.79\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_147\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Togo\&#34;/&gt;\n    &lt;circle cx=\&#34;335.68\&#34; cy=\&#34;94.21\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_148\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Tunisia\&#34;/&gt;\n    &lt;circle cx=\&#34;413.61\&#34; cy=\&#34;75.25\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_149\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Turkey\&#34;/&gt;\n    &lt;circle cx=\&#34;375.85\&#34; cy=\&#34;60.08\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_150\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Turkmenistan\&#34;/&gt;\n    &lt;circle cx=\&#34;513.36\&#34; cy=\&#34;71.46\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_151\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;UAE\&#34;/&gt;\n    &lt;circle cx=\&#34;155.12\&#34; cy=\&#34;60.08\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_152\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Uganda\&#34;/&gt;\n    &lt;circle cx=\&#34;461.28\&#34; cy=\&#34;101.79\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_153\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;UK\&#34;/&gt;\n    &lt;circle cx=\&#34;304.73\&#34; cy=\&#34;276.21\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_154\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Ukraine\&#34;/&gt;\n    &lt;circle cx=\&#34;397.88\&#34; cy=\&#34;113.16\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_155\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Uruguay\&#34;/&gt;\n    &lt;circle cx=\&#34;492.34\&#34; cy=\&#34;113.16\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_156\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;USA\&#34;/&gt;\n    &lt;circle cx=\&#34;282.04\&#34; cy=\&#34;33.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_157\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Uzbekistan\&#34;/&gt;\n    &lt;circle cx=\&#34;373.92\&#34; cy=\&#34;33.54\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_158\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Venezuela\&#34;/&gt;\n    &lt;circle cx=\&#34;280.76\&#34; cy=\&#34;116.96\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_159\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Vietnam\&#34;/&gt;\n    &lt;circle cx=\&#34;187.54\&#34; cy=\&#34;67.66\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_160\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Yemen\&#34;/&gt;\n    &lt;circle cx=\&#34;230.03\&#34; cy=\&#34;82.83\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_161\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Zambia\&#34;/&gt;\n    &lt;circle cx=\&#34;180.87\&#34; cy=\&#34;63.87\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_162\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34; title=\&#34;Zimbabwe\&#34;/&gt;\n    &lt;polygon points=\&#34;64.36,33.18 69.00,34.07 73.64,34.96 78.28,35.85 82.93,36.73 87.57,37.61 92.21,38.49 96.85,39.37 101.50,40.24 106.14,41.11 110.78,41.98 115.42,42.84 120.07,43.69 124.71,44.54 129.35,45.39 133.99,46.23 138.64,47.06 143.28,47.89 147.92,48.71 152.56,49.52 157.21,50.32 161.85,51.11 166.49,51.90 171.13,52.67 175.78,53.43 180.42,54.18 185.06,54.91 189.70,55.63 194.35,56.34 198.99,57.02 203.63,57.69 208.27,58.34 212.92,58.96 217.56,59.57 222.20,60.15 226.84,60.70 231.49,61.23 236.13,61.74 240.77,62.21 245.41,62.66 250.06,63.08 254.70,63.47 259.34,63.84 263.99,64.17 268.63,64.48 273.27,64.77 277.91,65.03 282.56,65.26 287.20,65.47 291.84,65.67 296.48,65.84 301.13,65.99 305.77,66.12 310.41,66.24 315.05,66.34 319.70,66.43 324.34,66.51 328.98,66.57 333.62,66.62 338.27,66.66 342.91,66.69 347.55,66.72 352.19,66.73 356.84,66.74 361.48,66.74 366.12,66.74 370.76,66.72 375.41,66.71 380.05,66.69 384.69,66.66 389.33,66.63 393.98,66.59 398.62,66.55 403.26,66.51 407.90,66.46 412.55,66.41 417.19,66.36 421.83,66.31 426.47,66.25 431.12,66.19 431.12,112.29 426.47,111.39 421.83,110.50 417.19,109.62 412.55,108.73 407.90,107.85 403.26,106.97 398.62,106.10 393.98,105.23 389.33,104.36 384.69,103.49 380.05,102.64 375.41,101.78 370.76,100.93 366.12,100.09 361.48,99.25 356.84,98.42 352.19,97.59 347.55,96.78 342.91,95.97 338.27,95.17 333.62,94.38 328.98,93.60 324.34,92.83 319.70,92.07 315.05,91.33 310.41,90.60 305.77,89.88 301.13,89.19 296.48,88.51 291.84,87.84 287.20,87.20 282.56,86.58 277.91,85.99 273.27,85.41 268.63,84.86 263.99,84.34 259.34,83.85 254.70,83.38 250.06,82.94 245.41,82.53 240.77,82.14 236.13,81.79 231.49,81.46 226.84,81.16 222.20,80.88 217.56,80.63 212.92,80.40 208.27,80.19 203.63,80.01 198.99,79.84 194.35,79.70 189.70,79.57 185.06,79.46 180.42,79.36 175.78,79.27 171.13,79.20 166.49,79.14 161.85,79.09 157.21,79.05 152.56,79.02 147.92,79.00 143.28,78.99 138.64,78.98 133.99,78.99 129.35,78.99 124.71,79.01 120.07,79.03 115.42,79.05 110.78,79.08 106.14,79.11 101.50,79.15 96.85,79.19 92.21,79.23 87.57,79.28 82.93,79.33 78.28,79.38 73.64,79.44 69.00,79.49 64.36,79.55\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_163\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#999999\&#34; fill-opacity=\&#34;0.4\&#34; stroke=\&#34;none\&#34;/&gt;\n    &lt;polyline points=\&#34;64.36,33.18 69.00,34.07 73.64,34.96 78.28,35.85 82.93,36.73 87.57,37.61 92.21,38.49 96.85,39.37 101.50,40.24 106.14,41.11 110.78,41.98 115.42,42.84 120.07,43.69 124.71,44.54 129.35,45.39 133.99,46.23 138.64,47.06 143.28,47.89 147.92,48.71 152.56,49.52 157.21,50.32 161.85,51.11 166.49,51.90 171.13,52.67 175.78,53.43 180.42,54.18 185.06,54.91 189.70,55.63 194.35,56.34 198.99,57.02 203.63,57.69 208.27,58.34 212.92,58.96 217.56,59.57 222.20,60.15 226.84,60.70 231.49,61.23 236.13,61.74 240.77,62.21 245.41,62.66 250.06,63.08 254.70,63.47 259.34,63.84 263.99,64.17 268.63,64.48 273.27,64.77 277.91,65.03 282.56,65.26 287.20,65.47 291.84,65.67 296.48,65.84 301.13,65.99 305.77,66.12 310.41,66.24 315.05,66.34 319.70,66.43 324.34,66.51 328.98,66.57 333.62,66.62 338.27,66.66 342.91,66.69 347.55,66.72 352.19,66.73 356.84,66.74 361.48,66.74 366.12,66.74 370.76,66.72 375.41,66.71 380.05,66.69 384.69,66.66 389.33,66.63 393.98,66.59 398.62,66.55 403.26,66.51 407.90,66.46 412.55,66.41 417.19,66.36 421.83,66.31 426.47,66.25 431.12,66.19\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_164\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke=\&#34;none\&#34;/&gt;\n    &lt;polyline points=\&#34;431.12,112.29 426.47,111.39 421.83,110.50 417.19,109.62 412.55,108.73 407.90,107.85 403.26,106.97 398.62,106.10 393.98,105.23 389.33,104.36 384.69,103.49 380.05,102.64 375.41,101.78 370.76,100.93 366.12,100.09 361.48,99.25 356.84,98.42 352.19,97.59 347.55,96.78 342.91,95.97 338.27,95.17 333.62,94.38 328.98,93.60 324.34,92.83 319.70,92.07 315.05,91.33 310.41,90.60 305.77,89.88 301.13,89.19 296.48,88.51 291.84,87.84 287.20,87.20 282.56,86.58 277.91,85.99 273.27,85.41 268.63,84.86 263.99,84.34 259.34,83.85 254.70,83.38 250.06,82.94 245.41,82.53 240.77,82.14 236.13,81.79 231.49,81.46 226.84,81.16 222.20,80.88 217.56,80.63 212.92,80.40 208.27,80.19 203.63,80.01 198.99,79.84 194.35,79.70 189.70,79.57 185.06,79.46 180.42,79.36 175.78,79.27 171.13,79.20 166.49,79.14 161.85,79.09 157.21,79.05 152.56,79.02 147.92,79.00 143.28,78.99 138.64,78.98 133.99,78.99 129.35,78.99 124.71,79.01 120.07,79.03 115.42,79.05 110.78,79.08 106.14,79.11 101.50,79.15 96.85,79.19 92.21,79.23 87.57,79.28 82.93,79.33 78.28,79.38 73.64,79.44 69.00,79.49 64.36,79.55\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_165\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke=\&#34;none\&#34;/&gt;\n    &lt;polyline points=\&#34;64.36,56.37 69.00,56.78 73.64,57.20 78.28,57.61 82.93,58.03 87.57,58.45 92.21,58.86 96.85,59.28 101.50,59.69 106.14,60.11 110.78,60.53 115.42,60.94 120.07,61.36 124.71,61.77 129.35,62.19 133.99,62.61 138.64,63.02 143.28,63.44 147.92,63.86 152.56,64.27 157.21,64.69 161.85,65.10 166.49,65.52 171.13,65.94 175.78,66.35 180.42,66.77 185.06,67.18 189.70,67.60 194.35,68.02 198.99,68.43 203.63,68.85 208.27,69.26 212.92,69.68 217.56,70.10 222.20,70.51 226.84,70.93 231.49,71.35 236.13,71.76 240.77,72.18 245.41,72.59 250.06,73.01 254.70,73.43 259.34,73.84 263.99,74.26 268.63,74.67 273.27,75.09 277.91,75.51 282.56,75.92 287.20,76.34 291.84,76.75 296.48,77.17 301.13,77.59 305.77,78.00 310.41,78.42 315.05,78.84 319.70,79.25 324.34,79.67 328.98,80.08 333.62,80.50 338.27,80.92 342.91,81.33 347.55,81.75 352.19,82.16 356.84,82.58 361.48,83.00 366.12,83.41 370.76,83.83 375.41,84.24 380.05,84.66 384.69,85.08 389.33,85.49 393.98,85.91 398.62,86.33 403.26,86.74 407.90,87.16 412.55,87.57 417.19,87.99 421.83,88.41 426.47,88.82 431.12,89.24\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_166\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;2.13396\&#34; stroke=\&#34;#3366FF\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;polygon points=\&#34;297.01,103.17 300.16,103.61 303.32,104.04 306.48,104.47 309.64,104.89 312.79,105.31 315.95,105.73 319.11,106.14 322.26,106.55 325.42,106.95 328.58,107.35 331.74,107.74 334.89,108.12 338.05,108.50 341.21,108.87 344.36,109.24 347.52,109.59 350.68,109.94 353.84,110.27 356.99,110.60 360.15,110.91 363.31,111.21 366.46,111.50 369.62,111.77 372.78,112.02 375.93,112.26 379.09,112.48 382.25,112.68 385.41,112.85 388.56,113.01 391.72,113.13 394.88,113.23 398.03,113.30 401.19,113.33 404.35,113.33 407.51,113.30 410.66,113.23 413.82,113.12 416.98,112.97 420.13,112.78 423.29,112.55 426.45,112.28 429.61,111.97 432.76,111.62 435.92,111.22 439.08,110.80 442.23,110.33 445.39,109.83 448.55,109.29 451.70,108.73 454.86,108.14 458.02,107.51 461.18,106.87 464.33,106.20 467.49,105.50 470.65,104.79 473.80,104.06 476.96,103.31 480.12,102.55 483.28,101.77 486.43,100.98 489.59,100.17 492.75,99.36 495.90,98.53 499.06,97.69 502.22,96.85 505.38,96.00 508.53,95.13 511.69,94.27 514.85,93.39 518.00,92.51 521.16,91.62 524.32,90.73 527.48,89.84 530.63,88.94 533.79,88.03 536.95,87.12 540.10,86.21 543.26,85.29 546.42,84.37 546.42,148.93 543.26,148.50 540.10,148.07 536.95,147.64 533.79,147.22 530.63,146.80 527.48,146.39 524.32,145.98 521.16,145.57 518.00,145.17 514.85,144.78 511.69,144.39 508.53,144.01 505.38,143.64 502.22,143.27 499.06,142.92 495.90,142.57 492.75,142.23 489.59,141.90 486.43,141.58 483.28,141.28 480.12,140.99 476.96,140.71 473.80,140.45 470.65,140.20 467.49,139.98 464.33,139.77 461.18,139.59 458.02,139.43 454.86,139.30 451.70,139.19 448.55,139.11 445.39,139.07 442.23,139.05 439.08,139.07 435.92,139.13 432.76,139.23 429.61,139.36 426.45,139.54 423.29,139.76 420.13,140.01 416.98,140.31 413.82,140.65 410.66,141.03 407.51,141.44 404.35,141.90 401.19,142.39 398.03,142.91 394.88,143.46 391.72,144.05 388.56,144.66 385.41,145.30 382.25,145.96 379.09,146.65 375.93,147.35 372.78,148.08 369.62,148.82 366.46,149.58 363.31,150.36 360.15,151.14 356.99,151.94 353.84,152.76 350.68,153.58 347.52,154.41 344.36,155.25 341.21,156.10 338.05,156.96 334.89,157.83 331.74,158.70 328.58,159.58 325.42,160.46 322.26,161.35 319.11,162.25 315.95,163.15 312.79,164.05 309.64,164.96 306.48,165.87 303.32,166.79 300.16,167.71 297.01,168.63\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_167\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;#999999\&#34; fill-opacity=\&#34;0.4\&#34; stroke=\&#34;none\&#34;/&gt;\n    &lt;polyline points=\&#34;297.01,103.17 300.16,103.61 303.32,104.04 306.48,104.47 309.64,104.89 312.79,105.31 315.95,105.73 319.11,106.14 322.26,106.55 325.42,106.95 328.58,107.35 331.74,107.74 334.89,108.12 338.05,108.50 341.21,108.87 344.36,109.24 347.52,109.59 350.68,109.94 353.84,110.27 356.99,110.60 360.15,110.91 363.31,111.21 366.46,111.50 369.62,111.77 372.78,112.02 375.93,112.26 379.09,112.48 382.25,112.68 385.41,112.85 388.56,113.01 391.72,113.13 394.88,113.23 398.03,113.30 401.19,113.33 404.35,113.33 407.51,113.30 410.66,113.23 413.82,113.12 416.98,112.97 420.13,112.78 423.29,112.55 426.45,112.28 429.61,111.97 432.76,111.62 435.92,111.22 439.08,110.80 442.23,110.33 445.39,109.83 448.55,109.29 451.70,108.73 454.86,108.14 458.02,107.51 461.18,106.87 464.33,106.20 467.49,105.50 470.65,104.79 473.80,104.06 476.96,103.31 480.12,102.55 483.28,101.77 486.43,100.98 489.59,100.17 492.75,99.36 495.90,98.53 499.06,97.69 502.22,96.85 505.38,96.00 508.53,95.13 511.69,94.27 514.85,93.39 518.00,92.51 521.16,91.62 524.32,90.73 527.48,89.84 530.63,88.94 533.79,88.03 536.95,87.12 540.10,86.21 543.26,85.29 546.42,84.37\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_168\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke=\&#34;none\&#34;/&gt;\n    &lt;polyline points=\&#34;546.42,148.93 543.26,148.50 540.10,148.07 536.95,147.64 533.79,147.22 530.63,146.80 527.48,146.39 524.32,145.98 521.16,145.57 518.00,145.17 514.85,144.78 511.69,144.39 508.53,144.01 505.38,143.64 502.22,143.27 499.06,142.92 495.90,142.57 492.75,142.23 489.59,141.90 486.43,141.58 483.28,141.28 480.12,140.99 476.96,140.71 473.80,140.45 470.65,140.20 467.49,139.98 464.33,139.77 461.18,139.59 458.02,139.43 454.86,139.30 451.70,139.19 448.55,139.11 445.39,139.07 442.23,139.05 439.08,139.07 435.92,139.13 432.76,139.23 429.61,139.36 426.45,139.54 423.29,139.76 420.13,140.01 416.98,140.31 413.82,140.65 410.66,141.03 407.51,141.44 404.35,141.90 401.19,142.39 398.03,142.91 394.88,143.46 391.72,144.05 388.56,144.66 385.41,145.30 382.25,145.96 379.09,146.65 375.93,147.35 372.78,148.08 369.62,148.82 366.46,149.58 363.31,150.36 360.15,151.14 356.99,151.94 353.84,152.76 350.68,153.58 347.52,154.41 344.36,155.25 341.21,156.10 338.05,156.96 334.89,157.83 331.74,158.70 328.58,159.58 325.42,160.46 322.26,161.35 319.11,162.25 315.95,163.15 312.79,164.05 309.64,164.96 306.48,165.87 303.32,166.79 300.16,167.71 297.01,168.63\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_169\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke=\&#34;none\&#34;/&gt;\n    &lt;polyline points=\&#34;297.01,135.90 300.16,135.66 303.32,135.41 306.48,135.17 309.64,134.93 312.79,134.68 315.95,134.44 319.11,134.19 322.26,133.95 325.42,133.71 328.58,133.46 331.74,133.22 334.89,132.98 338.05,132.73 341.21,132.49 344.36,132.25 347.52,132.00 350.68,131.76 353.84,131.51 356.99,131.27 360.15,131.03 363.31,130.78 366.46,130.54 369.62,130.30 372.78,130.05 375.93,129.81 379.09,129.56 382.25,129.32 385.41,129.08 388.56,128.83 391.72,128.59 394.88,128.35 398.03,128.10 401.19,127.86 404.35,127.62 407.51,127.37 410.66,127.13 413.82,126.88 416.98,126.64 420.13,126.40 423.29,126.15 426.45,125.91 429.61,125.67 432.76,125.42 435.92,125.18 439.08,124.93 442.23,124.69 445.39,124.45 448.55,124.20 451.70,123.96 454.86,123.72 458.02,123.47 461.18,123.23 464.33,122.99 467.49,122.74 470.65,122.50 473.80,122.25 476.96,122.01 480.12,121.77 483.28,121.52 486.43,121.28 489.59,121.04 492.75,120.79 495.90,120.55 499.06,120.30 502.22,120.06 505.38,119.82 508.53,119.57 511.69,119.33 514.85,119.09 518.00,118.84 521.16,118.60 524.32,118.36 527.48,118.11 530.63,117.87 533.79,117.62 536.95,117.38 540.10,117.14 543.26,116.89 546.42,116.65\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_170\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_3)\&#34; fill=\&#34;none\&#34; stroke-width=\&#34;2.13396\&#34; stroke=\&#34;#3366FF\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;butt\&#34;/&gt;\n    &lt;defs&gt;\n      &lt;clipPath id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4\&#34;&gt;\n        &lt;rect x=\&#34;0.00\&#34; y=\&#34;0.00\&#34; width=\&#34;576.00\&#34; height=\&#34;360.00\&#34;/&gt;\n      &lt;\/clipPath&gt;\n    &lt;\/defs&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;24.13\&#34; y=\&#34;237.71\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_171\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;40&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;24.13\&#34; y=\&#34;161.87\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_172\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;60&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;24.13\&#34; y=\&#34;86.04\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_173\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;80&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;18.53\&#34; y=\&#34;10.20\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_174\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;100&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;100.66\&#34; y=\&#34;300.45\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_175\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;7&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;198.02\&#34; y=\&#34;300.45\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_176\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;8&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;295.38\&#34; y=\&#34;300.45\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_177\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;9&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;389.94\&#34; y=\&#34;300.45\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_178\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;10&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;487.30\&#34; y=\&#34;300.45\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_179\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;11&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;290.64\&#34; y=\&#34;313.03\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_180\&#34; font-size=\&#34;8.25pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;l_gdp&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text transform=\&#34;translate(13.50,162.31) rotate(-90)\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_181\&#34; font-size=\&#34;8.25pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;value&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;154.22\&#34; y=\&#34;344.41\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_182\&#34; font-size=\&#34;8.25pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;rank&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;circle cx=\&#34;192.94\&#34; cy=\&#34;340.40\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_183\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34; fill=\&#34;#F8766D\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#F8766D\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34;/&gt;\n    &lt;circle cx=\&#34;333.18\&#34; cy=\&#34;340.40\&#34; r=\&#34;1.47pt\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_184\&#34; clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34; fill=\&#34;#00BFC4\&#34; fill-opacity=\&#34;1\&#34; stroke-width=\&#34;0.708661\&#34; stroke=\&#34;#00BFC4\&#34; stroke-opacity=\&#34;1\&#34; stroke-linejoin=\&#34;round\&#34; stroke-linecap=\&#34;round\&#34;/&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;207.06\&#34; y=\&#34;343.61\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_185\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;High infant mortality rate&lt;\/text&gt;\n    &lt;\/g&gt;\n    &lt;g clip-path=\&#34;url(#svg_790a8806-4836-491a-8eea-2a594d6a280b_cl_4)\&#34;&gt;\n      &lt;text x=\&#34;347.30\&#34; y=\&#34;343.61\&#34; id=\&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b_el_186\&#34; font-size=\&#34;6.60pt\&#34; fill=\&#34;#FFFFFF\&#34; fill-opacity=\&#34;1\&#34; font-family=\&#34;DejaVu Sans\&#34;&gt;Low infant mortality rate&lt;\/text&gt;\n    &lt;\/g&gt;\n  &lt;\/g&gt;\n&lt;\/svg&gt;\n&#34;,&#34;js&#34;:null,&#34;uid&#34;:&#34;svg_790a8806-4836-491a-8eea-2a594d6a280b&#34;,&#34;ratio&#34;:1.6,&#34;settings&#34;:{&#34;tooltip&#34;:{&#34;css&#34;:&#34; .tooltip_SVGID_ { padding:5px;background:black;color:white;border-radius:2px 2px 2px 2px ; position:absolute;pointer-events:none;z-index:999;}\n&#34;,&#34;offx&#34;:10,&#34;offy&#34;:0,&#34;use_cursor_pos&#34;:true,&#34;opacity&#34;:0.9,&#34;usefill&#34;:false,&#34;usestroke&#34;:false,&#34;delay&#34;:{&#34;over&#34;:200,&#34;out&#34;:500}},&#34;hover&#34;:{&#34;css&#34;:&#34; .hover_SVGID_ { fill:orange;stroke:gray; }\n&#34;},&#34;hoverkey&#34;:{&#34;css&#34;:&#34; .hover_key_SVGID_ { stroke:red; }\n&#34;},&#34;hovertheme&#34;:{&#34;css&#34;:&#34; .hover_theme_SVGID_ { fill:green; }\n&#34;},&#34;zoom&#34;:{&#34;min&#34;:1,&#34;max&#34;:1},&#34;capture&#34;:{&#34;css&#34;:&#34; .selected_SVGID_ { fill:red;stroke:gray; }\n&#34;,&#34;type&#34;:&#34;multiple&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturekey&#34;:{&#34;css&#34;:&#34; .selected_key_SVGID_ { stroke:gray; }\n&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;capturetheme&#34;:{&#34;css&#34;:&#34; .selected_theme_SVGID_ { stroke:gray; }\n&#34;,&#34;type&#34;:&#34;single&#34;,&#34;only_shiny&#34;:true,&#34;selected&#34;:[]},&#34;toolbar&#34;:{&#34;position&#34;:&#34;topright&#34;,&#34;saveaspng&#34;:true},&#34;sizing&#34;:{&#34;rescale&#34;:true,&#34;width&#34;:1}}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;All of a sudden, the relationship turns positive for high income countries. This is the famous
Simpson’s paradox in action. If you don’t know about Simpson’s paradox, you can read about it &lt;a href=&#34;https://en.wikipedia.org/wiki/Simpson%27s_paradox&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now what? Should we stop here? No.&lt;/p&gt;
&lt;p&gt;Let’s not even consider Simpson’s paradox. Even though the authors never claim to have found any
causal mechanism (and the Economist made no such claim, even though they tried hard to find some after
the fact explanation to justify their findings), authors of such studies do very often imply that
their simple analysis has at the very least some predictive power. We already know that this is bullocks, because
the R² is so low. But let’s try something fun; let’s split the dataset into a training set and a testing
set, and let’s see if we can accurately predict the points from the test set. Also, I won’t do
this once, because, who knows, maybe that one regression we did had some very hard to predict points
in the test set, so I’ll do it 100 times, always with new randomly generated training and testing sets.
The way I’m evaluating the accuracy of the regression is visually: I’ll be doing a plot like before,
where I’m showing the points from the training set, the points from the test set, as well as the
predictions. I’ll also be showing the distance between the prediction and the points from the test set.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Click to see the code to run the 100 regressions.
&lt;/summary&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;run_regression &amp;lt;- function(dataset){

  training_index &amp;lt;- sample(1:nrow(dataset), 120)

  training_set &amp;lt;- dataset[training_index, ]

  testing_set &amp;lt;- dataset[-training_index, ]

  fitted_model &amp;lt;- lm(value ~ l_gdp, data = training_set)

  predicted_points &amp;lt;- predict.lm(fitted_model, newdata = testing_set)

  predicted_points &amp;lt;- cbind(testing_set, &amp;quot;prediction&amp;quot; = predicted_points)

  rbind(training_set, predicted_points)
}

results &amp;lt;- tribble(~id,
                   seq(1, 100)) %&amp;gt;%
  mutate(dataset = list(filter(plot_data, country_name != &amp;quot;Taiwan&amp;quot;))) %&amp;gt;%  
  unnest(cols = c(id)) %&amp;gt;%
  mutate(regression = map(dataset, run_regression))&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;Now that I ran the 100 regressions, let’s create some visualisations:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Click to see the code to create the plots.
&lt;/summary&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- results %&amp;gt;%
  mutate(regression = map(regression,
                          ~mutate(., type_set = ifelse(is.na(prediction),
                                                    &amp;quot;training_set&amp;quot;,
                                                    &amp;quot;testing_set&amp;quot;))))


make_plots &amp;lt;- function(dataset){
ggplot() +
  geom_point(data = dataset,
             aes(y = value, x = l_gdp, shape = type_set), size = 5) +
  geom_smooth(data = dataset,
              aes(y = value, x = l_gdp),
              method = &amp;quot;lm&amp;quot;) +
  geom_point(data = {dataset %&amp;gt;%
                      filter(!is.na(prediction)) %&amp;gt;%
                       pivot_longer(c(value, prediction), names_to = &amp;quot;values&amp;quot;) %&amp;gt;%
                       mutate(values = ifelse(values == &amp;quot;value&amp;quot;,
                                              &amp;quot;Actual value&amp;quot;,
                                              &amp;quot;Prediction&amp;quot;))},
             aes(y = value, x = l_gdp, colour = values, group = country_name)) +
  geom_path(data = {dataset %&amp;gt;%
                      filter(!is.na(prediction)) %&amp;gt;%
                      pivot_longer(c(value, prediction), names_to = &amp;quot;values&amp;quot;) %&amp;gt;%
                      mutate(values = ifelse(values == &amp;quot;value&amp;quot;,
                                             &amp;quot;Actual value&amp;quot;,
                                             &amp;quot;Prediction&amp;quot;))},
               aes(y = value, x = l_gdp, colour = values, group = country_name),
               arrow = arrow(length = unit(0.03, &amp;quot;npc&amp;quot;))) +
  brotools::theme_blog()
} 

results &amp;lt;- results %&amp;gt;%
  mutate(plots = map(regression, make_plots))&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;Finally, let’s take a look at some of them:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
Click to see some plots.
&lt;/summary&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results$plots[1:3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-12-12-ethics_statistics_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-12-12-ethics_statistics_files/figure-html/unnamed-chunk-9-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/blog/2020-12-12-ethics_statistics_files/figure-html/unnamed-chunk-9-3.png&#34; width=&#34;672&#34; /&gt;
&lt;/details&gt;
&lt;p&gt;The red dots are the actual values in the test set (the triangles are the points in the training set).
The blue dots are the predictions. See what happens? They all get very close to the regression line.
This is of course completely normal; after all, the line is what the model is predicting, so
how else could it be? I don’t know if this is exactly what is named “regression towards the mean”,
but it does look very much like it. But in general, we speak of regression towards the mean when there’s time
involved in whatever you’re studying (for example students that score very well on a first test
tend to score worse, on average, on a second test, and vice-versa).
But what matters here, is that a regression line cannot even be useful to make any type of prediction.&lt;/p&gt;
&lt;p&gt;So where does that leave us? Should we avoid using simple methods like linear regression and only
use very complex methods? Should we stop communicating numbers and stats and graphs to the general
public? Certainly not. But using the excuse that the general public does not understand complex
methods to justify using faulty stats is also not an option.
In an article that mentions trust in vaccines, it also seems crucial to give more context; trust in
vaccines may be higher on average in poorer countries (and that’s an assumption, the article of
the Economist does not allow to conclude that), but distrust is also more
&lt;a href=&#34;https://www.nytimes.com/2013/02/09/world/africa/in-nigeria-polio-vaccine-workers-are-killed-by-gunmen.html&#34;&gt;extreme&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I don’t think I’ve ever seen the general public distrust science and stats so much than during this
pandemic. Many scientists made many predictions that of course never materialized, because
scientists should not give out single point forecasts. Unfortunately, that’s what they do because
that’s how they get people’s attention, and unfortunately, many also confuse science with stats. I
think Millenials are very guilty of this. We all were thought critical thinking in school, and now
all arguments devolve very quickly to “I have data and models to back my opinions up so my opinions
are actually facts, and your data and models are wrong and you’re a terrible human being by the
way”. The problem is that having data and models is not a sufficient condition for being right.&lt;/p&gt;
&lt;p&gt;As statisticians, we have a responsibility to use the right methods, and make more and better
efforts to communicate our results to the general public, even if the methods used are complex.
Sometimes there’s simply no simplifying further. Anything else is just charlatanism.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.
You can also watch my videos on &lt;a href=&#34;https://www.youtube.com/c/BrunoRodrigues1988/&#34;&gt;youtube&lt;/a&gt;.
So much content for you to consoom!&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Poorman&#39;s automated translation with R and Google Sheets using {googlesheets4}</title>
      <link>/blog/2020-12-05-poorman_translate/</link>
      <pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-12-05-poorman_translate/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2kArCRjT29w&#34;&gt;
&lt;img src=&#34;/img/omelette_du_fromage.gif&#34; title = &#34;A classic.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A little trick I thought about this week; using Google Sheets, which includes a “googletranslate()”
function to translate a survey that we’re preparing at work, from French to English, and using
R of course. You’ll need a Google account for this. Also, keep in mind that you’ll be sending
the text you want to translate to Google, so don’t go sending out anything sensitive.&lt;/p&gt;
&lt;p&gt;First, let’s load the needed packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googlesheets4)
library(dplyr)
library(tibble)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, I’ll be defining a tibble with one column, and two rows. Each cell contains a
sentence in French from the best show in the entire French speaking world, Kaamelott:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_french_tibble &amp;lt;- tribble(~french,
                  &amp;quot;J&amp;#39;apprécie les fruits au sirop&amp;quot;,
                  &amp;quot;C&amp;#39;est pas faux&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To this tibble, I’m now adding two more columns, that contain the following string: “=googletranslate(A:A,”fr“,”en“)”.
This is exactly what you would write in the formula bar in Sheets. Then, we need to convert that to
an actual Google Sheets formula using &lt;code&gt;gs4_formula()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
my_french_tibble &amp;lt;- my_french_tibble %&amp;gt;%
  mutate(english = &amp;#39;=googletranslate(A:A, &amp;quot;fr&amp;quot;, &amp;quot;en&amp;quot;)&amp;#39;) %&amp;gt;%
  mutate(portuguese = &amp;#39;=googletranslate(A:A, &amp;quot;fr&amp;quot;, &amp;quot;pt&amp;quot;)&amp;#39;) %&amp;gt;%
  mutate(english = gs4_formula(english),
         portuguese = gs4_formula(portuguese))
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `...` is not empty.
## 
## We detected these problematic arguments:
## * `needs_dots`
## 
## These dots only exist to allow future extensions and should be empty.
## Did you misspecify an argument?&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   french     english                           portuguese                       
##   &amp;lt;chr&amp;gt;      &amp;lt;fmla&amp;gt;                            &amp;lt;fmla&amp;gt;                           
## 1 J&amp;#39;appréci… =googletranslate(A:A, &amp;quot;fr&amp;quot;, &amp;quot;en&amp;quot;) =googletranslate(A:A, &amp;quot;fr&amp;quot;, &amp;quot;pt&amp;quot;)
## 2 C&amp;#39;est pas… =googletranslate(A:A, &amp;quot;fr&amp;quot;, &amp;quot;en&amp;quot;) =googletranslate(A:A, &amp;quot;fr&amp;quot;, &amp;quot;pt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re ready to send this to Google Sheets. As soon as the sheet gets uploaded, the formulas will be
evaluated, yielding translations in both English and Portuguese.&lt;/p&gt;
&lt;p&gt;To upload the tibble to sheets, run the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;french_sheet &amp;lt;- gs4_create(&amp;quot;repliques_kaamelott&amp;quot;,
                           sheets = list(perceval = my_french_tibble))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll be asked if you want to cache your credentials so that you don’t need to re-authenticate
between R sessions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/gs4_oauth.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Your browser will the open a tab asking you to login to Google:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/gs4_login.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this point, you might get a notification on your phone, alerting you that there was a login to your account:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/gs4_android_notification.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you go on your Google Sheets account, this is what you’ll see:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/gs4_repliques_kaamelott.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And if you open the sheet:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/gs4_repliques_kaamelott_result.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pretty nice, no? You can of course download the workbook, or better yet, never leave your R session at all
and simply get back the workbook using either the &lt;code&gt;{googledrive}&lt;/code&gt; package, which simply needs the name
of the workbook (&lt;code&gt;{googledrive}&lt;/code&gt; also needs authentication):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
translations &amp;lt;- googledrive::drive_get(&amp;quot;repliques_kaamelott&amp;quot;) %&amp;gt;%
  read_sheet
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll get a new data frame with the translation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Reading from &amp;quot;repliques_kaamelott&amp;quot;
Range &amp;quot;perceval&amp;quot;
# A tibble: 2 x 3
  french                    english                     portuguese              
  &amp;lt;chr&amp;gt;                     &amp;lt;chr&amp;gt;                       &amp;lt;chr&amp;gt;                   
1 J&amp;#39;apprécie les fruits au… I appreciate the fruits in… I apreciar os frutos em…
2 C&amp;#39;est pas faux            It is not false             Não é falsa             &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or you can use the link to the sheet (which does not require to re-authenticate at this point):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;translations &amp;lt;- read_sheet(&amp;quot;the_link_goes_here&amp;quot;, &amp;quot;perceval&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could of course encapsulate all these steps into a function and have any text translated
very easily! Just be careful not to send out any confidential information out…&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.
You can also watch my videos on &lt;a href=&#34;https://www.youtube.com/c/BrunoRodrigues1988/&#34;&gt;youtube&lt;/a&gt;.
So much content for you to consoom!&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graphical User Interfaces were a mistake but you can still make things right</title>
      <link>/blog/2020-11-21-guis_mistake/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-11-21-guis_mistake/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/cHw4GER-MiE?t=2&#34;&gt;
&lt;img src=&#34;/img/just_kill_me.png&#34; title = &#34;Welcome to Hell.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Some weeks ago I tweeted this:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
GUIs were a mistake
&lt;/p&gt;
— Bruno Rodrigues (&lt;span class=&#34;citation&#34;&gt;@brodriguesco&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/brodriguesco/status/1314505586172624898?ref_src=twsrc%5Etfw&#34;&gt;October 9, 2020&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;you might think that I tweeted this as an unfunny joke, but it’s not. GUIs were one of the worst
things to happen for statisticians. Clickable interfaces for data analysis is probably one of the
greatest source of mistakes and errors in data processing, very likely costing many millions to
companies worldwide and is a source of constant embarassment when mistakes happen which cost the
reputation, and money, of institutions or people.&lt;/p&gt;
&lt;p&gt;Remember the infamous Excel mistake by Reinhard and Rogoff? If you don’t know what I’m
talking about, you can get up to speed by reading &lt;a href=&#34;https://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646&#34;&gt;this&lt;/a&gt;.
I think the most interesting sentence is this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The most serious was that, in their Excel spreadsheet, Reinhart and Rogoff had not selected the entire row when averaging growth figures: they omitted data from Australia, Austria, Belgium, Canada and Denmark.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a typical mistake that happens when a mouse is used to select data in a GUI, instead of
typing whatever you need in a scripting language. Many other mistakes like that happen, and they
remain hidden, potentially for years, or go unreported.&lt;/p&gt;
&lt;p&gt;Recently there was another Excel-related problem in England where positive Covid tests got lost. For some
obscure reason, the raw data, which was encoded in a CSV file, got converted into an Excel
spreadsheet, most likely for further analysis. The problem is that the format that was used was the
now obsolete XLS format, instead of the latest XLSX format, which can handle millions of rows.
Because the data was converted in the XLS format, up to 15841 cases were lost. You can get all the
details from this BBC &lt;a href=&#34;https://www.bbc.com/news/technology-54423988&#34;&gt;article&lt;/a&gt;. Again, not entirely
Excel’s fault, as it was misused. The problem is that when all you have is a hammer, everything
looks like a nail, and Excel is that data analytics hammer. So to the uncultured, everything
looks like an Excel problem.&lt;/p&gt;
&lt;p&gt;Now don’t misunderstand me; I’m not blaming Excel specifically, or any other specific GUI
application for this. In many cases, the problem lies between the keyboard and the chair. But GUI
applications have a part of responsibility, as they allow users to implement GUI-based workflows. I
think that complex GUI based workflows were an unintended consequence of developing GUIs. Who could
have expected, 40 years ago, that office jobs would evolve so much and that they would require such
complex workflows to generate an output? Consider the life-cycle of a shared Excel file in your
typical run-of-the-mill financial advisory firm. In many cases, it starts with an already existing
file that was made for another client and that is now used as a starting point. The first thing to
do, is to assign a poor junior to update the file and adapt it for the current assignment. He or
she will spend hours trying to reverse engineer this Excel file and then update it. This file
will at some point go to more senior members that will continue working on it, until it gets send
off for review to a manager. This manager, already overworked and with little time between meetings
to review the file correctly, just gives it a cursory glance and might find some mistakes here and
there. As a review method, colours and comments will be used.
The file goes back for a round of updates and reviews. As time goes by, and as the file gets
more and more complex, it starts to become impossible to manage and review properly. It eventually
gets used to give advice to a client, which might be totally wrong, because just as in the case of Reinhard and
Rogoff, someone, at some point, somewhere, did not select the right cells for the right formula.
Good luck ever finding this mistake, and who did it. During my consulting years, I have been
involved with very, very, big clients that were completely overwhelmed because all their
workflows were GUI based. They had been working like that for years, and kept recruiting highly
educated people en masse just to manage Excel and Word files. They were looking for a magic,
AI-based solution, because in their minds, if AIs could drive fricking cars, they should
also be able to edit and send Excel files around for review. Well, we’re not quite there yet,
so we told them, after our review of their processes and data sources (which in many
cases were Excel AND Word files), that what they needed was for their
company to go through an in-depth optimisation process “journey”. They weren’t interested
so they kept hiring very intelligent people to be office drones. I don’t think
that business model can remain sustainable.&lt;/p&gt;
&lt;p&gt;Now how much are situations like that the fault of Excel and how much personal responsibility do the
people involved have? I don’t know, but my point is that if, by magic, GUIs were
made to disappear, problems like that would also not exist. The reason is that if
you’re forced to write code to reach the results you want, you avoid a lot of these pitfalls
I just described. Working with scripts and the command line forces a discipline unto
you; you cannot be lazy and click around.
For example, reverse engineering a source code file is much easier that a finished
Excel spreadsheet. Even poorly written and undocumented code is always much better
than an Excel spreadsheet. If you throw a version control system in the mix, you have
the whole history of the file and the ability to know exactly what happened and when.
Add unit tests on the pile, and you start to get something that is very robust,
transparent, and much easier to audit.&lt;/p&gt;
&lt;p&gt;“But Bruno, not everyone is a programmer!” I hear you scream at your monitor.&lt;/p&gt;
&lt;p&gt;My point, again, is that if GUIs did not exist, people would have enough knowledge of these
tools to be able to work. What other choice would they have?&lt;/p&gt;
&lt;p&gt;Of course, GUIs have been invented, and they’re going nowhere. So what can you do?&lt;/p&gt;
&lt;p&gt;When it comes to statistics and data analysis/processing, you can at least not be part of the
problem and avoid using Excel altogether. If we go back to our previous scenario from the financial
advisory firm, the first step, which consisted in reverse engineering an Excel file, can be done
using &lt;code&gt;{tidyxl}&lt;/code&gt;. Let’s take a quick look; the spreadsheet I used as the header image for this blog
post comes from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Enron_Corpus&#34;&gt;Enron corpus&lt;/a&gt; , which is mostly
know for being a database of over 600000 emails from the US company Enron. But it also contains
spreadsheets, which are delightful. You can download the one from the picture
&lt;a href=&#34;https://github.com/rbind/b-rodrigues.github.com/raw/master/dutch_quigley__9378__modeldutch.xlsx&#34;&gt;here&lt;/a&gt;
(8mb xlsx warning). Opening it in your usual spreadsheet application will probably cause your heart
rate to increase to dangerous levels, so avoid that. Instead, let’s take a look at what &lt;code&gt;{tidyxl}&lt;/code&gt;
does with it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyxl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tidyxl&amp;#39; was built under R version 4.0.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: replacing previous import &amp;#39;vctrs::data_frame&amp;#39; by &amp;#39;tibble::data_frame&amp;#39;
## when loading &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.3.2     ✔ purrr   0.3.4
## ✔ tibble  3.0.1     ✔ dplyr   1.0.0
## ✔ tidyr   1.1.2     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.5.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tidyr&amp;#39; was built under R version 4.0.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dutch_quigley_9378 &amp;lt;- xlsx_cells(&amp;quot;~/six_to/spreadsheets/dutch_quigley__9378__modeldutch.xlsx&amp;quot;)


head(dutch_quigley_9378)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `...` is not empty.
## 
## We detected these problematic arguments:
## * `needs_dots`
## 
## These dots only exist to allow future extensions and should be empty.
## Did you misspecify an argument?&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 21
##   sheet address   row   col is_blank data_type error logical numeric
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;lgl&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;lgl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Swap… A1          1     1 FALSE    character &amp;lt;NA&amp;gt;  NA           NA
## 2 Swap… D2          2     4 FALSE    character &amp;lt;NA&amp;gt;  NA           NA
## 3 Swap… E2          2     5 FALSE    character &amp;lt;NA&amp;gt;  NA           NA
## 4 Swap… F2          2     6 FALSE    character &amp;lt;NA&amp;gt;  NA           NA
## 5 Swap… G2          2     7 FALSE    character &amp;lt;NA&amp;gt;  NA           NA
## 6 Swap… D3          3     4 FALSE    character &amp;lt;NA&amp;gt;  NA           NA
## # … with 12 more variables: date &amp;lt;dttm&amp;gt;, character &amp;lt;chr&amp;gt;,
## #   character_formatted &amp;lt;list&amp;gt;, formula &amp;lt;chr&amp;gt;, is_array &amp;lt;lgl&amp;gt;,
## #   formula_ref &amp;lt;chr&amp;gt;, formula_group &amp;lt;int&amp;gt;, comment &amp;lt;chr&amp;gt;, height &amp;lt;dbl&amp;gt;,
## #   width &amp;lt;dbl&amp;gt;, style_format &amp;lt;chr&amp;gt;, local_format_id &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That whole Excel workbook is inside a neat data frame. Imagine that you want
to quickly know where all the formulas are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dutch_quigley_9378 %&amp;gt;%
  filter(!is.na(formula)) %&amp;gt;%
  count(sheet, address)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `...` is not empty.
## 
## We detected these problematic arguments:
## * `needs_dots`
## 
## These dots only exist to allow future extensions and should be empty.
## Did you misspecify an argument?&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18,776 x 3
##    sheet address     n
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt;
##  1 Front B22         1
##  2 Front C13         1
##  3 Front C2          1
##  4 Front C22         1
##  5 Front C25         1
##  6 Front C26         1
##  7 Front C27         1
##  8 Front C28         1
##  9 Front C30         1
## 10 Front C31         1
## # … with 18,766 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the code above, you can quickly find, for each sheet, where the formulas
are. This workbook contains 18776 formulas. If Hell is a real place, it’s probably
an office building full of cubicles where you’ll sit for eternity looking
at these spreadsheets and trying to make sense of them.&lt;/p&gt;
&lt;p&gt;Now imagine that you’d like to know what these formulas are, let’s say, for the
&lt;code&gt;Swap&lt;/code&gt; sheet:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dutch_quigley_9378 %&amp;gt;%
  filter(sheet == &amp;quot;Swap&amp;quot;, !is.na(formula)) %&amp;gt;%
  select(address, formula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `...` is not empty.
## 
## We detected these problematic arguments:
## * `needs_dots`
## 
## These dots only exist to allow future extensions and should be empty.
## Did you misspecify an argument?&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6,773 x 2
##    address formula           
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;             
##  1 F1      DAY(EOMONTH(G1,0))
##  2 G1      A11               
##  3 E2      BE9               
##  4 A3      BQ5               
##  5 E3      BF9               
##  6 F3      SUM(G3:K3)        
##  7 H3      $F$1*H2           
##  8 I3      $F$1*I2           
##  9 J3      $F$1*J2           
## 10 K3      $F$1*K2           
## # … with 6,763 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Brilliant! Maybe you’re interested to find all the &lt;code&gt;&#34;SUM&#34;&lt;/code&gt; formulas? Easy!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dutch_quigley_9378 %&amp;gt;%
  filter(sheet == &amp;quot;Swap&amp;quot;, !is.na(formula)) %&amp;gt;%
  filter(grepl(&amp;quot;SUM&amp;quot;, formula)) %&amp;gt;%
  select(address, formula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `...` is not empty.
## 
## We detected these problematic arguments:
## * `needs_dots`
## 
## These dots only exist to allow future extensions and should be empty.
## Did you misspecify an argument?&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 31 x 2
##    address formula        
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          
##  1 F3      SUM(G3:K3)     
##  2 E4      SUM(D11:D309)  
##  3 F5      SUM(G5:K5)     
##  4 E6      SUM(F6:H6)     
##  5 BF8     SUM(BF11:BF242)
##  6 B9      SUM(B47:B294)  
##  7 AB9     SUM(AB11:AB253)
##  8 AC9     SUM(AC11:AC253)
##  9 AD9     SUM(AD11:AD253)
## 10 AE9     SUM(AE11:AE253)
## # … with 21 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You get the idea. There are many more things that you can extract such as
the formatting, the contents of the cells, the comments (and where to find them)
and much, much more. This will make making sense of a complex Excel file a breeze.&lt;/p&gt;
&lt;p&gt;The other thing that you can also do, once you’re done understanding this
monster Excel file, is not to perform the analysis inside Excel. Don’t fall into the temptation of
continuing this bad habit. As one on the data experts in your team/company, you have a
responsibility to bring the light to your colleagues. Be their Prometheus and decouple the data
from the code. Let the data be in Excel, but write all the required code to create whatever is
expected from you inside R. You can then export your finalized results back to Excel if needed. If
management tells you to do it in Excel, tell them that you’re the professional statistician/data
scientist, and that they shouldn’t tell you how to do your job. Granted, this is not always
possible, but you should plead your case as much as possible. In general, a good manager will be
all ears if you explain that not using GUIs like Excel makes it easier to spot and correct
mistakes, with the added benefit of being much easily audited and with huge time savings in the
long run. This is of course easier for completely new projects, and if you have an open-minded
manager. If you’re the manager, then you should ask your IT department to uninstall Excel
from your team member’s computers.&lt;/p&gt;
&lt;p&gt;Be brave, and ditch the GUI.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.
You can also watch my videos on &lt;a href=&#34;https://www.youtube.com/c/BrunoRodrigues1988/&#34;&gt;youtube&lt;/a&gt;.
So much content for you to consoom!&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>It&#39;s time to retire the &#34;data scientist&#34; label</title>
      <link>/blog/2020-11-05-retire_data_science/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-11-05-retire_data_science/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a&gt;
&lt;img src=&#34;/img/venn.png&#34; title = &#34;The correct data scientist venn diagram&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The “Data Scientist” label served its purpose; it allowed us to signal a transition happening in
our profession from using only applied mathematical statistical methods to something else, which
now also involves the use of a subset of software engineering practices. This transition was
mentioned back in 2010 by Deborah Nolan
(&lt;a href=&#34;https://www.stat.berkeley.edu/~statcur/Preprints/ComputingCurric3.pdf&#34; class=&#34;uri&#34;&gt;https://www.stat.berkeley.edu/~statcur/Preprints/ComputingCurric3.pdf&lt;/a&gt;), and this transition might
now be complete. Version control systems, document generation from annotated source code (or even
full reports generation &lt;em&gt;à la&lt;/em&gt; rmarkdown), containers and build automation tools have now entered
the toolbox of the run-of-the-mill statistician. Maybe not all of these tools, of course, it
largely depends on what it is exactly you do, but certainly some of these. Same goes for software
engineering practices. I have had the opportunity to work with some old-school statisticians (and
still do), and the difference is clear; just like old school users of WYSIWYG editors like Word
don’t use its “newest” features such as “Track changes” (and thus keep writing their text in
different colors to signal which paragraphs are new), or the concept of versions of a document
synced on Sharepoint (and thus keep multiple versions of the same document with different names)
old school statisticians have not included the tools I mentioned before in their toolbox.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.phdcomics.com/comics/archive/phd101212s.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Now don’t get me wrong here; that is absolutely ok. We need and respect old school statisticians
because they’ve been in the business of getting insights from data for longer than I’ve been alive.
This blog post is not a jab at them because they don’t know how to use git (if you interpret it
like that, that’s on you). Old school statisticians now have very senior positions and for many of
them, their job does not involve getting their hands dirty on data anymore; most of them are now
more like managers or mentors, and share their deep knowledge with their more junior team members.
(Obviously there’s exceptions, when I say &lt;em&gt;all&lt;/em&gt; old school statisticians do this or that, I don’t
mean &lt;em&gt;all&lt;/em&gt; of them, but most of them. Of course, I don’t have any evidence to back that up).&lt;/p&gt;
&lt;p&gt;What this blog post is about is the label “Data Scientist” that gets used by these more junior team
members and by companies that want to hire talented and motivated young people. This label,
and the purported difference between a “Data Scientist” and “statistician” does not make any sense
in 2020 anymore. (I know I’m beating a dead horse here, but this is my blog. I’ll blog about dead
horses as much as I want thank you very much.)&lt;/p&gt;
&lt;p&gt;Firstly, this label has always been confusing. “Data Scientist”… what does it even mean? The fact
it took so long to find a definition, and that almost everyone working in the profession has a
different one speaks volumes. Also, don’t all scientists use data? Data from experiments, from
observational studies, from surveys, from the literature?&lt;/p&gt;
&lt;p&gt;Secondly, I don’t believe that you can get a degree in statistics today without any exposition
whatsoever to at least some of the tools I mentioned before. I really doubt that there’s people out
there getting Master’s degrees in statistics without having &lt;em&gt;ever&lt;/em&gt; touched these tools, or the unix
command line. The degrees they’re going for might not focus a lot on these tools, true, but they
certainly touch upon them. And of course, once they join a team at their first job, they’ll get
more exposed to these tools and incorporate them in their day to day work. So, they’re not
statisticians anymore? Their degree magically transformed into a data science degree?&lt;/p&gt;
&lt;p&gt;But what about data science degrees? Are the students graduating with these degrees statisticians?
I’d argue that yes, they are indeed statisticians; it’s just that they took a statistics degree
that might have focused more than usual on these “new” practices/tools, and changed its name to
“Data Science degree” for marketing purposes.&lt;/p&gt;
&lt;p&gt;Anyways, the label “Data Scientist” is now completely defunct; as I mentioned in the very
beginning, it served us well to signal that a transition was happening in the profession. I believe
that this transition is now complete, or should be nearing its final stages. Also, this transition
was not only about the tools used, but also about the deliverables. Statisticians now don’t only
deliver tables, graphs and studies but more and more of them deliver &lt;em&gt;products&lt;/em&gt;. This product can
be a package implementing a bleeding edge statistical method for the profession as a whole, or it
can be part of a piece of software that needs it to run (like your smartphone keyboard using a
statistical model for word predictions). See
&lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/10691898.2020.1845109?scroll=top&amp;amp;needAccess=true&amp;amp;&#34;&gt;this paper&lt;/a&gt; for
an interesting exposition about how curricula and deliverables have evolved in the past two
decades.&lt;/p&gt;
&lt;p&gt;Currently, this label gets used by people that try to get insights from data. But we already have a
word for them; statisticians. It’s just that the tools of the statistician have evolved over the
past decade or so. Actually, I would perhaps even make another distinction; we should reserve the
label of “statistician” to people that do statistics without ever touching any data. The other
statisticians, the ones that get dirty wrestling in the mud with the data (they’re the &lt;em&gt;pigs that
like it&lt;/em&gt; from that famous quote) should be called “data janitors”. I’m not even joking; not only
does that term already exist and gets used, I think it suits what we do perfectly. What do janitors
do? They clean stuff and put things in order. We clean data and put it in order; meaning creating
summary tables, visualizations, interactive applications, and models. Oh, and we do so (preferably)
in a reproducible way.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building apps with {shinipsum} and {golem}</title>
      <link>/blog/2020-09-27-golemdemo/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-09-27-golemdemo/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=B35E8QleVhg&#34;&gt;
&lt;img src=&#34;/img/golem.png&#34; title = &#34;Only 90&#39;s kids will get it&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&#34;https://www.brodrigues.co/blog/2020-09-20-shiny_raspberry/&#34;&gt;In my previous blog post&lt;/a&gt; I showed you
how I set up my own Shiny server using a Raspberry Pi 4B. If you visited the following
&lt;a href=&#34;https://www.brodrigues.co/blog/2020-09-20-shiny_raspberry/&#34;&gt;link&lt;/a&gt; you’ll be connecting to my
Raspberry Pi and can play around with a Shiny app that I called &lt;code&gt;golemDemo&lt;/code&gt;.
It’s been quite a few months that I wanted to discuss this app:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;A little prototype app I&amp;#39;m working on to learn more about &lt;a href=&#34;https://twitter.com/thinkR_fr?ref_src=twsrc%5Etfw&#34;&gt;@thinkR_fr&lt;/a&gt; &amp;#39;s {golem} and interaction between modules, YouTube video coming later this week &lt;a href=&#34;https://t.co/DRfM2KqqQf&#34;&gt;pic.twitter.com/DRfM2KqqQf&lt;/a&gt;&lt;/p&gt;&amp;mdash; Bruno Rodrigues (@brodriguesco) &lt;a href=&#34;https://twitter.com/brodriguesco/status/1277671383573704706?ref_src=twsrc%5Etfw&#34;&gt;June 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;So the tweet mentions that a video was coming in the following week and you’ll notice that the tweet
was made on… June 29th, and still no video. As I said in my previous blog post, I’ve been busy.
Anyways, here’s already a blog post, and I might still do a video where I’ll go into greater detail.
I think that videos are quite nice to walk an audience through an app, but it works best with an
accompanying blog post where I can comment some more complicated snippets of code.&lt;/p&gt;
&lt;div id=&#34;why-golem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why &lt;code&gt;{golem}&lt;/code&gt;?&lt;/h1&gt;
&lt;p&gt;Why should you consider the &lt;code&gt;{golem}&lt;/code&gt; package to develop your Shiny apps? For me, there are two
main reasons. First of all, I’m already familiar with package development in R, having made some
little packages that I have on my Github account, and one out on CRAN (with the complete texts of
Luxembourguish author &lt;a href=&#34;https://cran.r-project.org/web/packages/michelRodange/index.html&#34;&gt;Michel Rodange&lt;/a&gt;)
so using &lt;code&gt;{golem}&lt;/code&gt; came at no additional costs. This is because a Shiny app built with &lt;code&gt;{golem}&lt;/code&gt; is
actually an R package! This has many advantages; all the steps of documenting, testing and sharing
the app are greatly simplified.
Another reason to use &lt;code&gt;{golem}&lt;/code&gt; is that it forces on you a certain way of working. Now this
might seem like a pretty bad thing, but I find that it is quite helpful. When you start working
on a Shiny app, you might get very quickly overwhelmed with both thinking about your server logic
and your UI. You might spend much time tinkering with getting the server functions working, while
still not having no UI to speak of, or you might work on one part of the server and then go to the UI,
then back on the server… You’ll spend hours working on the app without a clear approach, and
probably waste much time because of this back and forth.
The first recommended step when building a shiny app (with or without &lt;code&gt;{golem}&lt;/code&gt;) is a “UI first” approach.
For this, we’re going to use &lt;code&gt;{shinipsum}&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lorem-ipsum-dolor-server-amet-its-latin-for-dont-bother-with-the-server-logic-until-its-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lorem ipsum dolor server amet (it’s Latin for “don’t bother with the server logic until it’s time”)&lt;/h1&gt;
&lt;p&gt;The developers of &lt;code&gt;{golem}&lt;/code&gt;, French company &lt;a href=&#34;https://thinkr.fr/&#34;&gt;ThinkR&lt;/a&gt;
suggest an “UI” first approach. The idea is to focus on the UI, and to do so using their other package called
&lt;code&gt;{shinipsum}&lt;/code&gt; to randomly generate elements on the server side which you can then later replace
with your actual server logic. For instance, imagine that somewhere on your app, you want to show
a bar plot using the &lt;code&gt;{ggplot2}&lt;/code&gt; package. Using &lt;code&gt;{shinipsum}&lt;/code&gt;, you can generate a random bar plot
with the following line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shinipsum::random_ggplot(&amp;quot;bar&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-09-27-golemDemo_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and that’s it! Now simply ignore this bit on the server, and continue focusing on the UI. You need
to show a random table? No problem:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shinipsum::random_table(ncol = 7, nrow = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    conc rate   state conc.1 rate.1 state.1 conc.2
## 1  0.02   76 treated   0.02     76 treated   0.02
## 2  0.02   47 treated   0.02     47 treated   0.02
## 3  0.06   97 treated   0.06     97 treated   0.06
## 4  0.06  107 treated   0.06    107 treated   0.06
## 5  0.11  123 treated   0.11    123 treated   0.11
## 6  0.11  139 treated   0.11    139 treated   0.11
## 7  0.22  159 treated   0.22    159 treated   0.22
## 8  0.22  152 treated   0.22    152 treated   0.22
## 9  0.56  191 treated   0.56    191 treated   0.56
## 10 0.56  201 treated   0.56    201 treated   0.56&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your app might now look something like this (actually, it won’t because the little demo below
is not a &lt;code&gt;{golem}&lt;/code&gt; app, but it illustrates &lt;code&gt;{shinipsum}&lt;/code&gt; well):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(shiny)
library(reactable)
library(shinipsum)
library(ggiraph)

ui &amp;lt;- pageWithSidebar(
  
  headerPanel(&amp;quot;This is a shinipsum demo&amp;quot;),
  
  sidebarPanel(
    sliderInput(&amp;quot;rows&amp;quot;,
                &amp;quot;Number of rows:&amp;quot;,
                min = 1,
                max = 50,
                value = 5)
  ),
  
  mainPanel(
    reactableOutput(&amp;quot;table&amp;quot;),
    girafeOutput(&amp;quot;graph&amp;quot;)
  )
)


server &amp;lt;- function(input, output) {

  output$table &amp;lt;- renderReactable({
    reactable(random_table(ncol = 10, nrow = input$rows))
  })

  output$graph &amp;lt;- renderGirafe({
    girafe(ggobj = random_ggplot(&amp;quot;bar&amp;quot;))
  })
}

shinyApp(ui = ui, server = server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you have the required packages, running this on a fresh R session should start a little app.&lt;/p&gt;
&lt;p&gt;You see that the server is only a call to &lt;code&gt;shinipsum::random_table&lt;/code&gt;, and &lt;code&gt;shinipsum::random_ggplot&lt;/code&gt;.
Because I want a &lt;code&gt;reactable&lt;/code&gt; and an interactive plot using the &lt;code&gt;{ggiraph}&lt;/code&gt; package, I have already
written the minimum amount of code on the server side to get things working. Now I can focus on my
UI and then, when I’m done, I can start replacing the random objects from &lt;code&gt;{shinipsum}&lt;/code&gt; with
the actual code.&lt;/p&gt;
&lt;p&gt;Now proceeding in this way is not a requirement of &lt;code&gt;{golem}&lt;/code&gt;, but it helps to structure your thoughts
and your app, and you can use this approach for any type of app. The example above, after all, is
not a &lt;code&gt;{golem}&lt;/code&gt; app.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;get-modular-with-golem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Get modular with &lt;code&gt;{golem}&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;This is now where we get to some more interesting, and &lt;code&gt;{golem}&lt;/code&gt; specific things. If you’ve been
using R and Shiny for the past years, you’ve probably have heard a lot about functional programming.
Functional programming is a programming paradigm that encourages, and in some languages
forces, the use of functions. The idea is that everything you do should be a call to a function,
and functions should be chained together to achieve whatever it is you want to do; cleaning data,
visualizing data, modeling data… R has many functional tools out of the box, which can be complemented
using the &lt;code&gt;{purrr}&lt;/code&gt; package.
What does all of this have to do with Shiny and &lt;code&gt;{golem}&lt;/code&gt;? Well, &lt;code&gt;{golem}&lt;/code&gt; forces you to write
modules to build your apps, and modules are very similar to functions (they’re actually functions).
They’re bits of code that can be decoupled from your app, used in any other app, they can be linked together,
they can be easily documented and tested… If you are familiar with R’s functional programming
approach, modules should not be totally new to you. But if you’ve been using Shiny without module,
they’ll require some getting used to.&lt;/p&gt;
&lt;p&gt;To illustrate how a simple app can be written using modules, I have built &lt;code&gt;golemDemo&lt;/code&gt;, which, as implied
by its name, is a demonstration of a &lt;code&gt;{golem}&lt;/code&gt; app which I hope is simple enough for anyone to
start using. The app is quite simple and does only three things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it allows you to choose between two datasets;&lt;/li&gt;
&lt;li&gt;it shows a table of the selected dataset;&lt;/li&gt;
&lt;li&gt;it shows a map of Luxembourg with the data points;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each of these things is a module, which means that if I were to create another app with a map of
Luxembourg, I could simply reuse it. But remember, the app is actually an R package. Here is
the root of the app on my computer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;ls&amp;quot;, args = &amp;quot;-lFR ~/Documents/golemDemo&amp;quot;, stdout = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;/home/cbrunos/Documents/golemDemo:&amp;quot;                                    
##  [2] &amp;quot;total 56&amp;quot;                                                              
##  [3] &amp;quot;-rw-r--r-- 1 cbrunos users  302 Sep 19 11:28 app.R&amp;quot;                    
##  [4] &amp;quot;drwxr-xr-x 2 cbrunos users 4096 Jun 29 17:49 data-raw/&amp;quot;                
##  [5] &amp;quot;-rw-r--r-- 1 cbrunos users  729 Sep 19 21:27 DESCRIPTION&amp;quot;              
##  [6] &amp;quot;drwxr-xr-x 2 cbrunos users 4096 Sep 11 23:39 dev/&amp;quot;                     
##  [7] &amp;quot;-rw-r--r-- 1 cbrunos users 2723 Sep 12 15:04 Dockerfile&amp;quot;               
##  [8] &amp;quot;drwxr-xr-x 3 cbrunos users 4096 Jun 28 11:33 inst/&amp;quot;                    
##  [9] &amp;quot;-rw-r--r-- 1 cbrunos users  483 Apr  8 21:38 LICENSE.md&amp;quot;               
## [10] &amp;quot;drwxr-xr-x 2 cbrunos users 4096 Sep 19 21:27 man/&amp;quot;                     
## [11] &amp;quot;-rw-r--r-- 1 cbrunos users 1420 Sep 19 21:27 NAMESPACE&amp;quot;                
## [12] &amp;quot;drwxr-xr-x 2 cbrunos users 4096 Sep 19 21:27 R/&amp;quot;                       
## [13] &amp;quot;-rw-r--r-- 1 cbrunos users 1056 Jun 28 11:38 README.Rmd&amp;quot;               
## [14] &amp;quot;drwxr-xr-x 3 cbrunos users 4096 Sep 11 17:12 rsconnect/&amp;quot;               
## [15] &amp;quot;drwxr-xr-x 3 cbrunos users 4096 Jun 28 11:48 tests/&amp;quot;                   
## [16] &amp;quot;drwxr-xr-x 2 cbrunos users 4096 Jun 28 11:48 vignettes/&amp;quot;               
## [17] &amp;quot;&amp;quot;                                                                      
## [18] &amp;quot;/home/cbrunos/Documents/golemDemo/data-raw:&amp;quot;                           
## [19] &amp;quot;total 1168&amp;quot;                                                            
## [20] &amp;quot;-rw-r--r-- 1 cbrunos users 1176106 Jun 11 09:52 communes_df.csv&amp;quot;       
## [21] &amp;quot;-rw-r--r-- 1 cbrunos users      99 Jun 28 11:48 my_dataset.R&amp;quot;          
## [22] &amp;quot;-rw-r--r-- 1 cbrunos users    1998 Jun 28 17:00 radars.csv&amp;quot;            
## [23] &amp;quot;-rw-r--r-- 1 cbrunos users    6390 Jun 28 12:31 rettungspunkte.csv&amp;quot;    
## [24] &amp;quot;&amp;quot;                                                                      
## [25] &amp;quot;/home/cbrunos/Documents/golemDemo/dev:&amp;quot;                                
## [26] &amp;quot;total 16&amp;quot;                                                              
## [27] &amp;quot;-rw-r--r-- 1 cbrunos users 1935 Jun 28 11:33 01_start.R&amp;quot;               
## [28] &amp;quot;-rw-r--r-- 1 cbrunos users 2011 Sep 11 23:39 02_dev.R&amp;quot;                 
## [29] &amp;quot;-rw-r--r-- 1 cbrunos users 1012 Jun 28 11:33 03_deploy.R&amp;quot;              
## [30] &amp;quot;-rw-r--r-- 1 cbrunos users  318 Jun 28 11:33 run_dev.R&amp;quot;                
## [31] &amp;quot;&amp;quot;                                                                      
## [32] &amp;quot;/home/cbrunos/Documents/golemDemo/inst:&amp;quot;                               
## [33] &amp;quot;total 8&amp;quot;                                                               
## [34] &amp;quot;drwxr-xr-x 3 cbrunos users 4096 Jun 28 11:33 app/&amp;quot;                     
## [35] &amp;quot;-rw-r--r-- 1 cbrunos users  140 Jun 28 11:38 golem-config.yml&amp;quot;         
## [36] &amp;quot;&amp;quot;                                                                      
## [37] &amp;quot;/home/cbrunos/Documents/golemDemo/inst/app:&amp;quot;                           
## [38] &amp;quot;total 4&amp;quot;                                                               
## [39] &amp;quot;drwxr-xr-x 2 cbrunos users 4096 Jun 28 11:48 www/&amp;quot;                     
## [40] &amp;quot;&amp;quot;                                                                      
## [41] &amp;quot;/home/cbrunos/Documents/golemDemo/inst/app/www:&amp;quot;                       
## [42] &amp;quot;total 12&amp;quot;                                                              
## [43] &amp;quot;-rw-r--r-- 1 cbrunos users    0 Jun 28 11:48 custom.css&amp;quot;               
## [44] &amp;quot;-rw-r--r-- 1 cbrunos users 3774 Jun 28 11:33 favicon.ico&amp;quot;              
## [45] &amp;quot;-rw-r--r-- 1 cbrunos users  100 Jun 28 11:48 handlers.js&amp;quot;              
## [46] &amp;quot;-rw-r--r-- 1 cbrunos users   40 Jun 28 11:48 script.js&amp;quot;                
## [47] &amp;quot;&amp;quot;                                                                      
## [48] &amp;quot;/home/cbrunos/Documents/golemDemo/man:&amp;quot;                                
## [49] &amp;quot;total 8&amp;quot;                                                               
## [50] &amp;quot;-rw-r--r-- 1 cbrunos users 261 Sep 19 21:27 pipe.Rd&amp;quot;                   
## [51] &amp;quot;-rw-r--r-- 1 cbrunos users 291 Jun 28 11:33 run_app.Rd&amp;quot;                
## [52] &amp;quot;&amp;quot;                                                                      
## [53] &amp;quot;/home/cbrunos/Documents/golemDemo/R:&amp;quot;                                  
## [54] &amp;quot;total 48&amp;quot;                                                              
## [55] &amp;quot;-rw-r--r-- 1 cbrunos users  783 Jun 28 11:33 app_config.R&amp;quot;             
## [56] &amp;quot;-rw-r--r-- 1 cbrunos users  654 Jun 29 18:34 app_server.R&amp;quot;             
## [57] &amp;quot;-rw-r--r-- 1 cbrunos users 1790 Sep 12 15:00 app_ui.R&amp;quot;                 
## [58] &amp;quot;-rw-r--r-- 1 cbrunos users    0 Jun 28 11:48 fct_helpers.R&amp;quot;            
## [59] &amp;quot;-rw-rw-r-- 1 cbrunos users  997 Jun 28 11:38 golem_utils_server.R&amp;quot;     
## [60] &amp;quot;-rw-rw-r-- 1 cbrunos users 5849 Jun 28 11:38 golem_utils_ui.R&amp;quot;         
## [61] &amp;quot;-rw-r--r-- 1 cbrunos users  549 Jun 28 11:48 mod_filter_data.R&amp;quot;        
## [62] &amp;quot;-rw-r--r-- 1 cbrunos users 3118 Sep 19 11:16 mod_load_data.R&amp;quot;          
## [63] &amp;quot;-rw-r--r-- 1 cbrunos users 2088 Jun 29 18:30 mod_map_data.R&amp;quot;           
## [64] &amp;quot;-rw-r--r-- 1 cbrunos users  910 Jun 29 18:17 mod_table_data.R&amp;quot;         
## [65] &amp;quot;-rw-r--r-- 1 cbrunos users  337 Jun 28 11:33 run_app.R&amp;quot;                
## [66] &amp;quot;-rw-r--r-- 1 cbrunos users    0 Jun 28 11:48 utils_helpers.R&amp;quot;          
## [67] &amp;quot;-rw-r--r-- 1 cbrunos users  207 Sep 19 21:27 utils-pipe.R&amp;quot;             
## [68] &amp;quot;&amp;quot;                                                                      
## [69] &amp;quot;/home/cbrunos/Documents/golemDemo/rsconnect:&amp;quot;                          
## [70] &amp;quot;total 4&amp;quot;                                                               
## [71] &amp;quot;drwxr-xr-x 3 cbrunos users 4096 Sep 11 17:12 shinyapps.io/&amp;quot;            
## [72] &amp;quot;&amp;quot;                                                                      
## [73] &amp;quot;/home/cbrunos/Documents/golemDemo/rsconnect/shinyapps.io:&amp;quot;             
## [74] &amp;quot;total 4&amp;quot;                                                               
## [75] &amp;quot;drwxr-xr-x 2 cbrunos users 4096 Sep 11 17:12 brodriguesco/&amp;quot;            
## [76] &amp;quot;&amp;quot;                                                                      
## [77] &amp;quot;/home/cbrunos/Documents/golemDemo/rsconnect/shinyapps.io/brodriguesco:&amp;quot;
## [78] &amp;quot;total 4&amp;quot;                                                               
## [79] &amp;quot;-rw-r--r-- 1 cbrunos users 219 Sep 19 21:30 golemdemo.dcf&amp;quot;             
## [80] &amp;quot;&amp;quot;                                                                      
## [81] &amp;quot;/home/cbrunos/Documents/golemDemo/tests:&amp;quot;                              
## [82] &amp;quot;total 8&amp;quot;                                                               
## [83] &amp;quot;drwxr-xr-x 2 cbrunos users 4096 Jun 28 11:48 testthat/&amp;quot;                
## [84] &amp;quot;-rw-r--r-- 1 cbrunos users   62 Jun 28 11:48 testthat.R&amp;quot;               
## [85] &amp;quot;&amp;quot;                                                                      
## [86] &amp;quot;/home/cbrunos/Documents/golemDemo/tests/testthat:&amp;quot;                     
## [87] &amp;quot;total 4&amp;quot;                                                               
## [88] &amp;quot;-rw-r--r-- 1 cbrunos users 64 Jun 28 11:48 test-app.R&amp;quot;                 
## [89] &amp;quot;&amp;quot;                                                                      
## [90] &amp;quot;/home/cbrunos/Documents/golemDemo/vignettes:&amp;quot;                          
## [91] &amp;quot;total 4&amp;quot;                                                               
## [92] &amp;quot;-rw-r--r-- 1 cbrunos users 298 Jun 28 11:48 golemDemo.Rmd&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first 16 lines show the root of the folder, and then we see what’s inside each subfolder,
starting with &lt;code&gt;data-raw/&lt;/code&gt;, then &lt;code&gt;dev/&lt;/code&gt; etc (this is done via a call to the &lt;code&gt;ls -lFR&lt;/code&gt; Linux command,
invoked here with R’s &lt;code&gt;system2()&lt;/code&gt; function).&lt;/p&gt;
&lt;p&gt;If you’ve already developed a package in the past, you’ll recognize the structure. What’s important
here is the &lt;code&gt;dev/&lt;/code&gt; folder, which is &lt;code&gt;{golem}&lt;/code&gt; specific. This folder contains for files,
&lt;code&gt;01_start.R&lt;/code&gt;, &lt;code&gt;02_dev.R&lt;/code&gt;, &lt;code&gt;03_deploy.R&lt;/code&gt; and &lt;code&gt;run_dev.R&lt;/code&gt;. These files are the ones that will help
you develop your shiny app and you should follow the instructions contained in each of them. Let’s
take a look at &lt;code&gt;01_start.R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;cat&amp;quot;, args = &amp;quot;~/Documents/golemDemo/dev/01_start.R&amp;quot;, stdout = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;# Building a Prod-Ready, Robust Shiny Application.&amp;quot;                                     
##  [2] &amp;quot;# &amp;quot;                                                                                     
##  [3] &amp;quot;# README: each step of the dev files is optional, and you don&amp;#39;t have to &amp;quot;               
##  [4] &amp;quot;# fill every dev scripts before getting started. &amp;quot;                                      
##  [5] &amp;quot;# 01_start.R should be filled at start. &amp;quot;                                               
##  [6] &amp;quot;# 02_dev.R should be used to keep track of your development during the project.&amp;quot;        
##  [7] &amp;quot;# 03_deploy.R should be used once you need to deploy your app.&amp;quot;                         
##  [8] &amp;quot;# &amp;quot;                                                                                     
##  [9] &amp;quot;# &amp;quot;                                                                                     
## [10] &amp;quot;########################################&amp;quot;                                               
## [11] &amp;quot;#### CURRENT FILE: ON START SCRIPT #####&amp;quot;                                               
## [12] &amp;quot;########################################&amp;quot;                                               
## [13] &amp;quot;&amp;quot;                                                                                       
## [14] &amp;quot;## Fill the DESCRIPTION ----&amp;quot;                                                           
## [15] &amp;quot;## Add meta data about your application&amp;quot;                                                
## [16] &amp;quot;golem::fill_desc(&amp;quot;                                                                      
## [17] &amp;quot;  pkg_name = \&amp;quot;golemDemo\&amp;quot;, # The Name of the package containing the App &amp;quot;              
## [18] &amp;quot;  pkg_title = \&amp;quot;PKG_TITLE\&amp;quot;, # The Title of the package containing the App &amp;quot;            
## [19] &amp;quot;  pkg_description = \&amp;quot;PKG_DESC.\&amp;quot;, # The Description of the package containing the App &amp;quot;
## [20] &amp;quot;  author_first_name = \&amp;quot;AUTHOR_FIRST\&amp;quot;, # Your First Name&amp;quot;                              
## [21] &amp;quot;  author_last_name = \&amp;quot;AUTHOR_LAST\&amp;quot;, # Your Last Name&amp;quot;                                 
## [22] &amp;quot;  author_email = \&amp;quot;AUTHOR@MAIL.COM\&amp;quot;, # Your Email&amp;quot;                                     
## [23] &amp;quot;  repo_url = NULL # The URL of the GitHub Repo (optional) &amp;quot;                             
## [24] &amp;quot;)     &amp;quot;                                                                                 
## [25] &amp;quot;&amp;quot;                                                                                       
## [26] &amp;quot;## Set {golem} options ----&amp;quot;                                                            
## [27] &amp;quot;golem::set_golem_options()&amp;quot;                                                             
## [28] &amp;quot;&amp;quot;                                                                                       
## [29] &amp;quot;## Create Common Files ----&amp;quot;                                                            
## [30] &amp;quot;## See ?usethis for more information&amp;quot;                                                   
## [31] &amp;quot;usethis::use_mit_license( name = \&amp;quot;Golem User\&amp;quot; )  # You can set another license here&amp;quot;  
## [32] &amp;quot;usethis::use_readme_rmd( open = FALSE )&amp;quot;                                                
## [33] &amp;quot;usethis::use_code_of_conduct()&amp;quot;                                                         
## [34] &amp;quot;usethis::use_lifecycle_badge( \&amp;quot;Experimental\&amp;quot; )&amp;quot;                                       
## [35] &amp;quot;usethis::use_news_md( open = FALSE )&amp;quot;                                                   
## [36] &amp;quot;&amp;quot;                                                                                       
## [37] &amp;quot;## Use git ----&amp;quot;                                                                        
## [38] &amp;quot;usethis::use_git()&amp;quot;                                                                     
## [39] &amp;quot;&amp;quot;                                                                                       
## [40] &amp;quot;## Init Testing Infrastructure ----&amp;quot;                                                    
## [41] &amp;quot;## Create a template for tests&amp;quot;                                                         
## [42] &amp;quot;golem::use_recommended_tests()&amp;quot;                                                         
## [43] &amp;quot;&amp;quot;                                                                                       
## [44] &amp;quot;## Use Recommended Packages ----&amp;quot;                                                       
## [45] &amp;quot;golem::use_recommended_deps()&amp;quot;                                                          
## [46] &amp;quot;&amp;quot;                                                                                       
## [47] &amp;quot;## Favicon ----&amp;quot;                                                                        
## [48] &amp;quot;# If you want to change the favicon (default is golem&amp;#39;s one)&amp;quot;                           
## [49] &amp;quot;golem::remove_favicon()&amp;quot;                                                                
## [50] &amp;quot;golem::use_favicon() # path = \&amp;quot;path/to/ico\&amp;quot;. Can be an online file. &amp;quot;                 
## [51] &amp;quot;&amp;quot;                                                                                       
## [52] &amp;quot;## Add helper functions ----&amp;quot;                                                           
## [53] &amp;quot;golem::use_utils_ui()&amp;quot;                                                                  
## [54] &amp;quot;golem::use_utils_server()&amp;quot;                                                              
## [55] &amp;quot;&amp;quot;                                                                                       
## [56] &amp;quot;# You&amp;#39;re now set! ----&amp;quot;                                                                 
## [57] &amp;quot;&amp;quot;                                                                                       
## [58] &amp;quot;# go to dev/02_dev.R&amp;quot;                                                                   
## [59] &amp;quot;rstudioapi::navigateToFile( \&amp;quot;dev/02_dev.R\&amp;quot; )&amp;quot;                                         
## [60] &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script is a series of calls to &lt;code&gt;{usethis}&lt;/code&gt; functions; you can remove whatever you don’t need
and adapt the others that you need. As you can see, I did not change much here. Execute it line by
line when you’re done editing it. Once you’re done, you can go to &lt;code&gt;02_dev.R&lt;/code&gt; and this is probably
the script that you’ll change the most:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;cat&amp;quot;, args = &amp;quot;~/Documents/golemDemo/dev/02_dev.R&amp;quot;, stdout = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;# Building a Prod-Ready, Robust Shiny Application.&amp;quot;                             
##  [2] &amp;quot;# &amp;quot;                                                                             
##  [3] &amp;quot;# README: each step of the dev files is optional, and you don&amp;#39;t have to &amp;quot;       
##  [4] &amp;quot;# fill every dev scripts before getting started. &amp;quot;                              
##  [5] &amp;quot;# 01_start.R should be filled at start. &amp;quot;                                       
##  [6] &amp;quot;# 02_dev.R should be used to keep track of your development during the project.&amp;quot;
##  [7] &amp;quot;# 03_deploy.R should be used once you need to deploy your app.&amp;quot;                 
##  [8] &amp;quot;# &amp;quot;                                                                             
##  [9] &amp;quot;# &amp;quot;                                                                             
## [10] &amp;quot;###################################&amp;quot;                                            
## [11] &amp;quot;#### CURRENT FILE: DEV SCRIPT #####&amp;quot;                                            
## [12] &amp;quot;###################################&amp;quot;                                            
## [13] &amp;quot;&amp;quot;                                                                               
## [14] &amp;quot;# Engineering&amp;quot;                                                                  
## [15] &amp;quot;&amp;quot;                                                                               
## [16] &amp;quot;## Dependencies ----&amp;quot;                                                           
## [17] &amp;quot;## Add one line by package you want to add as dependency&amp;quot;                       
## [18] &amp;quot;usethis::use_package( \&amp;quot;shiny\&amp;quot; )&amp;quot;                                              
## [19] &amp;quot;usethis::use_package( \&amp;quot;shinydashboard\&amp;quot; )&amp;quot;                                     
## [20] &amp;quot;usethis::use_package(\&amp;quot;data.table\&amp;quot;) &amp;quot;                                          
## [21] &amp;quot;usethis::use_package(\&amp;quot;DT\&amp;quot;)&amp;quot;                                                   
## [22] &amp;quot;usethis::use_package(\&amp;quot;dplyr\&amp;quot;)&amp;quot;                                                
## [23] &amp;quot;usethis::use_package(\&amp;quot;rlang\&amp;quot;)&amp;quot;                                                
## [24] &amp;quot;usethis::use_package(\&amp;quot;ggiraph\&amp;quot;)&amp;quot;                                              
## [25] &amp;quot;usethis::use_package(\&amp;quot;ggplot2\&amp;quot;)&amp;quot;                                              
## [26] &amp;quot;usethis::use_package(\&amp;quot;htmlwidgets\&amp;quot;)&amp;quot;                                          
## [27] &amp;quot;usethis::use_package(\&amp;quot;dplyr\&amp;quot;)&amp;quot;                                                
## [28] &amp;quot;usethis::use_package(\&amp;quot;colorspace\&amp;quot;)&amp;quot;                                           
## [29] &amp;quot;usethis::use_package(\&amp;quot;shinycssloaders\&amp;quot;)&amp;quot;                                      
## [30] &amp;quot;usethis::use_package(\&amp;quot;lubridate\&amp;quot;)&amp;quot;                                            
## [31] &amp;quot;&amp;quot;                                                                               
## [32] &amp;quot;## Add modules ----&amp;quot;                                                            
## [33] &amp;quot;## Create a module infrastructure in R/&amp;quot;                                        
## [34] &amp;quot;golem::add_module( name = \&amp;quot;name_of_module1\&amp;quot; ) # Name of the module&amp;quot;           
## [35] &amp;quot;golem::add_module( name = \&amp;quot;name_of_module2\&amp;quot; ) # Name of the module&amp;quot;           
## [36] &amp;quot;&amp;quot;                                                                               
## [37] &amp;quot;## Add helper functions ----&amp;quot;                                                   
## [38] &amp;quot;## Creates ftc_* and utils_*&amp;quot;                                                   
## [39] &amp;quot;golem::add_fct( \&amp;quot;helpers\&amp;quot; ) &amp;quot;                                                 
## [40] &amp;quot;golem::add_utils( \&amp;quot;helpers\&amp;quot; )&amp;quot;                                                
## [41] &amp;quot;&amp;quot;                                                                               
## [42] &amp;quot;## External resources&amp;quot;                                                          
## [43] &amp;quot;## Creates .js and .css files at inst/app/www&amp;quot;                                  
## [44] &amp;quot;golem::add_js_file( \&amp;quot;script\&amp;quot; )&amp;quot;                                               
## [45] &amp;quot;golem::add_js_handler( \&amp;quot;handlers\&amp;quot; )&amp;quot;                                          
## [46] &amp;quot;golem::add_css_file( \&amp;quot;custom\&amp;quot; )&amp;quot;                                              
## [47] &amp;quot;&amp;quot;                                                                               
## [48] &amp;quot;## Add internal datasets ----&amp;quot;                                                  
## [49] &amp;quot;## If you have data in your package&amp;quot;                                            
## [50] &amp;quot;usethis::use_data_raw( name = \&amp;quot;my_dataset\&amp;quot;, open = FALSE ) &amp;quot;                  
## [51] &amp;quot;&amp;quot;                                                                               
## [52] &amp;quot;## Tests ----&amp;quot;                                                                  
## [53] &amp;quot;## Add one line by test you want to create&amp;quot;                                     
## [54] &amp;quot;usethis::use_test( \&amp;quot;app\&amp;quot; )&amp;quot;                                                   
## [55] &amp;quot;&amp;quot;                                                                               
## [56] &amp;quot;# Documentation&amp;quot;                                                                
## [57] &amp;quot;&amp;quot;                                                                               
## [58] &amp;quot;## Vignette ----&amp;quot;                                                               
## [59] &amp;quot;usethis::use_vignette(\&amp;quot;golemDemo\&amp;quot;)&amp;quot;                                           
## [60] &amp;quot;devtools::build_vignettes()&amp;quot;                                                    
## [61] &amp;quot;&amp;quot;                                                                               
## [62] &amp;quot;## Code coverage ----&amp;quot;                                                          
## [63] &amp;quot;## (You&amp;#39;ll need GitHub there)&amp;quot;                                                  
## [64] &amp;quot;usethis::use_github()&amp;quot;                                                          
## [65] &amp;quot;usethis::use_travis()&amp;quot;                                                          
## [66] &amp;quot;usethis::use_appveyor()&amp;quot;                                                        
## [67] &amp;quot;&amp;quot;                                                                               
## [68] &amp;quot;# You&amp;#39;re now set! ----&amp;quot;                                                         
## [69] &amp;quot;# go to dev/03_deploy.R&amp;quot;                                                        
## [70] &amp;quot;rstudioapi::navigateToFile(\&amp;quot;dev/03_deploy.R\&amp;quot;)&amp;quot;                                
## [71] &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where you will list the dependencies of your package (lines 18 to 30) as well as the
modules (lines 34 to 35). I have mostly used this file for the dependencies, as I already
had the modules from another app, so I didn’t bother listing them here. But if I would have started
from scratch, I would changed the line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;golem::add_module( name = \&amp;quot;name_of_module1\&amp;quot; ) # Name of the module&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to something like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;golem::add_module( name = \&amp;quot;import_data\&amp;quot; ) # Name of the module&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and executing it would have generated the needed files to start creating the module at the right
spot. Let’s go see how such a module looks like (I’m skipping the third script for now, as it is
only useful once you want to deploy).&lt;/p&gt;
&lt;p&gt;You can find the modules in the &lt;code&gt;R/&lt;/code&gt; folder. Let’s take a look at the module that allows the user
to load the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;cat&amp;quot;, args = &amp;quot;~/Documents/golemDemo/R/mod_load_data.R&amp;quot;, stdout = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;#&amp;#39; load_data UI Function&amp;quot;                                                                                                    
##   [2] &amp;quot;#&amp;#39;&amp;quot;                                                                                                                          
##   [3] &amp;quot;#&amp;#39; @description A shiny Module.&amp;quot;                                                                                             
##   [4] &amp;quot;#&amp;#39;&amp;quot;                                                                                                                          
##   [5] &amp;quot;#&amp;#39; @param id,input,output,session Internal parameters for {shiny}.&amp;quot;                                                          
##   [6] &amp;quot;#&amp;#39;&amp;quot;                                                                                                                          
##   [7] &amp;quot;#&amp;#39; @noRd &amp;quot;                                                                                                                   
##   [8] &amp;quot;#&amp;#39;&amp;quot;                                                                                                                          
##   [9] &amp;quot;#&amp;#39; @importFrom shiny NS tagList &amp;quot;                                                                                            
##  [10] &amp;quot;#&amp;#39; @importFrom data.table fread&amp;quot;                                                                                             
##  [11] &amp;quot;#&amp;#39; @importFrom DT renderDataTable dataTableOutput&amp;quot;                                                                           
##  [12] &amp;quot;#&amp;#39; @importFrom dplyr filter&amp;quot;                                                                                                 
##  [13] &amp;quot;#&amp;#39; @importFrom rlang quo `!!` as_name&amp;quot;                                                                                       
##  [14] &amp;quot;mod_load_data_ui &amp;lt;- function(id){&amp;quot;                                                                                           
##  [15] &amp;quot;  ns &amp;lt;- NS(id)&amp;quot;                                                                                                              
##  [16] &amp;quot;  tagList(&amp;quot;                                                                                                                  
##  [17] &amp;quot;    box(title = \&amp;quot;Select dataset\&amp;quot;,&amp;quot;                                                                                         
##  [18] &amp;quot;        radioButtons(ns(\&amp;quot;select_dataset\&amp;quot;),&amp;quot;                                                                                
##  [19] &amp;quot;                    label = \&amp;quot;Select dataset\&amp;quot;,&amp;quot;                                                                             
##  [20] &amp;quot;                    choices = c(\&amp;quot;Rescue points\&amp;quot;, \&amp;quot;Radars\&amp;quot;),&amp;quot;                                                             
##  [21] &amp;quot;                    selected = c(\&amp;quot;Rescue points\&amp;quot;)),&amp;quot;                                                                       
##  [22] &amp;quot;        conditionalPanel(&amp;quot;                                                                                                   
##  [23] &amp;quot;          condition = paste0(&amp;#39;input[\\&amp;#39;&amp;#39;, ns(&amp;#39;select_dataset&amp;#39;), \&amp;quot;\\&amp;#39;] == \\&amp;#39;Rescue points\\&amp;#39;\&amp;quot;),&amp;quot;                           
##  [24] &amp;quot;          selectInput(ns(\&amp;quot;selector_place\&amp;quot;), \&amp;quot;Place\&amp;quot;,&amp;quot;                                                                    
##  [25] &amp;quot;                      choices = c(\&amp;quot;test\&amp;quot;),&amp;quot;                                                                                
##  [26] &amp;quot;                      #choices = c(unique(output$dataset$place)),&amp;quot;                                                           
##  [27] &amp;quot;                      selected = c(\&amp;quot;Luxembourg, Ville (G)\&amp;quot;),&amp;quot;                                                              
##  [28] &amp;quot;                      multiple = TRUE)),&amp;quot;                                                                                    
##  [29] &amp;quot;        conditionalPanel(&amp;quot;                                                                                                   
##  [30] &amp;quot;          condition = paste0(&amp;#39;input[\\&amp;#39;&amp;#39;, ns(&amp;#39;select_dataset&amp;#39;), \&amp;quot;\\&amp;#39;] == \\&amp;#39;Radars\\&amp;#39;\&amp;quot;),&amp;quot;                                  
##  [31] &amp;quot;          selectInput(ns(\&amp;quot;selector_radar\&amp;quot;), \&amp;quot;Radar\&amp;quot;,&amp;quot;                                                                    
##  [32] &amp;quot;                      choices = c(\&amp;quot;test\&amp;quot;),&amp;quot;                                                                                
##  [33] &amp;quot;                      #choices = c(\&amp;quot;huhu\&amp;quot;),&amp;quot;                                                                               
##  [34] &amp;quot;                      selected = c(\&amp;quot;National road\&amp;quot;),&amp;quot;                                                                      
##  [35] &amp;quot;                      multiple = TRUE)),&amp;quot;                                                                                    
##  [36] &amp;quot;        width = NULL),&amp;quot;                                                                                                      
##  [37] &amp;quot;  )&amp;quot;                                                                                                                         
##  [38] &amp;quot;}&amp;quot;                                                                                                                           
##  [39] &amp;quot;&amp;quot;                                                                                                                            
##  [40] &amp;quot;#&amp;#39; load_data Server Function&amp;quot;                                                                                                
##  [41] &amp;quot;#&amp;#39;&amp;quot;                                                                                                                          
##  [42] &amp;quot;#&amp;#39; @noRd &amp;quot;                                                                                                                   
##  [43] &amp;quot;mod_load_data_server &amp;lt;- function(input, output, session){&amp;quot;                                                                   
##  [44] &amp;quot;  ns &amp;lt;- session$ns&amp;quot;                                                                                                          
##  [45] &amp;quot; &amp;quot;                                                                                                                           
##  [46] &amp;quot;  &amp;quot;                                                                                                                          
##  [47] &amp;quot;  read_dataset &amp;lt;- reactive({&amp;quot;                                                                                                
##  [48] &amp;quot;    if(input$select_dataset == \&amp;quot;Rescue points\&amp;quot;) {&amp;quot;                                                                         
##  [49] &amp;quot;&amp;quot;                                                                                                                            
##  [50] &amp;quot;      dataset &amp;lt;- fread(\&amp;quot;data-raw/rettungspunkte.csv\&amp;quot;)&amp;quot;                                                                     
##  [51] &amp;quot;      variable &amp;lt;- quo(place)&amp;quot;                                                                                                
##  [52] &amp;quot;      filter_values &amp;lt;- unique(dataset[, place])&amp;quot;                                                                             
##  [53] &amp;quot;    } else {&amp;quot;                                                                                                                
##  [54] &amp;quot;      dataset &amp;lt;- fread(\&amp;quot;data-raw/radars.csv\&amp;quot;)&amp;quot;                                                                             
##  [55] &amp;quot;      variable &amp;lt;- quo(type_road)&amp;quot;                                                                                            
##  [56] &amp;quot;      filter_values &amp;lt;- unique(dataset[, type_road])&amp;quot;                                                                         
##  [57] &amp;quot;    }&amp;quot;                                                                                                                       
##  [58] &amp;quot;    cat(\&amp;quot;reading data\\n\&amp;quot;)&amp;quot;                                                                                                
##  [59] &amp;quot;    list(dataset = dataset,&amp;quot;                                                                                                 
##  [60] &amp;quot;         variable = variable,&amp;quot;                                                                                               
##  [61] &amp;quot;         filter_values = filter_values)&amp;quot;                                                                                     
##  [62] &amp;quot;  })&amp;quot;                                                                                                                        
##  [63] &amp;quot;&amp;quot;                                                                                                                            
##  [64] &amp;quot;&amp;quot;                                                                                                                            
##  [65] &amp;quot;  observe({&amp;quot;                                                                                                                 
##  [66] &amp;quot;    updateSelectInput(session, \&amp;quot;selector_place\&amp;quot;, label = \&amp;quot;Select place:\&amp;quot;, choices = read_dataset()$filter_values,&amp;quot;       
##  [67] &amp;quot;                      selected = \&amp;quot;Luxembourg, Ville (G)\&amp;quot;)&amp;quot;                                                                 
##  [68] &amp;quot;  })&amp;quot;                                                                                                                        
##  [69] &amp;quot;&amp;quot;                                                                                                                            
##  [70] &amp;quot;  observe({&amp;quot;                                                                                                                 
##  [71] &amp;quot;    updateSelectInput(session, \&amp;quot;selector_radar\&amp;quot;, label = \&amp;quot;Select type of road:\&amp;quot;, choices = read_dataset()$filter_values,&amp;quot;
##  [72] &amp;quot;                      selected = \&amp;quot;National road\&amp;quot;)&amp;quot;                                                                         
##  [73] &amp;quot;  })&amp;quot;                                                                                                                        
##  [74] &amp;quot;&amp;quot;                                                                                                                            
##  [75] &amp;quot;  result &amp;lt;- reactive({&amp;quot;                                                                                                      
##  [76] &amp;quot;    return_dataset &amp;lt;- read_dataset()$dataset&amp;quot;                                                                                
##  [77] &amp;quot;&amp;quot;                                                                                                                            
##  [78] &amp;quot;    if(\&amp;quot;place\&amp;quot; %in% colnames(return_dataset)){&amp;quot;                                                                            
##  [79] &amp;quot;      return_dataset &amp;lt;- return_dataset %&amp;gt;%&amp;quot;                                                                                  
##  [80] &amp;quot;        filter((!!read_dataset()$variable) %in% input$selector_place)&amp;quot;                                                       
##  [81] &amp;quot;&amp;quot;                                                                                                                            
##  [82] &amp;quot;      result &amp;lt;- list(&amp;quot;                                                                                                       
##  [83] &amp;quot;        return_dataset = return_dataset,&amp;quot;                                                                                    
##  [84] &amp;quot;        variable = quo(place)&amp;quot;                                                                                               
##  [85] &amp;quot;      )&amp;quot;                                                                                                                     
##  [86] &amp;quot;    } else {&amp;quot;                                                                                                                
##  [87] &amp;quot;      return_dataset &amp;lt;- return_dataset %&amp;gt;%&amp;quot;                                                                                  
##  [88] &amp;quot;        filter((!!read_dataset()$variable) %in% input$selector_radar)&amp;quot;                                                       
##  [89] &amp;quot;&amp;quot;                                                                                                                            
##  [90] &amp;quot;      result &amp;lt;- list(&amp;quot;                                                                                                       
##  [91] &amp;quot;        return_dataset = return_dataset,&amp;quot;                                                                                    
##  [92] &amp;quot;        variable = quo(type_road)&amp;quot;                                                                                           
##  [93] &amp;quot;      )&amp;quot;                                                                                                                     
##  [94] &amp;quot;    }&amp;quot;                                                                                                                       
##  [95] &amp;quot;  })&amp;quot;                                                                                                                        
##  [96] &amp;quot;&amp;quot;                                                                                                                            
##  [97] &amp;quot;  result&amp;quot;                                                                                                                    
##  [98] &amp;quot;}&amp;quot;                                                                                                                           
##  [99] &amp;quot;    &amp;quot;                                                                                                                        
## [100] &amp;quot;## To be copied in the UI&amp;quot;                                                                                                   
## [101] &amp;quot;# mod_load_data_ui(\&amp;quot;load_data_ui_1\&amp;quot;)&amp;quot;                                                                                      
## [102] &amp;quot;    &amp;quot;                                                                                                                        
## [103] &amp;quot;## To be copied in the server&amp;quot;                                                                                               
## [104] &amp;quot;# callModule(mod_load_data_server, \&amp;quot;load_data_ui_1\&amp;quot;)&amp;quot;                                                                      
## [105] &amp;quot; &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This scripts looks like a mini Shiny app; there’s a UI defined at the top of the script, and
then a server defined at the bottom (I’m not describing what the module does here, I’ll do that
in the video). What’s important here, is that this is a module and as such it can be reused in any
app, by simply copying the right lines of code at the right spot. See lines 100 to 104 for this,
which tells you exactly where to copy the lines to use this module. All the modules will
look the same, and have this little explanation at the bottom to tell you where you need to copy
the lines to use the modules. While building each module, you can use &lt;code&gt;{shinipsum}&lt;/code&gt; instead of
having to bother about the server logic, just to get things going, as explained above.&lt;/p&gt;
&lt;p&gt;Now, finally, let’s take a look at the actual UI of the app:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;cat&amp;quot;, args = &amp;quot;~/Documents/golemDemo/R/app_ui.R&amp;quot;, stdout = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;#&amp;#39; The application User-Interface&amp;quot;                                                          
##  [2] &amp;quot;#&amp;#39; &amp;quot;                                                                                        
##  [3] &amp;quot;#&amp;#39; @param request Internal parameter for `{shiny}`. &amp;quot;                                       
##  [4] &amp;quot;#&amp;#39;     DO NOT REMOVE.&amp;quot;                                                                      
##  [5] &amp;quot;#&amp;#39; @import shiny&amp;quot;                                                                           
##  [6] &amp;quot;#&amp;#39; @import shinydashboard&amp;quot;                                                                  
##  [7] &amp;quot;#&amp;#39; @noRd&amp;quot;                                                                                   
##  [8] &amp;quot;app_ui &amp;lt;- function(request) {&amp;quot;                                                              
##  [9] &amp;quot;  tagList(&amp;quot;                                                                                 
## [10] &amp;quot;                                        # Leave this function for adding external resources&amp;quot;
## [11] &amp;quot;    golem_add_external_resources(),&amp;quot;                                                        
## [12] &amp;quot;                                        # List the first level UI elements here&amp;quot;            
## [13] &amp;quot;    dashboardPage(&amp;quot;                                                                         
## [14] &amp;quot;      dashboardHeader(title = \&amp;quot;Prototype: dashboard ecoles\&amp;quot;),&amp;quot;                            
## [15] &amp;quot;      dashboardSidebar(&amp;quot;                                                                    
## [16] &amp;quot;        sidebarMenu(&amp;quot;                                                                       
## [17] &amp;quot;          menuItem(\&amp;quot;Carte\&amp;quot;, tabName = \&amp;quot;Carte\&amp;quot;, icon = icon(\&amp;quot;map\&amp;quot;)),&amp;quot;                  
## [18] &amp;quot;          menuItem(\&amp;quot;Tab 2\&amp;quot;, tabName = \&amp;quot;tab_2\&amp;quot;, icon = icon(\&amp;quot;chart-line\&amp;quot;))&amp;quot;            
## [19] &amp;quot;        )&amp;quot;                                                                                  
## [20] &amp;quot;      ),&amp;quot;                                                                                   
## [21] &amp;quot;      dashboardBody(&amp;quot;                                                                       
## [22] &amp;quot;        tabItems(&amp;quot;                                                                          
## [23] &amp;quot;          tabItem(tabName = \&amp;quot;Carte\&amp;quot;,&amp;quot;                                                     
## [24] &amp;quot;                  fluidRow(&amp;quot;                                                                
## [25] &amp;quot;                    column(&amp;quot;                                                                
## [26] &amp;quot;                      width = 4,&amp;quot;                                                           
## [27] &amp;quot;                      mod_load_data_ui(\&amp;quot;load_data_ui_1\&amp;quot;),&amp;quot;                                
## [28] &amp;quot;                      mod_table_data_ui(\&amp;quot;table_data_ui_1\&amp;quot;)&amp;quot;                               
## [29] &amp;quot;                    ),&amp;quot;                                                                     
## [30] &amp;quot;                    column(&amp;quot;                                                                
## [31] &amp;quot;                      width = 6, offset = 2,&amp;quot;                                               
## [32] &amp;quot;                      mod_map_data_ui(\&amp;quot;map_data_ui_1\&amp;quot;)&amp;quot;                                   
## [33] &amp;quot;                    )&amp;quot;                                                                      
## [34] &amp;quot;                  ))&amp;quot;                                                                       
## [35] &amp;quot;        )&amp;quot;                                                                                  
## [36] &amp;quot;      )&amp;quot;                                                                                    
## [37] &amp;quot;    )&amp;quot;                                                                                      
## [38] &amp;quot;  )&amp;quot;                                                                                        
## [39] &amp;quot;}&amp;quot;                                                                                          
## [40] &amp;quot;&amp;quot;                                                                                           
## [41] &amp;quot;#&amp;#39; Add external Resources to the Application&amp;quot;                                               
## [42] &amp;quot;#&amp;#39; &amp;quot;                                                                                        
## [43] &amp;quot;#&amp;#39; This function is internally used to add external &amp;quot;                                       
## [44] &amp;quot;#&amp;#39; resources inside the Shiny application. &amp;quot;                                                
## [45] &amp;quot;#&amp;#39; &amp;quot;                                                                                        
## [46] &amp;quot;#&amp;#39; @import shiny&amp;quot;                                                                           
## [47] &amp;quot;#&amp;#39; @importFrom golem add_resource_path activate_js favicon bundle_resources&amp;quot;                
## [48] &amp;quot;#&amp;#39; @noRd&amp;quot;                                                                                   
## [49] &amp;quot;golem_add_external_resources &amp;lt;- function(){&amp;quot;                                                
## [50] &amp;quot;  &amp;quot;                                                                                         
## [51] &amp;quot;  add_resource_path(&amp;quot;                                                                       
## [52] &amp;quot;    &amp;#39;www&amp;#39;, app_sys(&amp;#39;app/www&amp;#39;)&amp;quot;                                                              
## [53] &amp;quot;  )&amp;quot;                                                                                        
## [54] &amp;quot; &amp;quot;                                                                                          
## [55] &amp;quot;  tags$head(&amp;quot;                                                                               
## [56] &amp;quot;    favicon(),&amp;quot;                                                                             
## [57] &amp;quot;    bundle_resources(&amp;quot;                                                                      
## [58] &amp;quot;      path = app_sys(&amp;#39;app/www&amp;#39;),&amp;quot;                                                           
## [59] &amp;quot;      app_title = &amp;#39;golemDemo&amp;#39;&amp;quot;                                                              
## [60] &amp;quot;    )&amp;quot;                                                                                      
## [61] &amp;quot;    # Add here other external resources&amp;quot;                                                    
## [62] &amp;quot;    # for example, you can add shinyalert::useShinyalert() &amp;quot;                                
## [63] &amp;quot;  )&amp;quot;                                                                                        
## [64] &amp;quot;}&amp;quot;                                                                                          
## [65] &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;this is the “global” UI of the app. This looks like any other Shiny UI, but instead of having
many many lines of code, there’s basically only calls to the UIs of each modules (see lines 27 and 28).
And that’s it! It keeps your code quite small and much easier to reason about. You’ll find
something even simpler for the server:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;cat&amp;quot;, args = &amp;quot;~/Documents/golemDemo/R/app_server.R&amp;quot;, stdout = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;#&amp;#39; The application server-side&amp;quot;                                              
##  [2] &amp;quot;#&amp;#39; &amp;quot;                                                                         
##  [3] &amp;quot;#&amp;#39; @param input,output,session Internal parameters for {shiny}. &amp;quot;            
##  [4] &amp;quot;#&amp;#39;     DO NOT REMOVE.&amp;quot;                                                       
##  [5] &amp;quot;#&amp;#39; @import shiny&amp;quot;                                                            
##  [6] &amp;quot;#&amp;#39; @noRd&amp;quot;                                                                    
##  [7] &amp;quot;app_server &amp;lt;- function( input, output, session ) {&amp;quot;                          
##  [8] &amp;quot;  # List the first level callModules here&amp;quot;                                   
##  [9] &amp;quot;&amp;quot;                                                                            
## [10] &amp;quot;  result &amp;lt;- callModule(mod_load_data_server, \&amp;quot;load_data_ui_1\&amp;quot;)&amp;quot;            
## [11] &amp;quot;&amp;quot;                                                                            
## [12] &amp;quot;  callModule(mod_table_data_server, \&amp;quot;table_data_ui_1\&amp;quot;, result)&amp;quot;            
## [13] &amp;quot;  &amp;quot;                                                                          
## [14] &amp;quot;&amp;quot;                                                                            
## [15] &amp;quot;  selected_lines &amp;lt;- reactive({&amp;quot;                                              
## [16] &amp;quot;    if(is.null(input$`table_data_ui_1-dataset_rows_selected`)){&amp;quot;             
## [17] &amp;quot;      return(TRUE)&amp;quot;                                                          
## [18] &amp;quot;    } else {&amp;quot;                                                                
## [19] &amp;quot;      as.numeric(input$`table_data_ui_1-dataset_rows_selected`)&amp;quot;             
## [20] &amp;quot;    }&amp;quot;                                                                       
## [21] &amp;quot;  })&amp;quot;                                                                        
## [22] &amp;quot;&amp;quot;                                                                            
## [23] &amp;quot;  callModule(mod_map_data_server, \&amp;quot;map_data_ui_1\&amp;quot;, result, selected_lines)&amp;quot;
## [24] &amp;quot;&amp;quot;                                                                            
## [25] &amp;quot;}&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Line 10 calls the server side of the “load data” module, and saves the result (a data frame) into
a variable called &lt;code&gt;result&lt;/code&gt;. This result is then passed as an argument to the server side of
table data module, which simply shows a table of the data. From lines 15 to 21, I define a
variable called &lt;code&gt;selected-lines&lt;/code&gt; in which the lines that the user selects in the data table are
saved. This gave me some headaches, because I needed to find the right syntax. I was able to find
it thanks to a Stackoverflow post that I have now lost since then… but the idea is that
the indices of the selected rows are saved into a variable called &lt;code&gt;dataset_rows_selected&lt;/code&gt; and this variable name
must be appended to the name of the UI of the table where the table is. If no row is selected, then
this object should be &lt;code&gt;TRUE&lt;/code&gt;; why? Because if you filter a data frame with a condition that simply
evaluates always to &lt;code&gt;TRUE&lt;/code&gt;, you get all the rows back, and thus, all of the data frame. If you start
selecting rows, say, rows number 2, 8 and 12, then &lt;code&gt;dataset_rows_selected&lt;/code&gt; will be equal to &lt;code&gt;c(2, 8, 12)&lt;/code&gt;, and the filter will return these rows.&lt;/p&gt;
&lt;p&gt;Finally, I call the module that returns a map of Luxembourg, and pass both the data frame, saved in
the &lt;code&gt;result&lt;/code&gt; variable, and the &lt;code&gt;selected_lines&lt;/code&gt; objects as arguments. And that’s how you make modules
communicate and share data with each other, just like you would chain functions together.
I won’t go through each module, but there’s several other interesting tricks that I’ll discuss
during the video; for instance, I’m quite happy with the module that loads the data; the user can
choose between two different dataset, and the select input will update with the right columns. This
also wasn’t so easy to do, but it’ll be easier to explain during a video, so stay tuned!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Raspberry Pi 4B as a shiny server</title>
      <link>/blog/2020-09-20-shiny_raspberry/</link>
      <pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-09-20-shiny_raspberry/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2mSEEHblJqw&#34;&gt;
&lt;img src=&#34;/img/virgin_chad.png&#34; title = &#34;Not everyone can be a chad shiny dev&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This blog post will not have any code, but will document how I went from hosting apps
on &lt;a href=&#34;https://www.shinyapps.io&#34;&gt;shinyapps.io&lt;/a&gt; to hosting shiny apps on my own server, which is a Raspberry Pi 4B with 8 gigs of ram.
First of all, why hosting apps on a Raspberry Pi? And why not continue on &lt;a href=&#34;https://www.shinyapps.io&#34;&gt;shinyapps.io&lt;/a&gt;?
Or why not get one of hose nifty droplets on DigitalOcean? Well for two reasons; one is that I wanted
to have full control of the server, and learn some basic web dev/web engineering skills that I lacked. These services
simplify the process of deploying and hosting a lot, which of course is a good thing if your only
goal is to deploy apps. But I wanted to learn how to do it myself from scratch for some time.
True, with a DigitalOcean droplet, I could have learned quite a lot about the whole process as well,
but there’s a second problem; the minimum amount of processing power that the droplet needed to run
shiny came at 10€ a month. Not a fortune, but already quite expensive for me, since I just wanted
to learn some stuff on my free time. Which is why I got a Raspberry Pi 4B with 8 gigs of ram. It’s less
than 100€, and now that I have it, I can do whatever I want whenever I want to. If I don’t touch it
for several months, no harm done. And if I get tired of it, I’ll make a retro console out of it and
play some old schools games. It’s a win-win situation if you ask me.&lt;/p&gt;
&lt;p&gt;So first, you should get a Raspberry Pi. Those are quite easy to find online, and there’s many
tutorials available on how to install Ubuntu (or any other Linux distro) on it, so I won’t bother
with that. I also won’t explain to you how to ssh into your Raspberry Pi, again, there’s many tutorials
online. More importantly, is how to get Shiny on it? There’s two solutions; you either install
it from source, or you use Docker. I chose to use Docker, but maybe not in the way you’d expect;
there’s a lot of talk online about dockerizing apps, complete with all their dependencies and
environment. The advantage is that you’re guaranteed that deployment with be very smooth. But the
big disadvantage is that these dockerized apps are huge, around 1GB, or sometimes more. It is true that disk space is
quite cheap nowadays, but still… so I prefer to run a Shiny server from Docker, and then run the
apps out of this server. My apps are thus very small, and it’s only the Shiny server that is huge.
I found a Github repository from user &lt;code&gt;havlev&lt;/code&gt; that explains how to do it &lt;a href=&#34;https://github.com/hvalev/rpi-shiny-server-docker&#34;&gt;here&lt;/a&gt;.
I have followed this guide, and created my own docker container, which is based on &lt;code&gt;havlev&lt;/code&gt;’s
one. I added some dependencies (to the base Debian distro included, as well as some more R packages).&lt;/p&gt;
&lt;p&gt;If you’re in a hurry, and want to use my Docker image, you can simply type the following on your
Raspberry pi:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir shiny-server
cd shiny-server
mkdir apps
mkdir conf
mkdir logs
docker run -d -p 3838:3838 -v shiny-apps:/srv/shiny-server/ -v shiny-logs:/var/log/ -v shiny-conf:/etc/shiny-server/ --name rpi-shiny-server brodriguesco/shiny_1_5:firstcommit&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first 5 commands will create some folders that we’ll need later on, while the last one will pull
my Docker container, which is based on &lt;code&gt;havlev&lt;/code&gt;’s one, launch the server and it’ll start listening to
port 3838.&lt;/p&gt;
&lt;p&gt;I made an app (another blog post, focusing on this app, will follow soon), hosted on my Raspberry Pi
that you can find &lt;a href=&#34;http://shinybrodriguesco.duckdns.org:3838/golemDemo/&#34;&gt;here&lt;/a&gt;. I’ll also give you
some pointers on how you can achieve that.&lt;/p&gt;
&lt;p&gt;But let’s start from the beginning.&lt;/p&gt;
&lt;div id=&#34;adding-dependencies-to-a-docker-container&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding dependencies to a Docker container&lt;/h2&gt;
&lt;p&gt;So let’s suppose that you’re me a few weeks ago, and that you find and follow &lt;code&gt;havlev&lt;/code&gt;’s guide &lt;a href=&#34;https://github.com/hvalev/rpi-shiny-server-docker&#34;&gt;here&lt;/a&gt;.
Getting the docker running is quite easy, you just need to set up Docker, and then find the line in the
tutorial that starts with &lt;code&gt;docker run&lt;/code&gt;…. You’ll get Shiny running with its hello world app. Now,
how can you add more packages, either to the base Debian image, or R packages? For this part, I
followed &lt;a href=&#34;https://ropenscilabs.github.io/r-docker-tutorial/03-install-packages.html&#34;&gt;this guide&lt;/a&gt;.
The idea is to “log in” to the console of the base Debian distro that is running from the container.
First, find the ID of the container by typing the following command in the terminal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker ps&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ubuntu@ubuntu:~$ docker ps
CONTAINER ID        IMAGE                                COMMAND                  CREATED              STATUS              PORTS                    NAMES
69420blazeit        brodriguesco/shiny_1_5:firstcommit   &amp;quot;/etc/shiny-server/i…&amp;quot;   About a minute ago   Up About a minute   0.0.0.0:3838-&amp;gt;3838/tcp   rpi-shiny-server&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;now with the ID in hand, you can start any command line program from your Docker container, for instance
bash:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker exec -it 69420blazeit bash&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll be “logged in” as root:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@69420blazeit:/# &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and from there, you can install Debian packages. The following two packages are necessary to install
many R packages from source, so I recommend you install them:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@69420blazeit:/# apt-get install libssl-dev libxml2-dev&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once these Debian packages are installed, you can start R by simply typing &lt;code&gt;R&lt;/code&gt; in the same console,
and install whatever packages your Shiny apps will need. In my case, I installed &lt;code&gt;{golem}&lt;/code&gt; and several
others, but this will be the subject of another blog post. We’re almost done with that; we now need
to save the changes because if you restart the container, you’ll lose all these changes. To save these
changes, let’s run the following command, but in a new terminal on your Raspberry Pi (on the
“local” Ubuntu, not the Debian running in the container):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ubuntu@ubuntu:~$ docker commit -m &amp;quot;added some dependencies&amp;quot; 69420blazeit shiny_with_deps&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now you could run this container with the command from above, by replacing the adequate parts:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d -p 3838:3838 -v shiny-apps:/srv/shiny-server/ -v shiny-logs:/var/log/ -v shiny-conf:/etc/shiny-server/ --name rpi-shiny-server shiny_with_depsshiny_with_deps&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;using-your-shiny-server&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using your Shiny server&lt;/h2&gt;
&lt;p&gt;Ok so now that the server is running, you can you deploy apps on it? Remember the folders that we
created at the beginning of the blog post (or that you created if you followed &lt;code&gt;havlev&lt;/code&gt;’s guide)?
This is where you’ll drop your apps, the usual way. You create a folder there, and simply put the
&lt;code&gt;ui.R&lt;/code&gt; and &lt;code&gt;server.R&lt;/code&gt; files in here, and that it. These folders can be found in your &lt;code&gt;$HOME&lt;/code&gt; directory,
and they are accessible to your docker container as well. Once you dropped one or two apps, you’ll be able to access them on a link
similar as this one:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://192.168.178.55:3838/hello/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where &lt;code&gt;192.168.178.55&lt;/code&gt; is the local IP address of the Raspberry Pi, &lt;code&gt;3838&lt;/code&gt; is the port the server
is listening to, and &lt;code&gt;/hello/&lt;/code&gt; is the name of the subfolder contained in the &lt;code&gt;~/shiny-server/apps&lt;/code&gt;
folder that you created before. What is left doing is making your Raspberry Pi a proper server that
can be accessed from the internet. For this, you’ll need to ask your ISP for a dynamic IP address.
Generally, you’ll have to pay some money for it; in my case, I’m paying 2€ a month. This address
can then be used to access your Raspberry Pi from the internet. The problem, is that being dynamic,
the address changes every time you restart your server. To solve this issue, you can use a free
dynamic DNS. I use &lt;a href=&#34;https://www.duckdns.org/&#34;&gt;duckdns&lt;/a&gt;. This will allow you to have domain that you
can share with the world. What’s nice is that if you follow their &lt;a href=&#34;https://www.duckdns.org/install.jsp&#34;&gt;guide&lt;/a&gt;
the redirection to the dynamic IP address will happen seamlessly every time it changes, so no need
to think about it and do it manually.&lt;/p&gt;
&lt;p&gt;Finally, you’ll also have to open up port &lt;code&gt;3838&lt;/code&gt; on your router. The procedure changes from router
to router, but you should be able to find the instructions for your router quite easily. If not, you
should also be able to get help from your ISP.&lt;/p&gt;
&lt;p&gt;The end result is that you’ll have your own Shiny server running off a Raspberry Pi, and accessible
over the internet! You’ll be able to deploy as many apps as you want, but of course, don’t forget
that you’re running all this on a Raspberry Pi. While these machines have become quite powerful over
the years, they won’t be powerful enough if you’re running some heavy duty apps with hundreds of
concurrent users.&lt;/p&gt;
&lt;p&gt;In my next blog post, I’ll walk you through the development of a Shiny app using the &lt;code&gt;{golem}&lt;/code&gt; package,
which you can find &lt;a href=&#34;http://shinybrodriguesco.duckdns.org:3838/golemDemo/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Gotta go fast with &#34;{tidytable}&#34;</title>
      <link>/blog/2020-09-05-tidytable/</link>
      <pubDate>Sat, 05 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-09-05-tidytable/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SXrbYw_AqQA&#34;&gt;
&lt;img src=&#34;/img/kaamelott.jpg&#34; title = &#34;If there&#39;s one good reason to learn French, it&#39;s Kaamelott&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I’m back in business! After almost 5 months of hiatus, during which I was very busy with my new job, and
new house, I’m in a position where I can write again. To celebrate my comeback, I’ll introduce to
you the &lt;code&gt;{tidytable}&lt;/code&gt; package, which I learned about this week on Twitter.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{tidytable}&lt;/code&gt; is a package that allows you to manipulate &lt;code&gt;data.table&lt;/code&gt; objects with the speed of
&lt;code&gt;{data.table}&lt;/code&gt; and the convenience of the &lt;code&gt;{tidyverse}&lt;/code&gt; syntax. My first reaction when I heard about
&lt;code&gt;{tidytable}&lt;/code&gt; was &lt;em&gt;how is that different from &lt;code&gt;{dtplyr}&lt;/code&gt;&lt;/em&gt;? Well, &lt;code&gt;{dtplyr}&lt;/code&gt; focuses on providing
a &lt;code&gt;{data.table}&lt;/code&gt; backend for &lt;code&gt;{dplyr}&lt;/code&gt;, while &lt;code&gt;{tidytable}&lt;/code&gt; also allows you to use other &lt;code&gt;{tidyverse}&lt;/code&gt;
verbs on &lt;code&gt;data.table&lt;/code&gt; objects, for instance some &lt;code&gt;{tidyr}&lt;/code&gt; and &lt;code&gt;{purrr}&lt;/code&gt; verbs.&lt;/p&gt;
&lt;p&gt;Another very interesting feature of &lt;code&gt;{tidytable}&lt;/code&gt; is that it supports &lt;code&gt;{rlang}&lt;/code&gt;, which means that
you can program with &lt;code&gt;{tidytable}&lt;/code&gt;, which, as far as I know, is not possible with &lt;code&gt;{dtplyr}&lt;/code&gt;
(but fact-check me on that please).&lt;/p&gt;
&lt;p&gt;So to summarise, the speed of &lt;code&gt;{data.table}&lt;/code&gt; and the syntax of the &lt;code&gt;{tidyverse}&lt;/code&gt;, plus verbs for
&lt;code&gt;{tidyr}&lt;/code&gt; and &lt;code&gt;{purrr}&lt;/code&gt;? Sign me up!&lt;/p&gt;
&lt;p&gt;To illustrate, I have downloaded a data set and wrote a function in both a &lt;code&gt;{tidyverse}&lt;/code&gt; version
and a &lt;code&gt;{tidytable}&lt;/code&gt; version. Even though it is true that &lt;code&gt;{tidytable}&lt;/code&gt;’s syntax is very much, almost
the same as the regular &lt;code&gt;{tidyverse}&lt;/code&gt; syntax, there are some minor differences. But more on that
later. First, let’s get the data, which you can find &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction&#34;&gt;here&lt;/a&gt;.
Then, let’s load the needed packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(data.table)
library(tidytable)
library(readr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and let’s take a look at the data a little bit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;energy &amp;lt;- read.csv(&amp;quot;~/Downloads/energydata_complete.csv&amp;quot;)

head(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  date Appliances lights    T1     RH_1   T2     RH_2    T3
## 1 2016-01-11 17:00:00         60     30 19.89 47.59667 19.2 44.79000 19.79
## 2 2016-01-11 17:10:00         60     30 19.89 46.69333 19.2 44.72250 19.79
## 3 2016-01-11 17:20:00         50     30 19.89 46.30000 19.2 44.62667 19.79
## 4 2016-01-11 17:30:00         50     40 19.89 46.06667 19.2 44.59000 19.79
## 5 2016-01-11 17:40:00         60     40 19.89 46.33333 19.2 44.53000 19.79
## 6 2016-01-11 17:50:00         50     40 19.89 46.02667 19.2 44.50000 19.79
##       RH_3       T4     RH_4       T5  RH_5       T6     RH_6       T7     RH_7
## 1 44.73000 19.00000 45.56667 17.16667 55.20 7.026667 84.25667 17.20000 41.62667
## 2 44.79000 19.00000 45.99250 17.16667 55.20 6.833333 84.06333 17.20000 41.56000
## 3 44.93333 18.92667 45.89000 17.16667 55.09 6.560000 83.15667 17.20000 41.43333
## 4 45.00000 18.89000 45.72333 17.16667 55.09 6.433333 83.42333 17.13333 41.29000
## 5 45.00000 18.89000 45.53000 17.20000 55.09 6.366667 84.89333 17.20000 41.23000
## 6 44.93333 18.89000 45.73000 17.13333 55.03 6.300000 85.76667 17.13333 41.26000
##     T8     RH_8       T9  RH_9    T_out Press_mm_hg RH_out Windspeed Visibility
## 1 18.2 48.90000 17.03333 45.53 6.600000       733.5     92  7.000000   63.00000
## 2 18.2 48.86333 17.06667 45.56 6.483333       733.6     92  6.666667   59.16667
## 3 18.2 48.73000 17.00000 45.50 6.366667       733.7     92  6.333333   55.33333
## 4 18.1 48.59000 17.00000 45.40 6.250000       733.8     92  6.000000   51.50000
## 5 18.1 48.59000 17.00000 45.40 6.133333       733.9     92  5.666667   47.66667
## 6 18.1 48.59000 17.00000 45.29 6.016667       734.0     92  5.333333   43.83333
##   Tdewpoint      rv1      rv2
## 1       5.3 13.27543 13.27543
## 2       5.2 18.60619 18.60619
## 3       5.1 28.64267 28.64267
## 4       5.0 45.41039 45.41039
## 5       4.9 10.08410 10.08410
## 6       4.8 44.91948 44.91948&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this data is wide, and not long. Variables, or features, &lt;code&gt;T1&lt;/code&gt; to &lt;code&gt;T9&lt;/code&gt; provide the
temperature of 9 rooms, and &lt;code&gt;RH_1&lt;/code&gt; to &lt;code&gt;RH_9&lt;/code&gt; provide the humidity of the same 9 rooms.&lt;/p&gt;
&lt;p&gt;What if I’d like to make a plot of each room’s temperature throughout the year? In this format,
it is not possible. So let’s reshape this a little bit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;flat_energy &amp;lt;- energy %&amp;gt;% 
  pivot_longer(cols = matches(&amp;quot;T\\d{1}&amp;quot;), names_to = &amp;quot;temperature&amp;quot;, values_to = &amp;quot;temp_value&amp;quot;) %&amp;gt;% 
  pivot_longer(cols = matches(&amp;quot;RH_\\d{1}&amp;quot;), names_to = &amp;quot;humidity&amp;quot;, values_to = &amp;quot;hum_value&amp;quot;) %&amp;gt;%
  mutate(temperature = case_when(temperature == &amp;quot;T1&amp;quot; ~ &amp;quot;kitchen&amp;quot;,
                                 temperature == &amp;quot;T2&amp;quot; ~ &amp;quot;living&amp;quot;,
                                 temperature == &amp;quot;T3&amp;quot; ~ &amp;quot;laundry&amp;quot;,
                                 temperature == &amp;quot;T4&amp;quot; ~ &amp;quot;office&amp;quot;,
                                 temperature == &amp;quot;T5&amp;quot; ~ &amp;quot;bathroom&amp;quot;,
                                 temperature == &amp;quot;T6&amp;quot; ~ &amp;quot;north&amp;quot;,
                                 temperature == &amp;quot;T7&amp;quot; ~ &amp;quot;ironing&amp;quot;,
                                 temperature == &amp;quot;T8&amp;quot; ~ &amp;quot;teenager&amp;quot;,
                                 temperature == &amp;quot;T9&amp;quot; ~ &amp;quot;parents&amp;quot;)) %&amp;gt;%  
  mutate(humidity = case_when(humidity == &amp;quot;RH_1&amp;quot; ~ &amp;quot;kitchen&amp;quot;,
                                 humidity == &amp;quot;RH_2&amp;quot; ~ &amp;quot;living&amp;quot;,
                                 humidity == &amp;quot;RH_3&amp;quot; ~ &amp;quot;laundry&amp;quot;,
                                 humidity == &amp;quot;RH_4&amp;quot; ~ &amp;quot;office&amp;quot;,
                                 humidity == &amp;quot;RH_5&amp;quot; ~ &amp;quot;bathroom&amp;quot;,
                                 humidity == &amp;quot;RH_6&amp;quot; ~ &amp;quot;north&amp;quot;,
                                 humidity == &amp;quot;RH_7&amp;quot; ~ &amp;quot;ironing&amp;quot;,
                                 humidity == &amp;quot;RH_8&amp;quot; ~ &amp;quot;teenager&amp;quot;,
                              humidity == &amp;quot;RH_9&amp;quot; ~ &amp;quot;parents&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As explained above, there are two variables that need this treatment; the temperature, and the humidity levels. In order
to plot the average monthly temperature in each room, I need to use &lt;code&gt;tidyr::pivot_longer()&lt;/code&gt; (a
little side note, I could have used &lt;code&gt;names_to = &#34;room&#34;&lt;/code&gt;, instead of &lt;code&gt;&#34;temperature&#34;&lt;/code&gt; and &lt;code&gt;&#34;humidity&#34;&lt;/code&gt;,
but there’s a reason for that. More on it below).&lt;/p&gt;
&lt;p&gt;Now let’s plot it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;flat_energy %&amp;gt;%
  mutate(month = month(date)) %&amp;gt;%  
  group_by(month, temperature) %&amp;gt;%
  summarise(avg_temp = mean(temp_value)) %&amp;gt;%  
  ggplot() +
  geom_line(aes(y = avg_temp, x = month, col = temperature)) +
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;month&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-09-05-tidytable_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
Ok great. But what if I had such a dataset per house for a whole city? How many datasets would that
be? And how long would these operations take?
The first step I would take if I were in this situation, would be to write a function. I would make
it general enough to work with temperature or humidity. Below is this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prepare_data &amp;lt;- function(energy, variable){

  variable &amp;lt;- enquo(variable)

  variable_label &amp;lt;- as_label(variable)

  regex_selector &amp;lt;- ifelse(variable_label == &amp;quot;temperature&amp;quot;,
                           &amp;quot;T\\d{1}&amp;quot;,
                           &amp;quot;RH_\\d{1}&amp;quot;)
energy %&amp;gt;%
  pivot_longer(cols = matches(regex_selector),
               names_to = variable_label,
               values_to = paste0(variable_label, &amp;quot;_value&amp;quot;)) %&amp;gt;%
    mutate(!!(variable) := case_when(grepl(&amp;quot;1$&amp;quot;, !!(variable)) ~ &amp;quot;kitchen&amp;quot;,
                                    grepl(&amp;quot;2$&amp;quot;, !!(variable)) ~ &amp;quot;living&amp;quot;,
                                    grepl(&amp;quot;3$&amp;quot;, !!(variable)) ~ &amp;quot;laundry&amp;quot;,
                                    grepl(&amp;quot;4$&amp;quot;, !!(variable)) ~ &amp;quot;office&amp;quot;,
                                    grepl(&amp;quot;5$&amp;quot;, !!(variable)) ~ &amp;quot;bathroom&amp;quot;,
                                    grepl(&amp;quot;6$&amp;quot;, !!(variable)) ~ &amp;quot;outside&amp;quot;,
                                    grepl(&amp;quot;7$&amp;quot;, !!(variable)) ~ &amp;quot;ironing&amp;quot;,
                                    grepl(&amp;quot;8$&amp;quot;, !!(variable)) ~ &amp;quot;teenager&amp;quot;,
                                    grepl(&amp;quot;9$&amp;quot;, !!(variable)) ~ &amp;quot;parents&amp;quot;)) %&amp;gt;%
  mutate(month = month(date)) %&amp;gt;%  
  group_by(month, !!(variable)) %&amp;gt;%
  summarise(across(.cols = ends_with(&amp;quot;_value&amp;quot;),
                   .fns = mean),
            .groups = &amp;quot;drop&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function does exactly the same thing as above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prepare_data(energy, temperature) %&amp;gt;%
  ggplot() +
  geom_line(aes(y = temperature_value, x = month, col = temperature)) +
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-09-05-tidytable_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, I have the exact same plot. What’s nice with this function, is that it uses many
verbs from the &lt;code&gt;{tidyverse}&lt;/code&gt; as well as the tidy eval framework for non-standard evaluation (
which is why I did not use &lt;code&gt;names_to = &#34;room&#34;&lt;/code&gt;, I wanted to use the variable label defined with
&lt;code&gt;as_label()&lt;/code&gt; and see if it works with &lt;code&gt;{tidytable}&lt;/code&gt; as well).
Ok, so now let’s imagine that I’m happy with this function, but I’d like it to run faster, and because
I’m lazy, the less I have to modify it, the happier I am. This is where &lt;code&gt;{tidytable}&lt;/code&gt; looks very
promising. Let’s rewrite the function to make it work with &lt;code&gt;{tidytable}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prepare_data_dt &amp;lt;- function(energy, variable){

  variable &amp;lt;- enquo(variable)

  variable_label &amp;lt;- as_label(variable)

  regex_selector &amp;lt;- ifelse(variable_label == &amp;quot;temperature&amp;quot;,
                           &amp;quot;T\\d{1}&amp;quot;,
                           &amp;quot;RH_\\d{1}&amp;quot;)
energy %&amp;gt;%
  pivot_longer.(cols = matches(regex_selector),
               names_to = variable_label,
               values_to = paste0(variable_label, &amp;quot;_value&amp;quot;)) %&amp;gt;%
    mutate.(!!(variable) := case_when(grepl(&amp;quot;1$&amp;quot;, !!(variable)) ~ &amp;quot;kitchen&amp;quot;,
                                    grepl(&amp;quot;2$&amp;quot;, !!(variable)) ~ &amp;quot;living&amp;quot;,
                                    grepl(&amp;quot;3$&amp;quot;, !!(variable)) ~ &amp;quot;laundry&amp;quot;,
                                    grepl(&amp;quot;4$&amp;quot;, !!(variable)) ~ &amp;quot;office&amp;quot;,
                                    grepl(&amp;quot;5$&amp;quot;, !!(variable)) ~ &amp;quot;bathroom&amp;quot;,
                                    grepl(&amp;quot;6$&amp;quot;, !!(variable)) ~ &amp;quot;outside&amp;quot;,
                                    grepl(&amp;quot;7$&amp;quot;, !!(variable)) ~ &amp;quot;ironing&amp;quot;,
                                    grepl(&amp;quot;8$&amp;quot;, !!(variable)) ~ &amp;quot;teenager&amp;quot;,
                                    grepl(&amp;quot;9$&amp;quot;, !!(variable)) ~ &amp;quot;parents&amp;quot;)) %&amp;gt;%  
  mutate.(month = month(date)) %&amp;gt;%  
  summarise_across.(.cols = ends_with(&amp;quot;_value&amp;quot;),
                    .fns = mean,
                    .by = c(month, !!(variable))) %&amp;gt;%  
  ungroup()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, it’s &lt;em&gt;almost&lt;/em&gt; the same thing. &lt;code&gt;{tidytable}&lt;/code&gt; verbs end with a &lt;code&gt;&#39;.&#39;&lt;/code&gt; and that’s
it. Well almost (again), the biggest difference is how &lt;code&gt;{tidytable}&lt;/code&gt; groups by a variable. It’s very
similar to how it’s done in &lt;code&gt;{data.table}&lt;/code&gt;, by using a &lt;code&gt;.by =&lt;/code&gt; argument to verbs that support it,
such as &lt;code&gt;summarise_across()&lt;/code&gt; (which is also, by the way, another difference with standard
&lt;code&gt;{tidyverse}&lt;/code&gt; syntax). While I’ll have to remember these, I’d argue that they’re minor differences
and if it can make my function run faster, I don’t mind!&lt;/p&gt;
&lt;p&gt;Now let’s run a little benchmark. But first, let’s define our data as a &lt;code&gt;tidytable&lt;/code&gt; object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;energy_tidytable &amp;lt;- as_tidytable(energy)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re good to go:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark::microbenchmark(
                  energy %&amp;gt;%
                  prepare_data(temperature),
                  energy_tidytable %&amp;gt;%
                  prepare_data_dt(temperature),
                  times = 10
                )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##                                               expr      min       lq     mean
##               energy %&amp;gt;% prepare_data(temperature) 847.9709 849.6671 868.6524
##  energy_tidytable %&amp;gt;% prepare_data_dt(temperature) 820.2051 838.6647 861.9685
##    median       uq      max neval
##  861.0652 880.8200 914.4685    10
##  858.9454 873.3268 936.0147    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is nice! It does indeed run faster, and with only some minor changes to the function! And
how about using some more cores to run this function?
This can be done using &lt;code&gt;data.table::setDTthreads(n_cores)&lt;/code&gt; where &lt;code&gt;n_cores&lt;/code&gt; is the number of
cores you want to use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.table::setDTthreads(12)
microbenchmark::microbenchmark(
                  energy %&amp;gt;%
                  prepare_data(temperature),
                  energy_tidytable %&amp;gt;%
                  prepare_data_dt(temperature),
                  times = 10
                )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##                                               expr      min       lq     mean
##               energy %&amp;gt;% prepare_data(temperature) 832.9876 840.8000 874.3047
##  energy_tidytable %&amp;gt;% prepare_data_dt(temperature) 829.7937 831.2868 866.4383
##    median       uq      max neval
##  889.2684 898.6861 914.7178    10
##  836.8712 893.0613 997.8511    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe surprisingly, it did not run faster. It could very well be that my function does not really
lend itself to running in parallel, and the overhead induced by distributing the work to the
cpu cores cancels out the gains from running it in parallel. But in any case, this is really looking
very interesting. I have not tested the whole package yet, but
since the syntax is so similar to the &lt;code&gt;{tidyverse}&lt;/code&gt;, you can try really quickly to see if the &lt;code&gt;{tidytable}&lt;/code&gt;
version of the function runs faster, and if yes, I don’t really see a reason not to use it!&lt;/p&gt;
&lt;p&gt;Check out the project’s website &lt;a href=&#34;https://markfairbanks.github.io/tidytable/index.html&#34;&gt;here&lt;/a&gt;, and
follow the author’s twitter &lt;a href=&#34;https://twitter.com/markfairbanks10&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring NACE codes</title>
      <link>/blog/2020-04-27-nace_explorer/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-04-27-nace_explorer/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6n9ESFJTnHs&#34;&gt;
&lt;img src=&#34;/img/industry.png&#34; title = &#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A quick one today. If you work with economic data, you’ll be confronted to NACE code sooner or later.
NACE stands for &lt;em&gt;Nomenclature statistique des Activités économiques dans la Communauté Européenne&lt;/em&gt;.
It’s a standard classification of economic activities. It has 4 levels, and you can learn more
about it &lt;a href=&#34;https://ec.europa.eu/eurostat/en/web/products-manuals-and-guidelines/-/KS-RA-07-015&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Each level adds more details; consider this example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;C - Manufacturing
C10 - Manufacture of food products
C10.1 - Processing and preserving of meat and production of meat products
C10.1.1 - Processing and preserving of meat
C10.1.2 - Processing and preserving of poultry meat
C10.1.3 - Production of meat and poultry meat products&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So a company producing meat and poultry meat products would have NACE code level 4 &lt;code&gt;C10.1.3&lt;/code&gt; with it.
Today for work I had to create a nice visualisation of the hierarchy of the NACE classification.
It took me a bit of time to find a nice solution, so that’s why I’m posting it here. Who knows, it
might be useful for other people. First let’s get the data. Because finding it is not necessarily
very easy if you’re not used to navigating Eurostat’s website, I’ve put the CSV into a gist:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(data.tree)
library(igraph)
library(GGally)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nace_code &amp;lt;- read_csv(&amp;quot;https://gist.githubusercontent.com/b-rodrigues/4218d6daa8275acce80ebef6377953fe/raw/99bb5bc547670f38569c2990d2acada65bb744b3/nace_rev2.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   Order = col_double(),
##   Level = col_double(),
##   Code = col_character(),
##   Parent = col_character(),
##   Description = col_character(),
##   `This item includes` = col_character(),
##   `This item also includes` = col_character(),
##   Rulings = col_character(),
##   `This item excludes` = col_character(),
##   `Reference to ISIC Rev. 4` = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(nace_code)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 10
##    Order Level Code  Parent Description `This item incl… `This item also…
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           
## 1 398481     1 A     &amp;lt;NA&amp;gt;   AGRICULTUR… &amp;quot;This section i… &amp;lt;NA&amp;gt;            
## 2 398482     2 01    A      Crop and a… &amp;quot;This division … This division a…
## 3 398483     3 01.1  01     Growing of… &amp;quot;This group inc… &amp;lt;NA&amp;gt;            
## 4 398484     4 01.11 01.1   Growing of… &amp;quot;This class inc… &amp;lt;NA&amp;gt;            
## 5 398485     4 01.12 01.1   Growing of… &amp;quot;This class inc… &amp;lt;NA&amp;gt;            
## 6 398486     4 01.13 01.1   Growing of… &amp;quot;This class inc… &amp;lt;NA&amp;gt;            
## # … with 3 more variables: Rulings &amp;lt;chr&amp;gt;, `This item excludes` &amp;lt;chr&amp;gt;,
## #   `Reference to ISIC Rev. 4` &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So there’s a bunch of columns we don’t need, so we’re going to ignore them. What I’ll be doing is
transforming this data frame into a data tree, using the &lt;code&gt;{data.tree}&lt;/code&gt; package. For this, I need
columns that provide the hierarchy. I’m doing this with the next chunk of code. I won’t explain
each step, but the idea is quite simple. I’m using the &lt;code&gt;Level&lt;/code&gt; column to create new columns called
&lt;code&gt;Level1&lt;/code&gt;, &lt;code&gt;Level2&lt;/code&gt;, etc. I’m then doing some cleaning:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nace_code &amp;lt;- nace_code %&amp;gt;%
  select(Level, Code)

nace_code &amp;lt;- nace_code %&amp;gt;%
  mutate(Level1 = ifelse(Level == 1, Code, NA)) %&amp;gt;%
  fill(Level1, .direction = &amp;quot;down&amp;quot;) %&amp;gt;%  
  mutate(Level2 = ifelse(Level == 2, Code, NA)) %&amp;gt;%
  fill(Level2, .direction = &amp;quot;down&amp;quot;) %&amp;gt;%  
  mutate(Level3 = ifelse(Level == 3, Code, NA)) %&amp;gt;%
  fill(Level3, .direction = &amp;quot;down&amp;quot;) %&amp;gt;%  
  mutate(Level4 = ifelse(Level == 4, Code, NA)) %&amp;gt;%  
  filter(!is.na(Level4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at how the data looks now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(nace_code)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   Level Code  Level1 Level2 Level3 Level4
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; 
## 1     4 01.11 A      01     01.1   01.11 
## 2     4 01.12 A      01     01.1   01.12 
## 3     4 01.13 A      01     01.1   01.13 
## 4     4 01.14 A      01     01.1   01.14 
## 5     4 01.15 A      01     01.1   01.15 
## 6     4 01.16 A      01     01.1   01.16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now create the hierarchy using by creating a column called &lt;code&gt;pathString&lt;/code&gt; and passing that
data frame to &lt;code&gt;data.tree::as.Node()&lt;/code&gt;. Because some sections, like C (manufacturing) are very large,
I do this separately for each section by using the &lt;code&gt;group_by()&lt;/code&gt;-&lt;code&gt;nest()&lt;/code&gt; trick. This way, I can
create a &lt;code&gt;data.tree&lt;/code&gt; object for each section. Finally, to create the plots, I use &lt;code&gt;igraph::as.igraph()&lt;/code&gt;
and pass this to &lt;code&gt;GGally::ggnet2()&lt;/code&gt;, which takes care of creating the plots. This took me quite
some time to figure out, but the result is a nice looking PDF that the colleagues can now use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nace_code2 &amp;lt;- nace_code %&amp;gt;%
  group_by(Level1, Level2) %&amp;gt;%
  nest() %&amp;gt;%
  mutate(nace = map(data, ~mutate(., pathString = paste(&amp;quot;NACE2&amp;quot;,
                                       Level1,
                                       Level2,
                                       Level3,
                                       Level4,
                                       sep = &amp;quot;/&amp;quot;)))) %&amp;gt;%
  mutate(plots = map(nace, ~as.igraph(as.Node(.)))) %&amp;gt;%
  mutate(plots = map(plots, ggnet2, label = TRUE))


pdf(&amp;quot;nace_maps.pdf&amp;quot;)
pull(nace_code2, plots)
dev.off()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how the pdf looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/nace_c_10.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;If you want to read more about &lt;code&gt;{data.tree}&lt;/code&gt;, you can do so &lt;a href=&#34;https://cran.r-project.org/web/packages/data.tree/vignettes/data.tree.html&#34;&gt;here&lt;/a&gt;
and you can also read more about the &lt;code&gt;ggnet2()&lt;/code&gt; &lt;a href=&#34;https://briatte.github.io/ggnet/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>No excuse not to be a Bayesian anymore</title>
      <link>/blog/2020-04-20-no_excuse/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-04-20-no_excuse/</guid>
      <description>&lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
&lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Thomas_Bayes&#34;&gt;
&lt;img src=&#34;/img/mudasir.png&#34; title = &#34;There&#39;s a meme for anything nowadays&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;My first encounter with Bayesian statistics was around 10 years ago, when I was doing my econometrics
master’s degree. I was immediately very interested by the Bayesian approach to fit econometric
models, because, when you’re reading about Bayesian approaches, it just sounds so easy and natural.
You have a model, you &lt;em&gt;might&lt;/em&gt; have some prior beliefs about the models parameters, and Bayes’ rule
tells you how your beliefs should change when confronting the model to data (evidence). It is really
appealing, and what I really liked as well was the interpretation of the results. It was very natural
as well. Once your software is done estimating/training the model, you don’t actually get a vector
of values for the parameters of the model. You get whole distributions for each parameter, so-called
posterior distributions. You can then make statements like “there’s a 95% probability that this
parameter lies between 0.12 and 0.21” for instance, which is a statement that you cannot make
in a frequentist/classical framework.&lt;/p&gt;
&lt;p&gt;However, while this was very appealing to me at the time, there is no free lunch as they say. At the
time, and it was not that long ago, doing Bayesian statistics was not as straightforward as it is
now, as I will show you in this blog post. At the time, the BUGS language was still the
standard way to describe a Bayesian model and the actual estimation procedure. However, using
the BUGS language was tricky; there was WinBUGS, a Windows tool that had already been discontinued
at the time in favour of OpenBUGS, which was what I was using. The professor teaching this class,
who eventually became one of my PhD advisors, was using WinBUGS to teach, but I was using Linux at the
time already, so I went with OpenBUGS which worked with WINE, I think (WINE is a compatibility layer
that allows running some Windows programs on Linux. It is quite amazing what WINE is able to do,
so much so that Valve forked it to create Proton, which enables running Windows games on Linux
on their popular Steam platform).
Plus, I found out that there was an R package to call OpenBUGS from R and get the results back into
R seamlessly! I think that I remember that there was one for WINBUGS as well, but I also think I
remember I could not get it to work. Anyways, after some digging, I found an even better alternative
called JAGS. JAGS is open source, and is natively available for Linux. It is also able to run in parallel, which is really
useful for Bayesian inference. In any case, these were separate
languages/programs from R, and as such the user had to learn how to use them. The way they worked
was that users needed to write the Bayesian model in a separate text file. I will illustrate this
with an example that I worked on during my Master’s, back in 2011.
The example is taken from Ioannis Ntzoufras’ book &lt;em&gt;Bayesian Modeling Using WinBUGS&lt;/em&gt;.
The example can be found in chapter 7, section 7.4.2. The dataset is from Montgomery et al. (2006) and
&lt;em&gt;refers to the number of aircarft damages in 30 strike missions during the Vietnam War&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Here’s a description of the data, taken from Ntzoufras’ book:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;damage: the number of damaged locations of the aircraft&lt;/li&gt;
&lt;li&gt;type: binary variable which indicates the type of the plane (0 for A4, B for A6)&lt;/li&gt;
&lt;li&gt;bombload: the aircraft bomb load in tons&lt;/li&gt;
&lt;li&gt;airexp: the total months of aircrew experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal is to find a model for &lt;em&gt;damage&lt;/em&gt;, what variables explain the amount of damage on the aircraft?
Now, something quite interesting here, is that we only have 30 observations, and getting more
observations is &lt;em&gt;very&lt;/em&gt; costly. So what are you to do with this?&lt;/p&gt;
&lt;p&gt;First, let’s write the model down:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\text{damage}_i &amp;amp; = \text{Poisson}(\lambda_i) \\
\log(\lambda_i) &amp;amp; = \beta_1 + \beta_2*\text{type}_i + \beta_3 * \text{bombload}_i + \beta_4*\text{airexp}_i
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(i = 1, 2, 3, ..., 30\)&lt;/span&gt;. Since &lt;em&gt;damage&lt;/em&gt; is a count variable, we go with a Poisson distribution,
which only has one parameter, &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.
&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is defined on the second line. Both these definition form the likelihood, which should be
familiar to any statistician. In JAGS and BUGS, this likelihood had to be written in a separate text file, as I mentioned
above, with a syntax that was very similar to R’s (at least for JAGS, because if memory serves, BUGS’s syntax
was &lt;em&gt;more different&lt;/em&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model{
for (i in 1:30){damage[i] ~ dpois( lambda[i] )
        log(lambda[i]) &amp;lt;- b1 + b2 * type[i] 
        + b3 * bombload[i] + b4 * airexp[i]}
b1 ~ dnorm(0, 1.0E-4)
b2 ~ dnorm(0, 1.0E-4)
b3 ~ dnorm(0, 1.0E-4)
b4 ~ dnorm(0, 1.0E-4)}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last four lines are the prior distributions on the parameters. This is something that does
not exist in frequentist statistics. Frequentists would maximise the above likelihood and call it a day.
However, the Bayesian framework allows the practitioner to add prior knowledge into the model. This
prior knowledge can come from domain knowledge or the literature. However, if the practitioner
does not have a clue about good priors, then diffuse priors can be used. Diffuse priors do not
carry much information, if at all. The priors above are diffuse; they’re normally distributed, centered
around 0 with very small precision (in the Bayesian framework, the normal distribution is
defined with two parameters, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\tau = \dfrac{1}{\sigma}\)&lt;/span&gt;). But since my student
years I have learned that using such priors is actually not a very good idea, and better alternatives
exist (priors that at least provide some regularization for instance). The Bayesian approach to
statistics is often criticized for this, because priors are not objective. If you’re not using
diffuse priors, then you’re using priors that carry some information. This information is subjective
and subjectivity is a big No-No. But should subjectivity be a No-No? After all, if you can defend
your priors, either because of domain knowledge, or because of past studies that provide some clue
why not use this information? Especially when here is very little data like in this example. Also,
you can perform a sensitivity analysis, and show how the posterior distribution of the parameters
change when your priors change. What is important is to be fully transparent about the priors you’re using, and
have clear justification for them. If these conditions are met, I don’t see why you should not use
prior information in your model. Plus, even in frequentist statistics prior knowledge is used
as well, for instance by pre-processing the data in a certain way, or by constraining the values
the parameters are allowed to take in the optimisation routine (I’m looking at you, &lt;code&gt;L-BFGS-B&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now, let’s continue with the data. To load the data, I had to manually created each variable (but maybe
JAGS now uses data frames) to pass it to &lt;code&gt;jags()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We load the data this way since jags only takes numerical vectors, matrices or lists
# containing the names as input
damage &amp;lt;- c(0, 1, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 0, 1, 1, 2, 5, 1, 1, 5, 5, 7) 
type &amp;lt;- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) 
bombload &amp;lt;- c(4, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 7, 8, 8, 8, 7, 7, 7, 10, 10, 10, 12, 12, 12, 8, 8, 8, 14, 14, 14)
airexp &amp;lt;- c(91.5, 84, 76.5, 69, 61.5, 80, 72.5, 65, 57.5, 50, 103, 95.5, 88, 80.5, 73, 116.1, 100.6, 85, 69.4, 53.9, 
112.3, 96.7, 81.1, 65.6, 50, 120, 104.4, 88.9, 73.7, 57.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we loaded the data, we can fit the model, by first defining a vector of parameters, and
a named list for the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;parameters &amp;lt;- c(&amp;quot;b1&amp;quot;, &amp;quot;b2&amp;quot;, &amp;quot;b3&amp;quot;,&amp;quot;b4&amp;quot;)
data &amp;lt;- list(&amp;quot;damage&amp;quot;,&amp;quot;type&amp;quot;,&amp;quot;bombload&amp;quot;,&amp;quot;airexp&amp;quot;)

#We don&amp;#39;t give inits to jags since it can generate appropriate initial values

#Use this on single core machines, and/or windows machines
model_fit&amp;lt;-jags(data,inits=NULL,parameters,n.iter=50000,
                model.file=&amp;quot;bugsjags.txt&amp;quot;,n.chains=4,DIC=T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that &lt;code&gt;jags()&lt;/code&gt; has an argument called &lt;code&gt;model.file&lt;/code&gt;, which is the file I showed above.
Below, the code to take a look at the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Let&amp;#39;s see the results
model_fit$BUGSoutput


model.mcmc&amp;lt;-as.mcmc(model_fit)

traceplot(model.mcmc)

xyplot(model.mcmc)

heidel.diag(model.mcmc) 

par(mfrow=c(2,3))
autocorr.plot(model.mcmc[1],auto.layout=F,ask=F)

geweke.plot(model.mcmc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re actually not looking at this, because I’m not running the code; I only wanted to show you how
this was done 8 years ago. But why? Because now I can show you how this is done nowadays with &lt;code&gt;{rstanarm}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstanarm)

model_fit_stan &amp;lt;- stan_glm(damage ~ ., data = bombs, family = poisson)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;count&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 1.7e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.04083 seconds (Warm-up)
## Chain 1:                0.043647 seconds (Sampling)
## Chain 1:                0.084477 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;count&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 5e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.037986 seconds (Warm-up)
## Chain 2:                0.041253 seconds (Sampling)
## Chain 2:                0.079239 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;count&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 5e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.041033 seconds (Warm-up)
## Chain 3:                0.042982 seconds (Sampling)
## Chain 3:                0.084015 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;count&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 5e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.036928 seconds (Warm-up)
## Chain 4:                0.041124 seconds (Sampling)
## Chain 4:                0.078052 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a lot of output, but the input was a single line that should look very familiar to
practitioners used to the &lt;code&gt;glm()&lt;/code&gt; function. I only used default options, as well as the default priors.
Specifying different priors is quite simple, but I won’t discuss this here, because this blot post, while it
might look like a tutorial, is not a tutorial. What I wanted to show you is the difference that 9
years make in software development. &lt;code&gt;{stan}&lt;/code&gt; is an R package for Bayesian statistics that came
out in 2012 and which has been developed ever since. Just like JAGS and BUGS, users can write
external files with very detailed models. But for smaller, or more standard problems, &lt;code&gt;{rstanarm}&lt;/code&gt;,
makes Bayesian inference very easy and feel familiar to the traditional way of doing things and
as its name implies, uses &lt;code&gt;{stan}&lt;/code&gt; under the hood.&lt;/p&gt;
&lt;p&gt;Now let’s continue a little bit and take a look at the model summary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(model_fit_stan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Model Info:
##  function:     stan_glm
##  family:       poisson [log]
##  formula:      damage ~ .
##  algorithm:    sampling
##  sample:       4000 (posterior sample size)
##  priors:       see help(&amp;#39;prior_summary&amp;#39;)
##  observations: 30
##  predictors:   4
## 
## Estimates:
##               mean   sd   10%   50%   90%
## (Intercept) -0.5    0.9 -1.6  -0.5   0.7 
## type         0.6    0.5 -0.1   0.6   1.2 
## bombload     0.2    0.1  0.1   0.2   0.3 
## airexp       0.0    0.0  0.0   0.0   0.0 
## 
## Fit Diagnostics:
##            mean   sd   10%   50%   90%
## mean_PPD 1.5    0.3  1.1   1.5   1.9  
## 
## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&amp;#39;summary.stanreg&amp;#39;)).
## 
## MCMC diagnostics
##               mcse Rhat n_eff
## (Intercept)   0.0  1.0  2504 
## type          0.0  1.0  2412 
## bombload      0.0  1.0  2262 
## airexp        0.0  1.0  2652 
## mean_PPD      0.0  1.0  3813 
## log-posterior 0.0  1.0  1902 
## 
## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just like for Bayesian stats, we get our parameter’s estimates. But wait! In the intro of this blog
post, I said that in Bayesian statistics, we estimate full parameter distributions. So why are we
getting point estimates? Well, these are statistics from the posterior distribution, the mean,
standard deviation and some deciles.&lt;/p&gt;
&lt;p&gt;To explore the results, I like to use &lt;code&gt;{bayestestR}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayestestR)

describe_posterior(model_fit_stan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Description of Posterior Distributions
## 
## Parameter   | Median | CI | CI_low | CI_high |    pd | ROPE_CI | ROPE_low | ROPE_high | ROPE_Percentage |  Rhat |  ESS
## ----------------------------------------------------------------------------------------------------------------------
## (Intercept) | -0.473 | 89 | -1.898 |   0.929 | 0.711 |      89 |   -0.100 |     0.100 |           0.082 | 0.999 | 2504
## type        |  0.577 | 89 | -0.230 |   1.365 | 0.869 |      89 |   -0.100 |     0.100 |           0.099 | 1.002 | 2412
## bombload    |  0.169 | 89 |  0.065 |   0.275 | 0.994 |      89 |   -0.100 |     0.100 |           0.108 | 1.001 | 2262
## airexp      | -0.014 | 89 | -0.028 |  -0.001 | 0.953 |      89 |   -0.100 |     0.100 |           1.000 | 1.000 | 2652&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s also actually see the posterior of, say, &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(insight)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;insight&amp;#39; was built under R version 3.6.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posteriors &amp;lt;- get_parameters(model_fit_stan)

ggplot(posteriors, aes(x = bombload)) +
  geom_density(fill = &amp;quot;cyan&amp;quot;) +
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-20-no_excuse_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I won’t again go into much detail, because you can read the very detailed Vignettes on &lt;code&gt;{bayestestR}&lt;/code&gt;’s
website: &lt;a href=&#34;https://easystats.github.io/bayestestR/articles/bayestestR.html&#34;&gt;Get started with Bayesian Analysis&lt;/a&gt; and
&lt;a href=&#34;https://easystats.github.io/bayestestR/articles/example1.html#describing-the-posterior-1&#34;&gt;Describing the posterior&lt;/a&gt;
which explain all of this much better than I would ever do. The code I’m showing here is basically
a copy paste of these Vignettes, so if I piqued your interest, go read those Vignettes! I also
highly recommend reading &lt;code&gt;{rstanarm}&lt;/code&gt;’s Vignettes, and grabbing the second edition of &lt;em&gt;Statistical Rethinking&lt;/em&gt;,
by Richard McElreath, it is a great intro to Bayesian statistics with &lt;code&gt;{stan}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, as the title of this blog post reads, there is no excuse not to use Bayesian statistics; from
a software point of view, it’s as simple as ever. And by the way, &lt;code&gt;{stan}&lt;/code&gt; models are supported in
&lt;code&gt;{tidymodels}&lt;/code&gt;’ &lt;code&gt;{parsnip}&lt;/code&gt; package as well, which makes things even easier!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to basic: bar plots</title>
      <link>/blog/2020-04-12-basic_ggplot2/</link>
      <pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-04-12-basic_ggplot2/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=OisvDHvmKuM&#34;&gt;
&lt;img src=&#34;/img/chef.jpg&#34; title = &#34;Specialty from the chef!&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This blog post shows how to make bar plots and area charts. It’s mostly a list of recipes, indented
for myself. These are plots I have often to do in reports and would like to have the code handy
somewhere. Maybe this will be helpful to some of you as well. Actually, this post is exactly how
I started my blog post. I wanted to have a repository of recipes, and with time the blog grew to
what it is now (tutorials and me exploring methods and datasets with R).&lt;/p&gt;
&lt;div id=&#34;bar-charts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bar charts&lt;/h2&gt;
&lt;p&gt;Bar charts are quite simple plots, but there are enough variations of them that they deserve
one single blog post. However, don’t expect many explanations.&lt;/p&gt;
&lt;p&gt;Let’s first start by loading some data, and the usually required packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(janitor)
library(colorspace)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(gss_cat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Very often, what one wants to show are counts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gss_cat %&amp;gt;%
  count(marital, race)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 x 3
##    marital       race      n
##  * &amp;lt;fct&amp;gt;         &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt;
##  1 No answer     Other     2
##  2 No answer     Black     2
##  3 No answer     White    13
##  4 Never married Other   633
##  5 Never married Black  1305
##  6 Never married White  3478
##  7 Separated     Other   110
##  8 Separated     Black   196
##  9 Separated     White   437
## 10 Divorced      Other   212
## 11 Divorced      Black   495
## 12 Divorced      White  2676
## 13 Widowed       Other    70
## 14 Widowed       Black   262
## 15 Widowed       White  1475
## 16 Married       Other   932
## 17 Married       Black   869
## 18 Married       White  8316&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s lump marital statuses that appear less than 10% of the time into an “Other” category:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
  counts_marital_race &amp;lt;- gss_cat %&amp;gt;%
    mutate(marital = fct_lump(marital, prop = 0.1)) %&amp;gt;%
    count(marital, race)
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 12 x 3
##    marital       race      n
##  * &amp;lt;fct&amp;gt;         &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt;
##  1 Never married Other   633
##  2 Never married Black  1305
##  3 Never married White  3478
##  4 Divorced      Other   212
##  5 Divorced      Black   495
##  6 Divorced      White  2676
##  7 Married       Other   932
##  8 Married       Black   869
##  9 Married       White  8316
## 10 Other         Other   182
## 11 Other         Black   460
## 12 Other         White  1925&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simplest bar plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(counts_marital_race) +
  geom_col(aes(x = marital, y = n, fill = race)) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now with &lt;code&gt;position = &amp;quot;dodge&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(counts_marital_race) +
  geom_col(aes(x = marital, y = n, fill = race), position = &amp;quot;dodge&amp;quot;) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Moving the legend around with &lt;code&gt;theme(legend.position = ...)&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(counts_marital_race) +
  geom_col(aes(x = marital, y = n, fill = race), position = &amp;quot;dodge&amp;quot;) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() +
  theme(legend.position = &amp;quot;left&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Counting by year as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
  counts_marital_race_year &amp;lt;- gss_cat %&amp;gt;%
    mutate(marital = fct_lump(marital, prop = 0.1)) %&amp;gt;%
    count(year, marital, race) %&amp;gt;%
    ungroup()
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 96 x 4
##     year marital       race      n
##  * &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt;         &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt;
##  1  2000 Never married Other    60
##  2  2000 Never married Black   157
##  3  2000 Never married White   495
##  4  2000 Divorced      Other    20
##  5  2000 Divorced      Black    60
##  6  2000 Divorced      White   361
##  7  2000 Married       Other    78
##  8  2000 Married       Black   121
##  9  2000 Married       White  1079
## 10  2000 Other         Other    17
## # … with 86 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you want to show how a variable evolves through time, area chart are handy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;%
  group_by(year, marital) %&amp;gt;%
  summarise(n = sum(n)) %&amp;gt;%
  ggplot() +
  geom_area(aes(x = year, y = n, fill = marital)) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now with facets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;%
  ggplot() +
  geom_area(aes(x = year, y = n, fill = marital)) +
  facet_wrap(facets = vars(race), ncol = 1) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But what if I want each plot to have its own y axis?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;%
  ggplot() +
  geom_area(aes(x = year, y = n, fill = marital)) +
  facet_wrap(facets = vars(race), ncol = 1, scales = &amp;quot;free_y&amp;quot;) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now doing an area chart but with relative frequencies:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;% 
  group_by(year, marital) %&amp;gt;% 
  summarise(n = sum(n)) %&amp;gt;%  
  mutate(freq = n/sum(n)) %&amp;gt;% 
  ggplot() +
  geom_area(aes(x = year, y = freq, fill = marital)) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;facet_wrap()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;% 
  group_by(race, year, marital) %&amp;gt;% 
  summarise(n = sum(n)) %&amp;gt;%  
  mutate(freq = n/sum(n)) %&amp;gt;% 
  ggplot() +
  geom_area(aes(x = year, y = freq, fill = marital)) +
  facet_wrap(facets = vars(race), ncol = 1, scales = &amp;quot;free_y&amp;quot;) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Want to replace 2000 with “2000-01-01”? First need to create vector of prettier dates and positions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pretty_dates &amp;lt;- counts_marital_race_year %&amp;gt;%
  mutate(pretty_dates = paste0(year, &amp;quot;-01-01&amp;quot;)) %&amp;gt;%
  pull(pretty_dates) %&amp;gt;%
  unique()

position_dates &amp;lt;- counts_marital_race_year %&amp;gt;%
  pull(year) %&amp;gt;%
  unique() %&amp;gt;%
  sort() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;scale_x_continuous()&lt;/code&gt; can now use this. Using &lt;code&gt;guide = guide_axis(n.dodge = 2)&lt;/code&gt; to avoid
overlapping labels:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;% 
  group_by(race, year, marital) %&amp;gt;% 
  summarise(n = sum(n)) %&amp;gt;%  
  mutate(freq = n/sum(n)) %&amp;gt;%
  ggplot() +
  geom_area(aes(x = year, y = freq, fill = marital)) +
  facet_wrap(facets = vars(race), ncol = 1, scales = &amp;quot;free_y&amp;quot;) +
  scale_x_continuous(&amp;quot;Year of survey&amp;quot;, labels = pretty_dates,
                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Adding labels is not trivial. Here it is not working:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;% 
  group_by(race, year, marital) %&amp;gt;% 
  summarise(n = sum(n)) %&amp;gt;%  
  mutate(freq = n/sum(n)) %&amp;gt;% 
  ggplot() +
  geom_area(aes(x = year, y = freq, fill = marital)) +
  facet_wrap(facets = vars(race), ncol = 1, scales = &amp;quot;free_y&amp;quot;) +
  scale_x_continuous(&amp;quot;Year of survey&amp;quot;, labels = pretty_dates,
                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +
  geom_label(aes(x = year, y = freq, label = round(100 * freq))) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another failed attempt. I leave it here for posterity.
My first idea was first to sort the grouped data set by descending frequency, and then
to reorder the factor variable &lt;code&gt;marital&lt;/code&gt; by descending position, which is the cumulative percentage.
This would work fine, if the same factor levels would have had the same order for each of the
race categories. However, this is not the case. For blacks, the most frequent category is “Never Married”.
As you can see below, this trick worked well for 2 categories out of 3:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;% 
  group_by(race, year, marital) %&amp;gt;% 
  summarise(n = sum(n)) %&amp;gt;%  
  mutate(freq = n/sum(n)) %&amp;gt;%
  group_by(year, race) %&amp;gt;%  
  arrange(desc(freq)) %&amp;gt;% 
  mutate(position = cumsum(freq)) %&amp;gt;% 
  mutate(marital = fct_reorder(marital, desc(position))) %&amp;gt;% 
  ggplot() +
  geom_area(aes(x = year, y = freq, fill = marital)) +
  facet_wrap(facets = vars(race), ncol = 1, scales = &amp;quot;free&amp;quot;) +
  scale_x_continuous(&amp;quot;Year of survey&amp;quot;, labels = pretty_dates,
                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +
  geom_label(aes(x = year, y = position, label = round(100 * freq))) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So to remedy this, is not reorder too early; first, we need to reorder the factor variable by
frequency. Then, we arrange the data by the now reordered &lt;code&gt;marital&lt;/code&gt; variable, and then we can
compute the position using the cumulative frequency.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;% 
  group_by(race, year, marital) %&amp;gt;% 
  summarise(n = sum(n)) %&amp;gt;%  
  mutate(freq = n/sum(n)) %&amp;gt;%
  group_by(year, race) %&amp;gt;%  
  mutate(marital = fct_reorder(marital, freq)) %&amp;gt;% 
  arrange(desc(marital)) %&amp;gt;% 
  mutate(position = cumsum(freq)) %&amp;gt;% 
  ggplot() +
  geom_area(aes(x = year, y = freq, fill = marital)) +
  facet_wrap(facets = vars(race), ncol = 1, scales = &amp;quot;free&amp;quot;) +
  scale_x_continuous(&amp;quot;Year of survey&amp;quot;, labels = pretty_dates,
                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +
  geom_label(aes(x = year, y = position, label = round(100 * freq))) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can place the labels a bit better (in the middle of their respective areas), like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;counts_marital_race_year %&amp;gt;% 
  group_by(race, year, marital) %&amp;gt;% 
  summarise(n = sum(n)) %&amp;gt;%  
  mutate(freq = n/sum(n)) %&amp;gt;%
  group_by(year, race) %&amp;gt;%  
  mutate(marital = fct_reorder(marital, freq)) %&amp;gt;% 
  arrange(desc(marital)) %&amp;gt;% 
  mutate(position = cumsum(freq)) %&amp;gt;% mutate(prev_pos = lag(position, default = 0)) %&amp;gt;%
  mutate(position = (position + prev_pos)/2) %&amp;gt;%  
  ggplot() +
  geom_area(aes(x = year, y = freq, fill = marital)) +
  facet_wrap(facets = vars(race), ncol = 1, scales = &amp;quot;free&amp;quot;) +
  scale_x_continuous(&amp;quot;Year of survey&amp;quot;, labels = pretty_dates,
                     breaks = position_dates, guide = guide_axis(n.dodge = 2)) +
  geom_label(aes(x = year, y = position, label = round(100 * freq))) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s focus on the variable &lt;code&gt;tvhours&lt;/code&gt;. We want to show the total watched hours, but also
the total across all the categories of &lt;code&gt;race&lt;/code&gt; and &lt;code&gt;marital&lt;/code&gt; in a faceted bar plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
  total_tv &amp;lt;- gss_cat %&amp;gt;%
    group_by(year, race, marital) %&amp;gt;%
    summarise(total_tv = sum(tvhours, na.rm = TRUE))
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 127 x 4
## # Groups:   year, race [24]
##     year race  marital       total_tv
##    &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;
##  1  2000 Other No answer            2
##  2  2000 Other Never married      103
##  3  2000 Other Separated           16
##  4  2000 Other Divorced            17
##  5  2000 Other Widowed             24
##  6  2000 Other Married            122
##  7  2000 Black Never married      452
##  8  2000 Black Separated          135
##  9  2000 Black Divorced           156
## 10  2000 Black Widowed            183
## # … with 117 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tibble has the total watched hours by year, race and marital status variables. How to add the total
by year and race categories? For this, by are first going to use the &lt;code&gt;group_split()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;total_tv_split &amp;lt;- total_tv %&amp;gt;%
  select(race, year, marital, total_tv) %&amp;gt;%
  mutate(year = as.character(year)) %&amp;gt;%  
  group_split(year, race)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: ... is ignored in group_split(&amp;lt;grouped_df&amp;gt;), please use
## group_by(..., .add = TRUE) %&amp;gt;% group_split()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have to re-order the columns with &lt;code&gt;select()&lt;/code&gt;, because when using &lt;code&gt;janitor::adorn_totals()&lt;/code&gt;, which
I will be using below to add totals, the first column must be a character column (it serves as
an identifier column).&lt;/p&gt;
&lt;p&gt;This creates a list with 3 races times 6 years, so 24 elements. Each element of the list is a tibble
with each unique combination of year and race:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(total_tv_split)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;total_tv_split[1:2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;list_of&amp;lt;
##   tbl_df&amp;lt;
##     race    : factor&amp;lt;f4a07&amp;gt;
##     year    : character
##     marital : factor&amp;lt;82ceb&amp;gt;
##     total_tv: integer
##   &amp;gt;
## &amp;gt;[2]&amp;gt;
## [[1]]
## # A tibble: 6 x 4
##   race  year  marital       total_tv
##   &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;
## 1 Other 2000  No answer            2
## 2 Other 2000  Never married      103
## 3 Other 2000  Separated           16
## 4 Other 2000  Divorced            17
## 5 Other 2000  Widowed             24
## 6 Other 2000  Married            122
## 
## [[2]]
## # A tibble: 5 x 4
##   race  year  marital       total_tv
##   &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;fct&amp;gt;            &amp;lt;int&amp;gt;
## 1 Black 2000  Never married      452
## 2 Black 2000  Separated          135
## 3 Black 2000  Divorced           156
## 4 Black 2000  Widowed            183
## 5 Black 2000  Married            320&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do this? To use &lt;code&gt;janitor::adorn_totals()&lt;/code&gt;, which adds row-wise totals to a data frame, or to
each data frame if a list of data frames gets passed to it. I need to still transform the data a little
bit. After using &lt;code&gt;adorn_totals()&lt;/code&gt;, I bind my list of data frames together, and then fill down the year
column (when using &lt;code&gt;adorn_totals()&lt;/code&gt;, character columns like &lt;code&gt;year&lt;/code&gt; are filled with &lt;code&gt;&amp;quot;-&amp;quot;&lt;/code&gt;, but I chose
to fill it with &lt;code&gt;NA_character_&lt;/code&gt;). I then replace the NA value from the marital column with the
string &lt;code&gt;&amp;quot;Total&amp;quot;&lt;/code&gt; and then reorder the &lt;code&gt;marital&lt;/code&gt; column by value of &lt;code&gt;total_tv&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;total_tv_split &amp;lt;- total_tv_split %&amp;gt;%
  adorn_totals(fill = NA_character_) %&amp;gt;%
  map(as.data.frame) %&amp;gt;%  
  bind_rows() %&amp;gt;%
  fill(year, .direction = &amp;quot;down&amp;quot;) %&amp;gt;%
  mutate(marital = ifelse(is.na(marital), &amp;quot;Total&amp;quot;, marital)) %&amp;gt;%
  mutate(marital = fct_reorder(marital, total_tv))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can finally create my plot. Because I have added “Total” as a level in the &lt;code&gt;marital&lt;/code&gt; column, it
now appears seamlessly in the plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(total_tv_split) +
  geom_col(aes(x = marital, y = total_tv, fill = race)) +
  facet_wrap(facets = vars(year), nrow = 2) +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  scale_x_discrete(guide = guide_axis(n.dodge = 3)) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To finish this list of recipes, let’s do a pyramid plot now (inspiration from &lt;a href=&#34;https://stackoverflow.com/questions/14680075/simpler-population-pyramid-in-ggplot2&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_pyramid &amp;lt;- gss_cat %&amp;gt;%
  filter(year == &amp;quot;2000&amp;quot;, marital %in% c(&amp;quot;Married&amp;quot;, &amp;quot;Never married&amp;quot;)) %&amp;gt;%
  group_by(race, marital, rincome) %&amp;gt;%  
  summarise(total_tv = sum(tvhours, na.rm = TRUE))

ggplot(data_pyramid, aes(x = rincome, y = total_tv, fill = marital)) +
  geom_col(data = filter(data_pyramid, marital == &amp;quot;Married&amp;quot;)) +
  geom_col(data = filter(data_pyramid, marital == &amp;quot;Never married&amp;quot;), aes(y = total_tv * (-1))) +
  facet_wrap(facets = vars(race), nrow = 1, scales = &amp;quot;free_x&amp;quot;) +
  coord_flip() +
  scale_fill_discrete_qualitative(palette = &amp;quot;Dark 3&amp;quot;) +
  brotools::theme_blog() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-04-12-basic_ggplot2_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Happy Easter!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What would a keyboard optimised for Luxembourguish look like?</title>
      <link>/blog/2020-03-26-bepo_lu/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-03-26-bepo_lu/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/user/TheReportOfTheWeek&#34;&gt;
&lt;img src=&#34;/img/small_head.jpeg&#34; title = &#34;I highly recommend his youtube channel&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
&lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;I’ve been using the BÉPO layout for my keyboard since 2010-ish, and it’s been one of the best computing
decisions I’ve ever taken. The BÉPO layout is an optimized layout for French, but it works quite well
for many European languages, English included (the only issue you might have with the BÉPO layout
for English is that the &lt;code&gt;w&lt;/code&gt; is a bit far away).&lt;/p&gt;
&lt;p&gt;To come up with the BÉPO layout, ideas from a man named August Dvorak were applied for the French
language. Today, the keyboard layout that is optimised for English is called after him, the DVORAK
layout. Dvorak’s ideas were quite simple; unlike the QWERTY layout, his layout had to be based on
character frequency of the English language. The main idea is that the most used
characters of the language should be on the home row of the keyboard. The home row is the row where
you lay your fingers on the keyboard when you are not typing (see picture below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/home_row.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The problem with the “standard” layouts, such as QWERTY, is that they’re all absolute garbage, and
not optimized at all for typing on a computer. For instance, look at the heatmap below, which shows
the most used characters on a QWERTY keyboard when typing an a standard English text:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/qwerty_heatmap.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;(Heatmap generated on &lt;a href=&#34;https://www.patrick-wied.at/projects/heatmap-keyboard/&#34; class=&#34;uri&#34;&gt;https://www.patrick-wied.at/projects/heatmap-keyboard/&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;As you can see, most of the characters used to type this text are actually outside of the home row, and
the majority of them on the left hand side of the keyboard. The idea of Dvorak was to first, put the
most used characters on the home row, and second to try to have an equal split of characters, 50% for each hand.&lt;/p&gt;
&lt;p&gt;The same text on the DVORAK layout, shows how superior it is:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/dvorak_heatmap.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;As you can see, this is much much better. The same idea was applied to develop the BÉPO layout for
French. And because character frequency is quite similar across languages, learning a layout such as
the BÉPO not only translates to more efficient typing for French, but also for other languages, such
as English, as already explained above.&lt;/p&gt;
&lt;p&gt;The reason I’m writing this blog post is due, in part, to the confinement situation
that many people on Earth are currently facing due to the Corona virus. I have a job where I spend
my whole day typing, and am lucky enough to be able to work from home. Which means that I’m lucky
enough to use my mechanical keyboard to work, which is really great. (I avoid taking my mechanical
keyboard with me at work, because I am never very long in the same spot, between meeting and client
assignments…). But to have a mechanical keyboard that’s easy to take with me,
I decided to buy a second mechanical keyboard, a 40% keyboard from Ergodox (see picture below):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/planck.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Because I don’t even want to see the QWERTY keycaps, I bought blank keycaps to replace the ones that
come with the keyboard. Anyway, this made me think about how crazy it is that in 2020 people still
use absolute garbage keyboard layouts (and keyboards by the way) to type on, when their job is
basically only typing all day long. It made me so angry that I even made a video, which you enjoy
&lt;a href=&#34;https://www.youtube.com/watch?v=LMkFdqEpISo&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The other thing I thought about was the specific case of Luxembourg, a country with 3 official
languages (Luxembourguish, French and German), a very large Portuguese minority, and where English
became so important in recent years that the government distributed leaflets in English to the
population (along with leaflets in French, Luxembourguish, German and Portuguese of course) explaining
what is and is not allowed during the period of containment. What would a keyboard optimized for
such a unique country look like?&lt;/p&gt;
&lt;p&gt;Of course, the answer that comes to mind quickly is to use the BÉPO layout; even though people routinely
write in at least 3 of the above-mentioned languages, French is still the one that people use most
of the time for written communication (at least, that’s my perception). The reason is that while
Luxembourguish is the national language, and the language of the native population, French has
always been the administrative language, and laws are still written in French only, even though
they’re debated in Luxembourguish in the parliament.
However, people also routinely write emails in German or English, and more and more people also
write in Luxembourguish. This means that a keyboard optimized for Luxembourguish, or rather, for
the multilinguistic nature of the Luxembourguish country, should take into account all these
different languages. Another thing to keep in mind is that Luxembourguish uses many French words,
and as such, writing these words should be easy.&lt;/p&gt;
&lt;p&gt;So let’s start with the BÉPO layout as a base. This is what it looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/bepo_layout.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;A heatmap of character frequencies of a French, or even English, text would show that the most used
characters are on the home row. If you compare DVORAK to BÉPO, you will see that the home row is
fairly similar. But what strikes my colleagues when they see a picture of the BÉPO layout, is the
fact that the characters &lt;code&gt;é&lt;/code&gt;, &lt;code&gt;è&lt;/code&gt;, &lt;code&gt;ê&lt;/code&gt;, &lt;code&gt;à&lt;/code&gt; and &lt;code&gt;ç&lt;/code&gt; can be accessed directly. They are so used to having
these characters only accessible by using some kind of modifier key that their first reaction is to
think that this is completely stupid. However, what is stupid, is not having these letters easily
accessible, and instead having, say, &lt;code&gt;z&lt;/code&gt; easily accessible (the French “standard” layout is called
AZERTY, which is very similar and just as stupid as the QWERTY layout. The letter &lt;code&gt;Z&lt;/code&gt; is so easy to
type on, but is almost non-existing in French!).&lt;/p&gt;
&lt;p&gt;So let’s analyze character frequencies of a Luxembourguish text and see if the BÉPO layout could be
a good fit. I used several text snippets from the Bible in Luxembourguish for this, and a few lines
of R code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;root_url &amp;lt;- &amp;quot;https://cathol.lu/article&amp;quot;

texts &amp;lt;- seq(4869,4900)

urls &amp;lt;- c(&amp;quot;https://cathol.lu/article4887&amp;quot;,
          &amp;quot;https://cathol.lu/article1851&amp;quot;,
          &amp;quot;https://cathol.lu/article1845&amp;quot;,
          &amp;quot;https://cathol.lu/article1863&amp;quot;,
          &amp;quot;https://cathol.lu/article1857&amp;quot;,
          &amp;quot;https://cathol.lu/article4885&amp;quot;,
          &amp;quot;https://cathol.lu/article1648&amp;quot;,
          &amp;quot;https://cathol.lu/article1842&amp;quot;,
          &amp;quot;https://cathol.lu/article1654&amp;quot;,
          &amp;quot;https://cathol.lu/article1849&amp;quot;,
          &amp;quot;https://cathol.lu/article1874&amp;quot;,
          &amp;quot;https://cathol.lu/article4884&amp;quot;,
          &amp;quot;https://cathol.lu/article1878&amp;quot;,
          &amp;quot;https://cathol.lu/article2163&amp;quot;,
          &amp;quot;https://cathol.lu/article2127&amp;quot;,
          &amp;quot;https://cathol.lu/article2185&amp;quot;,
          &amp;quot;https://cathol.lu/article4875&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I’ve get the urls, let’s get the text out of it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pages &amp;lt;- urls %&amp;gt;%
  map(read_html)

texts &amp;lt;- pages %&amp;gt;%
  map(~html_node(., xpath = &amp;#39;//*[(@id = &amp;quot;art_texte&amp;quot;)]&amp;#39;)) %&amp;gt;%
  map(html_text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;texts&lt;/code&gt; is a list containing the raw text from the website. I used several functions from the &lt;code&gt;{rvest}&lt;/code&gt;
package to do this. I won’t comment on them, because this is not a tutorial about webscraping (I’ve
written several of those already), but a rant about keyboard layout gosh darn it.&lt;/p&gt;
&lt;p&gt;Anyway, let’s now take a look at the character frequencies, and put that in a neat data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;characters &amp;lt;- texts %&amp;gt;%
  map(~strsplit(., split = &amp;quot;&amp;quot;)) %&amp;gt;%
  unlist() %&amp;gt;%
  map(~strsplit(., split = &amp;quot;&amp;quot;)) %&amp;gt;%
  unlist() %&amp;gt;%
  tolower() %&amp;gt;%
  str_extract_all(pattern = &amp;quot;[:alpha:]&amp;quot;) %&amp;gt;%
  unlist() %&amp;gt;%
  table() %&amp;gt;%  
  as.data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Computing the frequencies is now easy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;characters &amp;lt;- characters %&amp;gt;%
  mutate(frequencies = round(Freq/sum(Freq)*100, digits = 2)) %&amp;gt;%
  arrange(desc(frequencies)) %&amp;gt;%  
  janitor::clean_names()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s start with the obvious differences: there is not a single instance of the characters &lt;code&gt;è&lt;/code&gt;, &lt;code&gt;ê&lt;/code&gt;
or &lt;code&gt;ç&lt;/code&gt;, which are used in French only. There are however instances of &lt;code&gt;ü&lt;/code&gt;, &lt;code&gt;ä&lt;/code&gt;, and &lt;code&gt;ë&lt;/code&gt;. These
characters should be easily accessible, however their frequencies are so low, that they could still
only be accessible using a modifier key, and it would not be a huge issue. However, since &lt;code&gt;ç&lt;/code&gt; does
not appear at all, maybe it could be replaced by &lt;code&gt;ä&lt;/code&gt; and &lt;code&gt;ê&lt;/code&gt; could be replaced by &lt;code&gt;ë&lt;/code&gt;. But we must
keep in mind that since the average Luxembourger has to very often switch between so many languages,
I would suggest that these French characters that would be replaced should still be accessible
using a modifier such as &lt;code&gt;Alt Gr&lt;/code&gt;. As for the rest, the layout as it stands is likely quite ok.
Well, actually I know it’s ok, because when I write in Luxembourguish using the BÉPO layout, I find
it quite easy to do. But let’s grab a French and a German text, and see how the ranking
of the characters compare. Let’s get some French text:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to read the French text&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;french &amp;lt;- &amp;quot;Au commencement, Dieu créa les cieux et la terre.
La terre était informe et vide: il y avait des ténèbres à la surface de l&amp;#39;abîme, et l&amp;#39;esprit de Dieu se mouvait au-dessus des eaux.
Dieu dit: Que la lumière soit! Et la lumière fut.
Dieu vit que la lumière était bonne; et Dieu sépara la lumière d&amp;#39;avec les ténèbres.
Dieu appela la lumière jour, et il appela les ténèbres nuit. Ainsi, il y eut un soir, et il y eut un matin: ce fut le premier jour.
Dieu dit: Qu&amp;#39;il y ait une étendue entre les eaux, et qu&amp;#39;elle sépare les eaux d&amp;#39;avec les eaux.
Et Dieu fit l&amp;#39;étendue, et il sépara les eaux qui sont au-dessous de l&amp;#39;étendue d&amp;#39;avec les eaux qui sont au-dessus de l&amp;#39;étendue. Et cela fut ainsi.
Dieu appela l&amp;#39;étendue ciel. Ainsi, il y eut un soir, et il y eut un matin: ce fut le second jour.
Dieu dit: Que les eaux qui sont au-dessous du ciel se rassemblent en un seul lieu, et que le sec paraisse. Et cela fut ainsi.
Dieu appela le sec terre, et il appela l&amp;#39;amas des eaux mers. Dieu vit que cela était bon.
Puis Dieu dit: Que la terre produise de la verdure, de l&amp;#39;herbe portant de la semence, des arbres fruitiers donnant du fruit selon leur espèce et ayant en eux leur semence sur la terre. Et cela fut ainsi.
La terre produisit de la verdure, de l&amp;#39;herbe portant de la semence selon son espèce, et des arbres donnant du fruit et ayant en eux leur semence selon leur espèce. Dieu vit que cela était bon.
Ainsi, il y eut un soir, et il y eut un matin: ce fut le troisième jour.
Dieu dit: Qu&amp;#39;il y ait des luminaires dans l&amp;#39;étendue du ciel, pour séparer le jour d&amp;#39;avec la nuit; que ce soient des signes pour marquer les époques, les jours et les années;
et qu&amp;#39;ils servent de luminaires dans l&amp;#39;étendue du ciel, pour éclairer la terre. Et cela fut ainsi.
Dieu fit les deux grands luminaires, le plus grand luminaire pour présider au jour, et le plus petit luminaire pour présider à la nuit; il fit aussi les étoiles.
Dieu les plaça dans l&amp;#39;étendue du ciel, pour éclairer la terre,
pour présider au jour et à la nuit, et pour séparer la lumière d&amp;#39;avec les ténèbres. Dieu vit que cela était bon.
Ainsi, il y eut un soir, et il y eut un matin: ce fut le quatrième jour.
Dieu dit: Que les eaux produisent en abondance des animaux vivants, et que des oiseaux volent sur la terre vers l&amp;#39;étendue du ciel.
Dieu créa les grands poissons et tous les animaux vivants qui se meuvent, et que les eaux produisirent en abondance selon leur espèce; il créa aussi tout oiseau ailé selon son espèce. Dieu vit que cela était bon.
Dieu les bénit, en disant: Soyez féconds, multipliez, et remplissez les eaux des mers; et que les oiseaux multiplient sur la terre.
Ainsi, il y eut un soir, et il y eut un matin: ce fut le cinquième jour.
Dieu dit: Que la terre produise des animaux vivants selon leur espèce, du bétail, des reptiles et des animaux terrestres, selon leur espèce. Et cela fut ainsi.
Dieu fit les animaux de la terre selon leur espèce, le bétail selon son espèce, et tous les reptiles de la terre selon leur espèce. Dieu vit que cela était bon.
Puis Dieu dit: Faisons l&amp;#39;homme à notre image, selon notre ressemblance, et qu&amp;#39;il domine sur les poissons de la mer, sur les oiseaux du ciel, sur le bétail, sur toute la terre, et sur tous les reptiles qui rampent sur la terre.
Dieu créa l&amp;#39;homme à son image, il le créa à l&amp;#39;image de Dieu, il créa l&amp;#39;homme et la femme.
Dieu les bénit, et Dieu leur dit: Soyez féconds, multipliez, remplissez la terre, et l&amp;#39;assujettissez; et dominez sur les poissons de la mer, sur les oiseaux du ciel, et sur tout animal qui se meut sur la terre.
Et Dieu dit: Voici, je vous donne toute herbe portant de la semence et qui est à la surface de toute la terre, et tout arbre ayant en lui du fruit d&amp;#39;arbre et portant de la semence: ce sera votre nourriture.
Et à tout animal de la terre, à tout oiseau du ciel, et à tout ce qui se meut sur la terre, ayant en soi un souffle de vie, je donne toute herbe verte pour nourriture. Et cela fut ainsi.
Dieu vit tout ce qu&amp;#39;il avait fait et voici, cela était très bon. Ainsi, il y eut un soir, et il y eut un matin: ce fut le sixième jour.
Joe Paterno, né le 21 décembre 1926 à Brooklyn et mort le 22 janvier 2012 à State College, est un joueur et entraîneur américain de football américain universitaire. Figure historique et emblématique des Nittany Lions de Penn State entre 1966 et 2011, il est l&amp;#39;entraîneur le plus victorieux de l&amp;#39;histoire du football américain universitaire avec 409 succès en Division I. Son image est toutefois ternie en fin de carrière à cause de soupçons de négligence dans une affaire d&amp;#39;agressions sexuelles sur mineurs.

Lors de ses brillantes études de droit à l&amp;#39;université Brown, Joe Paterno joue au football américain et est entraîné par Rip Engle. Ce dernier, embauché par l&amp;#39;université de Penn State, le recrute comme entraîneur assistant en 1950. Pendant quinze saisons, l&amp;#39;assistant fait ses preuves avant de devenir entraîneur principal des Nittany Lions en 1965. Surnommé JoePa, il connaît rapidement le succès. Invaincu en 1968 et 1969, il est désiré par plusieurs franchises de la National Football League (NFL), mais refuse pour conserver son rôle d&amp;#39;éducateur. Entraîneur de l&amp;#39;équipe universitaire championne en 1982 et 1986, vainqueur des quatre principaux Bowls universitaires, il intègre le College Football Hall of Fame en 2007 alors qu&amp;#39;il est encore en activité, un accomplissement rare.

Reconnu pour ses succès sportifs, académiques et son exemplarité, JoePa est adulé comme une icône populaire dans la région de State College. Onze jours après avoir célébré sa 409e victoire avec les Lions, il est démis de ses fonctions à la suite du scandale des agressions sexuelles de l&amp;#39;Université d&amp;#39;État de Pennsylvanie. Accusé d&amp;#39;avoir couvert les abus sexuels de Jerry Sandusky, son image est ternie par cette affaire au retentissement international. Il meurt deux mois plus tard des suites d&amp;#39;un cancer du poumon.
Chacun peut publier immédiatement du contenu en ligne, à condition de respecter les règles essentielles établies par la Fondation Wikimedia et par la communauté ; par exemple, la vérifiabilité du contenu, l&amp;#39;admissibilité des articles et garder une attitude cordiale.

De nombreuses pages d’aide sont à votre disposition, notamment pour créer un article, modifier un article ou insérer une image. N’hésitez pas à poser une question pour être aidé dans vos premiers pas, notamment dans un des projets thématiques ou dans divers espaces de discussion.

Les pages de discussion servent à centraliser les réflexions et les remarques permettant d’améliorer les articles.
En 1894, l’explorateur Gustav Adolf von Götzen suivait les traces d’un missionnaire en provenance de la cote orientale d’Afrique. Pendant qu’il se rendait au Rwanda, il découvre un petit village des pécheurs appelé Ngoma qui traduit signifie tam tam, par déformation il écrivit Goma. Ngoma devint un poste belge en face de celui de Rubavu (au Rwanda) habité par les Allemands. Au début, la cohabitation entre ces deux postes n’était pas facile. À un certain moment, les chefs coutumiers du Rwanda, en complicité avec les Allemands attaquent les Belges de Goma. Ces derniers se réfugient à Bukavu et laissent les envahisseurs occuper la ville. Après des négociations, les Allemands replient vers le Rwanda et les Belges reprennent leur position initiale comme poste colonial. L’afflux des colonisateurs dans ce village joue un rôle important dans son évolution pour devenir une grande agglomération. Les colonisateurs venaient d’installer le chef lieu du district Belge à Rutshuru ou vivait l’administrateur colonial. Le chef lieu passera de Rutshuru à Goma.

En ce moment, Goma reste un poste de transaction lacustre avec Bukavu qui était une ville minière. Plus tard, Rutshuru, Masisi, Kalehe, Gisenyi, etc. déverseront leurs populations dans Goma, à la rechercher de l’emploi au près des colonisateurs. C’est en cette période que vu le jour le quartier Birere (un bidonville de Goma) autour des entrepôts, bureaux et habitations des colons. Le nom Birere (littéralement feuilles de bananier) vient du fait qu’à l’époque, les gens y construisaient en feuilles des bananiers.

La ville est la base arrière de l&amp;#39;opération Turquoise organisée en 1994 à la fin du génocide rwandais.

La ville et ses environs abriteront dans des camps autour de 650 000 réfugiés hutus de 1994 jusqu&amp;#39;à la chute du Zaïre, dont certains supposés anciens génocidaires. Selon des ONG, l&amp;#39;AFDL procède à des massacres dans les camps entre 1996 et 19971.

De 1998 à 2002/2003, la ville, sous contrôle du Rassemblement congolais pour la démocratie (RCD) pro-rwandais échappe au contrôle du gouvernement congolais.

De nombreux viols, massacres et crimes de guerre y ont été perpétrés entre 1996 et 2006 par les troupes des généraux rebelles du RCD, essentiellement sous les généraux Nkundabatware et Mutebusi.

En 2002, le Nyiragongo entra en éruption, et une coulée de lave atteignit le centre de la ville. La lave n&amp;#39;a pas atteint le lac Kivu fort heureusement, en effet ce lac est un lac méromictique et un changement brutal de chaleur aurait des conséquences graves : Éruption limnique.

Débordant de populations fuyant les violences, Goma compte en 2012 plus de 400 000 habitants. Ceux qui ne peuvent pas trouver d&amp;#39;abri remplissent les camps de réfugiés, où l&amp;#39;ONU et les ONG se débattent pour leur fournir nourriture, eau et combustible.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;characters_fr &amp;lt;- french %&amp;gt;%
  map(~strsplit(., split = &amp;quot;&amp;quot;)) %&amp;gt;%
  unlist() %&amp;gt;%
  map(~strsplit(., split = &amp;quot;&amp;quot;)) %&amp;gt;%
  unlist() %&amp;gt;%
  tolower() %&amp;gt;%
  str_extract_all(pattern = &amp;quot;[:alpha:]&amp;quot;) %&amp;gt;%
  unlist() %&amp;gt;%
  table() %&amp;gt;%  
  as.data.frame() %&amp;gt;%  
  mutate(frequencies = round(Freq/sum(Freq)*100, digits = 2)) %&amp;gt;%
  arrange(desc(frequencies)) %&amp;gt;%  
  janitor::clean_names()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now do the same for German:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to read the German text&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;german &amp;lt;- &amp;quot;Am Anfang schuf Gott Himmel und Erde.
Und die Erde war wüst und leer, und es war finster auf der Tiefe; und der Geist Gottes schwebte auf dem Wasser.
Und Gott sprach: Es werde Licht! und es ward Licht.
Und Gott sah, daß das Licht gut war. Da schied Gott das Licht von der Finsternis
und nannte das Licht Tag und die Finsternis Nacht. Da ward aus Abend und Morgen der erste Tag.
Und Gott sprach: Es werde eine Feste zwischen den Wassern, und die sei ein Unterschied zwischen den Wassern.
Da machte Gott die Feste und schied das Wasser unter der Feste von dem Wasser über der Feste. Und es geschah also.
Und Gott nannte die Feste Himmel. Da ward aus Abend und Morgen der andere Tag.
Und Gott sprach: Es sammle sich das Wasser unter dem Himmel an besondere Örter, daß man das Trockene sehe. Und es geschah also.
Und Gott nannte das Trockene Erde, und die Sammlung der Wasser nannte er Meer. Und Gott sah, daß es gut war.
Und Gott sprach: Es lasse die Erde aufgehen Gras und Kraut, das sich besame, und fruchtbare Bäume, da ein jeglicher nach seiner Art Frucht trage und habe seinen eigenen Samen bei sich selbst auf Erden. Und es geschah also.
Und die Erde ließ aufgehen Gras und Kraut, das sich besamte, ein jegliches nach seiner Art, und Bäume, die da Frucht trugen und ihren eigenen Samen bei sich selbst hatten, ein jeglicher nach seiner Art. Und Gott sah, daß es gut war.
Da ward aus Abend und Morgen der dritte Tag.
Und Gott sprach: Es werden Lichter an der Feste des Himmels, die da scheiden Tag und Nacht und geben Zeichen, Zeiten, Tage und Jahre
und seien Lichter an der Feste des Himmels, daß sie scheinen auf Erden. Und es geschah also.
Und Gott machte zwei große Lichter: ein großes Licht, das den Tag regiere, und ein kleines Licht, das die Nacht regiere, dazu auch Sterne.
Und Gott setzte sie an die Feste des Himmels, daß sie schienen auf die Erde
und den Tag und die Nacht regierten und schieden Licht und Finsternis. Und Gott sah, daß es gut war.
Da ward aus Abend und Morgen der vierte Tag.
Und Gott sprach: Es errege sich das Wasser mit webenden und lebendigen Tieren, und Gevögel fliege auf Erden unter der Feste des Himmels.
Und Gott schuf große Walfische und allerlei Getier, daß da lebt und webt, davon das Wasser sich erregte, ein jegliches nach seiner Art, und allerlei gefiedertes Gevögel, ein jegliches nach seiner Art. Und Gott sah, daß es gut war.
Und Gott segnete sie und sprach: Seid fruchtbar und mehrt euch und erfüllt das Wasser im Meer; und das Gefieder mehre sich auf Erden.
Da ward aus Abend und Morgen der fünfte Tag.
Und Gott sprach: Die Erde bringe hervor lebendige Tiere, ein jegliches nach seiner Art: Vieh, Gewürm und Tiere auf Erden, ein jegliches nach seiner Art. Und es geschah also.
Und Gott machte die Tiere auf Erden, ein jegliches nach seiner Art, und das Vieh nach seiner Art, und allerlei Gewürm auf Erden nach seiner Art. Und Gott sah, daß es gut war.
Und Gott sprach: Laßt uns Menschen machen, ein Bild, das uns gleich sei, die da herrschen über die Fische im Meer und über die Vögel unter dem Himmel und über das Vieh und über die ganze Erde und über alles Gewürm, das auf Erden kriecht.
Und Gott schuf den Menschen ihm zum Bilde, zum Bilde Gottes schuf er ihn; und schuf sie einen Mann und ein Weib.
Und Gott segnete sie und sprach zu ihnen: Seid fruchtbar und mehrt euch und füllt die Erde und macht sie euch untertan und herrscht über die Fische im Meer und über die Vögel unter dem Himmel und über alles Getier, das auf Erden kriecht.
Und Gott sprach: Seht da, ich habe euch gegeben allerlei Kraut, das sich besamt, auf der ganzen Erde und allerlei fruchtbare Bäume, die sich besamen, zu eurer Speise,
und allem Getier auf Erden und allen Vögeln unter dem Himmel und allem Gewürm, das da lebt auf Erden, daß sie allerlei grünes Kraut essen. Und es geschah also.
Und Gott sah alles an, was er gemacht hatte; und siehe da, es war sehr gut. Da ward aus Abend und Morgen der sechste Tag.
Während des Bürgerkrieges und Völkermords im nahe angrenzenden Ruanda 1994 war Goma eines der Hauptziele für Flüchtlinge. Unter diesen waren nebst Zivilisten auch Mittäter des Genozids. Nachdem über eine Million Flüchtlinge die Stadt erreicht hatten, brach in den Lagern eine Cholera-Epidemie aus, die mehrere Tausend Opfer forderte. In den Jahren 1997 und 1998, als der Bürgerkrieg im Kongo nach dem Sturz von Präsident Mobutu Sese Seko eskalierte, eroberten ruandische Regierungstruppen Goma. Im Zuge der Verfolgung von Hutu, die in der Stadt Zuflucht gesucht hatten, töteten sie auch Hunderte Unbeteiligte.

Im Jahre 2002 wurde die Stadt von einem Lavastrom aus dem etwa 14 km entfernten Nyiragongo im Norden zu großen Teilen zerstört. Viele Gebäude gerade im Stadtzentrum sowie der Flughafen Goma waren betroffen. Von den 3.000 Metern der Start- und Landebahn sind bis heute noch fast 1.000 Meter unter einer Lavaschicht begraben, so dass der internationale Verkehr ihn meidet. Rund 250.000 Einwohner der Stadt mussten flüchten. Es gab 147 Todesopfer, viele Flüchtlinge blieben obdachlos oder haben sich am Rande der Lavafelder Notunterkünfte gebaut. Seit April 2009 wird unter Führung der Welthungerhilfe das Rollfeld des Flughafens von der Lava befreit. Die Bedrohung, dass sich bei einer erneuten Eruption Lavamassen aus dem innerhalb des Vulkankraters befindlichen Lavasee erneut ins Tal und auf die Stadt ergießen, besteht nach wie vor.[3]

Am 15. April 2008 raste nach dem Start vom Flughafen Goma eine Douglas DC-9 mit 79 Passagieren und 6 Besatzungsmitgliedern über das südliche Startbahnende hinaus in das Wohn- und Marktgebiet Birere. Etwa 40 Personen aus dem angrenzenden Siedlungsgebiet kamen ums Leben, mindestens 53 Passagiere und die 6 Besatzungsmitglieder überlebten jedoch. Das Feuer aus dem brennenden Wrack konnte sich aufgrund des starken Regens nicht ausbreiten, Anwohner konnten das Feuer zusätzlich eindämmen.

Zehntausende Menschen flohen Ende Oktober 2008 aufgrund einer Offensive von Tutsi-Rebellen aus der Stadt.[4]

Am 21. November 2012 wurden große Teile der Stadt von der gegen die Zentralregierung unter Präsident Joseph Kabila kämpfenden Rebellenbewegung M23 eingenommen. Dort stationierte UNO-Friedens-Truppen griffen im Gegensatz zu früheren Aktivitäten nicht mehr ein.[5] Am 1. Dezember begannen sie nach Überschreitung eines Ultimatums der Internationalen Konferenz der Großen Seen Afrikas und zwei Resolutionen des UN-Sicherheitsrats, sich aus der Stadt zurückzuziehen.

Im Jahre 2019 wurden mehrere Einzelfälle von Ebola in der Stadt registriert, nachdem die Ebola Epidemie bereits zuvor im Ostkongo ausgebrochen war.[6]

Seit 1959 ist Goma Sitz des römisch-katholischen Bistums Goma.
Die Transporteure werden Frachtführer (in Österreich Frächter) genannt. Sie organisieren nicht den Transport, sondern führen diesen aus, meistens im Auftrag eines Spediteurs. Die Höhe der Fracht wird im Frachtvertrag vereinbart und in der Regel im Frachtbrief festgehalten. Seit mit der Transportrechtsreform 1998 in Deutschland die Erstellung eines Frachtbriefes für nationale Transporte nicht mehr zwingend erforderlich ist, sondern auch Lieferscheine, Ladelisten oder vergleichbare Papiere als Warenbegleitdokument verwendet werden können, wird zunehmend kein Frachtbrief mehr ausgestellt. Beim Frachtbrief gibt es drei Originalausfertigungen. Eine Ausfertigung verbleibt beim Absender, nachdem ihm darauf der Frachtführer die Übernahme des Frachtguts bestätigt hat. Die zweite verbleibt nach Ablieferung des Frachtguts als Ablieferbestätigung beim Frachtführer und die dritte erhält der Empfänger.

Für die Verladung des Frachtguts ist der Absender zuständig. Er ist dabei gem. § 412 HGB für eine beförderungssichere Verladung des Frachtguts verantwortlich, wohingegen der Frachtführer für die verkehrssichere Verladung (z. B. Gewichtsverteilung, Einhaltung der zulässigen Achslasten), als auch für die Ladungssicherung zu sorgen hat.

Bei Kontrollen muss der Frachtbrief den Zoll- und Polizeibehörden, sowie dem Bundesamt für Güterverkehr (BAG) ausgehändigt werden.

Es gibt anmeldepflichtige Frachtgüter, für deren Transport es einer ausdrücklichen behördlichen Genehmigung bedarf. Schwertransporte erfordern eine behördliche Ausnahmegenehmigung und bei Überschreiten bestimmter Abmessungen sind gemäß § 29 Absatz. 3 StVO (Übermäßige Straßennutzung) definitiv Begleitfahrzeuge und/oder eine Begleitung durch die Polizei vorgeschrieben, um Sicherungsmaßnahmen einzuleiten und für einen reibungslosen Ablauf zu sorgen. Fällt das zu befördernde Frachtgut unter die Gefahrgutverordnung, muss das Transportfahrzeug neben der Einhaltung gefahrgutrelevanter Vorschriften auch mit entsprechenden Warntafeln gekennzeichnet sein. Darüber hinaus benötigt dann der Fahrzeugführer und ein eventueller Beifahrer auch eine ADR-Bescheinigung.

Die Aufteilung der Frachtkosten zwischen Absender und Empfänger wird über die im Kaufvertrag festgehaltenen Lieferbedingungen geregelt, im internationalen Warenverkehr durch die Incoterms.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;characters_gr &amp;lt;- german %&amp;gt;%
  map(~strsplit(., split = &amp;quot;&amp;quot;)) %&amp;gt;%
  unlist() %&amp;gt;%
  map(~strsplit(., split = &amp;quot;&amp;quot;)) %&amp;gt;%
  unlist() %&amp;gt;%
  tolower() %&amp;gt;%
  str_extract_all(pattern = &amp;quot;[:alpha:]&amp;quot;) %&amp;gt;%
  unlist() %&amp;gt;%
  table() %&amp;gt;%  
  as.data.frame() %&amp;gt;%  
  mutate(frequencies = round(Freq/sum(Freq)*100, digits = 2)) %&amp;gt;%
  arrange(desc(frequencies)) %&amp;gt;%
  janitor::clean_names()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now visualize how the rankings evolve between these three languages. For this, I’m using the &lt;code&gt;newggslopegraph()&lt;/code&gt;
function from the &lt;code&gt;{CGPfunctions}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;characters$rank &amp;lt;- seq(1, 30)
characters_fr$rank &amp;lt;- seq(1, 36)
characters_gr$rank &amp;lt;- seq(1, 28)

characters_fr &amp;lt;- characters_fr %&amp;gt;%
  select(letters = x, rank, frequencies) %&amp;gt;%
  mutate(language = &amp;quot;french&amp;quot;)

characters_gr &amp;lt;- characters_gr %&amp;gt;%
  select(letters = x, rank, frequencies) %&amp;gt;%
  mutate(language = &amp;quot;german&amp;quot;)

characters &amp;lt;- characters %&amp;gt;%
  select(letters = x, rank, frequencies) %&amp;gt;%
  mutate(language = &amp;quot;luxembourguish&amp;quot;)

characters_df &amp;lt;- bind_rows(characters, characters_fr, characters_gr)

CGPfunctions::newggslopegraph(characters_df, 
                              language,
                              rank,
                              letters,
                              Title = &amp;quot;Character frequency ranking for the Luxembourguish official languages&amp;quot;,
                              SubTitle = NULL,
                              Caption = NULL,
                              YTextSize = 4) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;lme4&amp;#39;:
##   method                          from
##   cooks.distance.influence.merMod car 
##   influence.merMod                car 
##   dfbeta.influence.merMod         car 
##   dfbetas.influence.merMod        car&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-26-bepo_lu_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;2100&#34; /&gt;&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to look at the raw data, which contains the frequencies&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;characters_df &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    letters rank frequencies       language
## 1        e    1       16.19 luxembourguish
## 2        n    2        9.61 luxembourguish
## 3        s    3        6.94 luxembourguish
## 4        a    4        6.56 luxembourguish
## 5        i    5        6.44 luxembourguish
## 6        t    6        6.16 luxembourguish
## 7        d    7        5.56 luxembourguish
## 8        r    8        5.42 luxembourguish
## 9        h    9        5.21 luxembourguish
## 10       u   10        3.76 luxembourguish
## 11       g   11        3.70 luxembourguish
## 12       m   12        3.26 luxembourguish
## 13       o   13        3.07 luxembourguish
## 14       l   14        2.81 luxembourguish
## 15       c   15        2.51 luxembourguish
## 16       w   16        2.23 luxembourguish
## 17       é   17        1.56 luxembourguish
## 18       k   18        1.42 luxembourguish
## 19       f   19        1.34 luxembourguish
## 20       ä   20        1.18 luxembourguish
## 21       z   21        1.03 luxembourguish
## 22       p   22        1.02 luxembourguish
## 23       j   23        0.78 luxembourguish
## 24       ë   24        0.72 luxembourguish
## 25       b   25        0.68 luxembourguish
## 26       v   26        0.68 luxembourguish
## 27       ü   27        0.13 luxembourguish
## 28       q   28        0.01 luxembourguish
## 29       x   29        0.01 luxembourguish
## 30       y   30        0.01 luxembourguish
## 31       e    1       15.40         french
## 32       s    2        7.81         french
## 33       i    3        7.63         french
## 34       t    4        7.52         french
## 35       a    5        7.47         french
## 36       u    6        7.03         french
## 37       r    7        6.74         french
## 38       n    8        6.70         french
## 39       l    9        6.31         french
## 40       o   10        4.74         french
## 41       d   11        4.14         french
## 42       c   12        3.00         french
## 43       p   13        2.43         french
## 44       m   14        2.39         french
## 45       é   15        1.84         french
## 46       v   16        1.42         french
## 47       b   17        1.08         french
## 48       f   18        1.08         french
## 49       g   19        0.96         french
## 50       q   20        0.82         french
## 51       x   21        0.57         french
## 52       è   22        0.51         french
## 53       h   23        0.51         french
## 54       y   24        0.44         french
## 55       à   25        0.40         french
## 56       j   26        0.37         french
## 57       z   27        0.18         french
## 58       w   28        0.14         french
## 59       î   29        0.11         french
## 60       k   30        0.11         french
## 61       ô   31        0.08         french
## 62       ç   32        0.03         french
## 63       ê   33        0.01         french
## 64       ï   34        0.01         french
## 65       ö   35        0.01         french
## 66       ù   36        0.01         french
## 67       e    1       16.58         german
## 68       n    2        9.00         german
## 69       r    3        8.05         german
## 70       a    4        6.71         german
## 71       d    5        6.55         german
## 72       t    6        6.47         german
## 73       s    7        6.38         german
## 74       i    8        6.35         german
## 75       u    9        4.63         german
## 76       h   10        4.44         german
## 77       g   11        3.78         german
## 78       l   12        3.09         german
## 79       c   13        2.80         german
## 80       m   14        2.49         german
## 81       o   15        2.29         german
## 82       f   16        2.28         german
## 83       b   17        2.12         german
## 84       w   18        1.11         german
## 85       v   19        0.85         german
## 86       z   20        0.84         german
## 87       ü   21        0.77         german
## 88       p   22        0.65         german
## 89       k   23        0.63         german
## 90       ä   24        0.34         german
## 91       ß   25        0.33         german
## 92       ö   26        0.26         german
## 93       j   27        0.19         german
## 94       y   28        0.01         german&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;Certain things pop out of this plot: the rankings of the German and Luxembourguish languages are
more similar than the rankings of French and Luxembourguish, but overall, the three languages have
practically the same top 10 characters. Using the same base as the BÉPO layout should be
comfortable enough, but the characters &lt;code&gt;h&lt;/code&gt; and &lt;code&gt;g&lt;/code&gt;, which are not very common in French, are much
more common in Luxembourguish, and should thus be better placed. I would advise against using the
German ergonomic/optimized layout, however, because as I said in the beginning, French is still
probably the most written language, certainly more often written than German. So even though the
frequencies of characters are very similar between Luxembourguish and German, I would still prefer
to use the French BÉPO layout.&lt;/p&gt;
&lt;p&gt;I don’t know if there ever will be an ergonomic/optimized layout for Luxembourguish, but I sure
hope that more and more people will start using layouts such as the BÉPO, which are really great
to use. It takes some time to get used to, but in general in about one week of usage, maybe two,
you should be as fast as you were on the legacy layout.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and watch my
&lt;a href=&#34;https://www.youtube.com/channel/UCTZXht1RTL2Duc3eU8MYGzQ&#34;&gt;youtube channel&lt;/a&gt;. If you want to support
my blog and channel, you could &lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or
&lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Explainbility of {tidymodels} models with {iml}</title>
      <link>/blog/2020-03-10-exp_tidymodels/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-03-10-exp_tidymodels/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rhetoric&#34;&gt;
&lt;img src=&#34;/img/exp_tidymodels.jpg&#34; title = &#34;&#39;{rethoric}&#39; would be a sick package name for explainability&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
&lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;In my previous &lt;a href=&#34;https://www.brodrigues.co/blog/2020-03-08-tidymodels/&#34;&gt;blog post&lt;/a&gt;, I have shown how
you could use &lt;code&gt;{tidymodels}&lt;/code&gt; to train several machine learning models. Now, let’s take a look at
getting some explanations out of them, using the &lt;code&gt;{iml}&lt;/code&gt; package. Originally I did not intend to create
a separate blog post, but I have encountered… an issue, or bug, when using both &lt;code&gt;{iml}&lt;/code&gt; and
&lt;code&gt;{tidymodels}&lt;/code&gt; and I felt that it was important that I write about it. Maybe it’s just me that’s missing
something, and you, kind reader, might be able to give me an answer. But let’s first reload the
models from last time (the same packages as on the previous blog post are loaded):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trained_models_list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 
## [[2]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 
## [[3]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 
## [[4]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 
## [[5]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see which of the models performed best (in cross-validation):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trained_models_list %&amp;gt;%
  map(show_best, metric = &amp;quot;accuracy&amp;quot;, n = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 1 x 7
##    penalty mixture .metric  .estimator  mean     n std_err
##      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 6.57e-10  0.0655 accuracy binary     0.916    10 0.00179
## 
## [[2]]
## # A tibble: 1 x 7
##    mtry trees .metric  .estimator  mean     n std_err
##   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1    13  1991 accuracy binary     0.929    10 0.00172
## 
## [[3]]
## # A tibble: 1 x 7
##   num_terms prune_method .metric  .estimator  mean     n std_err
##       &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1         5 backward     accuracy binary     0.904    10 0.00186
## 
## [[4]]
## # A tibble: 1 x 9
##    mtry trees tree_depth learn_rate .metric  .estimator  mean     n std_err
##   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1    12  1245         12     0.0770 accuracy binary     0.929    10 0.00175
## 
## [[5]]
## # A tibble: 1 x 7
##   hidden_units    penalty .metric  .estimator  mean     n std_err
##          &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1           10 0.00000307 accuracy binary     0.917    10 0.00209&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Seems like the second model, the random forest performed the best (highest mean accuracy with lowest
standard error). So let’s retrain the model on the whole training set and see how it fares on the
testing set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_specs &amp;lt;- trained_models_list[[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s save the best model specification in a variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_rf_spec &amp;lt;- show_best(rf_specs, &amp;quot;accuracy&amp;quot;, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now retrain this model, using a workflow:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_rf_model &amp;lt;- rand_forest(mode = &amp;quot;classification&amp;quot;, mtry = best_rf_spec$mtry,
                           trees = best_rf_spec$trees) %&amp;gt;%
  set_engine(&amp;quot;ranger&amp;quot;)

preprocess &amp;lt;- recipe(job_search ~ ., data = pra) %&amp;gt;%
  step_dummy(all_predictors())

pra_wflow_best &amp;lt;- workflow() %&amp;gt;%
  add_recipe(preprocess) %&amp;gt;%
  add_model(best_rf_model)

best_model_fitted &amp;lt;- fit(pra_wflow_best, data = pra_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The following variables are not factor vectors and will be ignored:
## `hours`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and let’s take a look at the confusion matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- predict(best_model_fitted, new_data = pra_test) %&amp;gt;%
  bind_cols(pra_test)

predictions %&amp;gt;%
  mutate(job_search = as.factor(job_search)) %&amp;gt;%  
  accuracy(job_search, .pred_class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.924&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions %&amp;gt;%
  mutate(job_search = as.factor(job_search)) %&amp;gt;%  
  conf_mat(job_search, .pred_class) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Truth
## Prediction    N    S
##          N 2539  156
##          S   64  149&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that predicting class &lt;code&gt;S&lt;/code&gt; (“Si”, meaning, “yes” in Spanish) is tricky. One would probably need
to use techniques such as &lt;code&gt;SMOTE&lt;/code&gt; to deal with this (see this &lt;a href=&#34;https://www.brodrigues.co/blog/2018-02-11-census-random_forest/&#34;&gt;blog post&lt;/a&gt;
for more info). Anyways, this is not today’s topic.&lt;/p&gt;
&lt;p&gt;Let’s say that we are satisfied with the model and want some explanations out of it. I have already
blogged about it in the past, so if you want more details, you can read this &lt;a href=&#34;https://www.brodrigues.co/blog/2018-02-11-census-random_forest/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, what is important, is that I have defined a complete workflow to deal with the data preprocessing
and then the training of the model. So I’ll be using this workflow as well to get my explainability. What I mean
with this is the following: to get explanations, we need a model, and a way to get predictions out
of it. As I have shown before, my fitted workflow is able to give me predictions. So I should have
every needed ingredient; &lt;code&gt;{iml}&lt;/code&gt;, the package that I am using for explainability provides several
functions that work all the same; you first define an object that takes as an input the fitted model,
the design matrix, the target variable and the prediction function. Let’s start with defining the
design matrix and the target variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;iml&amp;quot;)

features &amp;lt;- pra_test %&amp;gt;%
  select(-job_search)

target &amp;lt;- pra_test %&amp;gt;%
  mutate(job_search = as.factor(job_search)) %&amp;gt;%  
  select(job_search)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s define the predict function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict_wrapper &amp;lt;- function(model, newdata){
  workflows:::predict.workflow(object = model, new_data = newdata)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because a workflow is a bit special, I need to define this wrapper function that wraps the
&lt;code&gt;workflows:::predict.workflow()&lt;/code&gt; function. Normally, users should not have to deal with this function;
as you can see, to access it I had to use the very special &lt;code&gt;:::&lt;/code&gt; function. &lt;code&gt;:::&lt;/code&gt; permits users
to access &lt;em&gt;private&lt;/em&gt; functions (not sure if this is the right term; what I mean is that private functions
are used internally by the package and should not be available to users. AFAIK, this is how these
functions are called in Python). I tried simply using the &lt;code&gt;predict()&lt;/code&gt; function, which works interactively
but I was getting issues with it when I was providing it to the constructor below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictor &amp;lt;- Predictor$new(
                         model = best_model_fitted,
                         data = features, 
                         y = target,
                         predict.fun = predict_wrapper
                       )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a &lt;code&gt;Predictor&lt;/code&gt; object from which I am now able to get explanations. For example, for
feature importance, I would write the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;feature_importance &amp;lt;- FeatureImp$new(predictor, loss = &amp;quot;ce&amp;quot;)

plot(feature_importance)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-10-exp_tidymodels_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And this is where I noticed that something was wrong; the variables we are looking at are
categorical variables. So why am I not seeing the categories? Why is the most important variable
the contract type, without the category of the contract type that is the most important?
Remember that I created dummy variables using a recipe. So I was expecting something like
&lt;code&gt;type_of_contract_type_1&lt;/code&gt;, &lt;code&gt;type_of_contract_type_2&lt;/code&gt;, etc… as variables.&lt;/p&gt;
&lt;p&gt;This made me want to try to fit the model “the old way”, without using workflows. So for this
I need to use the &lt;code&gt;prep()&lt;/code&gt;, &lt;code&gt;juice()&lt;/code&gt; and &lt;code&gt;bake()&lt;/code&gt; functions, which are included in the &lt;code&gt;{recipes}&lt;/code&gt;
package. I won’t go into much detail, but the idea is that &lt;code&gt;prep()&lt;/code&gt; is used to train the recipe, and
compute whatever is needed to preprocess the data (such as means and standard deviations for
normalization). For this, you should use the training data only. &lt;code&gt;juice()&lt;/code&gt; returns the preprocessed
training set, and &lt;code&gt;bake()&lt;/code&gt; is then used to preprocessed a new data set, for instance the test set,
using the same estimated parameters that were obtained with &lt;code&gt;prep()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Using workflows avoids having to do these steps manually, but what I am hoping is that doing this
manually will solve my issue. So let’s try:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# without workflows
trained_recipe &amp;lt;- prep(preprocess, training = pra_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The following variables are not factor vectors and will be ignored:
## `hours`&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pra_train_prep &amp;lt;- juice(trained_recipe)


best_model_fit &amp;lt;- fit(best_rf_model, job_search ~ ., data = pra_train_prep)


pra_test_bake_features &amp;lt;- bake(trained_recipe, pra_test) %&amp;gt;%
  select(-job_search)


predict_wrapper2 &amp;lt;- function(model, newdata){
  predict(object = model, new_data = newdata)
}

predictor2 &amp;lt;- Predictor$new(
                          model = best_model_fit,
                          data = pra_test_bake_features, 
                          y = target,
                          predict.fun = predict_wrapper2
                        )

feature_importance2 &amp;lt;- FeatureImp$new(predictor2, loss = &amp;quot;ce&amp;quot;)

plot(feature_importance2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-10-exp_tidymodels_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Eureka! As you can see, the issue is now solved; we now have all the variables that were used
for training the model, also in our explanations. I don’t know exactly what’s going on; is this a
bug? Is it because the &lt;code&gt;{workflows}&lt;/code&gt; package makes this process too streamlined that it somehow
&lt;em&gt;rebuilds&lt;/em&gt; the features and then returns the results? I have no idea. In any case, it
would seem that for the time being, doing the training and explanations without the &lt;code&gt;{workflows}&lt;/code&gt;
package is the way to go if you require explanations as well.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and watch my
&lt;a href=&#34;https://www.youtube.com/channel/UCTZXht1RTL2Duc3eU8MYGzQ&#34;&gt;youtube channel&lt;/a&gt;. If you want to support
my blog and channel, you could &lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or
&lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine learning with {tidymodels}</title>
      <link>/blog/2020-03-08-tidymodels/</link>
      <pubDate>Sun, 08 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-03-08-tidymodels/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://autonxt.net/bosozoku-japans-car-tuning-subculture/&#34;&gt;
&lt;img src=&#34;/img/jap_tune.jpg&#34; title = &#34;Just because you tune your models, doesn&#39;t mean you can&#39;t overfit&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
&lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;intro-what-is-tidymodels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro: what is &lt;code&gt;{tidymodels}&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;I have already written about &lt;code&gt;{tidymodels}&lt;/code&gt; in the &lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-25-tidy_cv/&#34;&gt;past&lt;/a&gt;
but since then, the &lt;code&gt;{tidymodels}&lt;/code&gt; meta-package has evolved quite a lot. If you don’t know what
&lt;code&gt;{tidymodels}&lt;/code&gt; is, it is a suite of packages that make machine learning with R a breeze. R has many
packages for machine learning, each with their own syntax and function arguments. &lt;code&gt;{tidymodels}&lt;/code&gt; aims
at providing an unified interface which allows data scientists to focus on the problem they’re trying
to solve, instead of wasting time with learning package specificities.&lt;/p&gt;
&lt;p&gt;The packages included in &lt;code&gt;{tidymodels}&lt;/code&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://tidymodels.github.io/parsnip/articles/parsnip_Intro.html&#34;&gt;{parsnip}&lt;/a&gt; for model definition&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tidymodels.github.io/recipes/&#34;&gt;{recipes}&lt;/a&gt; for data preprocessing and feature engineering&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tidymodels.github.io/rsample/&#34;&gt;{rsample}&lt;/a&gt; to resample data (useful for cross-validation)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tidymodels.github.io/yardstick/index.html&#34;&gt;{yardstick}&lt;/a&gt; to evaluate model performance&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tidymodels.github.io/dials/index.html&#34;&gt;{dials}&lt;/a&gt; to define tuning parameters of your models&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tidymodels.github.io/tune/&#34;&gt;{tune}&lt;/a&gt; for model tuning&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://tidymodels.github.io/workflows/&#34;&gt;{workflows}&lt;/a&gt; which allows you to bundle everything together and train models easily&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some others, but I will not cover these. This is a lot of packages, and you might be
worried of getting lost; however, in practice I noticed that loading &lt;code&gt;{tidymodels}&lt;/code&gt; and then using
the functions I needed was good enough. Only rarely did I need to know from which package a certain
function came, and the more you use these, the better you know them, obviously. Before continuing,
one final and important note: these packages are still in heavy development, so you might not want
to use them in production yet. I don’t know how likely it is that the api still evolves, but my guess
is that it is likely. However, even though it might be a bit early to use these packages for production
code, I think it is important to learn about them as soon as possible and see what is possible with them.&lt;/p&gt;
&lt;p&gt;As I will show you, these packages do make the process of training machine learning models a breeze, and of
course they integrate very well with the rest of the &lt;code&gt;{tidyverse}&lt;/code&gt; packages. The problem we’re going
to tackle is to understand which variables play an important role in the probability of someone looking
for a job. I’ll use Eustat’s microdata, which I already discussed in my &lt;a href=&#34;https://www.brodrigues.co/blog/2020-02-23-synthpop/&#34;&gt;previous blog post&lt;/a&gt;.
The dataset can be downloaded from &lt;a href=&#34;https://en.eustat.eus/estadisticas/tema_37/opt_0/tipo_11/temas.html&#34;&gt;here&lt;/a&gt;, and is called
&lt;em&gt;Population with relation to activity (PRA)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem-at-hand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problem at hand&lt;/h2&gt;
&lt;p&gt;The dataset contains information on residents from the Basque country, and focuses on their labour supply.
Thus, we have information on how many hours people work a week, if they work, in which industry, what
is their educational attainment and whether they’re looking for a job.
The first step, as usual, is to load the data and required packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(readxl)
library(naniar)
library(janitor)
library(furrr)

list_data &amp;lt;- Sys.glob(&amp;quot;~/Documents/b-rodrigues.github.com/content/blog/MICRO*.csv&amp;quot;)

dataset &amp;lt;- map(list_data, read_csv2) %&amp;gt;%
  bind_rows()

dictionary &amp;lt;- read_xlsx(&amp;quot;~/Documents/b-rodrigues.github.com/content/blog/Microdatos_PRA_2019/diseño_registro_microdatos_pra.xlsx&amp;quot;, sheet=&amp;quot;Valores&amp;quot;,
                        col_names = FALSE)

col_names &amp;lt;- dictionary %&amp;gt;%
  filter(!is.na(...1)) %&amp;gt;%
  dplyr::select(1:2)

english &amp;lt;- readRDS(&amp;quot;~/Documents/b-rodrigues.github.com/content/blog/english_col_names.rds&amp;quot;)

col_names$english &amp;lt;- english

colnames(dataset) &amp;lt;- col_names$english

dataset &amp;lt;- janitor::clean_names(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 33
##   household_number survey_year reference_quart… territory capital   sex
##              &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1                1        2019                1 48              9     6
## 2                1        2019                1 48              9     1
## 3                2        2019                1 48              1     1
## 4                2        2019                1 48              1     6
## 5                2        2019                1 48              1     6
## 6                2        2019                1 48              1     1
## # … with 27 more variables: place_of_birth &amp;lt;dbl&amp;gt;, age &amp;lt;chr&amp;gt;, nationality &amp;lt;dbl&amp;gt;,
## #   level_of_studies_completed &amp;lt;dbl&amp;gt;, ruled_teaching_system &amp;lt;chr&amp;gt;,
## #   occupational_training &amp;lt;chr&amp;gt;, retirement_situation &amp;lt;dbl&amp;gt;,
## #   homework_situation &amp;lt;dbl&amp;gt;, part_time_employment &amp;lt;dbl&amp;gt;,
## #   short_time_cause &amp;lt;dbl&amp;gt;, job_search &amp;lt;chr&amp;gt;, search_reasons &amp;lt;dbl&amp;gt;,
## #   day_searched &amp;lt;dbl&amp;gt;, make_arrangements &amp;lt;chr&amp;gt;, search_form &amp;lt;chr&amp;gt;,
## #   search_months &amp;lt;dbl&amp;gt;, availability &amp;lt;chr&amp;gt;,
## #   relationship_with_the_activity &amp;lt;dbl&amp;gt;,
## #   relationship_with_the_activity_2 &amp;lt;chr&amp;gt;, main_occupation &amp;lt;dbl&amp;gt;,
## #   main_activity &amp;lt;chr&amp;gt;, main_professional_situation &amp;lt;dbl&amp;gt;,
## #   main_institutional_sector &amp;lt;dbl&amp;gt;, type_of_contract &amp;lt;dbl&amp;gt;, hours &amp;lt;dbl&amp;gt;,
## #   relationship &amp;lt;dbl&amp;gt;, elevator &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are many columns, most of them are categorical variables and unfortunately the levels in the
data are only some non-explicit codes. The excel file I have loaded, which I called &lt;code&gt;dictionary&lt;/code&gt;
contains the codes and their explanation. I kept the file opened while I was working, especially for
missing values imputation. Indeed, there are missing values in the data, and one should always try
to understand why before blindly imputing them. Indeed, there might be a very good reason why data
might be missing for a particular column. For instance, if children are also surveyed, they would
have an &lt;code&gt;NA&lt;/code&gt; in the, say, &lt;code&gt;main_occupation&lt;/code&gt; column which gives the main occupation of the surveyed
person. This might seem very obvious, but sometimes these reasons are not so obvious at all. You should
always go back with such questions to the data owners/producers, because if not, you will certainly
miss something very important. Anyway, the way I tackled this issue was by looking at the variables
with missing data and checking two-way tables with other variables. For instance, to go back to my
example from before, I would take a look at the two-way frequency table between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;main_occupation&lt;/code&gt;.
If all the missing values from &lt;code&gt;main_occupation&lt;/code&gt; where only for people 16 or younger, then it would
be quite safe to assume that I was right, and I could recode these &lt;code&gt;NA&lt;/code&gt;s in &lt;code&gt;main_occupation&lt;/code&gt; to
&lt;code&gt;&amp;quot;without occupation&amp;quot;&lt;/code&gt; for instance. I’ll spare you all this exploration, and go straight to the
data cleaning:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- dataset %&amp;gt;%
  mutate(main_occupation2 = ifelse(is.na(main_occupation),
                                   &amp;quot;without_occupation&amp;quot;,
                                   main_occupation))

dataset &amp;lt;- dataset %&amp;gt;%
  mutate(main_professional_situation2 = ifelse(is.na(main_professional_situation),
                                               &amp;quot;without_occupation&amp;quot;,
                                               main_professional_situation))

# People with missing hours are actually not working, so I put them to 0
dataset &amp;lt;- dataset %&amp;gt;%
  mutate(hours = ifelse(is.na(hours), 0, hours))

# Short time gives the reason why people are working less hours than specified in their contract
dataset &amp;lt;- dataset %&amp;gt;%
  mutate(short_time_cause = ifelse(hours == 0 | is.na(short_time_cause), 
                                   &amp;quot;without_occupation&amp;quot;,
                                   short_time_cause))

dataset &amp;lt;- dataset %&amp;gt;%
  mutate(type_of_contract = ifelse(is.na(type_of_contract),
                                   &amp;quot;other_contract&amp;quot;,
                                   type_of_contract))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now apply some further cleaning:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pra &amp;lt;- dataset %&amp;gt;%
  filter(age %in% c(&amp;quot;04&amp;quot;, &amp;quot;05&amp;quot;, &amp;quot;06&amp;quot;, &amp;quot;07&amp;quot;, &amp;quot;08&amp;quot;, &amp;quot;09&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;11&amp;quot;, &amp;quot;12&amp;quot;, &amp;quot;13&amp;quot;)) %&amp;gt;%
  filter(retirement_situation == 4) %&amp;gt;%    
  filter(!is.na(job_search)) %&amp;gt;%  
  select(capital, sex, place_of_birth, age, nationality, level_of_studies_completed,
         occupational_training, job_search, main_occupation2, type_of_contract,
         hours, short_time_cause, homework_situation,
         main_professional_situation2) %&amp;gt;%
  mutate_at(.vars = vars(-hours), .funs=as.character) %&amp;gt;%
  mutate(job_search = as.factor(job_search))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I only keep people that are not retired and of ages where they could work. I remove rows where
&lt;code&gt;job_search&lt;/code&gt;, the target, is missing, mutate all variables but &lt;code&gt;hours&lt;/code&gt; to character and &lt;code&gt;job_search&lt;/code&gt; to factor. At
first, I made every categorical column a factor but I got problems for certain models. I think the
issue came from the recipe that I defined (I’ll talk about it below), but the problem was resolved
if categorical variables were defined as character variables. However, for certain models, the target
(I think it was &lt;code&gt;xgboost&lt;/code&gt;) needs to be a factor variable for classification problems.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the data and check if any more data is missing:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(pra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;spec_tbl_df&amp;#39;, &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;: 29083 obs. of  14 variables:
##  $ capital                     : chr  &amp;quot;9&amp;quot; &amp;quot;9&amp;quot; &amp;quot;1&amp;quot; &amp;quot;1&amp;quot; ...
##  $ sex                         : chr  &amp;quot;6&amp;quot; &amp;quot;1&amp;quot; &amp;quot;1&amp;quot; &amp;quot;6&amp;quot; ...
##  $ place_of_birth              : chr  &amp;quot;1&amp;quot; &amp;quot;1&amp;quot; &amp;quot;1&amp;quot; &amp;quot;1&amp;quot; ...
##  $ age                         : chr  &amp;quot;09&amp;quot; &amp;quot;09&amp;quot; &amp;quot;11&amp;quot; &amp;quot;10&amp;quot; ...
##  $ nationality                 : chr  &amp;quot;1&amp;quot; &amp;quot;1&amp;quot; &amp;quot;1&amp;quot; &amp;quot;1&amp;quot; ...
##  $ level_of_studies_completed  : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;3&amp;quot; ...
##  $ occupational_training       : chr  &amp;quot;N&amp;quot; &amp;quot;N&amp;quot; &amp;quot;N&amp;quot; &amp;quot;N&amp;quot; ...
##  $ job_search                  : Factor w/ 2 levels &amp;quot;N&amp;quot;,&amp;quot;S&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ main_occupation2            : chr  &amp;quot;5&amp;quot; &amp;quot;7&amp;quot; &amp;quot;3&amp;quot; &amp;quot;2&amp;quot; ...
##  $ type_of_contract            : chr  &amp;quot;1&amp;quot; &amp;quot;other_contract&amp;quot; &amp;quot;other_contract&amp;quot; &amp;quot;1&amp;quot; ...
##  $ hours                       : num  36 40 40 40 0 0 22 38 40 0 ...
##  $ short_time_cause            : chr  &amp;quot;2&amp;quot; &amp;quot;2&amp;quot; &amp;quot;2&amp;quot; &amp;quot;2&amp;quot; ...
##  $ homework_situation          : chr  &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;2&amp;quot; &amp;quot;2&amp;quot; ...
##  $ main_professional_situation2: chr  &amp;quot;4&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vis_miss(pra)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-03-08-tidymodels_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The final dataset contains 29083 observations. Look’s like we’re good to go.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-up-the-training-resampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up the training: resampling&lt;/h2&gt;
&lt;p&gt;In order to properly train a model, one needs to split the data into two: a part for trying out
models with different configuration of hyper-parameters, and another part for final evaluation of
the model. This is achieved with &lt;code&gt;rsample::initial_split()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pra_split &amp;lt;- initial_split(pra, prop = 0.9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;pra_split&lt;/code&gt; now contains a training set and a testing set. We can get these by using the
&lt;code&gt;rsample::training()&lt;/code&gt; and &lt;code&gt;rsample::testing()&lt;/code&gt; functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pra_train &amp;lt;- training(pra_split)
pra_test &amp;lt;- testing(pra_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can’t stop here though. First we need to split the training set further, in order to perform
cross validation. Cross validation will allow us to select the best model; by best I mean a model
that has a good hyper-parameter configuration, enabling the model to generalize well to unseen data.
I do this by creating 10 splits from the training data (I won’t touch the testing data up until
the very end. This testing data is thus sometimes called the holdout set as well):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pra_cv_splits &amp;lt;- vfold_cv(pra_train, v = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at this object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pra_cv_splits&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## #  10-fold cross-validation 
## # A tibble: 10 x 2
##    splits               id    
##    &amp;lt;named list&amp;gt;         &amp;lt;chr&amp;gt; 
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocessing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocessing the data&lt;/h2&gt;
&lt;p&gt;I have already pre-processed the missing values in the dataset, so there is not much more that
I can do. I will simply create dummy variables out of the categorical variables using &lt;code&gt;step_dummy()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preprocess &amp;lt;- recipe(job_search ~ ., data = pra) %&amp;gt;%
  step_dummy(all_predictors())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;preprocess&lt;/code&gt; is a recipe that defines the transformations that must be applied to the training data
before fitting. In this case there is only one step; transforming all the predictors into dummies
(&lt;code&gt;hours&lt;/code&gt; is a numeric variable and will be ignored by this step). The recipe also defines the
formula that will be fitted by the models, &lt;code&gt;job_search ~ .&lt;/code&gt;, and takes &lt;code&gt;data&lt;/code&gt; as a further argument.
This is only to give the data frame specification to &lt;code&gt;recipe()&lt;/code&gt;: it could even be an empty data frame
with the right column names and types. This is why I give it the original data &lt;code&gt;pra&lt;/code&gt; and not the
training set &lt;code&gt;pra_train&lt;/code&gt;. Because this recipe is very simple, it could be applied to the original
raw data &lt;code&gt;pra&lt;/code&gt; and then I could do the split into training and testing set, as well as further
splitting the training set into 10 cross-validation sets. However, this is not the recommended way
of applying pre-processing steps. Pre-processing needs to happen inside the cross-validation loop,
not outside of it. Why? Suppose that you are normalizing a numeric variable, meaning, substracting
its mean from it and dividing by its standard deviation. If you do this operation outside of
cross-validation, and even worse, before splitting the data into training and testing set, you will
be leaking information from the testing set into the training set. The mean will contain information
from the testing set, which will be picked up by the model.
It is much better and “realistic” to first split the data and then apply
the pre-processing (remember that &lt;em&gt;hiding&lt;/em&gt; the test set from the model is supposed to simulate
the fact that new, completely unseen data, is thrown at your model once it’s put into production). The
same logic applies to cross-validation splits; each split contains now also a training and a testing
set (which I will be calling analysis and assessment sets, following &lt;code&gt;{tidymodels}&lt;/code&gt;’s author,
&lt;a href=&#34;https://twitter.com/topepos/status/1066131042615140353?s=20&#34;&gt;Max Kuhn&lt;/a&gt;) and thus the pre-processing
needs to be applied inside the cross-validation loop, meaning that the analysis set will be processed
on the fly.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-definition&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model definition&lt;/h2&gt;
&lt;p&gt;We come now to the very interesting part: model definition. With &lt;code&gt;{parsnip}&lt;/code&gt;, another &lt;code&gt;{tidymodels}&lt;/code&gt;
package, defining models is always the same, regardless of the underlying package doing the heavy
lifting. For instance, to define a logistic regression one would simply write:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# logistic regression 
logit_tune_pra &amp;lt;- logistic_reg() %&amp;gt;%
  set_engine(&amp;quot;glm&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This defines a standard logistic regression, powered by the &lt;code&gt;glm()&lt;/code&gt; &lt;em&gt;engine&lt;/em&gt; or function. The way
to do this in vanilla R would be :&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm(y ~ ., data = mydata, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The difference here is that the formula is contained in the &lt;code&gt;glm()&lt;/code&gt; function; in our case it is
contained in the recipe, which is why I don’t repeat it in the model definition above. You might
wonder what the added value of using &lt;code&gt;{tidymodels}&lt;/code&gt; for this is. Well, suppose now that I would like
to run a logistic regression but with regularization. I would use &lt;code&gt;{glmnet}&lt;/code&gt; for this but would need
to know the specific syntax of &lt;code&gt;glmnet()&lt;/code&gt; which, as you will see, is very different than the one
for &lt;code&gt;glm()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = 1.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;glmnet()&lt;/code&gt;, unlike &lt;code&gt;glm()&lt;/code&gt;, does not use a formula as an input, but two matrices, one for the design
matrix, and another for the target variable. Using &lt;code&gt;{parsnip}&lt;/code&gt;, however, I simply need to change the
engine from &lt;code&gt;&amp;quot;glm&amp;quot;&lt;/code&gt; to &lt;code&gt;&amp;quot;glmnet&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# logistic regression 
logit_tune_pra &amp;lt;- logistic_reg() %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This makes things much simpler as now users only need to learn how to use &lt;code&gt;{parsnip}&lt;/code&gt;. However,
it is of course still important to read the documentation of the original packages, because it is
were hyper-parameters are discussed. Another advantage of &lt;code&gt;{parsnip}&lt;/code&gt; is that the same words
are used to speak of the same hyper-parameters . For instance for tree-based methods, the number of
trees is sometimes &lt;code&gt;ntree&lt;/code&gt; then in another package &lt;code&gt;num_trees&lt;/code&gt;, and is again different in yet another package.
In &lt;code&gt;{parsnip}&lt;/code&gt;’s interface for tree-based methods, this parameter is simply
called &lt;code&gt;tree&lt;/code&gt;. Users can fix the value of hyper-parameters directly by passing values to, say, &lt;code&gt;tree&lt;/code&gt;
(as in &lt;code&gt;&amp;quot;tree&amp;quot; = 200&lt;/code&gt;), or they can tune these hyper-parameters. To do so, one needs to tag them, like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# logistic regression 
logit_tune_pra &amp;lt;- logistic_reg(penalty = tune(), mixture = tune()) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This defines &lt;code&gt;logit_tune_pra&lt;/code&gt; with 2 hyper-parameters that must be tuned using cross-validation,
the penalty and the amount of mixture between penalties (this is for elasticnet regularization).&lt;/p&gt;
&lt;p&gt;Now, I will define 5 different models, with different hyper-parameters to tune, and I will also
define a grid of hyper-parameters of size 10 for each model. This means that I will train these 5
models 10 times, each time with a different hyper-parameter configuration. To define the grid, I use
the &lt;code&gt;grid_max_entropy()&lt;/code&gt; function from the &lt;code&gt;{dials}&lt;/code&gt; package. This creates a grid with points that
are randomly drawn from the parameter space in a way that ensures that the combination we get
covers the whole space, or at least are not too far away from any portion of the space. Of course,
the more configuration you try, the better, but the longer the training will run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Logistic regression
logit_tune_pra &amp;lt;- logistic_reg(penalty = tune(), mixture = tune()) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)

# Hyperparameter grid
logit_grid &amp;lt;- logit_tune_pra %&amp;gt;%
  parameters() %&amp;gt;%
  grid_max_entropy(size = 10)

# Workflow bundling every step 
logit_wflow &amp;lt;- workflow() %&amp;gt;%
  add_recipe(preprocess) %&amp;gt;%
  add_model(logit_tune_pra)

# random forest
rf_tune_pra &amp;lt;- rand_forest(mtry = tune(), trees = tune()) %&amp;gt;%
  set_engine(&amp;quot;ranger&amp;quot;) %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;)

rf_grid &amp;lt;- rf_tune_pra %&amp;gt;%
  parameters() %&amp;gt;%
  finalize(select(pra, -job_search)) %&amp;gt;%  
  grid_max_entropy(size = 10)

rf_wflow &amp;lt;- workflow() %&amp;gt;%
  add_recipe(preprocess) %&amp;gt;%
  add_model(rf_tune_pra)

# mars model
mars_tune_pra &amp;lt;- mars(num_terms = tune(), prod_degree = 2, prune_method = tune()) %&amp;gt;%
  set_engine(&amp;quot;earth&amp;quot;) %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;)

mars_grid &amp;lt;- mars_tune_pra %&amp;gt;%
  parameters() %&amp;gt;%
  grid_max_entropy(size = 10)

mars_wflow &amp;lt;- workflow() %&amp;gt;%
  add_recipe(preprocess) %&amp;gt;%
  add_model(mars_tune_pra)

#boosted trees
boost_tune_pra &amp;lt;- boost_tree(mtry = tune(), tree = tune(),
                             learn_rate = tune(), tree_depth = tune()) %&amp;gt;%
  set_engine(&amp;quot;xgboost&amp;quot;) %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;)

boost_grid &amp;lt;- boost_tune_pra %&amp;gt;%
  parameters() %&amp;gt;%
  finalize(select(pra, -job_search)) %&amp;gt;%  
  grid_max_entropy(size = 10)

boost_wflow &amp;lt;- workflow() %&amp;gt;%
  add_recipe(preprocess) %&amp;gt;%
  add_model(boost_tune_pra)

#neural nets
keras_tune_pra &amp;lt;- mlp(hidden_units = tune(), penalty = tune(), activation = &amp;quot;relu&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;keras&amp;quot;) %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;)

keras_grid &amp;lt;- keras_tune_pra %&amp;gt;%
  parameters() %&amp;gt;%
  grid_max_entropy(size = 10)

keras_wflow &amp;lt;- workflow() %&amp;gt;%
  add_recipe(preprocess) %&amp;gt;%
  add_model(keras_tune_pra)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each model, I defined three objects; the model itself, for instance &lt;code&gt;keras_tune_pra&lt;/code&gt;, then a
grid of hyper-parameters, and finally a workflow. To define the grid, I need to extract the parameters
to tune using the &lt;code&gt;parameters()&lt;/code&gt; function, and for tree based methods, I also need to use &lt;code&gt;finalize()&lt;/code&gt;
to set the &lt;code&gt;mtry&lt;/code&gt; parameter. This is because &lt;code&gt;mtry&lt;/code&gt; depends on the dimensions of the data (the value
of &lt;code&gt;mtry&lt;/code&gt; cannot be larger than the number of features), so I need to pass on this information
to…well, finalize the grid. Then I can choose the size of the grid and how I want to create it
(randomly, or using max entropy, or regularly spaced…).
A workflow bundles the pre-processing and the model definition together, and makes fitting the model
very easy. Workflows make it easy to run the pre-processing inside the cross-validation loop.
Workflow objects can be passed to the fitting function, as we shall see in the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-models-with-tidymodels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting models with &lt;code&gt;{tidymodels}&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Fitting one model with &lt;code&gt;{tidymodels}&lt;/code&gt; is quite easy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_model &amp;lt;- fit(model_formula, data = data_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and that’s it. If you define a workflow, which bundles pre-processing and model definition
in one package, you need to pass it to &lt;code&gt;fit()&lt;/code&gt; as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_wflow &amp;lt;- fit(model_wflow, data = data_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, a single call to fit does not perform cross-validation. This simply trains the model on
the training data, and that’s it. To perform cross validation, you can use either &lt;code&gt;fit_resamples()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitted_resamples &amp;lt;- fit_resamples(model_wflow,
                               resamples = my_cv_splits,
                               control = control_resamples(save_pred = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or &lt;code&gt;tune_grid()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tuned_model &amp;lt;- tune_grid(model_wflow,
                         resamples = my_cv_splits,
                         grid = my_grid,
                         control = control_resamples(save_pred = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you probably guessed it, &lt;code&gt;fit_resamples()&lt;/code&gt; does not perform tuning; it simply fits a model
specification (without varying hyper-parameters) to all the analysis sets contained in the
&lt;code&gt;my_cv_splits&lt;/code&gt; object (which contains the resampled training data for cross-validation), while
&lt;code&gt;tune_grid()&lt;/code&gt; does the same, but allows for varying hyper-parameters.&lt;/p&gt;
&lt;p&gt;We thus are going to use &lt;code&gt;tune_grid()&lt;/code&gt; to fit our models and perform hyper-paramater tuning.
However, since I have 5 models and 5 grids, I’ll be using &lt;code&gt;map2()&lt;/code&gt; for this. If you’re not familiar
with &lt;code&gt;map2()&lt;/code&gt;, here’s a quick example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map2(c(1, 1, 1), c(2,2,2), `+`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 3
## 
## [[2]]
## [1] 3
## 
## [[3]]
## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;map2()&lt;/code&gt; maps the &lt;code&gt;+()&lt;/code&gt; function to each element of both vectors successively. I’m going to use
this to map the &lt;code&gt;tune_grid()&lt;/code&gt; function to a list of models and a list of grids. But because this is
going to take some time to run, and because I have an AMD Ryzen 5 1600X processor with 6 physical
cores and 12 logical cores, I’ll by running this in parallel using &lt;code&gt;furrr::future_map2()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;furrr::future_map2()&lt;/code&gt; will run one model per core, and the way to do it is to simply define
how many cores I want to use, then replace &lt;code&gt;map2()&lt;/code&gt; in my code by &lt;code&gt;future_map2()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wflow_list &amp;lt;- list(logit_wflow, rf_wflow, mars_wflow, boost_wflow, keras_wflow)
grid_list &amp;lt;- list(logit_grid, rf_grid, mars_grid, boost_grid, keras_grid)

plan(multiprocess, workers = 6)

trained_models_list &amp;lt;- future_map2(.x = wflow_list,
                                   .y = grid_list,
                                   ~tune_grid(.x , resamples = pra_cv_splits, grid = .y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running this code took almost 3 hours. In the end, here is the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trained_models_list&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 
## [[2]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 
## [[3]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 
## [[4]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 7]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 
## [[5]]
## #  10-fold cross-validation 
## # A tibble: 10 x 4
##    splits               id     .metrics          .notes          
##  * &amp;lt;list&amp;gt;               &amp;lt;chr&amp;gt;  &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;          
##  1 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold01 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  2 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold02 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  3 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold03 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  4 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold04 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  5 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold05 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  6 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold06 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  7 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold07 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  8 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold08 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
##  9 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold09 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;
## 10 &amp;lt;split [23.6K/2.6K]&amp;gt; Fold10 &amp;lt;tibble [20 × 5]&amp;gt; &amp;lt;tibble [1 × 1]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now have a list of 5 tibbles containing the analysis/assessment splits, the id identifying the
cross-validation fold, a list-column containing information on model performance for that given
split and some notes (if everything goes well, notes are empty). Let’s take a look at the column
&lt;code&gt;.metrics&lt;/code&gt; of the first model and for the first fold:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trained_models_list[[1]]$.metrics[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 5
##     penalty mixture .metric  .estimator .estimate
##       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
##  1 4.25e- 3  0.0615 accuracy binary         0.906
##  2 4.25e- 3  0.0615 roc_auc  binary         0.895
##  3 6.57e-10  0.0655 accuracy binary         0.908
##  4 6.57e-10  0.0655 roc_auc  binary         0.897
##  5 1.18e- 6  0.167  accuracy binary         0.908
##  6 1.18e- 6  0.167  roc_auc  binary         0.897
##  7 2.19e-10  0.371  accuracy binary         0.907
##  8 2.19e-10  0.371  roc_auc  binary         0.897
##  9 2.73e- 1  0.397  accuracy binary         0.885
## 10 2.73e- 1  0.397  roc_auc  binary         0.5  
## 11 1.72e- 6  0.504  accuracy binary         0.907
## 12 1.72e- 6  0.504  roc_auc  binary         0.897
## 13 1.25e- 9  0.633  accuracy binary         0.907
## 14 1.25e- 9  0.633  roc_auc  binary         0.897
## 15 6.62e- 6  0.880  accuracy binary         0.907
## 16 6.62e- 6  0.880  roc_auc  binary         0.897
## 17 6.00e- 1  0.899  accuracy binary         0.885
## 18 6.00e- 1  0.899  roc_auc  binary         0.5  
## 19 4.57e-10  0.989  accuracy binary         0.907
## 20 4.57e-10  0.989  roc_auc  binary         0.897&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows how the 10 different configurations of the elasticnet model performed. To see how the
model performed on the second fold:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trained_models_list[[1]]$.metrics[[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 5
##     penalty mixture .metric  .estimator .estimate
##       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
##  1 4.25e- 3  0.0615 accuracy binary         0.913
##  2 4.25e- 3  0.0615 roc_auc  binary         0.874
##  3 6.57e-10  0.0655 accuracy binary         0.913
##  4 6.57e-10  0.0655 roc_auc  binary         0.877
##  5 1.18e- 6  0.167  accuracy binary         0.913
##  6 1.18e- 6  0.167  roc_auc  binary         0.878
##  7 2.19e-10  0.371  accuracy binary         0.913
##  8 2.19e-10  0.371  roc_auc  binary         0.878
##  9 2.73e- 1  0.397  accuracy binary         0.901
## 10 2.73e- 1  0.397  roc_auc  binary         0.5  
## 11 1.72e- 6  0.504  accuracy binary         0.913
## 12 1.72e- 6  0.504  roc_auc  binary         0.878
## 13 1.25e- 9  0.633  accuracy binary         0.913
## 14 1.25e- 9  0.633  roc_auc  binary         0.878
## 15 6.62e- 6  0.880  accuracy binary         0.913
## 16 6.62e- 6  0.880  roc_auc  binary         0.878
## 17 6.00e- 1  0.899  accuracy binary         0.901
## 18 6.00e- 1  0.899  roc_auc  binary         0.5  
## 19 4.57e-10  0.989  accuracy binary         0.913
## 20 4.57e-10  0.989  roc_auc  binary         0.878&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hyper-Parameters are the same; it is only the cross validation fold that is different. To get the
best performing model from such objects you can use &lt;code&gt;show_best()&lt;/code&gt; which will extract the best
performing models across all the cross validation folds:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show_best(trained_models_list[[1]], metric = &amp;quot;accuracy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 7
##    penalty mixture .metric  .estimator  mean     n std_err
##      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 6.57e-10  0.0655 accuracy binary     0.916    10 0.00179
## 2 1.18e- 6  0.167  accuracy binary     0.916    10 0.00180
## 3 1.72e- 6  0.504  accuracy binary     0.916    10 0.00182
## 4 4.57e-10  0.989  accuracy binary     0.916    10 0.00181
## 5 6.62e- 6  0.880  accuracy binary     0.916    10 0.00181&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows the 5 best configurations for elasticnet when looking at accuracy. Now how to get the best
performing elasticnet regression, random forest, boosted trees, etc? Easy, using &lt;code&gt;map()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(trained_models_list, show_best, metric = &amp;quot;accuracy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 5 x 7
##    penalty mixture .metric  .estimator  mean     n std_err
##      &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 6.57e-10  0.0655 accuracy binary     0.916    10 0.00179
## 2 1.18e- 6  0.167  accuracy binary     0.916    10 0.00180
## 3 1.72e- 6  0.504  accuracy binary     0.916    10 0.00182
## 4 4.57e-10  0.989  accuracy binary     0.916    10 0.00181
## 5 6.62e- 6  0.880  accuracy binary     0.916    10 0.00181
## 
## [[2]]
## # A tibble: 5 x 7
##    mtry trees .metric  .estimator  mean     n std_err
##   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1    13  1991 accuracy binary     0.929    10 0.00172
## 2    13  1180 accuracy binary     0.929    10 0.00168
## 3    12   285 accuracy binary     0.928    10 0.00168
## 4     8  1567 accuracy binary     0.927    10 0.00171
## 5     8   647 accuracy binary     0.927    10 0.00191
## 
## [[3]]
## # A tibble: 5 x 7
##   num_terms prune_method .metric  .estimator  mean     n std_err
##       &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1         5 backward     accuracy binary     0.904    10 0.00186
## 2         5 forward      accuracy binary     0.902    10 0.00185
## 3         4 exhaustive   accuracy binary     0.901    10 0.00167
## 4         4 seqrep       accuracy binary     0.901    10 0.00167
## 5         2 backward     accuracy binary     0.896    10 0.00209
## 
## [[4]]
## # A tibble: 5 x 9
##    mtry trees tree_depth learn_rate .metric  .estimator  mean     n std_err
##   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1    12  1245         12   7.70e- 2 accuracy binary     0.929    10 0.00175
## 2     1   239          8   8.23e- 2 accuracy binary     0.927    10 0.00186
## 3     1   835         14   8.53e-10 accuracy binary     0.913    10 0.00232
## 4     4  1522         12   2.22e- 5 accuracy binary     0.896    10 0.00209
## 5     6   313          2   1.21e- 8 accuracy binary     0.896    10 0.00209
## 
## [[5]]
## # A tibble: 5 x 7
##   hidden_units  penalty .metric  .estimator  mean     n std_err
##          &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1           10 3.07e- 6 accuracy binary     0.917    10 0.00209
## 2            6 1.69e-10 accuracy binary     0.917    10 0.00216
## 3            4 2.32e- 7 accuracy binary     0.916    10 0.00194
## 4            7 5.52e- 5 accuracy binary     0.916    10 0.00163
## 5            8 1.13e- 9 accuracy binary     0.916    10 0.00173&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we need to test these models on the holdout set, but this post is already quite long. In the next
blog post, I will retrain the top best performing models for each type of model and see how they
fare against the holdout set. I’ll be also looking at explainability, so stay tuned!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and watch my
&lt;a href=&#34;https://www.youtube.com/channel/UCTZXht1RTL2Duc3eU8MYGzQ&#34;&gt;youtube channel&lt;/a&gt;. If you want to support
my blog and channel, you could &lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or
&lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Synthetic micro-datasets: a promising middle ground between data privacy and data analysis</title>
      <link>/blog/2020-02-23-synthpop/</link>
      <pubDate>Sun, 23 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-02-23-synthpop/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/scienceshitpost/status/1218199897654120449&#34;&gt;
&lt;img src=&#34;/img/fake_car.png&#34; title = &#34;A purpoise can be assumed to be a kind of fake car.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
&lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;div id=&#34;intro-the-need-for-microdata-and-the-risk-of-disclosure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro: the need for microdata, and the risk of disclosure&lt;/h2&gt;
&lt;p&gt;Survey and administrative data are essential for scientific research, however accessing such datasets
can be very tricky, or even impossible. In my previous job I was responsible for getting access to
such “scientific micro-datasets” from institutions like Eurostat.
In general, getting access to these micro datasets was only a question of filling out some forms
and signing NDAs. But this was true only because my previous employer was an accredited research entity.
Companies from the private sector or unaffiliated, individual, researchers cannot get access to
the microdata sets. This is because institutions that produce such datasets absolutely do not want
any type of personal information to be disclosed.&lt;/p&gt;
&lt;p&gt;For instance, with the labour force survey, a National Statistical Institute (NSI) collects
information about wages, family structure, educational attainment and much more.
If, say, a politician would answer to the survey and his answers would leak to the public that would
be disastrous for NSIs. So this is why access is restricted to accredited research institutions.
You may be asking yourself, “how could the politicians answers leak? The data is anonymized!”
Indeed it is, but in some cases that may not be enough to ensure that information does not get
disclosed. Suppose that the dataset contains enough information to allow you to know for certain
that you found said politician, assume that this politician is a 43 year old man, has two children,
a PhD in theology and lives in Strassen, one of Luxembourg-City very nice neighborhoods. It would be quite easy to
find him in the dataset and then find out his wage.&lt;/p&gt;
&lt;p&gt;To avoid this, researchers are required to perform output checking, which means going through the
set of outputs (summary tables, graphs, tables with regression coefficients…) and making sure that
it is not possible to find out individuals. For instance, in Luxembourg there are two companies in
the tobacco industry. Luxembourg’s NSI cannot release the total turnover
of the industry, because then company A would subtract its turnover from the total and find out its
competitor’s turnover. Now these are all hypothetical examples, and we might argue that the risk of
leakage is quite low, especially if NSIs make sure to lower the precision of the variables, by
providing age categories instead of the exact age for example. Or capping wages that exceed a certain
fixed amount.
In any case for now most NSIs don’t release micro data to the public, and this poses some challenges
for research. First of all, even for researchers, it would be great if the data was freely accessible.
It would allow research to go straight to data analysis and look at the structure of the data before
applying for access, with the risk of getting access to useless data.
And of course it would be great for the public at large to be able to freely access such data, for
educational purposes at the very least. It would also increase competition between research institutions
and the private sector when it comes to conducting studies using such data. Free access to the
microdata would level the playing field.
Now, some NSIs do release micro data to the public, see Eustat, the NSI from the Basque country,
an autonomous region of Spain. It is not clear to me if they also have more detailed data that is
only accessible to researchers, but the data they offer is already quite interesting.&lt;/p&gt;
&lt;p&gt;A middle ground between only releasing data to researchers and making it completely freely accessible
is to create a synthetic dataset, which does not contain any of the original records, but which still
allows to perform meaningful analyses.&lt;/p&gt;
&lt;p&gt;I’m not yet very familiar with the details of the procedure, but in this blog post I’ll use Eustat’s
microdata to generate a synthetic dataset. I’ll then perform the same analysis on both the original
dataset and the synthetic dataset. The dataset I’ll be using can be found
&lt;a href=&#34;https://en.eustat.eus/estadisticas/tema_37/opt_0/tipo_11/temas.html&#34;&gt;here&lt;/a&gt;, and is called
&lt;em&gt;Population with relation to activity (PRA)&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The Survey on the Population in Relation to Activity operation is a continuous source of
information on the characteristics and dynamics of the labour force of the Basque Country.
It records the relation to productive activity of the population resident in family households,
as well as the changes produced in labour situations; it produces indicators of conjunctural
variations in the evolution of the active population; it also estimates the degree of participation
of the population in economically non-productive activities. It offers information on the province
and capital level.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I’ll then compare the results of the analyses performed on the two datasets which will hopefully be
very similar. To create the synthetic dataset, I’ll be using the &lt;code&gt;{synthpop}&lt;/code&gt; package. You can read
the detailed vignette &lt;a href=&#34;https://cran.r-project.org/web/packages/synthpop/vignettes/synthpop.pdf&#34;&gt;here - pdf warning -&lt;/a&gt;.
First, let me perform some cleaning steps. There are four datasets included in the
archive. Let’s load them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidymodels)
library(readxl)
library(synthpop)

list_data &amp;lt;- Sys.glob(&amp;quot;MICRO*.csv&amp;quot;)

dataset &amp;lt;- map(list_data, read_csv2) %&amp;gt;%
  bind_rows()

head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The columns are labeled in Spanish so I’m copy pasting the labels into Google translate and paste
them back into my script. I saved the English names into the english.rds object for posterity.
These steps are detailed in the next lines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dictionary &amp;lt;- read_xlsx(&amp;quot;Microdatos_PRA_2019/diseño_registro_microdatos_pra.xlsx&amp;quot;, sheet=&amp;quot;Valores&amp;quot;,
                        col_names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## New names:
## * `` -&amp;gt; ...1
## * `` -&amp;gt; ...2
## * `` -&amp;gt; ...3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;col_names &amp;lt;- dictionary %&amp;gt;%
  filter(!is.na(...1)) %&amp;gt;%
  dplyr::select(1:2)

# copy to clipboard, paste to google translate
# couldn&amp;#39;t be bothered to use an api and google cloud or whatever
#clipr::write_clip(col_names$`...2`)

#english &amp;lt;- clipr::read_clip()

english &amp;lt;- readRDS(&amp;quot;english_col_names.rds&amp;quot;)

col_names$english &amp;lt;- english

colnames(dataset) &amp;lt;- col_names$english

dataset &amp;lt;- janitor::clean_names(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now create a function that will perform the cleaning steps:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;basic_cleaning &amp;lt;- function(dataset){
  dataset %&amp;gt;%
  dplyr::filter(age %in% c(&amp;quot;05&amp;quot;, &amp;quot;06&amp;quot;, &amp;quot;07&amp;quot;, &amp;quot;08&amp;quot;, &amp;quot;09&amp;quot;, &amp;quot;10&amp;quot;, &amp;quot;11&amp;quot;)) %&amp;gt;%
  dplyr::filter(!is.na(job_search)) %&amp;gt;%  
  dplyr::select(territory, capital, sex, place_of_birth, age, nationality, level_of_studies_completed,
                job_search, main_occupation, type_of_contract, hours) %&amp;gt;%
  dplyr::mutate_at(.vars = vars(-hours), .funs=as.factor)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-on-my-econometricians-hat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting on my econometricians hat&lt;/h2&gt;
&lt;p&gt;Let’s now suppose that I’m only interested in running a logistic regression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pra &amp;lt;- basic_cleaning(dataset)

head(pra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 11
##   territory capital sex   place_of_birth age   nationality level_of_studie…
##   &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;          &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;           
## 1 48        9       6     1              09    1           1               
## 2 48        9       1     1              09    1           2               
## 3 48        1       1     1              11    1           3               
## 4 48        1       6     1              10    1           3               
## 5 48        9       6     1              07    1           3               
## 6 48        9       1     1              09    1           1               
## # … with 4 more variables: job_search &amp;lt;fct&amp;gt;, main_occupation &amp;lt;fct&amp;gt;,
## #   type_of_contract &amp;lt;fct&amp;gt;, hours &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit_model &amp;lt;- glm(job_search ~ ., data = pra, family = binomial())

# Create a tidy dataset with the results of the regression
tidy_logit_model &amp;lt;- tidy(logit_model, conf.int = TRUE) %&amp;gt;%
  mutate(dataset = &amp;quot;true&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now take a look at the coefficients, by plotting their value along with their confidence
intervals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(tidy_logit_model, aes(x = term, y = estimate)) +
  geom_point(colour = &amp;quot;#82518c&amp;quot;) +
  geom_hline(yintercept = 0, colour = &amp;quot;red&amp;quot;) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), colour = &amp;quot;#657b83&amp;quot;) +
  brotools::theme_blog() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-02-23-synthpop_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, so now, how would the results change if I run the same analysis on the synthetic dataset? First,
I need to generate this synthetic dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_seed &amp;lt;- 1234

synthetic_data &amp;lt;- syn(pra, seed = my_seed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Synthesis
## -----------
##  territory capital sex place_of_birth age nationality level_of_studies_completed job_search main_occupation type_of_contract
##  hours&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The synthetic data is generated by a single call to the &lt;code&gt;syn()&lt;/code&gt; function included in the &lt;code&gt;{synthpop}&lt;/code&gt;
package. Let’s take a look at the generated object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;synthetic_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## ($call) syn(data = pra, seed = my_seed)
## 
## Number of synthesised data sets: 
## ($m)  1 
## 
## First rows of synthesised data set: 
## ($syn)
##   territory capital sex place_of_birth age nationality
## 1        48       9   1              1  06           1
## 2        01       9   6              3  09           1
## 3        48       3   1              1  08           1
## 4        48       9   6              1  11           1
## 5        20       2   6              1  09           1
## 6        48       1   6              1  11           1
##   level_of_studies_completed job_search main_occupation type_of_contract hours
## 1                          3          N               2                1    40
## 2                          1          S               9                6    10
## 3                          1          N               6             &amp;lt;NA&amp;gt;    32
## 4                          2          N               4                1    32
## 5                          3          N               5             &amp;lt;NA&amp;gt;    40
## 6                          1          S               7             &amp;lt;NA&amp;gt;    NA
## ...
## 
## Synthesising methods: 
## ($method)
##                  territory                    capital 
##                   &amp;quot;sample&amp;quot;                     &amp;quot;cart&amp;quot; 
##                        sex             place_of_birth 
##                     &amp;quot;cart&amp;quot;                     &amp;quot;cart&amp;quot; 
##                        age                nationality 
##                     &amp;quot;cart&amp;quot;                     &amp;quot;cart&amp;quot; 
## level_of_studies_completed                 job_search 
##                     &amp;quot;cart&amp;quot;                     &amp;quot;cart&amp;quot; 
##            main_occupation           type_of_contract 
##                     &amp;quot;cart&amp;quot;                     &amp;quot;cart&amp;quot; 
##                      hours 
##                     &amp;quot;cart&amp;quot; 
## 
## Order of synthesis: 
## ($visit.sequence)
##                  territory                    capital 
##                          1                          2 
##                        sex             place_of_birth 
##                          3                          4 
##                        age                nationality 
##                          5                          6 
## level_of_studies_completed                 job_search 
##                          7                          8 
##            main_occupation           type_of_contract 
##                          9                         10 
##                      hours 
##                         11 
## 
## Matrix of predictors: 
## ($predictor.matrix)
##                            territory capital sex place_of_birth age nationality
## territory                          0       0   0              0   0           0
## capital                            1       0   0              0   0           0
## sex                                1       1   0              0   0           0
## place_of_birth                     1       1   1              0   0           0
## age                                1       1   1              1   0           0
## nationality                        1       1   1              1   1           0
## level_of_studies_completed         1       1   1              1   1           1
## job_search                         1       1   1              1   1           1
## main_occupation                    1       1   1              1   1           1
## type_of_contract                   1       1   1              1   1           1
## hours                              1       1   1              1   1           1
##                            level_of_studies_completed job_search
## territory                                           0          0
## capital                                             0          0
## sex                                                 0          0
## place_of_birth                                      0          0
## age                                                 0          0
## nationality                                         0          0
## level_of_studies_completed                          0          0
## job_search                                          1          0
## main_occupation                                     1          1
## type_of_contract                                    1          1
## hours                                               1          1
##                            main_occupation type_of_contract hours
## territory                                0                0     0
## capital                                  0                0     0
## sex                                      0                0     0
## place_of_birth                           0                0     0
## age                                      0                0     0
## nationality                              0                0     0
## level_of_studies_completed               0                0     0
## job_search                               0                0     0
## main_occupation                          0                0     0
## type_of_contract                         1                0     0
## hours                                    1                1     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, &lt;code&gt;synthetic_data&lt;/code&gt; is a list with several elements. The data is inside the &lt;code&gt;syn&lt;/code&gt; element.
Let’s extract it, and perform the same analysis from before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;syn_pra &amp;lt;- synthetic_data$syn

head(syn_pra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   territory capital sex place_of_birth age nationality
## 1        48       9   1              1  06           1
## 2        01       9   6              3  09           1
## 3        48       3   1              1  08           1
## 4        48       9   6              1  11           1
## 5        20       2   6              1  09           1
## 6        48       1   6              1  11           1
##   level_of_studies_completed job_search main_occupation type_of_contract hours
## 1                          3          N               2                1    40
## 2                          1          S               9                6    10
## 3                          1          N               6             &amp;lt;NA&amp;gt;    32
## 4                          2          N               4                1    32
## 5                          3          N               5             &amp;lt;NA&amp;gt;    40
## 6                          1          S               7             &amp;lt;NA&amp;gt;    NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;syn_pra &amp;lt;- basic_cleaning(syn_pra)

logit_model_syn &amp;lt;- glm(job_search ~ ., data = syn_pra, family = binomial())

tidy_logit_syn &amp;lt;- tidy(logit_model_syn, conf.int = TRUE) %&amp;gt;%
  mutate(dataset = &amp;quot;syn&amp;quot;)

ggplot(tidy_logit_syn, aes(x = term, y = estimate)) +
  geom_point(colour = &amp;quot;#82518c&amp;quot;) +
  geom_hline(yintercept = 0, colour = &amp;quot;red&amp;quot;) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), colour = &amp;quot;#657b83&amp;quot;) +
  brotools::theme_blog() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-02-23-synthpop_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To ease the comparison between the coefficients of the model, let’s create a single graph:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coeff_models &amp;lt;- bind_rows(list(tidy_logit_model, tidy_logit_syn))

ggplot(coeff_models, aes(x = term, y = estimate, colour = dataset)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  brotools::theme_blog() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-02-23-synthpop_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is quite interesting; generally, there is quite some overlap between the synthetic data and
the real data! There are some differences though, for instance, &lt;code&gt;main_occupation6&lt;/code&gt; is significant
with the synthetic data, but is not with the real data. There’s the possibility to generate more
than one synthetic dataset, which would very likely reduce the noise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-on-my-data-scientist-hat&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting on my data scientist hat&lt;/h2&gt;
&lt;p&gt;Now let’s suppose that I am only interested into prediction. For this, I am going to split my dataset
into a training and testing set, then run a logistic regression and a random forest, assess the
models’ performance with 10-fold cross validation. I’ll do this on both the real and the synthetic
data. To perform the analysis, I’ll be using the &lt;code&gt;{tidymodels}&lt;/code&gt; framework; I’m going to explain
the code that follows line by line, because I’ll very likely write a blog post focusing on &lt;code&gt;{tidymodels}&lt;/code&gt;
soon.&lt;/p&gt;
&lt;p&gt;So, let’s write a function that does exactly what I explained above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_and_evaluating &amp;lt;- function(dataset){

  pra_split &amp;lt;- initial_split(dataset, prop = 0.8)
  
  pra_train &amp;lt;- training(pra_split)
  pra_test &amp;lt;- testing(pra_split)
  
  pra_cv_splits &amp;lt;- vfold_cv(pra_train, v = 10)
  
  preprocess &amp;lt;- recipe(job_search ~ ., data = pra) %&amp;gt;%
    step_knnimpute(all_predictors())
  
  logit_pra &amp;lt;- logistic_reg() %&amp;gt;%
    set_engine(&amp;quot;glm&amp;quot;)
  
  fitted_logit &amp;lt;- fit_resamples(preprocess,
                                model = logit_pra,
                                resamples = pra_cv_splits,
                                control = control_resamples(save_pred = TRUE))
  
  metric_logit &amp;lt;- fitted_logit$.metrics %&amp;gt;%
    bind_rows() %&amp;gt;%
    group_by(.metric) %&amp;gt;%
    summarise_at(.vars = vars(.estimate), .funs = lst(mean, sd)) %&amp;gt;%
    mutate(model = &amp;quot;logit&amp;quot;)
  
  rf_pra &amp;lt;- rand_forest(mode = &amp;quot;classification&amp;quot;) %&amp;gt;%
    set_engine(engine = &amp;quot;ranger&amp;quot;)
  
  fitted_forest &amp;lt;- fit_resamples(preprocess,
                                model = rf_pra,
                                resamples = pra_cv_splits,
                                control = control_resamples(save_pred = TRUE))
  
  metric_forest &amp;lt;- fitted_forest$.metrics %&amp;gt;%
    bind_rows() %&amp;gt;%
    group_by(.metric) %&amp;gt;%
    summarise_at(.vars = vars(.estimate), .funs = lst(mean, sd)) %&amp;gt;%
    mutate(model = &amp;quot;forest&amp;quot;)


  bind_rows(list(metric_logit, metric_forest))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can run this function on both the real and the synthetic data, and look at the performance
of the logistic regression and of the random forest:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;true_data_performance &amp;lt;- training_and_evaluating(pra)

syn_data_performance &amp;lt;- training_and_evaluating(syn_pra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;true_data_performance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##   .metric   mean      sd model 
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; 
## 1 accuracy 0.882 0.00816 logit 
## 2 roc_auc  0.708 0.0172  logit 
## 3 accuracy 0.907 0.00619 forest
## 4 roc_auc  0.879 0.0123  forest&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;syn_data_performance&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##   .metric   mean      sd model 
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; 
## 1 accuracy 0.882 0.00758 logit 
## 2 roc_auc  0.691 0.0182  logit 
## 3 accuracy 0.899 0.00615 forest
## 4 roc_auc  0.857 0.0124  forest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The performance is pretty much the same!&lt;/p&gt;
&lt;p&gt;Generating synthetic data is a very promising approach, that I certainly will be using more; I think
that such approaches can also be very interesting in the private sector (not only to ease access
to microdata for researchers) especially within large
companies. For instance, it can happen that the data owners from say, an insurance company, are
not very keen on sharing sensitive client information with their data scientists. However, by generating
a synthetic dataset and sharing the synthetic data with their data science team, the data owners
avoid any chance of disclosure of sensitive information, while at the same time allowing their
data scientists to develop interesting analyses or applications on the data!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and watch my
&lt;a href=&#34;https://www.youtube.com/channel/UCTZXht1RTL2Duc3eU8MYGzQ&#34;&gt;youtube channel&lt;/a&gt;. If you want to support
my blog and channel, you could &lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or
&lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic discrete choice models, reinforcement learning and Harold, part 2</title>
      <link>/blog/2020-02-08-harold_part2/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-02-08-harold_part2/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/ZwRuneeLsCQ?t=229&#34;&gt;
&lt;img src=&#34;/img/bus.jpg&#34; title = &#34;very nice&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
&lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;em&gt;In this blog post, I present a paper that has really interested me for a long time. This is part2,
where I will briefly present the model of the paper, and try to play around with the data.
If you haven’t, I suggest you read
&lt;a href=&#34;https://www.brodrigues.co/blog/2020-01-26-harold/&#34;&gt;part 1&lt;/a&gt; where I provide more context.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;rusts-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Rust’s model&lt;/h2&gt;
&lt;p&gt;Welcome to part 2 of this series, which might or might not have a part 3. I have been quite busy
with this paper and especially with reinforcement learning these past couple of weeks, but in the
meantime, other &lt;a href=&#34;https://www.youtube.com/channel/UCTZXht1RTL2Duc3eU8MYGzQ&#34;&gt;things&lt;/a&gt; have taken
some of my time, so who knows if I’ll keep discussing this paper.&lt;/p&gt;
&lt;p&gt;Before going into the data, let me describe the model very broadly.
The problem is as follows: each month, Harold Zurcher must decide whether to simply perform some
basic maintenance on the buses he’s responsible for, or he can decide to completely replace the
engine. Let his utility function be as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
u(x_t, i_t, \theta_1) = \left\{
\begin{array}{lcl}
-c(x_t, \theta_1) &amp;amp; \text{if} &amp;amp; i_t = 0, \\
-[\overline{P} - \underline{P} + c(0, \theta_1)] &amp;amp; \text{if} &amp;amp; i_t = 1,\\
\end{array}\right.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt; is the state variable, the reading of the odometer at month &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i_t\)&lt;/span&gt; is Harold Zurcher’s
decision at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(i_t = 0\)&lt;/span&gt; is the decision to keep the engine, &lt;span class=&#34;math inline&#34;&gt;\(i_t = 1\)&lt;/span&gt; is the decision to
replace. Each time the engine is replaced, the state variable &lt;span class=&#34;math inline&#34;&gt;\(x_t\)&lt;/span&gt; regenerates to 0. That is why
John Rust, the paper’s author, calls the problem under study a regenerative optimal stopping model.
If &lt;span class=&#34;math inline&#34;&gt;\(i_t = 0\)&lt;/span&gt; (keep the engine) is chosen, then the cost of normal maintenance is &lt;span class=&#34;math inline&#34;&gt;\(c(x_t, \theta_1)\)&lt;/span&gt;,
if &lt;span class=&#34;math inline&#34;&gt;\(i_t = 1\)&lt;/span&gt; (change the engine) then the cost is &lt;span class=&#34;math inline&#34;&gt;\(\overline{P}\)&lt;/span&gt;, which is the price of the new
engine. However, it is still possible to sell the old engine for scrap value, &lt;span class=&#34;math inline&#34;&gt;\(\underline{P}\)&lt;/span&gt;. The
replacement cost is equal to &lt;span class=&#34;math inline&#34;&gt;\(c(0, \theta_1)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; is a vector of parameters of the
cost function to estimate.
Because Harold Zurcher is forward looking, and does not want to simply maximize the current month’s
utility, he seeks to maximize his intertemporal utility function. The optimal policy would be the
solution to the following equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_{\theta} = \max E\left\{ \sum_{j = t}^\infty \beta^{j-t}u(x_j, f_j, \theta_1) | x_t\right\}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is a so-called value function, which is the total reward at the solution of the problem.&lt;/p&gt;
&lt;p&gt;The state variable evolves according to a stochastic process given by the following transition
probability:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(x_{t+1} | x_t, i_t, \theta_2) = \left\{
\begin{array}{lllll}
\theta_2 \exp\{\theta_2(x_{t+1} - x_t)\} &amp;amp; \text{if} &amp;amp; i_t = 0 &amp;amp; \text{and} &amp;amp; x_{t+1} \geq x_t \\
\theta_2 \exp\{\theta_2(x_{t+1})\} &amp;amp; \text{if} &amp;amp; i_t = 0 &amp;amp; \text{and} &amp;amp; x_{t+1} \geq 0 \\
0 &amp;amp; \text{otherwise}\\
\end{array}\right.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt; is the parameter of the exponential distribution, another parameter to estimate.
I’ll stop with one more equation, the Bellman equation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_\theta(x_t) = \max_{i_t \in C(x_t)} [u(x_t, i_t, \theta_1) + \beta EV_\theta(x_t, i_t)]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(C(x_t) = {0, 1}\)&lt;/span&gt; is the action set. The value function is the unique solution to this Bellman equation.&lt;/p&gt;
&lt;p&gt;As you can see, this is quite complex (and I have not detailed everything!) but the advantage
of models is that one can estimate its structural parameters and put a dollar value on the expected
replacement cost, &lt;span class=&#34;math inline&#34;&gt;\(\overline{P} - \underline{P}\)&lt;/span&gt; in addition to validating the very first
hypothesis of the paper; does Harold Zurcher behave optimally?&lt;/p&gt;
&lt;p&gt;In what follows, I’ll use the &lt;code&gt;{ReinforcementLearning}&lt;/code&gt; package to try to find the optimal policy rule.
The optimal policy rule tells us what is the best action at each period. Reinforcement learning is
an approach that is widely used in machine learning to solve problems very similar to the one that
I described above. However, as we shall see, it will fail here, and there’s a very good reason
for that. First, let’s load the data that was prepared last time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_bus_data &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/b-rodrigues/rust/ee15fb87fc4ba5db28d055c97a898b328725f53c/datasets/processed_data/all_buses.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   bus_id = col_double(),
##   date = col_date(format = &amp;quot;&amp;quot;),
##   odometer_reading = col_double(),
##   replacement = col_double(),
##   bus_family = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(all_bus_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   bus_id date       odometer_reading replacement bus_family
##    &amp;lt;dbl&amp;gt; &amp;lt;date&amp;gt;                &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     
## 1   4239 1974-12-01           140953           0 a452372   
## 2   4239 1975-01-01           142960           0 a452372   
## 3   4239 1975-02-01           145380           0 a452372   
## 4   4239 1975-03-01           148140           0 a452372   
## 5   4239 1975-04-01           150921           0 a452372   
## 6   4239 1975-05-01           153839           0 a452372&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the paper, the author groups the 4 following bus families, so I’ll be doing the same:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;family_group &amp;lt;- c(&amp;quot;g870&amp;quot;, &amp;quot;rt50&amp;quot;, &amp;quot;t8h203&amp;quot;, &amp;quot;a530875&amp;quot;)

group1_4 &amp;lt;- all_bus_data %&amp;gt;%
  filter(bus_family %in% family_group)

ggplot(group1_4) + 
  geom_line(aes(y = odometer_reading, x = date, group = bus_id, col = bus_family)) + 
  geom_point(aes(y = ifelse(odometer_reading*replacement == 0, NA, odometer_reading*replacement), 
                 x = date), col = &amp;quot;red&amp;quot;) +
  labs(title = paste0(&amp;quot;Odometer readings for bus families &amp;quot;, paste0(family_group, collapse = &amp;quot;, &amp;quot;)),
       caption = &amp;quot;The red dots are replacement events.&amp;quot;) + 
  theme(plot.caption = element_text(colour = &amp;quot;white&amp;quot;)) +
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 8200 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-02-08-harold_part2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are 104 buses in this subset of data. Let’s discretize
the odometer reading using the &lt;code&gt;ntile()&lt;/code&gt; function. Discretizing the state variable will make
computation faster:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group1_4 &amp;lt;- group1_4 %&amp;gt;%  
  mutate(state_at_replacement = ifelse(replacement == 1, odometer_reading, NA)) %&amp;gt;%
  group_by(bus_id) %&amp;gt;%
  fill(state_at_replacement, .direction = &amp;quot;down&amp;quot;) %&amp;gt;%
  ungroup() %&amp;gt;%  
  mutate(state_at_replacement = odometer_reading - state_at_replacement) %&amp;gt;%
  mutate(state_at_replacement = ifelse(is.na(state_at_replacement), odometer_reading, state_at_replacement)) %&amp;gt;%  
  mutate(state = ntile(state_at_replacement, 50))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me also save the bus ids in a vector, I’ll need it later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;buses &amp;lt;- unique(group1_4$bus_id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use the dataset with the &lt;code&gt;{ReinforcementLearning}&lt;/code&gt; package, it must first be prepared:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group1_4 &amp;lt;- group1_4 %&amp;gt;%
  group_by(bus_id) %&amp;gt;%  
  mutate(next_state = lead(state, 1)) %&amp;gt;%
  mutate(replacement = lead(replacement, 1)) %&amp;gt;%  
  mutate(action = replacement) %&amp;gt;% 
  select(state, action, reward = replacement, next_state) %&amp;gt;%
  mutate(reward = (-1)*reward) %&amp;gt;%
  mutate(action = ifelse(is.na(action), 0, action),
         reward = ifelse(is.na(reward), 0, reward)) %&amp;gt;%  
  mutate(next_state = ifelse(is.na(next_state), state + 1, next_state)) %&amp;gt;% 
  mutate(state = as.character(state),
         next_state = as.character(next_state),
         action = as.character(action)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Adding missing grouping variables: `bus_id`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see how the data looks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(group1_4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
## # Groups:   bus_id [1]
##   bus_id state action reward next_state
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     
## 1   5297 2     0           0 3         
## 2   5297 3     0           0 4         
## 3   5297 4     0           0 5         
## 4   5297 5     0           0 6         
## 5   5297 6     0           0 8         
## 6   5297 8     0           0 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So when action 0 (do nothing) is chosen, the value of the state is increased by one. If action
1 (replace) is chosen:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group1_4 %&amp;gt;%
  filter(action == &amp;quot;1&amp;quot;) %&amp;gt;%
  head&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
## # Groups:   bus_id [6]
##   bus_id state action reward next_state
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     
## 1   5297 34    1          -1 1         
## 2   5299 42    1          -1 1         
## 3   5300 43    1          -1 1         
## 4   5301 36    1          -1 1         
## 5   5302 30    1          -1 1         
## 6   5303 49    1          -1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The state goes back to 1, and the reward is -1.&lt;/p&gt;
&lt;p&gt;Now, let’s split the dataset into two: a training dataset and a testing dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
train_buses &amp;lt;- sample(buses, size = round(length(buses)*.8))

test_buses &amp;lt;- setdiff(buses, train_buses)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There will be 83 in the training data and 21 in the
testing data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_data &amp;lt;- group1_4 %&amp;gt;%
  filter(bus_id %in% train_buses)

test_data &amp;lt;- group1_4 %&amp;gt;%
  filter(bus_id %in% test_buses)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re finally ready to use the &lt;code&gt;{ReinforcementLearning}&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ReinforcementLearning)
model &amp;lt;- ReinforcementLearning(train_data,
                                         s = &amp;quot;state&amp;quot;,
                                         a = &amp;quot;action&amp;quot;,
                                         r = &amp;quot;reward&amp;quot;,
                                         s_new = &amp;quot;next_state&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now what’s the result?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## State-Action function Q
##     0        1
## X30 0 -0.19000
## X31 0  0.00000
## X1  0  0.00000
## X32 0  0.00000
## X2  0  0.00000
## X33 0 -0.10000
## X3  0  0.00000
## X34 0 -0.19000
## X4  0  0.00000
## X35 0  0.00000
## X5  0  0.00000
## X36 0 -0.19000
## X6  0  0.00000
## X37 0 -0.10000
## X7  0  0.00000
## X38 0  0.00000
## X8  0  0.00000
## X39 0 -0.34390
## X9  0  0.00000
## X10 0  0.00000
## X40 0 -0.10000
## X11 0  0.00000
## X41 0 -0.10000
## X12 0  0.00000
## X42 0 -0.34390
## X13 0  0.00000
## X43 0 -0.40951
## X14 0  0.00000
## X44 0 -0.19000
## X45 0 -0.34390
## X15 0  0.00000
## X46 0 -0.27100
## X16 0  0.00000
## X47 0 -0.19000
## X17 0  0.00000
## X48 0 -0.40951
## X18 0  0.00000
## X49 0 -0.34390
## X19 0  0.00000
## X50 0 -0.34390
## X20 0  0.00000
## X21 0  0.00000
## X22 0  0.00000
## X23 0  0.00000
## X24 0  0.00000
## X25 0  0.00000
## X26 0  0.00000
## X27 0  0.00000
## X28 0  0.00000
## X29 0 -0.10000
## 
## Policy
## X30 X31  X1 X32  X2 X33  X3 X34  X4 X35  X5 X36  X6 X37  X7 X38  X8 X39  X9 X10 
## &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; 
## X40 X11 X41 X12 X42 X13 X43 X14 X44 X45 X15 X46 X16 X47 X17 X48 X18 X49 X19 X50 
## &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; 
## X20 X21 X22 X23 X24 X25 X26 X27 X28 X29 
## &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; 
## 
## Reward (last iteration)
## [1] -48&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the optimal policy is always to do nothing! This is actually “normal” here, as we are using historical
data; and in this data, there is no information on the likelihood of severe engine failure if one
does not replace it completely at some point! So the agent does not see the point in replacing the
engine; it costs money and does not seem to bring in any benefit!&lt;/p&gt;
&lt;p&gt;Another way of using the &lt;code&gt;{ReinforcementLearning}&lt;/code&gt; package
is to write a function that simulates the environment. One could write such a function, and add in it
a probability of severe failure with a very big cost. This probability would increase as the state
(number of miles driven) increases as well. With such a function, there would be simulations where
the cost of doing nothing would be very high, and as such, hopefully, the agent would learn that
replacing the engine once might be a better course of action than doing nothing.&lt;/p&gt;
&lt;p&gt;This might be the subject of part 3 of this series!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and watch my
&lt;a href=&#34;https://www.youtube.com/channel/UCTZXht1RTL2Duc3eU8MYGzQ&#34;&gt;youtube channel&lt;/a&gt;. If you want to support
my blog and channel, you could &lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or
&lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dynamic discrete choice models, reinforcement learning and Harold, part 1</title>
      <link>/blog/2020-01-26-harold/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-01-26-harold/</guid>
      <description>&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=r30D3SW4OVw&#34;&gt;
&lt;img src=&#34;/img/fork.jpg&#34; title = &#34;If this blog post had an OST, this would likely be it.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I want to write about an &lt;em&gt;Econometrica&lt;/em&gt; paper written in 1987 (&lt;a href=&#34;https://www.jstor.org/stable/1911259&#34;&gt;jstor link&lt;/a&gt;) by John Rust, currently Professor of Economics at
Georgetown University, paper which has been on my mind for the past 10 years or so. Why? Because
it is a seminal paper in the econometric literature, but it is quite a bizarre one in some aspects.
In this paper, John Rust estimates a structural
dynamic discrete choice model on real data, and Professor Rust even had to develop his own novel
algorithm, which he called NFXP, which stands for &lt;em&gt;Nested Fixed Point&lt;/em&gt; algorithm, to estimate the model.
Such models hare now part of the toolbox of structural econometricians, because said models are
suited to model decision making in a changing environment. How much should you save today for
retirement? Should you go to university? If yes, which major should you choose? Should you get a
PhD? Should you have kids? How many? With whom?
As you see, kind reader, these models are at the center point of what makes life so interesting,
and sometimes so scary as well; what will be the impact of our decisions today on future rewards?
Some would say that only the Almighty would know, but structural econometricians now know as well,
thanks to John Rust.&lt;/p&gt;
&lt;p&gt;It is thus completely natural that Professor Rust chose a very important topic and gathered some
very important data to illustrate the inner workings of such a complicated, and yet fundamentally
important model.&lt;/p&gt;
&lt;p&gt;John Rust chose to tell the story of one named Harold Zurcher, superintendent of the Madison,
Wisconsin, Metropolitan Bus Company and his monthly decision making process on whether to replace
the engine of the buses of the company’s fleet, or not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;and-thine-ears-shall-hear-a-word-behind-thee-saying-this-is-the-way-walk-ye-in-it-when-ye-turn-to-the-right-hand-and-when-ye-turn-to-the-left.-isaiah-3021&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;em&gt;And thine ears shall hear a word behind thee, saying, This is the way, walk ye in it, when ye turn to the right hand, and when ye turn to the left.&lt;/em&gt;, Isaiah 30:21&lt;/h2&gt;
&lt;p&gt;John Rust’s goal is to write down a model of Harold Zurcher’s behaviour, which he assumes follows
an optimal stopping rule: &lt;em&gt;a strategy which specifies whether or not to replace
the current bus engine each period as a function of observed and unobserved
state variables.&lt;/em&gt; But, dear reader, you might wonder, &lt;em&gt;Why model the decisions of Harold Zurcher?
Why not any other, more pressing, issue?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Quoting the author gives an answer: &lt;em&gt;Admittedly, few people are likely to take particular interest in Harold Zurcher and bus engine replacement, per se. I focus on a particular individual and a
specific capital good because it provides a simple, concrete framework to illustrate
two ideas: (i) a “bottom-up” approach for modelling replacement investment
and (ii) a “nested fixed point” algorithm for estimating dynamic programming
models of discrete choice.&lt;/em&gt; And this is what made me absolutely love this paper; I am 100% certain
that today, anyone, especially when starting an academic career, could not, and would not, write
a paper where one would model something so… non-consequential. And yet, John Rust not only wrote
such a paper, his paper is seminal in the literature of structural econometrics. For me, this is
one of the best papers I ever read. I read this paper around 2010-ish, and have thought about
it on and off since then. I now want to explore the data from his paper, and make you discover
it as well.&lt;/p&gt;
&lt;p&gt;In this blog post, I will focus on the data of the paper, which you can download in its raw,
original format or tidy format in the github repo I set up
&lt;a href=&#34;https://github.com/b-rodrigues/rust/tree/master/datasets&#34;&gt;here&lt;/a&gt;. In the next blog post, I’ll
discuss the model in greater detail, with a focus on Harold Zurcher’s decisions. I’ll then discuss
the similarities between reinforcement learning (the title of this blog post was not 100% clickbait)
and dynamic discrete stochastic models and use the &lt;code&gt;{ReinforcementLearning}&lt;/code&gt; package to try to
estimate the optimal policy. I haven’t tried the package’s function on this paper’s data yet, so
I have no idea if it’s going to work out. We’ll see.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-papers-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The paper’s data&lt;/h2&gt;
&lt;p&gt;Harold Zurcher provided monthly data on odometer readings from 162 buses of the Madison Metro fleet
to John Rust.&lt;/p&gt;
&lt;p&gt;(&lt;/p&gt;
&lt;p&gt;I sometimes wonder how this discussion went.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;- Hello Mr Zurcher, I’m an economist, my name is John Rust, and I am interested in dynamic discrete
choice models and their estimation. I would like to write an empirical paper for a prestigious journal,
and would like to know if you would be so kind as to provide me with data for my paper.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;- You what?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;)&lt;/p&gt;
&lt;p&gt;The time period goes from December, 1974 to May, 1985. There are 9 groups of buses, but for a reason
that is not explained in the paper only 8 groups of buses are studied. In addition to the monthly
odometer readings, there is also the date of a first, or second engine replacement. This is the
decision that Harold Zurcher faces each month: should he replace, or not, the engine?
This is a simplification from the author; in actuality, Harold Zurcher could also perform a routine
maintenance or replace individual components as well. The idea to focus on the third option
(complete replacement of the engine) is justified by John Rust as being part of a general
“preventive maintenance” strategy. Indeed, if a component of the engine fails at low mileage, it
is rather safe to simply replace that component. However, should one component of the engine fail
at a much higher mileage, then it is very likely that other components would fail as well in the
near future. As such, it is much safer to completely replace the engine, either with a brand new one,
or with one freshly rebuilt from the company’s machine shop. John Rust points out that Harold Zurcher
assured him that &lt;em&gt;rebuilt engines are every bit as good, if not better, than engines purchased brand
new&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now, to the data itself. The data comes in a format unlike anything I had ever seen before. Let’s take a
look at the head of one single file, for instance &lt;code&gt;a452372.asc&lt;/code&gt; (&lt;code&gt;.asc&lt;/code&gt; stands for ascii, as far as I know):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   4239 
      2 
     72 
      1 
     76 
 166100 
      0 
      0 
      0 
     12 
     74 
 140953 
 142960 
 145380 
 148140 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, on line 138, the data for the second bus of this groups starts:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   4240 
      2 
     72 
      1 
     75 
 177900 
      0 
      0 
      0 
     12 
     74 
 174402 
 175116 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and so on for each bus of this group. The other files are structured in the same way.&lt;/p&gt;
&lt;p&gt;This is quite cryptic, but thankfully, the data is well documented in the manual
of the NFXP software that John Rust wrote for this paper (remember the algorithm he wrote to
estimate the model? He shared his code with a nice manual, a very good practice that
unfortunately is not widespread enough in econometric circles, even to this day).
From this manual, we can read that the 11 first lines of the file are some kind of metadata:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Row  &lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Observation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1  &lt;/td&gt;
&lt;td&gt;bus number&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4239&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2  &lt;/td&gt;
&lt;td&gt;month purchased&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3  &lt;/td&gt;
&lt;td&gt;year purchased&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4  &lt;/td&gt;
&lt;td&gt;month of 1st engine replacement&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5  &lt;/td&gt;
&lt;td&gt;year of 1st engine replacement&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;6  &lt;/td&gt;
&lt;td&gt;odometer at replacement&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;166100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;7  &lt;/td&gt;
&lt;td&gt;month of 2nd replacement&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;8  &lt;/td&gt;
&lt;td&gt;year of 2nd replacement&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;9  &lt;/td&gt;
&lt;td&gt;odometer at replacement&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;10  &lt;/td&gt;
&lt;td&gt;month odometer data begins&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;11  &lt;/td&gt;
&lt;td&gt;year odometer data begins&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;74&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;12  &lt;/td&gt;
&lt;td&gt;odometer reading&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;140953&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With this knowledge, the first step is thus to build a tidy data frame. To achieve this, I first
load the relevant packages, and read in all the data at once:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)

data_file_path &amp;lt;- Sys.glob(&amp;quot;datasets/*.asc&amp;quot;)

data_files &amp;lt;- map(data_file_path, read_lines)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;data_files&lt;/code&gt; is a list of 9 elements, where each element is one of the raw data files
(&lt;code&gt;a42372.asc&lt;/code&gt;, &lt;code&gt;a452374.asc&lt;/code&gt;, ….)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; str(data_files)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;List of 9
 $ : chr [1:2466] &amp;quot;   4239 &amp;quot; &amp;quot;      2 &amp;quot; &amp;quot;     72 &amp;quot; &amp;quot;      1 &amp;quot; ...
 $ : chr [1:1370] &amp;quot;   4287 &amp;quot; &amp;quot;     10 &amp;quot; &amp;quot;     74 &amp;quot; &amp;quot;     11 &amp;quot; ...
 $ : chr [1:2466] &amp;quot;   5257 &amp;quot; &amp;quot;      5 &amp;quot; &amp;quot;     72 &amp;quot; &amp;quot;      6 &amp;quot; ...
 $ : chr [1:1644] &amp;quot;   5275 &amp;quot; &amp;quot;     10 &amp;quot; &amp;quot;     74 &amp;quot; &amp;quot;      9 &amp;quot; ...
 $ : chr [1:4736] &amp;quot;   5297 &amp;quot; &amp;quot;      8 &amp;quot; &amp;quot;     75 &amp;quot; &amp;quot;      4 &amp;quot; ...
 $ : chr [1:440] &amp;quot;   1334 &amp;quot; &amp;quot;      3 &amp;quot; &amp;quot;     77 &amp;quot; &amp;quot;      0 &amp;quot; ...
 $ : chr [1:540] &amp;quot;   4403 &amp;quot; &amp;quot;      5 &amp;quot; &amp;quot;     83 &amp;quot; &amp;quot;      0 &amp;quot; ...
 $ : chr [1:240] &amp;quot;   2386 &amp;quot; &amp;quot;      5 &amp;quot; &amp;quot;     81 &amp;quot; &amp;quot;      0 &amp;quot; ...
 $ : chr [1:3888] &amp;quot;   4338 &amp;quot; &amp;quot;      3 &amp;quot; &amp;quot;     79 &amp;quot; &amp;quot;      3 &amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to process all this data, I wrote this monster function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;process_bus_data &amp;lt;- function(data_file){
  data_file &amp;lt;- as.numeric(data_file)
  first_bus &amp;lt;- data_file[1]
  second_bus &amp;lt;- first_bus + 1
  second_bus_index &amp;lt;- which(data_file == second_bus)

  nb_data_points &amp;lt;- second_bus_index - 1

  nb_buses &amp;lt;- length(data_file) / nb_data_points

  indices &amp;lt;- nb_data_points * seq(1, nb_buses)

  indices &amp;lt;- c(0, indices)

  sep_data_sets &amp;lt;- map(indices, ~`[`(data_file, (. + 1):(. + nb_data_points) ))

  headers_list &amp;lt;- map(sep_data_sets, ~`[`(., 1:11))

  header_elements &amp;lt;- c(&amp;quot;bus number&amp;quot;, &amp;quot;month purchased&amp;quot;, &amp;quot;year purchased&amp;quot;,
                       &amp;quot;month of 1st engine replacement&amp;quot;, &amp;quot;year of 1st engine replacement&amp;quot;,
                       &amp;quot;odometer at replacement&amp;quot;, &amp;quot;month of 2nd replacement&amp;quot;,
                       &amp;quot;year of 2nd replacement&amp;quot;, &amp;quot;odometer at replacement&amp;quot;,
                       &amp;quot;month odometer data begins&amp;quot;, &amp;quot;year odometer data begins&amp;quot;)

  create_start_date &amp;lt;- function(one_dataset){
      one_dataset &amp;lt;- pull(one_dataset)
      month &amp;lt;- one_dataset[10]
      year &amp;lt;- paste0(&amp;quot;19&amp;quot;, one_dataset[11])

      month &amp;lt;- ifelse(nchar(month) == 1, paste0(&amp;quot;0&amp;quot;, month), month)

      ymd(paste0(year, &amp;quot;-&amp;quot;, month, &amp;quot;-01&amp;quot;))
  }

  create_first_replacement &amp;lt;- function(one_dataset){
      one_dataset &amp;lt;- pull(one_dataset, odometer_reading)
      month &amp;lt;- one_dataset[4]
      year &amp;lt;- paste0(&amp;quot;19&amp;quot;, one_dataset[5])

      month &amp;lt;- ifelse(nchar(month) == 1, paste0(&amp;quot;0&amp;quot;, month), month)

      ymd(paste0(year, &amp;quot;-&amp;quot;, month, &amp;quot;-01&amp;quot;))
  }

  create_second_replacement &amp;lt;- function(one_dataset){
      one_dataset &amp;lt;- pull(one_dataset, odometer_reading)
      month &amp;lt;- one_dataset[7]
      year &amp;lt;- paste0(&amp;quot;19&amp;quot;, one_dataset[8])

      month &amp;lt;- ifelse(nchar(month) == 1, paste0(&amp;quot;0&amp;quot;, month), month)

      ymd(paste0(year, &amp;quot;-&amp;quot;, month, &amp;quot;-01&amp;quot;))
  }

  get_bus_id &amp;lt;- function(one_dataset){
      one_dataset &amp;lt;- pull(one_dataset, odometer_reading)
      one_dataset[1]
  }

  named_headers &amp;lt;- map(headers_list, ~set_names(., header_elements))


  raw_data &amp;lt;- map(sep_data_sets, ~tibble(&amp;quot;odometer_reading&amp;quot; = .))
  raw_data &amp;lt;- map(raw_data, ~mutate(., &amp;quot;date&amp;quot; = create_start_date(.)))
  raw_data &amp;lt;- map(raw_data, ~mutate(., &amp;quot;first_replacement_date&amp;quot; = create_first_replacement(.)))
  raw_data &amp;lt;- map(raw_data, ~mutate(., &amp;quot;second_replacement_date&amp;quot; = create_second_replacement(.)))
  raw_data &amp;lt;- map(raw_data, ~mutate(., &amp;quot;bus_id&amp;quot; = get_bus_id(.)))
  raw_data &amp;lt;- map(raw_data, ~slice(., -c(1:11)))

  fill_dates &amp;lt;- function(vector){
      for(i in 2:length(vector)){
          vector[i] &amp;lt;- add_with_rollback(vector[i-1], months(1))
          # the line below can be uncommented to skip the 2 months of strike in 1980
          #vector[i] &amp;lt;- if_else(vector[i] == ymd(&amp;quot;1980-07-01&amp;quot;), add_with_rollback(vector[i], months(2)),
          #                    vector[i])
      }
      vector
  }

  raw_data &amp;lt;- raw_data %&amp;gt;%
      map(~mutate(., date = fill_dates(date)))

  raw_data &amp;lt;- map(raw_data, ~mutate(., &amp;quot;replacement_1&amp;quot; = if_else(date == first_replacement_date, 1, 0, 0)))
  raw_data &amp;lt;- map(raw_data, ~mutate(., &amp;quot;replacement_2&amp;quot; = if_else(date == second_replacement_date, 1, 0, 0)))
  raw_data &amp;lt;- map(raw_data, ~mutate(., replacement = replacement_1 + replacement_2))
  raw_data &amp;lt;- map(raw_data, ~select(., bus_id, date, odometer_reading, replacement,
                                    -replacement_1, -replacement_2, -first_replacement_date, -second_replacement_date))

  return(raw_data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, as usual, I didn’t write this in one go. First, I experimented bits and pieces of code on
one single dataset, and then only started putting these pieces together into this big function.&lt;/p&gt;
&lt;p&gt;I won’t go through this function line by line, because it would take me ages. I think there are
two majors things to understand in this function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first identify the start of a particular bus’s data;&lt;/li&gt;
&lt;li&gt;second this function uses some intermediary &lt;code&gt;{purrr}&lt;/code&gt; magic.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So first step, identify the start of the monthly odometer reading for one bus. For the first bus
this is quite simple, as it is simply the start of the file. But when does the data for the
second bus start? Thankfully, buses’ ids are numbers, and they’re in incrementing order in the data.
I use this to get the index of the second bus, and compute the number of rows between the id of
the first and second bus, which gives me the number of months of odometer readings for the first
bus.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  data_file &amp;lt;- as.numeric(data_file)
  first_bus &amp;lt;- data_file[1]
  second_bus &amp;lt;- first_bus + 1
  second_bus_index &amp;lt;- which(data_file == second_bus)

  nb_data_points &amp;lt;- second_bus_index - 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I get the number of buses in the data, and create a vector with all the indices of the
buses’ ids:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  nb_buses &amp;lt;- length(data_file) / nb_data_points

  indices &amp;lt;- nb_data_points * seq(1, nb_buses)

  indices &amp;lt;- c(0, indices)

  sep_data_sets &amp;lt;- map(indices, ~`[`(data_file, (. + 1):(. + nb_data_points) ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I end up with a list of lists, &lt;code&gt;sep_data_sets&lt;/code&gt;.
The first element of my list is now a list, with the data from the
&lt;code&gt;a452372.asc&lt;/code&gt; file, where each element is the data for a single bus.&lt;/p&gt;
&lt;p&gt;For instance, here is the first element of &lt;code&gt;sep_data_sets&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(sep_data_sets[[1]])
List of 19
 $ : num [1:137] 4239 2 72 1 76 ...
 $ : num [1:137] 4240 2 72 1 75 ...
 $ : num [1:137] 4241 2 72 5 75 ...
 $ : num [1:137] 4242 2 72 2 76 ...
 $ : num [1:137] 4243 2 72 4 76 ...
 $ : num [1:137] 4244 2 72 3 78 ...
 $ : num [1:137] 4245 2 72 1 75 ...
 $ : num [1:137] 4246 2 72 3 75 ...
 $ : num [1:137] 4247 2 72 9 80 ...
 $ : num [1:137] 4248 2 72 2 75 ...
 $ : num [1:137] 4249 2 72 7 75 ...
 $ : num [1:137] 4250 2 72 4 80 ...
 $ : num [1:137] 4251 2 72 1 79 ...
 $ : num [1:137] 4252 2 72 5 76 ...
 $ : num [1:137] 4253 2 72 1 77 ...
 $ : num [1:137] 4254 2 72 3 76 ...
 $ : num [1:137] 4255 2 72 1 76 ...
 $ : num [1:137] 4256 2 72 9 77 ...
 $ : num [1:137] NA NA NA NA NA NA NA NA NA NA ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So there are 18 buses in the first group of data (the last line full of NA’s is due to the fact
that I messed up my indices vector, I’ll simply remove these at the end).&lt;/p&gt;
&lt;p&gt;That’s the first step. The second step, is to make use of this list structure to apply some
cleaning functions to each dataset using &lt;code&gt;{purrr}&lt;/code&gt;. I explain the approach in my ebook, which you
can read for free
&lt;a href=&#34;https://b-rodrigues.github.io/modern_R/functional-programming.html#list-based-workflows-for-efficiency&#34;&gt;here&lt;/a&gt;.
The idea is to use a function that would work on a single element of your list, and then mapping
this over all the elements of the list. For instance, remember that the 11 first elements of
the data are some kind of header? To extract those for one single vector of observations, one
would use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_vector[1:11]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or, equivalently:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`[`(my_vector, 1:11)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, when faced with a list of vectors, one maps this function over the whole list using &lt;code&gt;map()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(my_list_of_vectors, `[`(1:11))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the logic of this big &lt;code&gt;process_bus_data()&lt;/code&gt; function. If something’s not clear after you study
it, drop me an email or tweet.&lt;/p&gt;
&lt;p&gt;Anyways, now that I cleaned the data, here’s how it looks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_buses &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/b-rodrigues/rust/ee15fb87fc4ba5db28d055c97a898b328725f53c/datasets/processed_data/all_buses.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   bus_id = col_double(),
##   date = col_date(format = &amp;quot;&amp;quot;),
##   odometer_reading = col_double(),
##   replacement = col_double(),
##   bus_family = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(all_buses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   bus_id date       odometer_reading replacement bus_family
##    &amp;lt;dbl&amp;gt; &amp;lt;date&amp;gt;                &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     
## 1   4239 1974-12-01           140953           0 a452372   
## 2   4239 1975-01-01           142960           0 a452372   
## 3   4239 1975-02-01           145380           0 a452372   
## 4   4239 1975-03-01           148140           0 a452372   
## 5   4239 1975-04-01           150921           0 a452372   
## 6   4239 1975-05-01           153839           0 a452372&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tidy data frame now has the bus id, the odometer readings with the right date, and whether
a replacement occurred at that date. I said the right date, but in the original documentation of
the data, John Rust mentions a two month strike in July and August 1980, and he removed these
points from the data since the odometer readings where the same. I did not skip July and August
when I created the dates, even though I have added the code to do it in the function above, because
it does not matter.&lt;/p&gt;
&lt;p&gt;I have 166 in my sample, while John Rust writes in the paper that
his sample contains 162. I do not know why I have 4 more buses.&lt;/p&gt;
&lt;p&gt;Let’s try to reproduce Table 2a of the paper (mileage at replacement):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_buses %&amp;gt;% 
    group_by(bus_id) %&amp;gt;% 
    filter(replacement == 1) %&amp;gt;% 
    group_by(bus_family) %&amp;gt;% 
    summarise_at(.vars = vars(odometer_reading), 
                 .funs = list(~max(.), ~min(.), ~mean(.), ~sd(.)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   bus_family    max    min    mean     sd
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 a452372    334393 130810 193175. 53533.
## 2 a452374    237287  82370 151495  61246.
## 3 a530872    413132 170508 278292. 78529.
## 4 a530874    325336 117986 247119  60818.
## 5 a530875    388254 120709 263405. 64556.
## 6 t8h203     273369 125643 200685. 37120.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I find different slightly results, for instance, for bus family &lt;code&gt;t8h203&lt;/code&gt; I find an average of
200’685 miles, while the original author found 199’733. This difference comes very likely from
the fact that the author probably uses the value from the header, “odometer at replacement”, at
position 6, while I use the value of the odometer at that month, which is always slightly different.&lt;/p&gt;
&lt;p&gt;Let’s try to reproduce Table 2b, as well, mileage for buses who did not have a replacement:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_buses %&amp;gt;% 
    group_by(bus_id) %&amp;gt;% 
    filter(all(replacement == 0)) %&amp;gt;% 
    group_by(bus_family) %&amp;gt;% 
    summarise_at(.vars = vars(odometer_reading), 
                 .funs = list(~max(.), ~min(.), ~mean(.), ~sd(.)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 5
##   bus_family    max   min    mean      sd
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 a452374    299040  4249 156135.  81992.
## 2 a530874    326843 13065 197547.  86692.
## 3 a530875    352450   129 188193. 104453.
## 4 d309        65045   294  30643.  17063.
## 5 g870       120151   483  49582.  32353.
## 6 rt50       161748  1743  77506.  44674.
## 7 t8h203     280802  2950 127964.  72300.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I find exactly the same values as the author. To finish this quite long blog post, let’s
now plot the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(all_buses) + 
    geom_line(aes(y = odometer_reading, x = date, group = bus_id, col = bus_family)) + 
    labs(title = &amp;quot;Odometer readings&amp;quot;) +
    brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-01-26-harold_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s add some dots to mark the points in time where replacements happened:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(all_buses) + 
    geom_line(aes(y = odometer_reading, x = date, group = bus_id, col = bus_family)) + 
    geom_point(aes(y = ifelse(odometer_reading*replacement == 0, NA, odometer_reading*replacement), 
                              x = date), col = &amp;quot;red&amp;quot;) +
    labs(title = &amp;quot;Odometer readings and points in time where engine replacement occurred&amp;quot;) +
    brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 15840 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-01-26-harold_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s create a graph for each bus family:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(all_buses) + 
    geom_line(aes(y = odometer_reading, x = date, group = bus_id), col = &amp;quot;#82518c&amp;quot;) +
    geom_point(aes(y = ifelse(odometer_reading*replacement == 0, NA, odometer_reading*replacement), 
                              x = date), col = &amp;quot;red&amp;quot;) +
    facet_wrap(~bus_family) + 
    labs(title = &amp;quot;Odometer readings and points in time where engine replacement occurred&amp;quot;) +
    brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 15840 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2020-01-26-harold_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the next blog post, I’ll explore how recent reinforcement learning methods might help us get
the optimal policy from the data!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Intrumental variable regression and machine learning</title>
      <link>/blog/2019-11-06-explainability_econometrics/</link>
      <pubDate>Sat, 09 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-11-06-explainability_econometrics/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-Bu654lDuBk&#34;&gt; &lt;img src=&#34;/img/maybe.jpg&#34; title = &#34;Every time I look at observational data and wonder if 
  this correlation could imply causation, at least this one time.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;Just like the question “what’s the difference between machine learning and statistics” has shed a lot of ink (since at least &lt;a href=&#34;https://projecteuclid.org/euclid.ss/1009213726&#34;&gt;Breiman (2001)&lt;/a&gt;), the same question but where statistics is replaced by econometrics has led to a lot of discussion, as well. I like this presentation by &lt;a href=&#34;https://web.stanford.edu/class/ee380/Abstracts/140129-slides-Machine-Learning-and-Econometrics.pdf&#34;&gt;Hal Varian&lt;/a&gt; from almost 6 years ago. There’s a slide called “What econometrics can learn from machine learning”, which summarises in a few bullet points &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3&#34;&gt;Varian (2014)&lt;/a&gt; and the rest of the presentation discusses what machine learning can learn from econometrics. Varian argues that the difference between machine learning and econometrics is that machine learning focuses on prediction, while econometrics on inference and causality (and to a lesser extent prediction as well). Varian cites some methods that have been in the econometricians’ toolbox for decades (at least for some of them), such as regression discontinuity, difference in differences and instrumental variables regression. Another interesting paper is &lt;a href=&#34;https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87&#34;&gt;Mullainathan and Spiess&lt;/a&gt;, especially the section called &lt;em&gt;What Do We (Not) Learn from Machine Learning Output?&lt;/em&gt;. The authors discuss the tempting idea of using LASSO to perform variable (feature) selection. Econometricians might be tempted to use LASSO to perform variable selection, and draw conclusions such as &lt;em&gt;The variable (feature) “Number of rooms” has not been selected by LASSO, thus it plays no role in the prediction of house prices&lt;/em&gt;. However, when variables (features) are highly correlated, LASSO selects variables essentially randomly, without any meaningful impact on model performance (for prediction). I found this paragraph quite interesting (emphasis mine):&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This problem is ubiquitous in machine learning. The very appeal of these algorithms is that they can fit many different functions. But this creates an Achilles’ heel: more functions mean a greater chance that two functions with very different coefficients can produce similar prediction quality. As a result, how an algorithm chooses between two very different functions can effectively come down to the flip of a coin. In econometric terms, while the lack of &lt;strong&gt;standard errors&lt;/strong&gt; illustrates the limitations to making inference after model selection, the challenge here is (uniform) model selection consistency itself.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Assuming that we successfully dealt with model selection, we still have to content with significance of coefficients. There is recent research into this topic, such as &lt;a href=&#34;https://arxiv.org/abs/1902.06021v1&#34;&gt;Horel and Giesecke&lt;/a&gt;, but I wonder to what extent explainability could help with this. I have been looking around for papers that discuss explainability in the context of the social sciences but have not found any. If any of the readers of this blog are aware of such papers, please let me know.&lt;/p&gt;
&lt;p&gt;Just to wrap up Mullainathan and Spiess; the authors then suggest to use machine learning mainly for prediction tasks, such as using images taken using satellites to predict future harvest size (the authors cite &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0378429012002754&#34;&gt;Lobell (2013)&lt;/a&gt;), or for tasks that have an &lt;em&gt;implicit&lt;/em&gt; prediction component. For instance in the case of instrumental variables regression, two stages least squares is often used, and the first stage is a prediction task. Propensity score matching is another prediction task, where machine learning could be used. Other examples are presented as well. In this blog post, I’ll explore two stages least squares and see what happens when a random forest is used for the first step.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;instrumental-variables-regression-using-two-stage-least-squares&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Instrumental variables regression using two-stage least squares&lt;/h2&gt;
&lt;p&gt;Let’s work out a textbook (literally) example of instrumental variable regression. The below example is taken from Wooldrige’s &lt;em&gt;Econometric analysis of cross section and panel data&lt;/em&gt;, and is an exercise made using data from Mroz (1987) &lt;em&gt;The sensitivity of an empirical model of married women’s hours of work to economic and statistical assumptions&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let’s first load the needed packages and the data &lt;code&gt;&amp;quot;mroz&amp;quot;&lt;/code&gt; included in the &lt;code&gt;{wooldridge}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(randomForest)
library(wooldridge)
library(AER)
library(Metrics)

data(&amp;quot;mroz&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s only select the women that are in the labour force (&lt;code&gt;inlf == 1&lt;/code&gt;), and let’s run a simple linear regression. The dependent variable, or target, is &lt;code&gt;lwage&lt;/code&gt;, the logarithm of the wage, and the explanatory variables, or features are &lt;code&gt;exper&lt;/code&gt;, &lt;code&gt;expersq&lt;/code&gt; and &lt;code&gt;educ&lt;/code&gt;. For a full description of the data, click below:&lt;/p&gt;
&lt;p&gt;&lt;details&gt; &lt;summary&gt;Description of data&lt;/summary&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
mroz {wooldridge}   R Documentation
mroz

Description

Wooldridge Source: T.A. Mroz (1987), “The Sensitivity of an Empirical Model of Married Women’s Hours of Work to Economic and Statistical Assumptions,” Econometrica 55, 765-799. Professor Ernst R. Berndt, of MIT, kindly provided the data, which he obtained from Professor Mroz. Data loads lazily.

Usage

data(&amp;#39;mroz&amp;#39;)
Format

A data.frame with 753 observations on 22 variables:

inlf: =1 if in lab frce, 1975

hours: hours worked, 1975

kidslt6: # kids &amp;lt; 6 years

kidsge6: # kids 6-18

age: woman&amp;#39;s age in yrs

educ: years of schooling

wage: est. wage from earn, hrs

repwage: rep. wage at interview in 1976

hushrs: hours worked by husband, 1975

husage: husband&amp;#39;s age

huseduc: husband&amp;#39;s years of schooling

huswage: husband&amp;#39;s hourly wage, 1975

faminc: family income, 1975

mtr: fed. marg. tax rte facing woman

motheduc: mother&amp;#39;s years of schooling

fatheduc: father&amp;#39;s years of schooling

unem: unem. rate in county of resid.

city: =1 if live in SMSA

exper: actual labor mkt exper

nwifeinc: (faminc - wage*hours)/1000

lwage: log(wage)

expersq: exper^2

Used in Text

pages 249-251, 260, 294, 519-520, 530, 535, 535-536, 565-566, 578-579, 593- 595, 601-603, 619-620, 625

Source

https://www.cengage.com/cgi-wadsworth/course_products_wp.pl?fid=M20b&amp;amp;product_isbn_issn=9781111531041
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/details&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;working_w &amp;lt;- mroz %&amp;gt;% 
    filter(inlf == 1)

wage_lm &amp;lt;- lm(lwage ~ exper + expersq + educ, 
              data = working_w)

summary(wage_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = lwage ~ exper + expersq + educ, data = working_w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.08404 -0.30627  0.04952  0.37498  2.37115 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.5220406  0.1986321  -2.628  0.00890 ** 
## exper        0.0415665  0.0131752   3.155  0.00172 ** 
## expersq     -0.0008112  0.0003932  -2.063  0.03974 *  
## educ         0.1074896  0.0141465   7.598 1.94e-13 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6664 on 424 degrees of freedom
## Multiple R-squared:  0.1568, Adjusted R-squared:  0.1509 
## F-statistic: 26.29 on 3 and 424 DF,  p-value: 1.302e-15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we see that education is statistically significant and the effect is quite high. The return to education is about 11%. Now, let’s add some more explanatory variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wage_lm2 &amp;lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ, 
              data = working_w)

summary(wage_lm2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + 
##     huswage + city + educ, data = working_w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.07431 -0.30500  0.05477  0.37871  2.31157 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -0.3853695  0.3163043  -1.218  0.22378    
## exper        0.0398817  0.0133651   2.984  0.00301 ** 
## expersq     -0.0007400  0.0003985  -1.857  0.06402 .  
## kidslt6     -0.0564071  0.0890759  -0.633  0.52692    
## kidsge6     -0.0143165  0.0276579  -0.518  0.60499    
## husage      -0.0028828  0.0049338  -0.584  0.55934    
## huswage      0.0177470  0.0102733   1.727  0.08482 .  
## city         0.0119960  0.0725595   0.165  0.86877    
## educ         0.0986810  0.0151589   6.510 2.16e-10 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6669 on 419 degrees of freedom
## Multiple R-squared:  0.1654, Adjusted R-squared:  0.1495 
## F-statistic: 10.38 on 8 and 419 DF,  p-value: 2.691e-13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The return to education lowers a bit, but is still significant. Now, the issue is that education is not exogenous (randomly assigned), and is thus correlated with the error term of the regression, due to an omitted variable for instance contained in the error term, that is correlated with education (for example work ethic).&lt;/p&gt;
&lt;p&gt;To deal with this, econometricians use instrumental variables (IV) regression. I won’t go into detail here; just know that this method can deal with these types of issues. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Instrumental_variables_estimation&#34;&gt;Wikipedia&lt;/a&gt; page gives a good intro on what this is all about. This short &lt;a href=&#34;https://wol.iza.org/uploads/articles/250/pdfs/using-instrumental-variables-to-establish-causality.pdf&#34;&gt;paper&lt;/a&gt; is also quite interesting in introducing instrumental variables.&lt;/p&gt;
&lt;p&gt;In practice, IV is done in two steps. First, regress the endogenous variable, in our case education, on all the explanatory variables from before, plus so called instruments. Instruments are variables that are correlated with the endogenous variable, here education, but uncorrelated to the error term. They only affect the target variable through their correlation with the endogenous variable. We will be using the education level of the parents of the women, as well as the education levels of their husbands as intruments. The assumption is that the parents’, as well as the husband’s education are exogenous in the log wage of the woman. This assumption can of course be challenged, but let’s say that it holds.&lt;/p&gt;
&lt;p&gt;To conclude stage 1, we obtain the predictions of education:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first_stage &amp;lt;- lm(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage 
                  + city + motheduc +  fatheduc + huseduc, data = working_w)

working_w$predictions_first_stage &amp;lt;- predict(first_stage)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready for the second stage. In the regression from before:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wage_lm2 &amp;lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ, 
              data = working_w)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we now replace &lt;code&gt;educ&lt;/code&gt; with the predictions of stage 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second_stage &amp;lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage 
                   + city + predictions_first_stage,
                  data = working_w)

summary(second_stage)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + 
##     huswage + city + predictions_first_stage, data = working_w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.13493 -0.30004  0.03046  0.37142  2.27199 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)              0.1763588  0.4206911   0.419   0.6753   
## exper                    0.0419047  0.0139885   2.996   0.0029 **
## expersq                 -0.0007881  0.0004167  -1.891   0.0593 . 
## kidslt6                 -0.0255934  0.0941128  -0.272   0.7858   
## kidsge6                 -0.0234422  0.0291914  -0.803   0.4224   
## husage                  -0.0042628  0.0051919  -0.821   0.4121   
## huswage                  0.0263802  0.0114511   2.304   0.0217 * 
## city                     0.0215685  0.0759034   0.284   0.7764   
## predictions_first_stage  0.0531993  0.0263735   2.017   0.0443 * 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6965 on 419 degrees of freedom
## Multiple R-squared:  0.08988,    Adjusted R-squared:  0.0725 
## F-statistic: 5.172 on 8 and 419 DF,  p-value: 3.581e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that education, now instrumented by the parents’ and the husband’s education is still significant, but the effect is much lower. The return to education is now about 5%. However, should our assumption hold, this effect is now &lt;em&gt;causal&lt;/em&gt;. However there are some caveats. The IV estimate is a local average treatment effect, meaning that we only get the effect on those individuals that were affected by the treatment. In this case, it would mean that the effect we recovered is only for women who were not planning on, say, studying, but only did so under the influence of their parents (or vice-versa).&lt;/p&gt;
&lt;p&gt;IV regression can also be achieved using the &lt;code&gt;ivreg()&lt;/code&gt; function from the &lt;code&gt;{AER}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inst_reg &amp;lt;- ivreg(lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage + city + educ 
                  | .-educ + motheduc + fatheduc + huseduc,
                  data = working_w)

summary(inst_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## ivreg(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + 
##     husage + huswage + city + educ | . - educ + motheduc + fatheduc + 
##     huseduc, data = working_w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.10175 -0.30407  0.03379  0.35255  2.25107 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)  0.1763588  0.4071522   0.433   0.6651   
## exper        0.0419047  0.0135384   3.095   0.0021 **
## expersq     -0.0007881  0.0004033  -1.954   0.0514 . 
## kidslt6     -0.0255934  0.0910840  -0.281   0.7789   
## kidsge6     -0.0234422  0.0282519  -0.830   0.4071   
## husage      -0.0042628  0.0050249  -0.848   0.3967   
## huswage      0.0263802  0.0110826   2.380   0.0177 * 
## city         0.0215685  0.0734606   0.294   0.7692   
## educ         0.0531993  0.0255247   2.084   0.0377 * 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6741 on 419 degrees of freedom
## Multiple R-Squared: 0.1475,  Adjusted R-squared: 0.1312 
## Wald test: 5.522 on 8 and 419 DF,  p-value: 1.191e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, great, now let’s see how a machine learning practitioner who took an econometrics MOOC might tackle the issue. The first step will be to split the data into training and testing sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
sample &amp;lt;- sample.int(n = nrow(working_w), size = floor(.90*nrow(working_w)), replace = F)
train &amp;lt;- working_w[sample, ]
test  &amp;lt;- working_w[-sample, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now run the same analysis as above, but let’s compute the RMSE of the first stage regression on the testing data as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first_stage &amp;lt;- lm(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage 
                  + city + motheduc +  fatheduc + huseduc, data = train)

test$predictions_first_stage &amp;lt;- predict(first_stage, newdata = test)

lm_rmse &amp;lt;- rmse(predicted = test$predictions_first_stage, actual = test$educ)

train$predictions_first_stage &amp;lt;- predict(first_stage)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first stage is done, let’s go with the second stage:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second_stage &amp;lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + 
                       husage + huswage + city + predictions_first_stage,
                  data = train)

summary(second_stage)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + 
##     huswage + city + predictions_first_stage, data = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.09828 -0.28606  0.05248  0.37258  2.29947 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)             -0.0037711  0.4489252  -0.008  0.99330   
## exper                    0.0449370  0.0145632   3.086  0.00218 **
## expersq                 -0.0008394  0.0004344  -1.933  0.05404 . 
## kidslt6                 -0.0630522  0.0963953  -0.654  0.51345   
## kidsge6                 -0.0197164  0.0306834  -0.643  0.52089   
## husage                  -0.0034744  0.0054358  -0.639  0.52310   
## huswage                  0.0219622  0.0118602   1.852  0.06484 . 
## city                     0.0679668  0.0804317   0.845  0.39863   
## predictions_first_stage  0.0618777  0.0283253   2.185  0.02954 * 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6952 on 376 degrees of freedom
## Multiple R-squared:  0.1035, Adjusted R-squared:  0.08438 
## F-statistic: 5.424 on 8 and 376 DF,  p-value: 1.764e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficients here are a bit different due to the splitting, but that’s not an issue. Ok, great, but our machine learning engineer is in love with random forests, so he wants to use a random forest for the prediction task of the first stage:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomForest)

first_stage_rf &amp;lt;- randomForest(educ ~ exper + expersq + kidslt6 + kidsge6 + husage + huswage 
                  + city + motheduc +  fatheduc + huseduc, 
                               data = train)

test$predictions_first_stage_rf &amp;lt;- predict(first_stage_rf, newdata = test)

rf_rmse &amp;lt;- rmse(predicted = test$predictions_first_stage_rf, actual = test$educ)

train$predictions_first_stage_rf &amp;lt;- predict(first_stage_rf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s compare the RMSE’s of the two first stages. The RMSE of the first stage using linear regression was 2.0558723 and for the random forest 2.0000417. Our machine learning engineer is happy, because the random forest has better performance. Let’s now use the predictions for the second stage:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second_stage_rf_lm &amp;lt;- lm(lwage ~ exper + expersq + kidslt6 + kidsge6 + 
                             husage + huswage + city + predictions_first_stage_rf,
                  data = train)

summary(second_stage_rf_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = lwage ~ exper + expersq + kidslt6 + kidsge6 + husage + 
##     huswage + city + predictions_first_stage_rf, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0655 -0.3198  0.0376  0.3710  2.3277 
## 
## Coefficients:
##                              Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)                -0.0416945  0.4824998  -0.086  0.93118   
## exper                       0.0460311  0.0145543   3.163  0.00169 **
## expersq                    -0.0008594  0.0004344  -1.978  0.04863 * 
## kidslt6                    -0.0420827  0.0952030  -0.442  0.65872   
## kidsge6                    -0.0211208  0.0306490  -0.689  0.49117   
## husage                     -0.0033102  0.0054660  -0.606  0.54514   
## huswage                     0.0229111  0.0118142   1.939  0.05322 . 
## city                        0.0688384  0.0805209   0.855  0.39314   
## predictions_first_stage_rf  0.0629275  0.0306877   2.051  0.04100 * 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.6957 on 376 degrees of freedom
## Multiple R-squared:  0.1021, Adjusted R-squared:  0.08302 
## F-statistic: 5.346 on 8 and 376 DF,  p-value: 2.251e-06&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The results are pretty similar. Now, why not go a bit further and use a random forest for the second stage as well?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-stage-random-forests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two-stage random forests&lt;/h2&gt;
&lt;p&gt;I have tried to find literature on this, but did not find anything that really fits what I’ll be doing here. Honestly, I don’t know if this is sound theoretically, but it does have intuitive appeal. Using random forests instead of the linear regressions of each stages poses at least the following question: how can we interpret the results of the second stage? As you have seen above, interpretation of the coefficients and standard errors is important, and random forests do not provide this. My idea is to use explainability techniques of black box models, such as partial dependence plots. In this setting, the whole first stage could be interpreted as a feature engineering step. Let’s do it and see what happens.&lt;/p&gt;
&lt;p&gt;We already have the first step from before, so let’s go straight to the first step:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second_stage_rf_rf &amp;lt;- randomForest(lwage ~ exper + expersq + kidslt6 + kidsge6 + 
                                       husage + huswage + city + predictions_first_stage_rf,
                  data = train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now use the &lt;code&gt;{iml}&lt;/code&gt; package for explainability. Let’s start first by loading the package, defining a predictor object, and then get model-agnostic feature importance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;iml&amp;quot;)

predictor &amp;lt;- Predictor$new(
  model = second_stage_rf_rf, 
  data = select(test, exper, expersq,
                kidslt6, kidsge6,
                husage, huswage, city,
                predictions_first_stage_rf), 
  y = test$lwage, 
  predict.fun = predict,
  class = &amp;quot;regression&amp;quot;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plot below shows the ratio of the original model error and model error after permutation. A higher value indicates that this feature is important:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_rf &amp;lt;- FeatureImp$new(predictor, loss = &amp;quot;rmse&amp;quot;)

plot(imp_rf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-06-explainability_econometrics_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;According to this measure of feature importance, there does not seem to be any feature that is important in predicting log wage. This is similar to the result we had with linear regression: most coefficients were not statistically significant, but some were. Does that mean that we should not trust the results of linear regression? After all, how likely is it that log wages can be modeled as a linear combination of features?&lt;/p&gt;
&lt;p&gt;Let’s see if the random forest was able to undercover strong interaction effects:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interactions &amp;lt;- Interaction$new(predictor, feature = &amp;quot;predictions_first_stage_rf&amp;quot;)

plot(interactions)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-06-explainability_econometrics_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This can seem to be a surprising result: education interacts strongly with the number of kids greater than 6 in household. But is it? Depending on a woman’s age, in order to have 2 or 3 kids with ages between 6 and 18, she would have needed to start having them young, and thus could not have pursued a master’s degree, or a PhD. The interaction strength is measured as the share of variance that is explained by the interaction.&lt;/p&gt;
&lt;p&gt;Let’s now take a look at the partial dependence plots and individual conditional expectation curves. Let me quote the advantages of pdps from &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/pdp.html#advantages-5&#34;&gt;Christoph Molnar’s&lt;/a&gt; book on interpretable machine learning:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The calculation for the partial dependence plots has a causal interpretation. We intervene on a feature and measure the changes in the predictions. In doing so, we analyze the causal relationship between the feature and the prediction. The relationship is causal for the model – because we explicitly model the outcome as a function of the features – but not necessarily for the real world!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;That sounds good. If we can defend the assumption that our instruments are valid, then the relationship should between the feature and the prediction should be causal, and not only for the model. However, pdps have a shortcoming. Again, quoting Christoph Molnar:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The assumption of independence is the biggest issue with PD plots. It is assumed that the feature(s) for which the partial dependence is computed are not correlated with other features.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let’s take a look at the correlation of features&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corr_vars &amp;lt;- cor(select(test, exper, expersq,
                        kidslt6, kidsge6,
                        husage, huswage, city,
                        predictions_first_stage_rf)) %&amp;gt;% 
    as.data.frame() %&amp;gt;% 
    rownames_to_column() %&amp;gt;% 
    pivot_longer(-rowname, names_to = &amp;quot;vars2&amp;quot;) %&amp;gt;% 
    rename(vars1 = rowname)

head(corr_vars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   vars1 vars2    value
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 exper exper    1    
## 2 exper expersq  0.959
## 3 exper kidslt6 -0.272
## 4 exper kidsge6 -0.360
## 5 exper husage   0.487
## 6 exper huswage -0.181&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corr_vars %&amp;gt;% 
    mutate(value = abs(value)) %&amp;gt;% 
    filter(value != 1, value &amp;gt; 0.2) %&amp;gt;%
    filter(vars1 == &amp;quot;predictions_first_stage_rf&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
##   vars1                      vars2   value
##   &amp;lt;chr&amp;gt;                      &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 predictions_first_stage_rf expersq 0.243
## 2 predictions_first_stage_rf kidslt6 0.292
## 3 predictions_first_stage_rf husage  0.217
## 4 predictions_first_stage_rf huswage 0.494
## 5 predictions_first_stage_rf city    0.369&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Only 5 variables have a correlation greater than 0.2 with education, and the one with highest correlation is the husband’s wage. It would seem that this situation is ideal to use pdps and ice curves. Before computing them however, let’s read about ice curves:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Individual conditional expectation curves are even more intuitive to understand than partial dependence plots. One line represents the predictions for one instance if we vary the feature of interest.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;however:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ICE curves can only display one feature meaningfully, because two features would require the drawing of several overlaying surfaces and you would not see anything in the plot. ICE curves suffer from the same problem as PDPs: If the feature of interest is correlated with the other features, then some points in the lines might be invalid data points according to the joint feature distribution. If many ICE curves are drawn, the plot can become overcrowded and you will not see anything. The solution: Either add some transparency to the lines or draw only a sample of the lines. In ICE plots it might not be easy to see the average. This has a simple solution: Combine individual conditional expectation curves with the partial dependence plot. Unlike partial dependence plots, ICE curves can uncover heterogeneous relationships.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So great, let’s go:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inst_effect &amp;lt;- FeatureEffect$new(predictor, &amp;quot;predictions_first_stage_rf&amp;quot;, method = &amp;quot;pdp+ice&amp;quot;)

plot(inst_effect) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-06-explainability_econometrics_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Interesting, we see that the curves are fairly similar, but there seem to be two groups: one group were adding education years increases wages, and another where the effect seems to remain constant.&lt;/p&gt;
&lt;p&gt;Let’s try to dig a bit deeper, and get explanations for individual predictions. For this, I create two new observations that have exactly the same features, but one without children older than 6 and another with two children older than 6:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(new_obs &amp;lt;- data.frame(
    exper = rep(10, 2),
    expersq = rep(100, 2),
    kidslt6 = rep(1, 2),
    kidsge6 = c(0, 2),
    husage = rep(35, 2),
    huswage = rep(6, 2),
    city = rep(1, 2),
    predictions_first_stage_rf = rep(10, 2)
))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   exper expersq kidslt6 kidsge6 husage huswage city
## 1    10     100       1       0     35       6    1
## 2    10     100       1       2     35       6    1
##   predictions_first_stage_rf
## 1                         10
## 2                         10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what the model predicts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(second_stage_rf_rf, newdata = new_obs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2 
## 1.139720 1.216423&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try to understand the difference between these two predictions. For this, we will be using Shapley values as described &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/shapley.html&#34;&gt;here&lt;/a&gt;. Shapley values use game theory to compute the contribution of each feature towards the prediction of one particular observation. Interpretation of the Shapley values is as follows (quoting Christoph Molnar’s book): &lt;em&gt;Given the current set of feature values, the contribution of a feature value to the difference between the actual prediction and the mean prediction is the estimated Shapley value.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let’s compute the Shapley values of all the features:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shapley_1 &amp;lt;-  Shapley$new(predictor, x.interest = new_obs[1, ], sample.size = 100)
shapley_2 &amp;lt;-  Shapley$new(predictor, x.interest = new_obs[2, ], sample.size = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(shapley_1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-06-explainability_econometrics_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(shapley_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-06-explainability_econometrics_files/figure-html/unnamed-chunk-22-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The average prediction is 1.21 and the prediction for the first new observation is 1.14, which is 0.07 below the average prediction. This difference of 0.07 is the sum of the Shapley values. For the second observation, the prediction is 1.22, so 0.01 above the average prediction. The order and magnitude of contributions is not the same as well; and surprisingly, the contribution of the instrumented education to the prediction is negative.&lt;/p&gt;
&lt;p&gt;Ok, let’s end this here. I’m quite certain that explainability methods will help econometricians adopt more machine learning methods in the future, and I am also excited to see the research of causality in machine learning and AI continue.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and &lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiple data imputation and explainability</title>
      <link>/blog/2019-11-02-mice_exp/</link>
      <pubDate>Sat, 02 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-11-02-mice_exp/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://xkcd.com/303/&#34;&gt;
&lt;img src=&#34;/img/kermit.png&#34; title = &#34;It is always so tempting&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Imputing missing values is quite an important task, but in my experience, very often, it is performed
using very simplistic approaches. The basic approach is to impute missing values for
numerical features using the average of each feature, or using the mode for categorical features.
There are better ways of imputing missing values, for instance by predicting the values using
a regression model, or KNN. However, imputing only once is not enough, because each imputed
value carries with it a certain level of uncertainty. To account for this, it is better to perform
multiple imputation. This means that if you impute your dataset 10 times, you’ll end up with 10
different datasets. Then, you should perform your analysis 10 times, for instance, if training
a machine learning model, you should train it on the 10 datasets (and do a train/test split
for each, even potentially tune a model for each). Finally, you should pool the results of
these 10 analyses.&lt;/p&gt;
&lt;p&gt;I have met this approach in the social sciences and statistical literature in general, but
very rarely in machine learning. Usually, in the social sciences, explainability is the goal of
fitting statistical models to data, and the approach I described above is very well suited for this.
Fit 10 (linear) regressions to each imputed dataset, and then pool the estimated coefficients/weights
together. Rubin’s rule is used to pool these estimates. You can read more about this rule
&lt;a href=&#34;https://bookdown.org/mwheymans/bookmi/rubins-rules.html&#34;&gt;here&lt;/a&gt;.
In machine learning, the task is very often prediction; in this case, you should pool the
predictions. Computing the average and other statistics of the predictions
seem to work just fine in practice.&lt;/p&gt;
&lt;p&gt;However, if you are mainly interested in explainability, how should you proceed? I’ve thought a
bit about it, and the answer, is “exactly the same way”… I think.
What I’m sure about, is you should impute m times, run the analysis m times
(which in this case will include getting explanations) and then pool. So the idea is to be able to
pool explanations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explainability-in-the-standard-case-no-missing-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explainability in the “standard” case (no missing values)&lt;/h2&gt;
&lt;p&gt;To illustrate this idea, I’ll be using the &lt;code&gt;{mice}&lt;/code&gt; package for multiple imputation,
&lt;code&gt;{h2o}&lt;/code&gt; for the machine learning bit and&lt;code&gt;{iml}&lt;/code&gt; for explainability. Note that I could have used
any other machine learning package instead of &lt;code&gt;{h2o}&lt;/code&gt; as &lt;code&gt;{iml}&lt;/code&gt; is totally &lt;em&gt;package-agnostic&lt;/em&gt;.
However, I have been experimenting with &lt;code&gt;{h2o}&lt;/code&gt;’s automl implementation lately, so I happened
to have code on hand. Let’s start with the “standard” case where the data does not have any missing
values.&lt;/p&gt;
&lt;p&gt;First let’s load the needed packages and initialize &lt;code&gt;h2o&lt;/code&gt; functions with &lt;code&gt;h2o.init()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(Ecdat)
library(mice)
library(h2o)
library(iml)
h2o.init()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ll be using the &lt;code&gt;DoctorContacts&lt;/code&gt; data. Here’s a description:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to view the description of the data&lt;/summary&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DoctorContacts              package:Ecdat              R Documentation

Contacts With Medical Doctor

Description:

     a cross-section from 1977-1978

     _number of observations_ : 20186

Usage:

     data(DoctorContacts)
     
Format:

     A time serie containing :

     mdu number of outpatient visits to a medical doctor

     lc log(coinsrate+1) where coinsurance rate is 0 to 100

     idp individual deductible plan ?

     lpi log(annual participation incentive payment) or 0 if no payment

     fmde log(max(medical deductible expenditure)) if IDP=1 and MDE&amp;gt;1
          or 0 otherw

     physlim physical limitation ?

     ndisease number of chronic diseases

     health self-rate health (excellent,good,fair,poor)

     linc log of annual family income (in \$)

     lfam log of family size

     educdec years of schooling of household head

     age exact age

     sex sex (male,female)

     child age less than 18 ?

     black is household head black ?

Source:

     Deb, P.  and P.K.  Trivedi (2002) “The Structure of Demand for
     Medical Care: Latent Class versus Two-Part Models”, _Journal of
     Health Economics_, *21*, 601-625.

References:

     Cameron, A.C.  and P.K.  Trivedi (2005) _Microeconometrics :
     methods and applications_, Cambridge, pp. 553-556 and 565.&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;The task is to predict &lt;code&gt;&amp;quot;mdu&amp;quot;&lt;/code&gt;, the number of outpatient visits to an MD. Let’s prepare the data
and split it into 3; a training, validation and holdout set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;DoctorContacts&amp;quot;)

contacts &amp;lt;- as.h2o(DoctorContacts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;splits &amp;lt;- h2o.splitFrame(data=contacts, ratios = c(0.7, 0.2))

original_train &amp;lt;- splits[[1]]

validation &amp;lt;- splits[[2]]

holdout &amp;lt;- splits[[3]]

features_names &amp;lt;- setdiff(colnames(original_train), &amp;quot;mdu&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, the ratios argument &lt;code&gt;c(0.7, 0.2)&lt;/code&gt; does not add up to 1.
This means that the first of the splits will have 70% of the data, the second split 20% and
the final 10% will be the holdout set.&lt;/p&gt;
&lt;p&gt;Let’s first go with a poisson regression. To obtain the same results as with R’s built-in &lt;code&gt;glm()&lt;/code&gt;
function, I use the options below, as per H2o’s glm
&lt;a href=&#34;http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html#faq&#34;&gt;faq&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you read Cameron and Trivedi’s &lt;em&gt;Microeconometrics&lt;/em&gt;, where this data is presented in the
context of count models, you’ll see that they also fit a negative binomial model 2 to this data,
as it allows for overdispersion. Here, I’ll stick to a simple poisson regression, simply
because the goal of this blog post is not to get the best model; as explained in the beginning,
this is an attempt at pooling explanations when doing multiple imputation (and it’s also because
GBMs, which I use below, do not handle the negative binomial model).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm_model &amp;lt;- h2o.glm(y = &amp;quot;mdu&amp;quot;, x = features_names,
                     training_frame = original_train,
                     validation_frame = validation,
                     compute_p_values = TRUE,
                     solver = &amp;quot;IRLSM&amp;quot;,
                     lambda = 0,
                     remove_collinear_columns = TRUE,
                     score_each_iteration = TRUE,
                     family = &amp;quot;poisson&amp;quot;, 
                     link = &amp;quot;log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I have this simple model, which returns the (almost) same results as R’s &lt;code&gt;glm()&lt;/code&gt; function,
I can take a look at coefficients and see which are important, because GLMs are easily
interpretable:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to view &lt;code&gt;h2o.glm()&lt;/code&gt;’s output&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(glm_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model Details:
## ==============
## 
## H2ORegressionModel: glm
## Model Key:  GLM_model_R_1572735931328_5 
## GLM Model: summary
##    family link regularization number_of_predictors_total
## 1 poisson  log           None                         16
##   number_of_active_predictors number_of_iterations  training_frame
## 1                          16                    5 RTMP_sid_8588_3
## 
## H2ORegressionMetrics: glm
## ** Reported on training data. **
## 
## MSE:  17.6446
## RMSE:  4.200547
## MAE:  2.504063
## RMSLE:  0.8359751
## Mean Residual Deviance :  3.88367
## R^2 :  0.1006768
## Null Deviance :64161.44
## Null D.o.F. :14131
## Residual Deviance :54884.02
## Residual D.o.F. :14115
## AIC :83474.52
## 
## 
## H2ORegressionMetrics: glm
## ** Reported on validation data. **
## 
## MSE:  20.85941
## RMSE:  4.56721
## MAE:  2.574582
## RMSLE:  0.8403465
## Mean Residual Deviance :  4.153042
## R^2 :  0.09933874
## Null Deviance :19667.55
## Null D.o.F. :4078
## Residual Deviance :16940.26
## Residual D.o.F. :4062
## AIC :25273.25
## 
## 
## 
## 
## Scoring History: 
##             timestamp   duration iterations negative_log_likelihood
## 1 2019-11-03 00:33:46  0.000 sec          0             64161.43611
## 2 2019-11-03 00:33:46  0.004 sec          1             56464.99004
## 3 2019-11-03 00:33:46  0.020 sec          2             54935.05581
## 4 2019-11-03 00:33:47  0.032 sec          3             54884.19756
## 5 2019-11-03 00:33:47  0.047 sec          4             54884.02255
## 6 2019-11-03 00:33:47  0.063 sec          5             54884.02255
##   objective
## 1   4.54015
## 2   3.99554
## 3   3.88728
## 4   3.88368
## 5   3.88367
## 6   3.88367
## 
## Variable Importances: (Extract with `h2o.varimp`) 
## =================================================
## 
##        variable relative_importance scaled_importance  percentage
## 1    black.TRUE          0.67756097        1.00000000 0.236627982
## 2   health.poor          0.48287163        0.71266152 0.168635657
## 3  physlim.TRUE          0.33962316        0.50124369 0.118608283
## 4   health.fair          0.25602066        0.37785627 0.089411366
## 5      sex.male          0.19542639        0.28842628 0.068249730
## 6      ndisease          0.16661902        0.24591001 0.058189190
## 7      idp.TRUE          0.15703578        0.23176627 0.054842384
## 8    child.TRUE          0.09988003        0.14741114 0.034881600
## 9          linc          0.09830075        0.14508030 0.034330059
## 10           lc          0.08126160        0.11993253 0.028379394
## 11         lfam          0.07234463        0.10677213 0.025265273
## 12         fmde          0.06622332        0.09773781 0.023127501
## 13      educdec          0.06416087        0.09469387 0.022407220
## 14  health.good          0.05501613        0.08119732 0.019213558
## 15          age          0.03167598        0.04675000 0.011062359
## 16          lpi          0.01938077        0.02860373 0.006768444&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;As a bonus, let’s see the output of the &lt;code&gt;glm()&lt;/code&gt; function:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to view &lt;code&gt;glm()&lt;/code&gt;’s output&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_tibble &amp;lt;- as_tibble(original_train)

r_glm &amp;lt;- glm(mdu ~ ., data = train_tibble,
            family = poisson(link = &amp;quot;log&amp;quot;))

summary(r_glm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = mdu ~ ., family = poisson(link = &amp;quot;log&amp;quot;), data = train_tibble)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.7039  -1.7890  -0.8433   0.4816  18.4703  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  0.0005100  0.0585681   0.009   0.9931    
## lc          -0.0475077  0.0072280  -6.573 4.94e-11 ***
## idpTRUE     -0.1794563  0.0139749 -12.841  &amp;lt; 2e-16 ***
## lpi          0.0129742  0.0022141   5.860 4.63e-09 ***
## fmde        -0.0166968  0.0042265  -3.951 7.80e-05 ***
## physlimTRUE  0.3182780  0.0126868  25.087  &amp;lt; 2e-16 ***
## ndisease     0.0222300  0.0007215  30.811  &amp;lt; 2e-16 ***
## healthfair   0.2434235  0.0192873  12.621  &amp;lt; 2e-16 ***
## healthgood   0.0231824  0.0115398   2.009   0.0445 *  
## healthpoor   0.4608598  0.0329124  14.003  &amp;lt; 2e-16 ***
## linc         0.0826053  0.0062208  13.279  &amp;lt; 2e-16 ***
## lfam        -0.1194981  0.0106904 -11.178  &amp;lt; 2e-16 ***
## educdec      0.0205582  0.0019404  10.595  &amp;lt; 2e-16 ***
## age          0.0041397  0.0005152   8.035 9.39e-16 ***
## sexmale     -0.2096761  0.0104668 -20.032  &amp;lt; 2e-16 ***
## childTRUE    0.1529588  0.0179179   8.537  &amp;lt; 2e-16 ***
## blackTRUE   -0.6231230  0.0176758 -35.253  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 64043  on 14096  degrees of freedom
## Residual deviance: 55529  on 14080  degrees of freedom
## AIC: 84052
## 
## Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;I could also use the excellent &lt;code&gt;{ggeffects}&lt;/code&gt; package to see the marginal effects of
different variables, for instance &lt;code&gt;&amp;quot;linc&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggeffects)

ggeffect(r_glm, &amp;quot;linc&amp;quot;) %&amp;gt;% 
    ggplot(aes(x, predicted)) +
    geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = &amp;quot;#0f4150&amp;quot;) +
    geom_line(colour = &amp;quot;#82518c&amp;quot;) +
    brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-02-mice_exp_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that as “linc” (and other covariates are held constant), the target variable increases.&lt;/p&gt;
&lt;p&gt;Let’s also take a look at the marginal effect of a categorical variable, namely &lt;code&gt;&amp;quot;sex&amp;quot;&lt;/code&gt;:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to view another example of marginal effects&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggeffects)

ggeffect(r_glm, &amp;quot;sex&amp;quot;) %&amp;gt;% 
    ggplot(aes(x, predicted)) +
    geom_point(colour = &amp;quot;#82518c&amp;quot;) +
    geom_errorbar(aes(x, ymin = conf.low, ymax = conf.high), colour = &amp;quot;#82518c&amp;quot;) +
    brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/blog/2019-11-02-mice_exp_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;
&lt;/details&gt;
&lt;p&gt;In the case of the &lt;code&gt;&amp;quot;sex&amp;quot;&lt;/code&gt; variable, men have significantly less doctor contacts than women.&lt;/p&gt;
&lt;p&gt;Now, let’s suppose that I want to train a model with a more complicated name, in order to justify
my salary. Suppose I go with one of those nifty &lt;em&gt;black-box&lt;/em&gt; models, for instance a GBM, which
very likely will perform better than the GLM from before. GBMs are available in &lt;code&gt;{h2o}&lt;/code&gt; via the
&lt;code&gt;h2o.gbm()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gbm_model &amp;lt;- h2o.gbm(y = &amp;quot;mdu&amp;quot;, x = features_names,
            training_frame = original_train,
            validation_frame = validation,
            distribution = &amp;quot;poisson&amp;quot;,
            score_each_iteration = TRUE,
            ntrees = 110,
            max_depth = 20,
            sample_rate = 0.6,
            col_sample_rate = 0.8,
            col_sample_rate_per_tree = 0.9,
            learn_rate = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To find a set of good hyper-parameter values, I actually used &lt;code&gt;h2o.automl()&lt;/code&gt; and then used the
returned parameter values from the leader model. Maybe I’ll write another blog post about
&lt;code&gt;h2o.automl()&lt;/code&gt; one day, it’s quite cool. Anyways, now, how do I get me some explainability out of
this? The model does perform better than the GLM as indicated by all the different metrics, but
now I cannot compute any marginal effects, or anything like that. I do get feature importance
by default with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.varimp(gbm_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Variable Importances: 
##    variable relative_importance scaled_importance percentage
## 1       age       380350.093750          1.000000   0.214908
## 2      linc       282274.343750          0.742143   0.159492
## 3  ndisease       245862.718750          0.646412   0.138919
## 4       lpi       173552.734375          0.456297   0.098062
## 5   educdec       148186.265625          0.389605   0.083729
## 6      lfam       139174.312500          0.365911   0.078637
## 7      fmde        94193.585938          0.247650   0.053222
## 8    health        86160.679688          0.226530   0.048683
## 9       sex        63502.667969          0.166958   0.035881
## 10       lc        50674.968750          0.133232   0.028633
## 11  physlim        45328.382812          0.119175   0.025612
## 12    black        26376.841797          0.069349   0.014904
## 13      idp        24809.185547          0.065227   0.014018
## 14    child         9382.916992          0.024669   0.005302&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but that’s it. And had I chosen a different “black-box” model, not based on trees, then I would
not even have that.
Thankfully, there’s the amazing &lt;code&gt;{iml}&lt;/code&gt; package that contains a lot of functions for model-agnostic
explanations. If you are not familiar with this package and the methods it implements, I highly
encourage you to read the free online &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/&#34;&gt;ebook&lt;/a&gt;
written by the packages author, Christoph Molnar
(who you can follow on &lt;a href=&#34;https://twitter.com/ChristophMolnar&#34;&gt;Twitter&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Out of the box, &lt;code&gt;{iml}&lt;/code&gt; works with several machine learning frameworks, such as &lt;code&gt;{caret}&lt;/code&gt; or &lt;code&gt;{mlr}&lt;/code&gt;
but not with &lt;code&gt;{h2o}&lt;/code&gt;. However, this is not an issue; you only need to create a predict function
which returns a data frame (&lt;code&gt;h2o.predict()&lt;/code&gt; used for prediction with h2o models returns an
h2o frame). I have found this interesting blog post from
&lt;a href=&#34;https://www.business-science.io/business/2018/08/13/iml-model-interpretability.html&#34;&gt;business-science.io&lt;/a&gt;
which explains how to do this. I highly recommend you read this blog post, as it goes much deeper
into the capabilities of &lt;code&gt;{iml}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So let’s write a predict function that &lt;code&gt;{iml}&lt;/code&gt; can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#source: https://www.business-science.io/business/2018/08/13/iml-model-interpretability.html
predict_for_iml &amp;lt;- function(model, newdata){
  as_tibble(h2o.predict(model, as.h2o(newdata)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And let’s now create a &lt;code&gt;Predictor&lt;/code&gt; object. These objects are used by &lt;code&gt;{iml}&lt;/code&gt; to create explanations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;just_features &amp;lt;- as_tibble(holdout[, 2:15])
actual_target &amp;lt;- as_tibble(holdout[, 1])

predictor_original &amp;lt;- Predictor$new(
  model = gbm_model, 
  data = just_features, 
  y = actual_target, 
  predict.fun = predict_for_iml
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;predictor_original&lt;/code&gt; can now be used to compute all kinds of explanations. I won’t go into much
detail here, as this blog post is already quite long (and I haven’t even reached what I actually
want to write about yet) and you can read more on the before-mentioned blog post or directly
from Christoph Molnar’s ebook linked above.&lt;/p&gt;
&lt;p&gt;First, let’s compute a partial dependence plot, which shows the marginal effect of a variable
on the outcome. This is to compare it to the one from the GLM model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;feature_effect_original &amp;lt;- FeatureEffect$new(predictor_original, &amp;quot;linc&amp;quot;, method = &amp;quot;pdp&amp;quot;)

plot(feature_effect_original) +
    brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-02-mice_exp_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;feature_effect_original &amp;lt;- FeatureEffect$new(predictor_original, &amp;quot;linc&amp;quot;, method = &amp;quot;pdp&amp;quot;)

plot(feature_effect_original) +
    brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Quite similar to the marginal effects from the GLM!
Let’s now compute model-agnostic feature importances:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;feature_importance_original &amp;lt;- FeatureImp$new(predictor_original, loss = &amp;quot;mse&amp;quot;)

plot(feature_importance_original)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-02-mice_exp_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And finally, the interaction effect of the &lt;code&gt;sex&lt;/code&gt; variable interacted with all the others:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interaction_sex_original &amp;lt;- Interaction$new(predictor_original, feature = &amp;quot;sex&amp;quot;)

plot(interaction_sex_original)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-02-mice_exp_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok so let’s assume that I’m happy with these explanations, and do need or want to go further.
This would be the end of it in an ideal world, but this is not an ideal world unfortunately,
but it’s the best we’ve got. In the real world, it often happens that data comes with missing values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data-and-explainability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing data and explainability&lt;/h2&gt;
&lt;p&gt;As explained in the beginning, I’ve been wondering how to deal with missing values when the goal
of the analysis is explainability. How can the explanations be pooled? Let’s start
with creating a data set with missing values, then perform multiple imputation, then perform
the analysis.&lt;/p&gt;
&lt;p&gt;First, let me create a &lt;code&gt;patterns&lt;/code&gt; matrix, that I will pass to the &lt;code&gt;ampute()&lt;/code&gt; function from the
&lt;code&gt;{mice}&lt;/code&gt; package. This function creates a dataset with missing values, and by using its &lt;code&gt;patterns&lt;/code&gt;
argument, I can decide which columns should have missing values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;patterns &amp;lt;- -1*(diag(1, nrow = 15, ncol = 15) - 1)

patterns[ ,c(seq(1, 6), c(9, 13))] &amp;lt;- 0

amputed_train &amp;lt;- ampute(as_tibble(original_train), prop = 0.1, patterns = patterns, mech = &amp;quot;MNAR&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Data is made numeric because the calculation of weights requires
## numeric data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the missingness pattern:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;naniar::vis_miss(amputed_train$amp) + 
    brotools::theme_blog() + 
      theme(axis.text.x=element_text(angle=90, hjust=1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-02-mice_exp_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, so now let’s suppose that this was the dataset I was given. As a serious data scientist,
I decide to perform multiple imputation first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imputed_train_data &amp;lt;- mice(data = amputed_train$amp, m = 10)

long_train_data &amp;lt;- complete(imputed_train_data, &amp;quot;long&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So because I performed multiple imputation 10 times, I now have 10 different datasets. I should
now perform my analysis on these 10 datasets, which means I should run my GBM on each of them,
and then get out the explanations for each of them. So let’s do just that. But first, let’s
change the columns back to how they were; to perform amputation, the factor columns were
converted to numbers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long_train_data &amp;lt;- long_train_data %&amp;gt;% 
    mutate(idp = ifelse(idp == 1, FALSE, TRUE),
           physlim = ifelse(physlim == 1, FALSE, TRUE),
           health = as.factor(case_when(health == 1 ~ &amp;quot;excellent&amp;quot;,
                              health == 2 ~ &amp;quot;fair&amp;quot;,
                              health == 3 ~ &amp;quot;good&amp;quot;, 
                              health == 4 ~  &amp;quot;poor&amp;quot;)),
           sex = as.factor(ifelse(sex == 1, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;)),
           child = ifelse(child == 1, FALSE, TRUE),
           black = ifelse(black == 1, FALSE, TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, so now we’re ready to go. I will use the &lt;code&gt;h2o.gbm()&lt;/code&gt; function on each imputed data set.
For this, I’ll use the &lt;code&gt;group_by()&lt;/code&gt;-&lt;code&gt;nest()&lt;/code&gt; trick which consists in grouping the dataset by
the &lt;code&gt;.imp&lt;/code&gt; column, then nesting it, then mapping the &lt;code&gt;h2o.gbm()&lt;/code&gt; function to each imputed
dataset. If you are not familiar with this, you can read
&lt;a href=&#34;https://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/&#34;&gt;this other&lt;/a&gt; blog post, which
explains the approach. I define a custom function, &lt;code&gt;train_on_imputed_data()&lt;/code&gt; to run &lt;code&gt;h2o.gbm()&lt;/code&gt; on
each imputed data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_on_imputed_data &amp;lt;- function(long_data){
    long_data %&amp;gt;% 
        group_by(.imp) %&amp;gt;% 
        nest() %&amp;gt;% 
        mutate(model = map(data, ~h2o.gbm(y = &amp;quot;mdu&amp;quot;, x = features_names,
            training_frame = as.h2o(.),
            validation_frame = validation,
            distribution = &amp;quot;poisson&amp;quot;,
            score_each_iteration = TRUE,
            ntrees = 110,
            max_depth = 20,
            sample_rate = 0.6,
            col_sample_rate = 0.8,
            col_sample_rate_per_tree = 0.9,
            learn_rate = 0.05)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the training takes place:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_trained &amp;lt;- train_on_imputed_data(long_train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;imp_trained&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_trained&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
## # Groups:   .imp [10]
##     .imp            data model     
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;lt;df[,16]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    
##  1     1   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
##  2     2   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
##  3     3   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
##  4     4   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
##  5     5   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
##  6     6   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
##  7     7   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
##  8     8   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
##  9     9   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;
## 10    10   [14,042 × 16] &amp;lt;H2ORgrsM&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the column &lt;code&gt;model&lt;/code&gt; contains one model for each imputed dataset. Now comes the
part I wanted to write about (finally): getting explanations out of this. Getting the explanations
from each model is not the hard part, that’s easily done using some &lt;code&gt;{tidyverse}&lt;/code&gt; magic (if
you’re following along, run this bit of code below, and go make dinner, have dinner, and
wash the dishes, because it takes time to run):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;make_predictors &amp;lt;- function(model){
    Predictor$new(
        model = model, 
        data = just_features, 
        y = actual_target, 
        predict.fun = predict_for_iml
        )
}

make_effect &amp;lt;- function(predictor_object, feature = &amp;quot;linc&amp;quot;, method = &amp;quot;pdp&amp;quot;){
    FeatureEffect$new(predictor_object, feature, method)
}

make_feat_imp &amp;lt;- function(predictor_object, loss = &amp;quot;mse&amp;quot;){
    FeatureImp$new(predictor_object, loss)
}

make_interactions &amp;lt;- function(predictor_object, feature = &amp;quot;sex&amp;quot;){
    Interaction$new(predictor_object, feature = feature)
}

imp_trained &amp;lt;- imp_trained %&amp;gt;%
    mutate(predictors = map(model, make_predictors)) %&amp;gt;% 
    mutate(effect_linc = map(predictors, make_effect)) %&amp;gt;% 
    mutate(feat_imp = map(predictors, make_feat_imp)) %&amp;gt;% 
    mutate(interactions_sex = map(predictors, make_interactions))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok so now that I’ve got these explanations, I am done with my analysis. This is the time to
pool the results together. Remember, in the case of regression models as used in the social
sciences, this means averaging the estimated model parameters and using Rubin’s rule to
compute their standard errors. But in this case, this is not so obvious. Should the
explanations be averaged? Should I instead analyse them one by one, and see if they differ?
My gut feeling is that they shouldn’t differ much, but who knows? Perhaps the answer is doing
a bit of both. I have checked online for a paper that would shed some light into this, but
have not found any. So let’s take a closer look to the explanations. Let’s look at feature
importance:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to view the 10 feature importances&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_trained %&amp;gt;% 
    pull(feat_imp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1 ndisease     1.0421605   1.362672      1.467244          22.03037
## 2     fmde     0.8611917   1.142809      1.258692          18.47583
## 3      lpi     0.8706659   1.103367      1.196081          17.83817
## 4   health     0.8941010   1.098014      1.480508          17.75164
## 5       lc     0.8745229   1.024288      1.296668          16.55970
## 6    black     0.7537278   1.006294      1.095054          16.26879
## 
## [[2]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1      age      0.984304   1.365702      1.473146          22.52529
## 2     linc      1.102023   1.179169      1.457907          19.44869
## 3 ndisease      1.075821   1.173938      1.642938          19.36241
## 4     fmde      1.059303   1.150112      1.281291          18.96944
## 5       lc      0.837573   1.132719      1.200556          18.68257
## 6  physlim      0.763757   1.117635      1.644434          18.43379
## 
## [[3]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1      age     0.8641304   1.334382      1.821797          21.62554
## 2    black     1.0553001   1.301338      1.429119          21.09001
## 3     fmde     0.8965085   1.208761      1.360217          19.58967
## 4 ndisease     1.0577766   1.203418      1.651611          19.50309
## 5     linc     0.9299725   1.114041      1.298379          18.05460
## 6      sex     0.9854144   1.091391      1.361406          17.68754
## 
## [[4]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##   feature importance.05 importance importance.95 permutation.error
## 1 educdec     0.9469049   1.263961      1.358115          20.52909
## 2     age     1.0980269   1.197441      1.763202          19.44868
## 3  health     0.8539843   1.133338      1.343389          18.40753
## 4    linc     0.7608811   1.123423      1.328756          18.24649
## 5     lpi     0.8203850   1.103394      1.251688          17.92118
## 6   black     0.9476909   1.089861      1.328960          17.70139
## 
## [[5]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##   feature importance.05 importance importance.95 permutation.error
## 1     lpi     0.9897789   1.336405      1.601778          22.03791
## 2 educdec     0.8701162   1.236741      1.424602          20.39440
## 3     age     0.8537786   1.181242      1.261411          19.47920
## 4    lfam     1.0185313   1.133158      1.400151          18.68627
## 5     idp     0.9502284   1.069772      1.203147          17.64101
## 6    linc     0.8600586   1.042453      1.395231          17.19052
## 
## [[6]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##   feature importance.05 importance importance.95 permutation.error
## 1      lc     0.7707383   1.208190      1.379422          19.65436
## 2     sex     0.9309901   1.202629      1.479511          19.56391
## 3    linc     1.0549563   1.138404      1.624217          18.51912
## 4     lpi     0.9360817   1.135198      1.302084          18.46696
## 5 physlim     0.7357272   1.132525      1.312584          18.42349
## 6   child     1.0199964   1.109120      1.316306          18.04274
## 
## [[7]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1     linc     0.9403425   1.262994      1.511122          20.65942
## 2       lc     1.0481333   1.233136      1.602796          20.17103
## 3 ndisease     1.1612194   1.212454      1.320208          19.83272
## 4  educdec     0.7924637   1.197343      1.388218          19.58554
## 5     lfam     0.8423790   1.178545      1.349884          19.27805
## 6      age     0.9125829   1.168297      1.409525          19.11043
## 
## [[8]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1      age     1.1281736   1.261273      1.609524          20.55410
## 2   health     0.9134557   1.240597      1.432366          20.21716
## 3     lfam     0.7469043   1.182294      1.345910          19.26704
## 4      lpi     0.8088552   1.160863      1.491139          18.91779
## 5 ndisease     1.0756671   1.104357      1.517278          17.99695
## 6     fmde     0.6929092   1.093465      1.333544          17.81946
## 
## [[9]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1  educdec     1.0188109   1.287697      1.381982          20.92713
## 2      lpi     0.9853336   1.213095      1.479002          19.71473
## 3     linc     0.8354715   1.195344      1.254350          19.42625
## 4      age     0.9980451   1.179371      1.383545          19.16666
## 5 ndisease     1.0492685   1.176804      1.397398          19.12495
## 6     lfam     1.0814043   1.166626      1.264592          18.95953
## 
## [[10]]
## Interpretation method:  FeatureImp 
## error function: mse
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##    feature importance.05 importance importance.95 permutation.error
## 1      age     0.9538824   1.211869      1.621151          19.53671
## 2      sex     0.9148921   1.211253      1.298311          19.52678
## 3     lfam     0.8227355   1.093094      1.393815          17.62192
## 4 ndisease     0.8282127   1.090779      1.205994          17.58459
## 5       lc     0.7004401   1.060870      1.541697          17.10244
## 6   health     0.8137149   1.058324      1.183639          17.06138&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;As you can see, the feature importances are quite different from each other, but I don’t think
this comes from the imputations, but rather from the fact that feature importance
&lt;em&gt;depends on shuffling the feature, which adds randomness to the measurement&lt;/em&gt;
(source: &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/feature-importance.html#disadvantages-9&#34; class=&#34;uri&#34;&gt;https://christophm.github.io/interpretable-ml-book/feature-importance.html#disadvantages-9&lt;/a&gt;).
To mitigate this, Christoph Molnar suggests repeating the the permutation and averaging the
importance measures; I think that this would be my approach for &lt;em&gt;pooling&lt;/em&gt; as well.&lt;/p&gt;
&lt;p&gt;Let’s now take a look at interactions:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to view the 10 interactions&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_trained %&amp;gt;% 
    pull(interactions_sex)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.07635197
## 2      idp:sex   0.08172754
## 3      lpi:sex   0.10704357
## 4     fmde:sex   0.11267146
## 5  physlim:sex   0.04099073
## 6 ndisease:sex   0.16314524
## 
## [[2]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.10349820
## 2      idp:sex   0.07432519
## 3      lpi:sex   0.11651413
## 4     fmde:sex   0.18123926
## 5  physlim:sex   0.12952808
## 6 ndisease:sex   0.14528876
## 
## [[3]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.05919320
## 2      idp:sex   0.05586197
## 3      lpi:sex   0.24253335
## 4     fmde:sex   0.05240474
## 5  physlim:sex   0.06404969
## 6 ndisease:sex   0.14508072
## 
## [[4]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.02775529
## 2      idp:sex   0.02050390
## 3      lpi:sex   0.11781130
## 4     fmde:sex   0.11084240
## 5  physlim:sex   0.17932694
## 6 ndisease:sex   0.07181589
## 
## [[5]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.12873151
## 2      idp:sex   0.03681428
## 3      lpi:sex   0.15879389
## 4     fmde:sex   0.16952900
## 5  physlim:sex   0.07031520
## 6 ndisease:sex   0.10567463
## 
## [[6]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.15320481
## 2      idp:sex   0.08645037
## 3      lpi:sex   0.16674641
## 4     fmde:sex   0.14671054
## 5  physlim:sex   0.09236257
## 6 ndisease:sex   0.14605618
## 
## [[7]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.04072960
## 2      idp:sex   0.05641868
## 3      lpi:sex   0.19491959
## 4     fmde:sex   0.07119644
## 5  physlim:sex   0.05777469
## 6 ndisease:sex   0.16555363
## 
## [[8]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.04979709
## 2      idp:sex   0.06036898
## 3      lpi:sex   0.14009307
## 4     fmde:sex   0.10927688
## 5  physlim:sex   0.08761533
## 6 ndisease:sex   0.20544585
## 
## [[9]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.08572075
## 2      idp:sex   0.12254979
## 3      lpi:sex   0.17532347
## 4     fmde:sex   0.12557420
## 5  physlim:sex   0.05084209
## 6 ndisease:sex   0.13977328
## 
## [[10]]
## Interpretation method:  Interaction 
## 
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##       .feature .interaction
## 1       lc:sex   0.08636490
## 2      idp:sex   0.04807331
## 3      lpi:sex   0.17922280
## 4     fmde:sex   0.05728403
## 5  physlim:sex   0.09392774
## 6 ndisease:sex   0.13408956&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;It would seem that interactions are a bit more stable. Let’s average the values; for this
I need to access the &lt;code&gt;results&lt;/code&gt; element of the interactions object and the result out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interactions_sex_result &amp;lt;- imp_trained %&amp;gt;% 
    mutate(interactions_results = map(interactions_sex, function(x)(x$results))) %&amp;gt;% 
    pull()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;interactions_sex_result&lt;/code&gt; is a list of dataframes, which means I can bind the rows together and
compute whatever I need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interactions_sex_result %&amp;gt;% 
    bind_rows() %&amp;gt;% 
    group_by(.feature) %&amp;gt;% 
    summarise_at(.vars = vars(.interaction), 
                 .funs = funs(mean, sd, low_ci = quantile(., 0.05), high_ci = quantile(., 0.95)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 x 5
##    .feature       mean     sd low_ci high_ci
##    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 age:sex      0.294  0.0668 0.181    0.369
##  2 black:sex    0.117  0.0286 0.0763   0.148
##  3 child:sex    0.0817 0.0308 0.0408   0.125
##  4 educdec:sex  0.148  0.0411 0.104    0.220
##  5 fmde:sex     0.114  0.0443 0.0546   0.176
##  6 health:sex   0.130  0.0190 0.104    0.151
##  7 idp:sex      0.0643 0.0286 0.0278   0.106
##  8 lc:sex       0.0811 0.0394 0.0336   0.142
##  9 lfam:sex     0.149  0.0278 0.125    0.198
## 10 linc:sex     0.142  0.0277 0.104    0.179
## 11 lpi:sex      0.160  0.0416 0.111    0.221
## 12 ndisease:sex 0.142  0.0356 0.0871   0.187
## 13 physlim:sex  0.0867 0.0415 0.0454   0.157&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That seems pretty good. Now, what about the partial dependence? Let’s take a closer look:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to view the 10 pdps&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_trained %&amp;gt;% 
    pull(effect_linc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.652445   pdp
## 2 0.5312226 1.687522   pdp
## 3 1.0624453 1.687522   pdp
## 4 1.5936679 1.687522   pdp
## 5 2.1248905 1.685088   pdp
## 6 2.6561132 1.694112   pdp
## 
## [[2]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.813449   pdp
## 2 0.5312226 1.816195   pdp
## 3 1.0624453 1.816195   pdp
## 4 1.5936679 1.816195   pdp
## 5 2.1248905 1.804457   pdp
## 6 2.6561132 1.797238   pdp
## 
## [[3]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.906515   pdp
## 2 0.5312226 2.039318   pdp
## 3 1.0624453 2.039318   pdp
## 4 1.5936679 2.039318   pdp
## 5 2.1248905 2.002970   pdp
## 6 2.6561132 2.000922   pdp
## 
## [[4]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.799552   pdp
## 2 0.5312226 2.012634   pdp
## 3 1.0624453 2.012634   pdp
## 4 1.5936679 2.012634   pdp
## 5 2.1248905 1.982425   pdp
## 6 2.6561132 1.966392   pdp
## 
## [[5]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.929158   pdp
## 2 0.5312226 1.905171   pdp
## 3 1.0624453 1.905171   pdp
## 4 1.5936679 1.905171   pdp
## 5 2.1248905 1.879721   pdp
## 6 2.6561132 1.869113   pdp
## 
## [[6]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 2.147697   pdp
## 2 0.5312226 2.162393   pdp
## 3 1.0624453 2.162393   pdp
## 4 1.5936679 2.162393   pdp
## 5 2.1248905 2.119923   pdp
## 6 2.6561132 2.115131   pdp
## 
## [[7]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.776742   pdp
## 2 0.5312226 1.957938   pdp
## 3 1.0624453 1.957938   pdp
## 4 1.5936679 1.957938   pdp
## 5 2.1248905 1.933847   pdp
## 6 2.6561132 1.885287   pdp
## 
## [[8]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 2.020647   pdp
## 2 0.5312226 2.017981   pdp
## 3 1.0624453 2.017981   pdp
## 4 1.5936679 2.017981   pdp
## 5 2.1248905 1.981122   pdp
## 6 2.6561132 2.017604   pdp
## 
## [[9]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.811189   pdp
## 2 0.5312226 2.003053   pdp
## 3 1.0624453 2.003053   pdp
## 4 1.5936679 2.003053   pdp
## 5 2.1248905 1.938150   pdp
## 6 2.6561132 1.918518   pdp
## 
## [[10]]
## Interpretation method:  FeatureEffect 
## features: linc[numerical]
## grid size: 20
## 
## Analysed predictor: 
## Prediction task: unknown 
## 
## 
## Analysed data:
## Sampling from data.frame with 2013 rows and 14 columns.
## 
## Head of results:
##        linc   .y.hat .type
## 1 0.0000000 1.780325   pdp
## 2 0.5312226 1.850203   pdp
## 3 1.0624453 1.850203   pdp
## 4 1.5936679 1.850203   pdp
## 5 2.1248905 1.880805   pdp
## 6 2.6561132 1.881305   pdp&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;As you can see, the values are quite similar. I think that in the case of plots, the best way
to visualize the impact of the imputation is to simply plot all the lines in a single plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;effect_linc_results &amp;lt;- imp_trained %&amp;gt;% 
    mutate(effect_linc_results = map(effect_linc, function(x)(x$results))) %&amp;gt;% 
    select(.imp, effect_linc_results) %&amp;gt;% 
    unnest(effect_linc_results)

effect_linc_results %&amp;gt;% 
    bind_rows() %&amp;gt;% 
    ggplot() + 
    geom_line(aes(y = .y.hat, x = linc, group = .imp), colour = &amp;quot;#82518c&amp;quot;) + 
    brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-11-02-mice_exp_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Overall, the partial dependence plot seems to behave in a very similar way across the different
imputed datasets!&lt;/p&gt;
&lt;p&gt;To conclude, I think that the approach I suggest here is nothing revolutionary; it is consistent
with the way one should conduct an analysis with multiple imputed datasets. However, the pooling
step is non-trivial and there is no magic recipe; it really depends on the goal of the analysis
and what you want or need to show.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cluster multiple time series using K-means</title>
      <link>/blog/2019-10-12-cluster_ts/</link>
      <pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-10-12-cluster_ts/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Elbow_method_(clustering)&#34;&gt;
&lt;img src=&#34;/img/deepfried_elbow.jpg&#34; title = &#34;A life saving skill&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I have been recently confronted to the issue of finding similarities among time-series and though
about using k-means to cluster them. To illustrate the method, I’ll be using data from the
Penn World Tables, readily available in R (inside the &lt;code&gt;{pwt9}&lt;/code&gt; package):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(pwt9)
library(brotools)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, of all, let’s only select the needed columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt &amp;lt;- pwt9.0 %&amp;gt;%
select(country, year, avh)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;avh&lt;/code&gt; contains the average worked hours for a given country and year. The data looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(pwt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          country year avh
## ABW-1950   Aruba 1950  NA
## ABW-1951   Aruba 1951  NA
## ABW-1952   Aruba 1952  NA
## ABW-1953   Aruba 1953  NA
## ABW-1954   Aruba 1954  NA
## ABW-1955   Aruba 1955  NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each country, there’s yearly data on the &lt;code&gt;avh&lt;/code&gt; variable. The goal here is to cluster the different
countries by looking at how similar they are on the &lt;code&gt;avh&lt;/code&gt; variable. Let’s do some further cleaning.
The k-means implementation in R expects a wide data frame (currently my data frame is in the long
format) and no missing values. These could potentially be imputed, but I can’t be bothered:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt_wide &amp;lt;- pwt %&amp;gt;%
  pivot_wider(names_from = year, values_from = avh)  %&amp;gt;%
  filter(!is.na(`1950`)) %&amp;gt;%
  mutate_at(vars(-country), as.numeric)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To convert my data frame from long to wide, I use the fresh &lt;code&gt;pivot_wider()&lt;/code&gt; function, instead of the
less intuitive &lt;code&gt;spread()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;We’re ready to use the k-means algorithm. To know how many clusters I should aim for, I’ll be using
the elbow method (if you’re not familiar with this method, click on the image at the very top of
this post):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wss &amp;lt;- map_dbl(1:5, ~{kmeans(select(pwt_wide, -country), ., nstart=50,iter.max = 15 )$tot.withinss})

n_clust &amp;lt;- 1:5

elbow_df &amp;lt;- as.data.frame(cbind(&amp;quot;n_clust&amp;quot; = n_clust, &amp;quot;wss&amp;quot; = wss))


ggplot(elbow_df) +
geom_line(aes(y = wss, x = n_clust), colour = &amp;quot;#82518c&amp;quot;) +
theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-10-12-cluster_ts_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like 3 clusters is a good choice. Let’s now run the kmeans algorithm:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clusters &amp;lt;- kmeans(select(pwt_wide, -country), centers = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;clusters&lt;/code&gt; is a list with several interesting items. The item &lt;code&gt;centers&lt;/code&gt; contains the “average”
time series:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(centers &amp;lt;- rownames_to_column(as.data.frame(clusters$centers), &amp;quot;cluster&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   cluster     1950     1951     1952     1953     1954     1955     1956
## 1       1 2110.440 2101.273 2088.947 2074.273 2066.617 2053.391 2034.926
## 2       2 2086.509 2088.571 2084.433 2081.939 2078.756 2078.710 2074.175
## 3       3 2363.600 2350.774 2338.032 2325.375 2319.011 2312.083 2308.483
##       1957     1958     1959     1960     1961     1962     1963     1964
## 1 2021.855 2007.221 1995.038 1985.904 1978.024 1971.618 1963.780 1962.983
## 2 2068.807 2062.021 2063.687 2060.176 2052.070 2044.812 2038.939 2037.488
## 3 2301.355 2294.556 2287.556 2279.773 2272.899 2262.781 2255.690 2253.431
##       1965     1966     1967     1968     1969     1970     1971     1972
## 1 1952.945 1946.961 1928.445 1908.354 1887.624 1872.864 1855.165 1825.759
## 2 2027.958 2021.615 2015.523 2007.176 2001.289 1981.906 1967.323 1961.269
## 3 2242.775 2237.216 2228.943 2217.717 2207.037 2190.452 2178.955 2167.124
##       1973     1974     1975     1976     1977     1978     1979     1980
## 1 1801.370 1770.484 1737.071 1738.214 1713.395 1693.575 1684.215 1676.703
## 2 1956.755 1951.066 1933.527 1926.508 1920.668 1911.488 1904.316 1897.103
## 3 2156.304 2137.286 2125.298 2118.138 2104.382 2089.717 2083.036 2069.678
##       1981     1982     1983     1984     1985     1986     1987     1988
## 1 1658.894 1644.019 1636.909 1632.371 1623.901 1615.320 1603.383 1604.331
## 2 1883.376 1874.730 1867.266 1861.386 1856.947 1849.568 1848.748 1847.690
## 3 2055.658 2045.501 2041.428 2030.095 2040.210 2033.289 2028.345 2029.290
##       1989     1990     1991     1992     1993     1994     1995     1996
## 1 1593.225 1586.975 1573.084 1576.331 1569.725 1567.599 1567.113 1558.274
## 2 1842.079 1831.907 1823.552 1815.864 1823.824 1830.623 1831.815 1831.648
## 3 2031.741 2029.786 1991.807 1974.954 1973.737 1975.667 1980.278 1988.728
##       1997     1998     1999     2000     2001     2002     2003     2004
## 1 1555.079 1555.071 1557.103 1545.349 1530.207 1514.251 1509.647 1522.389
## 2 1835.372 1836.030 1839.857 1827.264 1813.477 1781.696 1786.047 1781.858
## 3 1985.076 1961.219 1966.310 1959.219 1946.954 1940.110 1924.799 1917.130
##       2005     2006     2007     2008     2009     2010     2011     2012
## 1 1514.492 1512.872 1515.299 1514.055 1493.875 1499.563 1503.049 1493.862
## 2 1775.167 1776.759 1773.587 1771.648 1734.559 1736.098 1742.143 1735.396
## 3 1923.496 1912.956 1902.156 1897.550 1858.657 1861.875 1861.608 1850.802
##       2013     2014
## 1 1485.589 1486.991
## 2 1729.973 1729.543
## 3 1848.158 1851.829&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;clusters&lt;/code&gt; also contains the &lt;code&gt;cluster&lt;/code&gt; item, which tells me to which cluster the different countries
belong to. I can easily add this to the original data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt_wide &amp;lt;- pwt_wide %&amp;gt;% 
  mutate(cluster = clusters$cluster)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s prepare the data for visualisation. I have to go back to a long data frame for this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pwt_long &amp;lt;- pwt_wide %&amp;gt;%
  pivot_longer(cols=c(-country, -cluster), names_to = &amp;quot;year&amp;quot;, values_to = &amp;quot;avh&amp;quot;) %&amp;gt;%
  mutate(year = ymd(paste0(year, &amp;quot;-01-01&amp;quot;)))

centers_long &amp;lt;- centers %&amp;gt;%
  pivot_longer(cols = -cluster, names_to = &amp;quot;year&amp;quot;, values_to = &amp;quot;avh&amp;quot;) %&amp;gt;%  
  mutate(year = ymd(paste0(year, &amp;quot;-01-01&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And I can now plot the different time series, by cluster and highlight the “average” time series
for each cluster as well (yellow line):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  geom_line(data = pwt_long, aes(y = avh, x = year, group = country), colour = &amp;quot;#82518c&amp;quot;) +
  facet_wrap(~cluster, nrow = 1) + 
  geom_line(data = centers_long, aes(y = avh, x = year, group = cluster), col = &amp;quot;#b58900&amp;quot;, size = 2) +
  theme_blog() +
  labs(title = &amp;quot;Average hours worked in several countries&amp;quot;, 
       caption = &amp;quot;The different time series have been clustered using k-means.
                 Cluster 1: Belgium, Switzerland, Germany, Denmark, France, Luxembourg, Netherlands,
                 Norway, Sweden.\nCluster 2: Australia, Colombia, Ireland, Iceland, Japan, Mexico,
                 Portugal, Turkey.\nCluster 3: Argentina, Austria, Brazil, Canada, Cyprus, Spain, Finland,
                 UK, Italy, New Zealand, Peru, USA, Venezuela&amp;quot;) +
  theme(plot.caption = element_text(colour = &amp;quot;white&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-10-12-cluster_ts_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Split-apply-combine for Maximum Likelihood Estimation of a linear model</title>
      <link>/blog/2019-10-05-parallel_maxlik/</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-10-05-parallel_maxlik/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.univ-orleans.fr/deg/masters/ESA/CH/Chapter2_MLE.pdf&#34;&gt;
&lt;img src=&#34;/img/hieforth.png&#34; title = &#34;click with thy mouse hither to wot moe about maximum plausability estimation&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;Maximum likelihood estimation is a very useful technique to fit a model to data used a lot in
econometrics and other sciences, but seems, at least to my knowledge, to not be so well known by
machine learning practitioners (but I may be wrong about that). Other useful techniques to confront models to data
used in econometrics are the minimum distance family of techniques such as the general method of
moments or Bayesian approaches, while machine learning practitioners seem to favor the minimization
of a loss function (the mean squared error in the case of linear regression for instance).&lt;/p&gt;
&lt;p&gt;When I taught at the university, students had often some problems to understand the technique.
It is true that it is not as easy to understand as ordinary least squares, but I’ll try to explain
to the best of my abilities.&lt;/p&gt;
&lt;p&gt;Given a sample of data, what is the unknown probability distribution
that &lt;em&gt;most likely&lt;/em&gt; generated it? For instance, if your sample only contains 0’s and 1’s, and
the proportion of 1’s is 80%, what do you think is the most likely distribution that generated it?
The probability distribution that &lt;em&gt;most likely&lt;/em&gt; generated such a dataset is a binomial distribution
with probability of success equal to 80%. It &lt;em&gt;might have been&lt;/em&gt; a binomial distribution with probability
of success equal to, say, 60%, but the &lt;em&gt;most likely&lt;/em&gt; one is one with probability of success equal
to 80%.&lt;/p&gt;
&lt;p&gt;To perform maximum likelihood estimation, one thus needs to assume a certain probability distribution,
and then look for the parameters that maximize the likelihood that this distribution generated the
observed data. So, now the question is, how to maximize this likelihood? And mathematically speaking,
what is a &lt;em&gt;likelihood&lt;/em&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-theory&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some theory&lt;/h2&gt;
&lt;p&gt;First of all, let’s assume that each observation from your dataset not only was generated from the
same distribution, but that each observation is also independent from each other. For instance, if in your sample
you have data on people’s wages and socio-economic background, it is safe to assume, under certain
circumstances, that the observations are independent.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; be random variables, and &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; be their realizations (actual observed values).
Let’s assume that the &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; are distributed according
to a certain probability distribution &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; with density &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is
a parameter of said distribution.
Because our sample is composed of i.i.d. random variables, the probability that it was generated by
our distribution &lt;span class=&#34;math inline&#34;&gt;\(D(\theta)\)&lt;/span&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\prod_{i=1}^N Pr(X_i = x_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is customary to take the log of this expression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log(\prod_{i=1}^N Pr(X_i = x_i)) = \sum_{i=1}^N \log(Pr(X_i = x_i))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The expression above is called the &lt;em&gt;log-likelihood&lt;/em&gt;, &lt;span class=&#34;math inline&#34;&gt;\(logL(\theta; x_1, ..., x_N)\)&lt;/span&gt;. Maximizing this
function yields &lt;span class=&#34;math inline&#34;&gt;\(\theta^*\)&lt;/span&gt;, the value of the parameter that makes the sample the most probable.
In the case of linear regression, the density function to use is the one from the Normal distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-likelihood-of-the-linear-model-as-an-example-of-the-split-apply-combine-strategy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maximum likelihood of the linear model as an example of the split-apply-combine strategy&lt;/h2&gt;
&lt;p&gt;Hadley Wickham’s seminal paper, &lt;a href=&#34;https://www.jstatsoft.org/article/view/v040i01/v40i01.pdf&#34;&gt;The Split-Apply-Combine Strategy for Data Analysis&lt;/a&gt;
presents the &lt;em&gt;split-apply-combine&lt;/em&gt; strategy, which should remind the reader of the map-reduce
framework from Google. The idea is to recognize that in some cases big problems are simply an
aggregation of smaller problems. This is the case for Maximum Likelihood Estimation of the linear
model as well.
The picture below illustrates how Maximum Likelihood works, in the standard case:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/maxlik_1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Let’s use R to do exactly this. Let’s first start by simulating some data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
size &amp;lt;- 500000

x1 &amp;lt;- rnorm(size)
x2 &amp;lt;- rnorm(size)
x3 &amp;lt;- rnorm(size)

dep_y &amp;lt;- 1.5 + 2*x1 + 3*x2 + 4*x3 + rnorm(size)

x_data &amp;lt;- cbind(dep_y, 1, x1, x2, x3)

x_df &amp;lt;- as.data.frame(x_data) %&amp;gt;%
  rename(iota = V2)

head(x_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       dep_y iota         x1          x2         x3
## 1  1.637044    1  0.2287198  0.91609653 -0.4006215
## 2 -1.684578    1  1.2780291 -0.02468559 -1.4020914
## 3  1.289595    1  1.0524842  0.30206515 -0.3553641
## 4 -3.769575    1 -2.5763576  0.13864796 -0.3181661
## 5 13.110239    1 -0.9376462  0.77965301  3.0351646
## 6  5.059152    1  0.7488792 -0.10049061  0.1307225&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that this is done, let’s write a function to perform Maximum Likelihood Estimation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loglik_linmod &amp;lt;- function(parameters, x_data){
  sum_log_likelihood &amp;lt;- x_data %&amp;gt;%
    mutate(log_likelihood =
             dnorm(dep_y,
                   mean = iota*parameters[1] + x1*parameters[2] + x2*parameters[3] + x3*parameters[4],
                   sd = parameters[5],
                   log = TRUE)) %&amp;gt;%
    summarise(sum(log_likelihood))

  -1 * sum_log_likelihood
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function returns minus the log likelihood, because &lt;code&gt;optim()&lt;/code&gt; which I will be using to optimize
the log-likelihood function minimizes functions by default (minimizing the opposite of a function is the
same as maximizing a function). Let’s optimize the function and see if we’re able to find the
parameters of the data generating process, &lt;code&gt;1.5, 2, 3, 4&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt; (the standard deviation of the
error term):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;optim(c(1,1,1,1,1), loglik_linmod, x_data = x_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We successfully find the parameters of our data generating process!&lt;/p&gt;
&lt;p&gt;Now, what if I’d like to distribute the computation of the contribution to the likelihood of each
observations across my 12 cores? The goal is not necessarily to speed up the computations but
to be able to handle larger than RAM data. If I have data that is too large to fit in memory,
I could split it into chunks, compute the contributions to the likelihood of each chunk, sum
everything again, and voila! This is illustrated below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/maxlik_2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;To do this, I use the &lt;code&gt;{disk.frame}&lt;/code&gt; package, and only need to change my &lt;code&gt;loglik_linmod()&lt;/code&gt; function
slightly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;disk.frame&amp;quot;)
x_diskframe &amp;lt;- as.disk.frame(x_df) #Convert the data frame to a disk.frame

loglik_linmod_df &amp;lt;- function(parameters, x_data){
  sum_log_likelihood &amp;lt;- x_data %&amp;gt;%
    mutate(log_likelihood =
             dnorm(dep_y,
                   mean = iota*parameters[1] + x1*parameters[2] + x2*parameters[3] + x3*parameters[4],
                   sd = parameters[5],
                   log = TRUE)) %&amp;gt;% 
    chunk_summarise(sum(log_likelihood))

  out &amp;lt;- sum_log_likelihood %&amp;gt;%
    collect() %&amp;gt;%
    pull() %&amp;gt;%
    sum()

  -out
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function is applied to each chunk, and &lt;code&gt;chunk_summarise()&lt;/code&gt; computes the sum of the contributions
inside each chunk. Thus, I first need to use &lt;code&gt;collect()&lt;/code&gt; to transfer the chunk-wise sums in memory
and then use &lt;code&gt;pull()&lt;/code&gt; to convert it to an atomic vector, and finally sum them all again.&lt;/p&gt;
&lt;p&gt;Let’s now optimize this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;optim(rep(1, 5), loglik_linmod_df, x_data = x_diskframe)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $par
## [1] 1.5351722 1.9566144 3.0067978 4.0202956 0.9889412
## 
## $value
## [1] 709977.2
## 
## $counts
## function gradient 
##      502       NA 
## 
## $convergence
## [1] 1
## 
## $message
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how you can use the split-apply-combine approach for maximum likelihood estimation of a
linear model! This approach is quite powerful, and the familiar &lt;code&gt;map()&lt;/code&gt; and &lt;code&gt;reduce()&lt;/code&gt; functions
included in &lt;code&gt;{purrr}&lt;/code&gt; can also help with this task. However, this only works if you can split your
problem into chunks, which is sometimes quite hard to achieve.&lt;/p&gt;
&lt;p&gt;However, as usual, there is rarely a need to write your own functions, as &lt;code&gt;{disk.frame}&lt;/code&gt; includes
the &lt;code&gt;dfglm()&lt;/code&gt; function which can be used to estimate any generalized linear model using &lt;code&gt;disk.frame&lt;/code&gt; objects!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>{disk.frame} is epic</title>
      <link>/blog/2019-09-03-disk_frame/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-09-03-disk_frame/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/3XMTyi_H4q4&#34;&gt;
&lt;img src=&#34;/img/disk_frame.png&#34; title = &#34;Zhuo Jia Dai&#39;s talk at useR!2019&#34; width=&#34;750&#34; height=&#34;500&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Note: When I started writing this blog post, I encountered a bug and filed a &lt;a href=&#34;https://github.com/xiaodaigh/disk.frame/issues/141&#34;&gt;bug report&lt;/a&gt;
that I encourage you to read. The responsiveness of the developer was exemplary. Not only did Zhuo
solve the issue in record time, he provided ample code snippets to illustrate the solutions. Hats off
to him!&lt;/p&gt;
&lt;p&gt;This blog post is a short presentation of &lt;code&gt;{disk.frame}&lt;/code&gt; a package that makes it easy to work with
data that is too large to fit on RAM, but not large enough that it could be called big data. Think
data that is around 30GB for instance, or more, but nothing at the level of TBs.&lt;/p&gt;
&lt;p&gt;I have already written a blog post about this topic, using Spark and the R library &lt;code&gt;{sparklyr}&lt;/code&gt;, where
I showed how to set up &lt;code&gt;{sparklyr}&lt;/code&gt; to import 30GB of data. I will import the same file here, and
run a very simple descriptive analysis. If you need context about the file I’ll be using, just
read the &lt;a href=&#34;https://www.brodrigues.co/blog/2018-02-16-importing_30gb_of_data/&#34;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first step, as usual, is to load the needed packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(disk.frame)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to specify how many cores you want to dedicate to &lt;code&gt;{disk.frame}&lt;/code&gt;; of course, the
more cores you use, the faster the operations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;setup_disk.frame(workers = 6)
options(future.globals.maxSize = Inf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;setup_disk.frame(workers = 6)&lt;/code&gt; means that 6 cpu threads will be dedicated to importing and working
on the data. The second line, &lt;code&gt;future.globals.maxSize = Inf&lt;/code&gt; means that an &lt;em&gt;unlimited amount of data will be passed from worker to worker&lt;/em&gt;,
as described in the documentation.&lt;/p&gt;
&lt;p&gt;Now comes the interesting bit. If you followed the previous blog post, you should have a 30GB
csv file. This file was obtained by merging a lot of smaller sized csv files. In practice, you should
keep the files separated, and NOT merge them. This makes things much easier. However, as I said before,
I want to be in the situation, which already happened to me in the past, where I get a big-sized
csv file and I am to provide an analysis on that data. So, let’s try to read in that big file, which
I called &lt;code&gt;combined.csv&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path_to_data &amp;lt;- &amp;quot;path/to/data/&amp;quot;

flights.df &amp;lt;- csv_to_disk.frame(
  paste0(path_to_data, &amp;quot;combined.csv&amp;quot;), 
  outdir = paste0(path_to_data, &amp;quot;combined.df&amp;quot;),
  in_chunk_size = 2e6,
  backend = &amp;quot;LaF&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s go through these lines, one at a time. In the first line, I simply define the path
to the folder that contains the data. The next chunk is where I read in the data using the
&lt;code&gt;csv_to_disk_frame()&lt;/code&gt; function. The first option is simply the path to the csv file. The second
option &lt;code&gt;outdir =&lt;/code&gt; is where you need to define the directory that will hold the intermediary files,
which are in the fst format. This folder, that contains these fst files, is the &lt;code&gt;disk.frame&lt;/code&gt;.
fst files are created by the &lt;code&gt;{fst}&lt;/code&gt; package, which &lt;em&gt;provides a fast, easy and flexible way to serialize data frames&lt;/em&gt;.
This means that files that are in that format can be read and written much much faster than by
other means (see a benchmark of &lt;code&gt;{fst}&lt;/code&gt; &lt;a href=&#34;https://www.fstpackage.org/&#34;&gt;here&lt;/a&gt;).
The next time you want to import the data, you can use the &lt;code&gt;disk.frame()&lt;/code&gt; function and point it to
the &lt;code&gt;combined.df&lt;/code&gt; folder. &lt;code&gt;in_chunk_size =&lt;/code&gt; specifies how many lines are to be read in one swoop,
and &lt;code&gt;backend =&lt;/code&gt; is the underlying engine that reads in the data, in this case the &lt;code&gt;{LaF}&lt;/code&gt; package.
The default backend is &lt;code&gt;{data.table}&lt;/code&gt; and there is also a &lt;code&gt;{readr}&lt;/code&gt; backend. As written in the
note at the beginning of the blog post, I encourage you to read the github issue to learn why I am
using the &lt;code&gt;LaF&lt;/code&gt; backend (the &lt;code&gt;{data.table}&lt;/code&gt; and &lt;code&gt;{readr}&lt;/code&gt; backend work as well).&lt;/p&gt;
&lt;p&gt;Now, let’s try to replicate what I did in my previous blog post, namely, computing the average
delay in departures per day. With &lt;code&gt;{disk.frame}&lt;/code&gt;, one has to be very careful about something
important however; all the &lt;code&gt;group_by()&lt;/code&gt; operations are performed &lt;em&gt;per chunk&lt;/em&gt;. This means that a second
&lt;code&gt;group_by()&lt;/code&gt; call might be needed. For more details, I encourage you to read the &lt;a href=&#34;http://diskframe.com/articles/intro-disk-frame.html#grouping&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code below is what I want to perform; group by day, and compute the average daily flight delay:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_dep_delay &amp;lt;- flights.df %&amp;gt;%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %&amp;gt;%
  summarise(mean_delay = mean(DEP_DELAY, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, because with &lt;code&gt;{disk.frame}&lt;/code&gt;, &lt;code&gt;group_by()&lt;/code&gt; calls are performed per chunk, the code must now
be changed. The first step is to compute the sum of delays within each chunk, and count the number
of days within each chunk. This is the time consuming part:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic &amp;lt;- Sys.time()
mean_dep_delay &amp;lt;- flights.df %&amp;gt;%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %&amp;gt;%
  summarise(sum_delay = sum(DEP_DELAY, na.rm = TRUE), n = n()) %&amp;gt;%
  collect()
(toc = Sys.time() - tic)
Time difference of 1.805515 mins&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is pretty impressive! It is much faster than with &lt;code&gt;{sparklyr}&lt;/code&gt;. But we’re not done yet, we
still need to compute the average:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_dep_delay &amp;lt;- mean_dep_delay %&amp;gt;%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %&amp;gt;%
  summarise(mean_delay = sum(sum_delay)/sum(n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to keep in mind that &lt;code&gt;group_by()&lt;/code&gt; works by chunks when dealing with &lt;code&gt;disk.frame&lt;/code&gt;
objects.&lt;/p&gt;
&lt;p&gt;To conclude, we can plot the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)
dep_delay &amp;lt;- mean_dep_delay %&amp;gt;%
  arrange(YEAR, MONTH, DAY_OF_MONTH) %&amp;gt;%
  mutate(date = ymd(paste(YEAR, MONTH, DAY_OF_MONTH, sep = &amp;quot;-&amp;quot;)))

ggplot(dep_delay, aes(date, mean_delay)) +
  geom_smooth(colour = &amp;quot;#82518c&amp;quot;) + 
  brotools::theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-09-03-disk_frame_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;{disk.frame}&lt;/code&gt; is really promising, and I will monitor this package very closely. I might write
another blog post about it, focusing this time on using machine learning with &lt;code&gt;disk.frame&lt;/code&gt; objects.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modern R with the tidyverse is available on Leanpub</title>
      <link>/blog/2019-08-17-modern_r/</link>
      <pubDate>Sat, 17 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-08-17-modern_r/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;
&lt;img src=&#34;/img/cover_modern.png&#34; title = &#34;Click here to go to Leanpub&#34; width=&#34;500&#34; height=&#34;647&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Yesterday I released an ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;,
called &lt;em&gt;Modern R with the tidyverse&lt;/em&gt;, which you can also
read for free &lt;a href=&#34;https://b-rodrigues.github.io/modern_R/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, I want to give some context.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Modern R with the tidyverse&lt;/em&gt; is the second ebook I release on Leanpub. I released the first one, called
&lt;a href=&#34;https://leanpub.com/fput&#34;&gt;Functional programming and unit testing for data munging with R&lt;/a&gt; around
Christmas 2016 (I’ve retired it on Leanpub, but you can still read it for free
&lt;a href=&#34;https://b-rodrigues.github.io/fput/&#34;&gt;here&lt;/a&gt;) . I just had moved back to my home country of
Luxembourg and started a new job as a research assistant at the statistical national institute.
Since then, lots of things happened; I’ve changed jobs and joined PwC Luxembourg as a data scientist,
was promoted to manager, finished my PhD, and most importantly of all, I became a father.&lt;/p&gt;
&lt;p&gt;Through all this, I continued blogging and working on a new ebook, called &lt;em&gt;Modern R with the tidyverse&lt;/em&gt;.
At first, this was supposed to be a separate book from the first one, but as I continued writing,
I realized that updating and finishing the first one, would take a lot of effort, and also, that
it wouldn’t make much sense in keeping both separated. So I decided to merge the content from the
first ebook with the second, and update everything in one go.&lt;/p&gt;
&lt;p&gt;My very first notes were around 50 pages if memory serves, and I used them to teach R at the
University of Strasbourg while I employed there as a research and teaching assistant and working
on my PhD. These notes were the basis of &lt;em&gt;Functional programming and unit testing for data munging with R&lt;/em&gt;
and now &lt;em&gt;Modern R&lt;/em&gt;. Chapter 2 of &lt;em&gt;Modern R&lt;/em&gt; is almost a simple copy and paste from these notes
(with more sections added). These notes were first written around 2012-2013ish.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Modern R&lt;/em&gt; is the kind of text I would like to have had when I first started playing around with R,
sometime around 2009-2010. It starts from the beginning, but also goes quite into details in the
later chapters. For instance, the section on
&lt;a href=&#34;https://b-rodrigues.github.io/modern_R/functional-programming.html#modeling-with-functional-programming&#34;&gt;modeling with functional programming&lt;/a&gt;
is quite advanced, but I believe that readers that read through all the book and reached that part
would be armed with all the needed knowledge to follow. At least, this is my hope.&lt;/p&gt;
&lt;p&gt;Now, the book is still not finished. Two chapters are missing, but it should not take me long to
finish them as I already have drafts lying around. However, exercises might still be in wrong
places, and more are required. Also, generally, more polishing is needed.&lt;/p&gt;
&lt;p&gt;As written in the first paragraph of this section, the book is available on
&lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;. Unlike my previous ebook, this one costs money;
a minimum price of 4.99$ and a recommended price of 14.99$, but as mentioned you can read it for
free &lt;a href=&#34;https://b-rodrigues.github.io/modern_R/&#34;&gt;online&lt;/a&gt;. I’ve hesitated to give it a minimum price of
0$, but I figured that since the book can be read for free online, and that Leanpub has a 45 days
return policy where readers can get 100% reimbursed, no questions asked (and keep the downloaded
ebook), readers were not taking a lot of risks by buying it for 5 bucks. I sure hope however that
readers will find that this ebook is worth at least 5 bucks!&lt;/p&gt;
&lt;p&gt;Now why should you read it? There’s already a lot of books on learning how to use R. Well, I don’t
really want to convince you to read it. But some people do seem to like my style of writing and my
blog posts, so I guess these same people, or similar people, might like the ebook. Also, I think
that this ebook covers a lot of different topics, enough of them to make you an efficient R user.
But as I’ve written in the introduction of &lt;em&gt;Modern R&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;So what you can expect from this book is that this book is not the only one you should read.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Anyways, hope you’ll enjoy &lt;em&gt;Modern R&lt;/em&gt;, suggestions, criticisms and reviews welcome!&lt;/p&gt;
&lt;p&gt;By the way, the cover of the book is a painting by John William Waterhouse, depicting Diogenes of Sinope,
an ancient Greek philosopher, and absolute mad lad. Read his Wikipedia page, it’s worth it.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;, or buy my ebook on &lt;a href=&#34;https://leanpub.com/modern_tidyverse&#34;&gt;Leanpub&lt;/a&gt;&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using linear models with binary dependent variables, a simulation study</title>
      <link>/blog/2019-08-14-lpm/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-08-14-lpm/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://psyarxiv.com/4gmbv&#34;&gt;
&lt;img src=&#34;/img/illegal.png&#34; title = &#34;Even psychologists are not safe&#34; width=&#34;800&#34; height=&#34;612&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for free
&lt;a href=&#34;https://b-rodrigues.github.io/modern_R/functional-programming.html#modeling-with-functional-programming&#34;&gt;here&lt;/a&gt;.
This is taken from Chapter 8, in which I discuss advanced functional programming methods for
modeling.&lt;/p&gt;
&lt;p&gt;As written just above (note: as written above &lt;em&gt;in the book&lt;/em&gt;), &lt;code&gt;map()&lt;/code&gt; simply applies a function
to a list of inputs, and in the previous
section we mapped &lt;code&gt;ggplot()&lt;/code&gt; to generate many plots at once. This approach can also be used to
map any modeling functions, for instance &lt;code&gt;lm()&lt;/code&gt; to a list of datasets.&lt;/p&gt;
&lt;p&gt;For instance, suppose that you wish to perform a Monte Carlo simulation. Suppose that you are
dealing with a binary choice problem; usually, you would use a logistic regression for this.&lt;/p&gt;
&lt;p&gt;However, in certain disciplines, especially in the social sciences, the so-called Linear Probability
Model is often used as well. The LPM is a simple linear regression, but unlike the standard setting
of a linear regression, the dependent variable, or target, is a binary variable, and not a continuous
variable. Before you yell “Wait, that’s illegal”, you should know that in practice LPMs do a good
job of estimating marginal effects, which is what social scientists and econometricians are often
interested in. Marginal effects are another way of interpreting models, giving how the outcome
(or the target) changes given a change in a independent variable (or a feature). For instance,
a marginal effect of 0.10 for age would mean that probability of success would increase by 10% for
each added year of age.&lt;/p&gt;
&lt;p&gt;There has been a lot of discussion on logistic regression vs LPMs, and there are pros and cons
of using LPMs. Micro-econometricians are still fond of LPMs, even though the pros of LPMs are
not really convincing. However, quoting Angrist and Pischke:&lt;/p&gt;
&lt;p&gt;“While a nonlinear model may fit the CEF (population conditional expectation function) for LDVs
(limited dependent variables) more closely than a linear model, when it comes to marginal effects,
this probably matters little” (source: &lt;em&gt;Mostly Harmless Econometrics&lt;/em&gt;)&lt;/p&gt;
&lt;p&gt;so LPMs are still used for estimating marginal effects.&lt;/p&gt;
&lt;p&gt;Let us check this assessment with one example. First, we simulate some data, then
run a logistic regression and compute the marginal effects, and then compare with a LPM:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
x1 &amp;lt;- rnorm(100)
x2 &amp;lt;- rnorm(100)
  
z &amp;lt;- .5 + 2*x1 + 4*x2

p &amp;lt;- 1/(1 + exp(-z))

y &amp;lt;- rbinom(100, 1, p)

df &amp;lt;- tibble(y = y, x1 = x1, x2 = x2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data generating process generates data from a binary choice model. Fitting the model using a
logistic regression allows us to recover the structural parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_regression &amp;lt;- glm(y ~ ., data = df, family = binomial(link = &amp;quot;logit&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see a summary of the model fit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(logistic_regression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ ., family = binomial(link = &amp;quot;logit&amp;quot;), data = df)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.91941  -0.44872   0.00038   0.42843   2.55426  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)   0.0960     0.3293   0.292 0.770630    
## x1            1.6625     0.4628   3.592 0.000328 ***
## x2            3.6582     0.8059   4.539 5.64e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 138.629  on 99  degrees of freedom
## Residual deviance:  60.576  on 97  degrees of freedom
## AIC: 66.576
## 
## Number of Fisher Scoring iterations: 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We do recover the parameters that generated the data, but what about the marginal effects? We can
get the marginal effects easily using the &lt;code&gt;{margins}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(margins)

margins(logistic_regression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Average marginal effects&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## glm(formula = y ~ ., family = binomial(link = &amp;quot;logit&amp;quot;), data = df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      x1     x2
##  0.1598 0.3516&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, even better, we can compute the &lt;em&gt;true&lt;/em&gt; marginal effects, since we know the data
generating process:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meffects &amp;lt;- function(dataset, coefs){
  X &amp;lt;- dataset %&amp;gt;% 
  select(-y) %&amp;gt;% 
  as.matrix()
  
  dydx_x1 &amp;lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[2])
  dydx_x2 &amp;lt;- mean(dlogis(X%*%c(coefs[2], coefs[3]))*coefs[3])
  
  tribble(~term, ~true_effect,
          &amp;quot;x1&amp;quot;, dydx_x1,
          &amp;quot;x2&amp;quot;, dydx_x2)
}

(true_meffects &amp;lt;- meffects(df, c(0.5, 2, 4)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   term  true_effect
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 x1          0.175
## 2 x2          0.350&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, so now what about using this infamous Linear Probability Model to estimate the marginal effects?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lpm &amp;lt;- lm(y ~ ., data = df)

summary(lpm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ ., data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.83953 -0.31588 -0.02885  0.28774  0.77407 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.51340    0.03587  14.314  &amp;lt; 2e-16 ***
## x1           0.16771    0.03545   4.732 7.58e-06 ***
## x2           0.31250    0.03449   9.060 1.43e-14 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.3541 on 97 degrees of freedom
## Multiple R-squared:  0.5135, Adjusted R-squared:  0.5034 
## F-statistic: 51.18 on 2 and 97 DF,  p-value: 6.693e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s not too bad, but maybe it could have been better in other circumstances. Perhaps if we had more
observations, or perhaps for a different set of structural parameters the results of the LPM
would have been closer. The LPM estimates the marginal effect of &lt;code&gt;x1&lt;/code&gt; to be
0.1677134 vs 0.1597956
for the logistic regression and for &lt;code&gt;x2&lt;/code&gt;, the LPM estimation is 0.3124966
vs 0.351607. The &lt;em&gt;true&lt;/em&gt; marginal effects are
0.1750963 and 0.3501926 for &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x2&lt;/code&gt; respectively.&lt;/p&gt;
&lt;p&gt;Just as to assess the accuracy of a model data scientists perform cross-validation, a Monte Carlo
study can be performed to asses how close the estimation of the marginal effects using a LPM is
to the marginal effects derived from a logistic regression. It will allow us to test with datasets
of different sizes, and generated using different structural parameters.&lt;/p&gt;
&lt;p&gt;First, let’s write a function that generates data. The function below generates 10 datasets of size
100 (the code is inspired by this &lt;a href=&#34;https://stats.stackexchange.com/a/46525&#34;&gt;StackExchange answer&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;generate_datasets &amp;lt;- function(coefs = c(.5, 2, 4), sample_size = 100, repeats = 10){

  generate_one_dataset &amp;lt;- function(coefs, sample_size){
  x1 &amp;lt;- rnorm(sample_size)
  x2 &amp;lt;- rnorm(sample_size)
  
  z &amp;lt;- coefs[1] + coefs[2]*x1 + coefs[3]*x2

  p &amp;lt;- 1/(1 + exp(-z))

  y &amp;lt;- rbinom(sample_size, 1, p)

  df &amp;lt;- tibble(y = y, x1 = x1, x2 = x2)
  }

  simulations &amp;lt;- rerun(.n = repeats, generate_one_dataset(coefs, sample_size))
 
  tibble(&amp;quot;coefs&amp;quot; = list(coefs), &amp;quot;sample_size&amp;quot; = sample_size, &amp;quot;repeats&amp;quot; = repeats, &amp;quot;simulations&amp;quot; = list(simulations))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s first generate one dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one_dataset &amp;lt;- generate_datasets(repeats = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;one_dataset&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one_dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 4
##   coefs     sample_size repeats simulations
##   &amp;lt;list&amp;gt;          &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;     
## 1 &amp;lt;dbl [3]&amp;gt;         100       1 &amp;lt;list [1]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the tibble with the simulated data is inside a list-column called &lt;code&gt;simulations&lt;/code&gt;.
Let’s take a closer look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(one_dataset$simulations)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 1
##  $ :List of 1
##   ..$ :Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;: 100 obs. of  3 variables:
##   .. ..$ y : int [1:100] 0 1 1 1 0 1 1 0 0 1 ...
##   .. ..$ x1: num [1:100] 0.437 1.06 0.452 0.663 -1.136 ...
##   .. ..$ x2: num [1:100] -2.316 0.562 -0.784 -0.226 -1.587 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The structure is quite complex, and it’s important to understand this, because it will have an
impact on the next lines of code; it is a list, containing a list, containing a dataset! No worries
though, we can still map over the datasets directly, by using &lt;code&gt;modify_depth()&lt;/code&gt; instead of &lt;code&gt;map()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, let’s fit a LPM and compare the estimation of the marginal effects with the &lt;em&gt;true&lt;/em&gt; marginal
effects. In order to have some confidence in our results,
we will not simply run a linear regression on that single dataset, but will instead simulate hundreds,
then thousands and ten of thousands of data sets, get the marginal effects and compare
them to the true ones (but here I won’t simulate more than 500 datasets).&lt;/p&gt;
&lt;p&gt;Let’s first generate 10 datasets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;many_datasets &amp;lt;- generate_datasets()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now comes the tricky part. I have this object, &lt;code&gt;many_datasets&lt;/code&gt; looking like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;many_datasets&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 4
##   coefs     sample_size repeats simulations
##   &amp;lt;list&amp;gt;          &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;     
## 1 &amp;lt;dbl [3]&amp;gt;         100      10 &amp;lt;list [10]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I would like to fit LPMs to the 10 datasets. For this, I will need to use all the power of functional
programming and the &lt;code&gt;{tidyverse}&lt;/code&gt;. I will be adding columns to this data frame using &lt;code&gt;mutate()&lt;/code&gt;
and mapping over the &lt;code&gt;simulations&lt;/code&gt; list-column using &lt;code&gt;modify_depth()&lt;/code&gt;. The list of data frames is
at the second level (remember, it’s a list containing a list containing data frames).&lt;/p&gt;
&lt;p&gt;I’ll start by fitting the LPMs, then using &lt;code&gt;broom::tidy()&lt;/code&gt; I will get a nice data frame of the
estimated parameters. I will then only select what I need, and then bind the rows of all the
data frames. I will do the same for the &lt;em&gt;true&lt;/em&gt; marginal effects.&lt;/p&gt;
&lt;p&gt;I highly suggest that you run the following lines, one after another. It is complicated to understand
what’s going on if you are not used to such workflows. However, I hope to convince you that once
it will click, it’ll be much more intuitive than doing all this inside a loop. Here’s the code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results &amp;lt;- many_datasets %&amp;gt;% 
  mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&amp;gt;% 
  mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&amp;gt;% 
  mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&amp;gt;% 
  mutate(lpm = modify_depth(lpm, 2, ~filter(., term != &amp;quot;(Intercept)&amp;quot;))) %&amp;gt;% 
  mutate(lpm = map(lpm, bind_rows)) %&amp;gt;% 
  mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&amp;gt;% 
  mutate(true_effect = map(true_effect, bind_rows))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how results looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##   coefs     sample_size repeats simulations lpm             true_effect    
##   &amp;lt;list&amp;gt;          &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;      &amp;lt;list&amp;gt;          &amp;lt;list&amp;gt;         
## 1 &amp;lt;dbl [3]&amp;gt;         100      10 &amp;lt;list [10]&amp;gt; &amp;lt;tibble [20 × … &amp;lt;tibble [20 × …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a closer look to the &lt;code&gt;lpm&lt;/code&gt; and &lt;code&gt;true_effect&lt;/code&gt; columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results$lpm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 20 x 2
##    term  estimate
##    &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 x1       0.228
##  2 x2       0.353
##  3 x1       0.180
##  4 x2       0.361
##  5 x1       0.165
##  6 x2       0.374
##  7 x1       0.182
##  8 x2       0.358
##  9 x1       0.125
## 10 x2       0.345
## 11 x1       0.171
## 12 x2       0.331
## 13 x1       0.122
## 14 x2       0.309
## 15 x1       0.129
## 16 x2       0.332
## 17 x1       0.102
## 18 x2       0.374
## 19 x1       0.176
## 20 x2       0.410&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results$true_effect&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 20 x 2
##    term  true_effect
##    &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 x1          0.183
##  2 x2          0.366
##  3 x1          0.166
##  4 x2          0.331
##  5 x1          0.174
##  6 x2          0.348
##  7 x1          0.169
##  8 x2          0.339
##  9 x1          0.167
## 10 x2          0.335
## 11 x1          0.173
## 12 x2          0.345
## 13 x1          0.157
## 14 x2          0.314
## 15 x1          0.170
## 16 x2          0.340
## 17 x1          0.182
## 18 x2          0.365
## 19 x1          0.161
## 20 x2          0.321&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s bind the columns, and compute the difference between the &lt;em&gt;true&lt;/em&gt; and estimated marginal
effects:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulation_results &amp;lt;- results %&amp;gt;% 
  mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&amp;gt;% 
  mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&amp;gt;% 
  mutate(difference = map(difference, ~select(., term, difference))) %&amp;gt;% 
  pull(difference) %&amp;gt;% 
  .[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the simulation results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulation_results %&amp;gt;% 
  group_by(term) %&amp;gt;% 
  summarise(mean = mean(difference), 
            sd = sd(difference))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   term     mean     sd
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 x1     0.0122 0.0370
## 2 x2    -0.0141 0.0306&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Already with only 10 simulated datasets, the difference in means is not significant. Let’s rerun
the analysis, but for difference sizes. In order to make things easier, we can put all the code
into a nifty function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monte_carlo &amp;lt;- function(coefs, sample_size, repeats){
  many_datasets &amp;lt;- generate_datasets(coefs, sample_size, repeats)
  
  results &amp;lt;- many_datasets %&amp;gt;% 
    mutate(lpm = modify_depth(simulations, 2, ~lm(y ~ ., data = .x))) %&amp;gt;% 
    mutate(lpm = modify_depth(lpm, 2, broom::tidy)) %&amp;gt;% 
    mutate(lpm = modify_depth(lpm, 2, ~select(., term, estimate))) %&amp;gt;% 
    mutate(lpm = modify_depth(lpm, 2, ~filter(., term != &amp;quot;(Intercept)&amp;quot;))) %&amp;gt;% 
    mutate(lpm = map(lpm, bind_rows)) %&amp;gt;% 
    mutate(true_effect = modify_depth(simulations, 2, ~meffects(., coefs = coefs[[1]]))) %&amp;gt;% 
    mutate(true_effect = map(true_effect, bind_rows))

  simulation_results &amp;lt;- results %&amp;gt;% 
    mutate(difference = map2(.x = lpm, .y = true_effect, bind_cols)) %&amp;gt;% 
    mutate(difference = map(difference, ~mutate(., difference = true_effect - estimate))) %&amp;gt;% 
    mutate(difference = map(difference, ~select(., term, difference))) %&amp;gt;% 
    pull(difference) %&amp;gt;% 
    .[[1]]

  simulation_results %&amp;gt;% 
    group_by(term) %&amp;gt;% 
    summarise(mean = mean(difference), 
              sd = sd(difference))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, let’s run the simulation for different parameters and sizes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monte_carlo(c(.5, 2, 4), 100, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   term      mean     sd
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 x1    -0.00826 0.0291
## 2 x2    -0.00732 0.0412&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monte_carlo(c(.5, 2, 4), 100, 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   term     mean     sd
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 x1    0.00360 0.0392
## 2 x2    0.00517 0.0446&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monte_carlo(c(.5, 2, 4), 100, 500)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   term       mean     sd
##   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 x1    -0.00152  0.0371
## 2 x2    -0.000701 0.0423&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monte_carlo(c(pi, 6, 9), 100, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   term      mean     sd
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 x1    -0.00829 0.0546
## 2 x2     0.00178 0.0370&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monte_carlo(c(pi, 6, 9), 100, 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   term     mean     sd
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 x1    0.0107  0.0608
## 2 x2    0.00831 0.0804&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monte_carlo(c(pi, 6, 9), 100, 500)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   term     mean     sd
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 x1    0.00879 0.0522
## 2 x2    0.0113  0.0668&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that, at least for this set of parameters, the LPM does a good job of estimating marginal
effects.&lt;/p&gt;
&lt;p&gt;Now, this study might in itself not be very interesting to you, but I believe the general approach
is quite useful and flexible enough to be adapted to all kinds of use-cases.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistical matching, or when one single data source is not enough</title>
      <link>/blog/2019-07-19-statmatch/</link>
      <pubDate>Fri, 19 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-07-19-statmatch/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Row_and_column_vectors&#34;&gt;
&lt;img src=&#34;/img/columns.jpg&#34; title = &#34;Not that kind of columns&#34; width=&#34;1119&#34; height=&#34;720&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I was recently asked how to go about matching several datasets where different samples of
individuals were interviewed. This sounds like a big problem; say that you have dataset A and B,
and that A contain one sample of individuals, and B another sample of individuals, then how could
you possibly match the datasets? Matching datasets requires a common identifier, for instance,
suppose that A contains socio-demographic information on a sample of individuals I, while B,
contains information on wages and hours worked on the same sample of individuals I, then yes,
it will be possible to match/merge/join both datasets.&lt;/p&gt;
&lt;p&gt;But that was not what I was asked about; I was asked about a situation where the same population
gets sampled twice, and each sample answers to a different survey. For example the first survey
is about labour market information and survey B is about family structure. Would it be possible to
combine the information from both datasets?&lt;/p&gt;
&lt;p&gt;To me, this sounded a bit like missing data imputation problem, but where all the information
about the variables of interest was missing! I started digging a bit, and found that not only there
was already quite some literature on it, there is even a package for this, called &lt;code&gt;{StatMatch}&lt;/code&gt; with
a very detailed &lt;a href=&#34;https://cran.r-project.org/web/packages/StatMatch/vignettes/Statistical_Matching_with_StatMatch.pdf&#34;&gt;vignette&lt;/a&gt;.
The vignette is so detailed, that I will not write any code, I just wanted to share this package!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Curly-Curly, the successor of Bang-Bang</title>
      <link>/blog/2019-06-20-tidy_eval_saga/</link>
      <pubDate>Sat, 29 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-06-20-tidy_eval_saga/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Row_and_column_vectors&#34;&gt;
&lt;img src=&#34;/img/curly.jpg&#34; title = &#34;Not that kind of columns&#34; width=&#34;1119&#34; height=&#34;720&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Writing functions that take data frame columns as arguments is a problem that most R users have been
confronted with at some point. There are different ways to tackle this issue, and this blog post will
focus on the solution provided by the latest release of the &lt;code&gt;{rlang}&lt;/code&gt; package. You can read the
announcement &lt;a href=&#34;https://www.tidyverse.org/articles/2019/06/rlang-0-4-0/&#34;&gt;here&lt;/a&gt;, which explains really
well what was wrong with the old syntax, and how the new syntax works now.&lt;/p&gt;
&lt;p&gt;I have written about the problem of writing functions that use data frame columns as arguments
&lt;a href=&#34;https://www.brodrigues.co/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/&#34;&gt;three years ago&lt;/a&gt;
and &lt;a href=&#34;https://www.brodrigues.co/blog/2017-08-27-why_tidyeval/&#34;&gt;two year ago&lt;/a&gt; too.
&lt;a href=&#34;https://www.brodrigues.co/blog/2018-01-19-mapping_functions_with_any_cols/&#34;&gt;Last year&lt;/a&gt;, I wrote a
blog post that showed how to map a list of functions to a list of datasets with a list of columns
as arguments that used the &lt;code&gt;!!quo(column_name)&lt;/code&gt; syntax (the &lt;code&gt;!!&lt;/code&gt; is pronounced &lt;em&gt;bang-bang&lt;/em&gt;).
Now, there is a new sheriff in town, &lt;code&gt;{{}}&lt;/code&gt;, introduced in &lt;code&gt;{rlang}&lt;/code&gt; version 0.4.0 that makes
things even easier. The suggested pronunciation of &lt;code&gt;{{}}&lt;/code&gt; is &lt;em&gt;curly-curly&lt;/em&gt;, but there is no
&lt;a href=&#34;https://twitter.com/JonTheGeek/status/1144815369766547456&#34;&gt;consensus yet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, let’s load the &lt;code&gt;{tidyverse}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s suppose that I need to write a function that takes a data frame, as well as a column from
this data frame as arguments:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;how_many_na &amp;lt;- function(dataframe, column_name){
  dataframe %&amp;gt;%
    filter(is.na(column_name)) %&amp;gt;%
    count()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try this function out on the &lt;code&gt;starwars&lt;/code&gt; data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(starwars)

head(starwars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 13
##   name  height  mass hair_color skin_color eye_color birth_year gender
##   &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; 
## 1 Luke…    172    77 blond      fair       blue            19   male  
## 2 C-3PO    167    75 &amp;lt;NA&amp;gt;       gold       yellow         112   &amp;lt;NA&amp;gt;  
## 3 R2-D2     96    32 &amp;lt;NA&amp;gt;       white, bl… red             33   &amp;lt;NA&amp;gt;  
## 4 Dart…    202   136 none       white      yellow          41.9 male  
## 5 Leia…    150    49 brown      light      brown           19   female
## 6 Owen…    178   120 brown, gr… light      blue            52   male  
## # … with 5 more variables: homeworld &amp;lt;chr&amp;gt;, species &amp;lt;chr&amp;gt;, films &amp;lt;list&amp;gt;,
## #   vehicles &amp;lt;list&amp;gt;, starships &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, there are missing values in the &lt;code&gt;hair_color&lt;/code&gt; column. Let’s try to count how many
missing values are in this column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;how_many_na(starwars, hair_color)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error: object &amp;#39;hair_color&amp;#39; not found&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R cannot find the &lt;code&gt;hair_color&lt;/code&gt; column, and yet it is in the data! Well, this is actually exactly
the issue. The issue is that the column is inside the dataframe, but when calling the function
with &lt;code&gt;hair_color&lt;/code&gt; as the second argument, R is looking for a variable called &lt;code&gt;hair_color&lt;/code&gt; that
does not exist. What about trying with &lt;code&gt;&amp;quot;hair_color&amp;quot;&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;how_many_na(starwars, &amp;quot;hair_color&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##       n
##   &amp;lt;int&amp;gt;
## 1     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we get something, but something wrong!&lt;/p&gt;
&lt;p&gt;One way to solve this issue, is to not use the &lt;code&gt;filter()&lt;/code&gt; function, and instead rely on base R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;how_many_na_base &amp;lt;- function(dataframe, column_name){
  na_index &amp;lt;- is.na(dataframe[, column_name])
  nrow(dataframe[na_index, column_name])
}

how_many_na_base(starwars, &amp;quot;hair_color&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This works, but not using the &lt;code&gt;{tidyverse}&lt;/code&gt; at all is not an option, at least for me. For instance,
the next function, which uses a grouping variable, would be difficult to implement without the
&lt;code&gt;{tidyverse}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summarise_groups &amp;lt;- function(dataframe, grouping_var, column_name){
  dataframe %&amp;gt;%
    group_by(grouping_var) %&amp;gt;%  
    summarise(mean(column_name, na.rm = TRUE))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calling this function results in the following error message:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error: Column `grouping_var` is unknown&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before the release of &lt;code&gt;{rlang}&lt;/code&gt; 0.4.0 this is was the solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summarise_groups &amp;lt;- function(dataframe, grouping_var, column_name){

  grouping_var &amp;lt;- enquo(grouping_var)
  column_name &amp;lt;- enquo(column_name)
  mean_name &amp;lt;- paste0(&amp;quot;mean_&amp;quot;, quo_name(column_name))

  dataframe %&amp;gt;%
    group_by(!!grouping_var) %&amp;gt;%  
    summarise(!!(mean_name) := mean(!!column_name, na.rm = TRUE))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The core of the function remained very similar to the version from before, but now one has to
use the &lt;code&gt;enquo()&lt;/code&gt;-&lt;code&gt;!!&lt;/code&gt; syntax. While not overly difficult to use, it is cumbersome.&lt;/p&gt;
&lt;p&gt;Now this can be simplified using the new &lt;code&gt;{{}}&lt;/code&gt; syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summarise_groups &amp;lt;- function(dataframe, grouping_var, column_name){

  dataframe %&amp;gt;%
    group_by({{grouping_var}}) %&amp;gt;%  
    summarise({{column_name}} := mean({{column_name}}, na.rm = TRUE))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Much easier and cleaner! You still have to use the &lt;code&gt;:=&lt;/code&gt; operator instead of &lt;code&gt;=&lt;/code&gt; for the column name
however. Also, from my understanding, if you want to modify the column names, for instance in this
case return &lt;code&gt;&amp;quot;mean_height&amp;quot;&lt;/code&gt; instead of &lt;code&gt;height&lt;/code&gt; you have to keep using the &lt;code&gt;enquo()&lt;/code&gt;-&lt;code&gt;!!&lt;/code&gt; syntax.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intermittent demand, Croston and Die Hard</title>
      <link>/blog/2019-06-12-intermittent/</link>
      <pubDate>Wed, 12 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-06-12-intermittent/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/List_of_Christmas_films&#34;&gt;
&lt;img src=&#34;/img/diehard.jpg&#34; title = &#34;Die Hard is the best Christmas movie&#34; width=&#34;600&#34; height=&#34;400&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I have recently been confronted to a kind of data set and problem that I was not even aware existed:
intermittent demand data. Intermittent demand arises when the demand for a certain good arrives
sporadically. Let’s take a look at an example, by analyzing the number of downloads for the &lt;code&gt;{RDieHarder}&lt;/code&gt;
package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tsintermittent)
library(nnfor)
library(cranlogs)
library(brotools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rdieharder &amp;lt;- cran_downloads(&amp;quot;RDieHarder&amp;quot;, from = &amp;quot;2017-01-01&amp;quot;)

ggplot(rdieharder) +
  geom_line(aes(y = count, x = date), colour = &amp;quot;#82518c&amp;quot;) +
  theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-06-12-intermittent_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s take a look at just one month of data, because the above plot is not very clear, because of
the outlier just before 2019… I wonder now, was that on Christmas day?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rdieharder %&amp;gt;%
  filter(count == max(count))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         date count    package
## 1 2018-12-21   373 RDieHarder&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not exactly on Christmas day, but almost! Anyways, let’s look at one month of data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;january_2018 &amp;lt;- rdieharder %&amp;gt;%
  filter(between(date, as.Date(&amp;quot;2018-01-01&amp;quot;), as.Date(&amp;quot;2018-02-01&amp;quot;)))

ggplot(january_2018) +
  geom_line(aes(y = count, x = date), colour = &amp;quot;#82518c&amp;quot;) +
  theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-06-12-intermittent_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, it is clear that this will be tricky to forecast. There is no discernible pattern,
no trend, no seasonality… nothing that would make it “easy” for a model to learn how to forecast
such data.&lt;/p&gt;
&lt;p&gt;This is typical intermittent demand data. Specific methods have been developed to forecast such
data, the most well-known being Croston, as detailed in
&lt;a href=&#34;https://www.jstor.org/stable/3007885?seq=1#page_scan_tab_contents&#34;&gt;this paper&lt;/a&gt;.
A function to estimate such models is available in the &lt;code&gt;{tsintermittent}&lt;/code&gt; package, written by
&lt;a href=&#34;https://kourentzes.com/forecasting/2014/06/23/intermittent-demand-forecasting-package-for-r/&#34;&gt;Nikolaos Kourentzes&lt;/a&gt;
who also wrote another package, &lt;code&gt;{nnfor}&lt;/code&gt;, which uses Neural Networks to forecast time series data.
I am going to use both to try to forecast the intermittent demand for the &lt;code&gt;{RDieHarder}&lt;/code&gt; package
for the year 2019.&lt;/p&gt;
&lt;p&gt;Let’s first load these packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tsintermittent)
library(nnfor)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And as usual, split the data into training and testing sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_data &amp;lt;- rdieharder %&amp;gt;%
  filter(date &amp;lt; as.Date(&amp;quot;2019-01-01&amp;quot;)) %&amp;gt;%
  pull(count) %&amp;gt;%
  ts()

test_data &amp;lt;- rdieharder %&amp;gt;%
  filter(date &amp;gt;= as.Date(&amp;quot;2019-01-01&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s consider three models; a naive one, which simply uses the mean of the training set as the
forecast for all future periods, Croston’s method, and finally a Neural Network from the &lt;code&gt;{nnfor}&lt;/code&gt;
package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;naive_model &amp;lt;- mean(train_data)

croston_model &amp;lt;- crost(train_data, h = 163)

nn_model &amp;lt;- mlp(train_data, reps = 1, hd.auto.type = &amp;quot;cv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in preprocess(y, m, lags, keep, difforder, sel.lag,
## allow.det.season, : No inputs left in the network after pre-selection,
## forcing AR(1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nn_model_forecast &amp;lt;- forecast(nn_model, h = 163)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;crost()&lt;/code&gt; function estimates Croston’s model, and the &lt;code&gt;h&lt;/code&gt; argument produces the
forecast for the next 163 days. &lt;code&gt;mlp()&lt;/code&gt; trains a multilayer perceptron, and the &lt;code&gt;hd.auto.type = &amp;quot;cv&amp;quot;&lt;/code&gt;
argument means that 5-fold cross-validation will be used to find the best number of hidden nodes. I
then obtain the forecast using the &lt;code&gt;forecast()&lt;/code&gt; function. As you can read from the Warning message
above, the Neural Network was replaced by an auto-regressive model, AR(1), because no inputs were
left after pre-selection… I am not exactly sure what that means, but if I remove the big outlier
from before, this warning message disappears, and a Neural Network is successfully trained.&lt;/p&gt;
&lt;p&gt;In order to rank the models, I follow &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0169207006000239&#34;&gt;this paper&lt;/a&gt;
from Rob J. Hyndman, who wrote a very useful book titled &lt;a href=&#34;https://otexts.com/fpp2/&#34;&gt;Forecasting: Principles and Practice&lt;/a&gt;,
and use the Mean Absolute Scaled Error, or MASE. You can also read &lt;a href=&#34;https://robjhyndman.com/papers/foresight.pdf&#34;&gt;this shorter pdf&lt;/a&gt;
which also details how to use MASE to measure the accuracy for intermittent demand. Here is the
function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mase &amp;lt;- function(train_ts, test_ts, outsample_forecast){

  naive_insample_forecast &amp;lt;- stats::lag(train_ts)

  insample_mae &amp;lt;- mean(abs(train_ts - naive_insample_forecast), na.rm = TRUE)
  error_outsample &amp;lt;- test_ts - outsample_forecast

  ase &amp;lt;- error_outsample / insample_mae
  mean(abs(ase), na.rm = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is now easy to compute the models’ accuracies:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mase(train_data, test_data$count, naive_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.764385&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mase(train_data, test_data$count, croston_model$component$c.out[1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.397611&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mase(train_data, test_data$count, nn_model_forecast$mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.767357&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Croston’s method is the one that performs best from the three. Maybe surprisingly, the naive method
performs just as well as the Neural Network! (or rather, the AR(1) model) Let’s also plot the predictions
with the true values from the test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_data &amp;lt;- test_data %&amp;gt;%
  mutate(naive_model_forecast = naive_model,
         croston_model_forecast = croston_model$component$c.out[1],
         nn_model_forecast = nn_model_forecast$mean) %&amp;gt;%
  select(-package) %&amp;gt;%
  rename(actual_value = count)


test_data_longer &amp;lt;- test_data %&amp;gt;%
  gather(models, value,
         actual_value, naive_model_forecast, croston_model_forecast, nn_model_forecast)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: attributes are not identical across measure variables;
## they will be dropped&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_data_longer) +
  geom_line(aes(y = value, x = date, colour = models)) +
  theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-06-12-intermittent_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Just to make sure I didn’t make a mistake when writing the &lt;code&gt;mase()&lt;/code&gt; function, let’s use the
&lt;code&gt;accuracy()&lt;/code&gt; function from the &lt;code&gt;{forecast}&lt;/code&gt; package and compare the result for the Neural Network:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(forecast)
accuracy(nn_model_forecast, x = test_data$actual_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                       ME     RMSE      MAE  MPE MAPE      MASE       ACF1
## Training set 0.001929409 14.81196 4.109577  NaN  Inf 0.8437033 0.05425074
## Test set     8.211758227 12.40199 8.635563 -Inf  Inf 1.7673570         NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is the same, so it does seem like the naive method is not that bad, actually! Now, in
general, intermittent demand series have a lot of 0 values, which is not really the case here. I
still think that the methodology fits to this particular data set.&lt;/p&gt;
&lt;p&gt;How else would you have forecast this data? Let me know via twitter!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using cosine similarity to find matching documents: a tutorial using Seneca&#39;s letters to his friend Lucilius</title>
      <link>/blog/2019-06-04-cosine_sim/</link>
      <pubDate>Tue, 04 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-06-04-cosine_sim/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Seneca_the_Younger&#34;&gt;
&lt;img src=&#34;/img/seneca.png&#34; title = &#34;Seneca the Younger&#34; width=&#34;400&#34; height=&#34;600&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Lately I’ve been interested in trying to cluster documents, and to find similar documents based on their contents.
In this blog post, I will use &lt;a href=&#34;https://en.wikisource.org/wiki/Moral_letters_to_Lucilius&#34;&gt;Seneca’s &lt;em&gt;Moral letters to Lucilius&lt;/em&gt;&lt;/a&gt;
and compute the pairwise &lt;a href=&#34;https://en.wikipedia.org/wiki/Cosine_similarity&#34;&gt;cosine similarity&lt;/a&gt; of his 124 letters.
Computing the cosine similarity between two vectors returns how similar these vectors are. A cosine
similarity of 1 means that the angle between the two vectors is 0, and thus both vectors have the
same direction.
Seneca’s Moral letters to Lucilius deal mostly with philosophical topics, as Seneca was, among many other
things, a philosopher of the stoic school. The stoic school of philosophy is quite
interesting, but it has been unfortunately misunderstood, especially in modern times. There is now a renewed interest for
this school, see &lt;a href=&#34;https://en.wikipedia.org/wiki/Modern_Stoicism&#34;&gt;Modern Stoicism&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The first step is to scrape the letters. The code below scrapes the letters, and saves them into a list.
I first start by writing a function that gets the raw text. Note the &lt;code&gt;xpath&lt;/code&gt; argument of the &lt;code&gt;html_nodes()&lt;/code&gt;
function. I obtained this complex expression by using the &lt;a href=&#34;https://selectorgadget.com/&#34;&gt;SelectorGadget&lt;/a&gt;
extension for Google Chrome, and then selecting the right element of the web page.
See this &lt;a href=&#34;https://i.imgur.com/2cntugt.png&#34;&gt;screenshot&lt;/a&gt; if my description was not very clear.&lt;/p&gt;
&lt;p&gt;Then, the &lt;code&gt;extract_text()&lt;/code&gt; function extracts the text from the letter. The only line that might be
a bit complex is &lt;code&gt;discard(~`==`(., &amp;quot;&amp;quot;))&lt;/code&gt; which removes every empty line.&lt;/p&gt;
&lt;p&gt;Finally, there’s the &lt;code&gt;get_letter()&lt;/code&gt; function that actually gets the letter by calling the first two
functions. In the last line, I get all the letters into a list by mapping the list of urls to the
&lt;code&gt;get_letter()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)

base_url &amp;lt;- &amp;quot;https://en.wikisource.org/wiki/Moral_letters_to_Lucilius/Letter_&amp;quot;

letter_numbers &amp;lt;- seq(1, 124)

letter_urls &amp;lt;- paste0(base_url, letter_numbers)

get_raw_text &amp;lt;- function(base_url, letter_number){
  paste0(base_url, letter_number) %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(xpath =&amp;#39;//*[contains(concat( &amp;quot; &amp;quot;, @class, &amp;quot; &amp;quot; ), concat( &amp;quot; &amp;quot;, &amp;quot;mw-parser-output&amp;quot;, &amp;quot; &amp;quot; ))]&amp;#39;) %&amp;gt;%  
    html_text()
}


extract_text &amp;lt;- function(raw_text, letter_number){
  raw_text &amp;lt;- raw_text %&amp;gt;%
    str_split(&amp;quot;\n&amp;quot;) %&amp;gt;%  
    flatten_chr() %&amp;gt;%  
    discard(~`==`(., &amp;quot;&amp;quot;))

  start &amp;lt;- 5

  end &amp;lt;- str_which(raw_text, &amp;quot;Footnotes*&amp;quot;)

  raw_text[start:(end-1)] %&amp;gt;%
    str_remove_all(&amp;quot;\\[\\d{1,}\\]&amp;quot;) %&amp;gt;%
    str_remove_all(&amp;quot;\\[edit\\]&amp;quot;)
}

get_letter &amp;lt;- function(base_url, letter_number){

  raw_text &amp;lt;- get_raw_text(base_url, letter_number)

  extract_text(raw_text, letter_number)
}

letters_to_lucilius &amp;lt;- map2(base_url, letter_numbers, get_letter)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the letters saved in a list, we need to process the text a little bit. In order
to compute the cosine similarity between the letters, I need to somehow represent them as vectors.
There are several ways of doing this, and I am going to compute the tf-idf of each letter. The tf-idf
will give me a vector for each letter, with zero and non-zero values. Zero values represent words
that are common to all letters, and thus do not have any &lt;em&gt;predictive power&lt;/em&gt;. Non-zero values are
words that are not present in all letters, but maybe only a few. I expect that letters that
discuss death for example, will have the word death in them, and letters that do not discuss death
will not have this word. The word death thus has what I call &lt;em&gt;predictive power&lt;/em&gt;, in that it helps
us distinguish the letters discussing death from the other letters that do not discuss it. The same
reasoning can be applied for any topic.&lt;/p&gt;
&lt;p&gt;So, to get the tf-idf of each letter, I first need to put them in a tidy dataset. I will use the
&lt;code&gt;{tidytext}&lt;/code&gt; package for this. First, I load the required packages, convert each letter to a
dataframe of one column that contains the text, and save the letter’s titles into another list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidytext)
library(SnowballC)
library(stopwords)
library(text2vec)

letters_to_lucilius_df &amp;lt;- map(letters_to_lucilius, ~tibble(&amp;quot;text&amp;quot; = .))

letter_titles &amp;lt;- letters_to_lucilius_df %&amp;gt;%
  map(~slice(., 1)) %&amp;gt;%
  map(pull)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I add this title to each dataframe as a new column, called title:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;letters_to_lucilius_df &amp;lt;-  map2(.x = letters_to_lucilius_df, .y = letter_titles,
                                ~mutate(.x, title = .y)) %&amp;gt;%
  map(~slice(., -1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now use &lt;code&gt;unnest_tokens()&lt;/code&gt; to transform the datasets. Before, I had the whole text of the letter
in one column. After using &lt;code&gt;unnest_tokens()&lt;/code&gt; I now have a dataset with one row per word. This will
make it easy to compute frequencies by letters, or what I am interested in, the tf-idf of each letter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tokenized_letters &amp;lt;- letters_to_lucilius_df %&amp;gt;%
  bind_rows() %&amp;gt;%
  group_by(title) %&amp;gt;%
  unnest_tokens(word, text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now remove stopwords, using the data containing in the &lt;code&gt;{stopwords}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stopwords_en &amp;lt;- tibble(&amp;quot;word&amp;quot; = stopwords(&amp;quot;en&amp;quot;, source  = &amp;quot;smart&amp;quot;))

tokenized_letters &amp;lt;- tokenized_letters %&amp;gt;%
  anti_join(stopwords_en) %&amp;gt;%
  filter(!str_detect(word, &amp;quot;\\d{1,}&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;word&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next step, wordstemming, meaning, going from “dogs” to “dog”, or from “was” to “be”. If you do not
do wordstemming, “dogs” and “dog” will be considered different words, even though they are not.
&lt;code&gt;wordStem()&lt;/code&gt; is a function from &lt;code&gt;{SnowballC}&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tokenized_letters &amp;lt;- tokenized_letters %&amp;gt;%
  mutate(word = wordStem(word, language = &amp;quot;en&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I can compute the tf-idf of each letter and cast the data as a sparse matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tfidf_letters &amp;lt;- tokenized_letters %&amp;gt;%
  count(title, word, sort  = TRUE) %&amp;gt;%
  bind_tf_idf(word, title, n)

sparse_matrix &amp;lt;- tfidf_letters %&amp;gt;%
  cast_sparse(title, word, tf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the sparse matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sparse_matrix[1:10, 1:4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10 x 4 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                                                                   thing
## CXIII. On the Vitality of the Soul and Its Attributes       0.084835631
## LXVI. On Various Aspects of Virtue                          0.017079890
## LXXXVII. Some Arguments in Favour of the Simple Life        0.014534884
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.025919732
## LXXVI. On Learning Wisdom in Old Age                        0.021588946
## CII. On the Intimations of Our Immortality                  0.014662757
## CXXIV. On the True Good as Attained by Reason               0.010139417
## XCIV. On the Value of Advice                                0.009266409
## LXXXI. On Benefits                                          0.007705479
## LXXXV. On Some Vain Syllogisms                              0.013254786
##                                                                     live
## CXIII. On the Vitality of the Soul and Its Attributes       0.0837751856
## LXVI. On Various Aspects of Virtue                          .           
## LXXXVII. Some Arguments in Favour of the Simple Life        0.0007267442
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.0050167224
## LXXVI. On Learning Wisdom in Old Age                        0.0025906736
## CII. On the Intimations of Our Immortality                  0.0019550342
## CXXIV. On the True Good as Attained by Reason               .           
## XCIV. On the Value of Advice                                0.0023166023
## LXXXI. On Benefits                                          0.0008561644
## LXXXV. On Some Vain Syllogisms                              0.0022091311
##                                                                   good
## CXIII. On the Vitality of the Soul and Its Attributes       0.01166490
## LXVI. On Various Aspects of Virtue                          0.04132231
## LXXXVII. Some Arguments in Favour of the Simple Life        0.04578488
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties 0.04849498
## LXXVI. On Learning Wisdom in Old Age                        0.04663212
## CII. On the Intimations of Our Immortality                  0.05180841
## CXXIV. On the True Good as Attained by Reason               0.06717364
## XCIV. On the Value of Advice                                0.01081081
## LXXXI. On Benefits                                          0.01626712
## LXXXV. On Some Vain Syllogisms                              0.01472754
##                                                                 precept
## CXIII. On the Vitality of the Soul and Its Attributes       .          
## LXVI. On Various Aspects of Virtue                          .          
## LXXXVII. Some Arguments in Favour of the Simple Life        .          
## CXVII. On Real Ethics as Superior to Syllogistic Subtleties .          
## LXXVI. On Learning Wisdom in Old Age                        .          
## CII. On the Intimations of Our Immortality                  .          
## CXXIV. On the True Good as Attained by Reason               0.001267427
## XCIV. On the Value of Advice                                0.020463320
## LXXXI. On Benefits                                          .          
## LXXXV. On Some Vain Syllogisms                              .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can consider each row of this matrix as the vector representing a letter, and thus compute the
cosine similarity between letters. For this, I am using the &lt;code&gt;sim2()&lt;/code&gt; function from the &lt;code&gt;{text2vec}&lt;/code&gt;
package. I then create the &lt;code&gt;get_similar_letters()&lt;/code&gt; function that returns similar letters for a
given reference letter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;similarities &amp;lt;- sim2(sparse_matrix, method = &amp;quot;cosine&amp;quot;, norm = &amp;quot;l2&amp;quot;) 

get_similar_letters &amp;lt;- function(similarities, reference_letter, n_recommendations = 3){
  sort(similarities[reference_letter, ], decreasing = TRUE)[1:(2 + n_recommendations)]
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_similar_letters(similarities, 19)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          XXX. On Conquering the Conqueror 
##                                 1.0000000 
##                  XXIV. On Despising Death 
##                                 0.6781600 
##      LXXXII. On the Natural Fear of Death 
##                                 0.6639736 
## LXX. On the Proper Time to Slip the Cable 
##                                 0.5981706 
## LXXVIII. On the Healing Power of the Mind 
##                                 0.4709679&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_similar_letters(similarities, 99)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                              LXI. On Meeting Death Cheerfully 
##                                                     1.0000000 
##                     LXX. On the Proper Time to Slip the Cable 
##                                                     0.5005015 
## XCIII. On the Quality, as Contrasted with the Length, of Life 
##                                                     0.4631796 
##                         CI. On the Futility of Planning Ahead 
##                                                     0.4503093 
##                              LXXVII. On Taking One&amp;#39;s Own Life 
##                                                     0.4147019&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_similar_letters(similarities, 32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                    LIX. On Pleasure and Joy 
##                                                   1.0000000 
##          XXIII. On the True Joy which Comes from Philosophy 
##                                                   0.4743672 
##                          CIX. On the Fellowship of Wise Men 
##                                                   0.4526835 
## XC. On the Part Played by Philosophy in the Progress of Man 
##                                                   0.4498278 
##         CXXIII. On the Conflict between Pleasure and Virtue 
##                                                   0.4469312&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_similar_letters(similarities, 101)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    X. On Living to Oneself 
##                                  1.0000000 
##          LXXIII. On Philosophers and Kings 
##                                  0.3842292 
##                  XLI. On the God within Us 
##                                  0.3465457 
##                       XXXI. On Siren Songs 
##                                  0.3451388 
## XCV. On the Usefulness of Basic Principles 
##                                  0.3302794&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see from these examples, this seems to be working quite well: the first title is the
title of the reference letter, will the next 3 are the suggested letters. The problem is that my
matrix is not in the right order, and thus reference letter 19 does not correspond to letter 19
of Seneca… I have to correct that, but not today.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The never-ending editor war (?)</title>
      <link>/blog/2019-05-19-spacemacs/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-05-19-spacemacs/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Death_mask&#34;&gt;
&lt;img src=&#34;/img/typical_emacs_user.gif&#34; title = &#34;typical emacs user working&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The creation of this blog post was prompted by this tweet, asking an age-old question:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;und&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/spacemacs?ref_src=twsrc%5Etfw&#34;&gt;@spacemacs&lt;/a&gt;&lt;/p&gt;&amp;mdash; Bruno Rodrigues (@brodriguesco) &lt;a href=&#34;https://twitter.com/brodriguesco/status/1128981852558123008?ref_src=twsrc%5Etfw&#34;&gt;May 16, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This is actually a very important question, that I have been asking myself for a long time. An IDE,
and plain text editors, are a very important tools to anyone writing code. Most working hours are spent
within such a program, which means that one has to be careful about choosing the right one, and
once a choice is made, one has, in my humble opinion, learn as many features of this program as
possible to become as efficient as possible.&lt;/p&gt;
&lt;p&gt;As you can notice from the tweet above, I suggested the use of &lt;a href=&#34;http://spacemacs.org/&#34;&gt;Spacemacs&lt;/a&gt;…
and my tweet did not get any likes or retweets (as of the 19th of May, sympathetic readers of this
blog have liked the tweet). It is to set this great injustice straight that I
decided to write this blog post.&lt;/p&gt;
&lt;p&gt;Spacemacs is a strange beast; if vi and Emacs had a baby, it would certainly look like Spacemacs.
So first of all, to understand what is Spacemacs, one has to know a bit about vi and Emacs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/vim.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;vi is a text editor with 43 years of history now. You might have heard of Vim (Vi IMproved) which is a
modern clone of vi, from 1991. More recently, another clone has been getting popular, Neovim, started
in 2014. Whatever version of vi however, its basic way of functioning remains the same. vi is a modal
editor, meaning that the user has to switch between different modes to work on a text file.
When vi is first started, the program will be in &lt;em&gt;Normal&lt;/em&gt; mode. In this mode, trying to type a word
will likely result in nothing, or unexpected behaviour; unexpected, if you’re not familiar with vi.
For instance, in &lt;em&gt;Normal&lt;/em&gt; mode, typing &lt;strong&gt;j&lt;/strong&gt; will not show the character &lt;strong&gt;j&lt;/strong&gt; on your screen.
Instead, this will move the cursor down one line. Typing &lt;strong&gt;p&lt;/strong&gt; will paste, &lt;strong&gt;u&lt;/strong&gt; will undo the
last action, &lt;strong&gt;y&lt;/strong&gt; will yank (copy) etc…&lt;/p&gt;
&lt;p&gt;To type text, first, one has to enter &lt;em&gt;Insert&lt;/em&gt; mode, by typing &lt;strong&gt;i&lt;/strong&gt; while in &lt;em&gt;Normal&lt;/em&gt; mode. Only
then is it possible to write text. To go back to &lt;em&gt;Normal&lt;/em&gt; mode, type &lt;strong&gt;ESC&lt;/strong&gt;. Other modes are
&lt;em&gt;Visual&lt;/em&gt; mode (from &lt;em&gt;Normal&lt;/em&gt; mode press &lt;strong&gt;v&lt;/strong&gt;), which allows the user to select text and &lt;em&gt;Command-line&lt;/em&gt;
mode which can be entered by keying &lt;strong&gt;:&lt;/strong&gt; from &lt;em&gt;Normal&lt;/em&gt; mode and allows to enter commands.&lt;/p&gt;
&lt;p&gt;Now you might be wondering why anyone would use such a convoluted way to type text. Well, this is
because one can chain these commands quite easily to perform repetitive tasks very quickly.
For instance, to delete a word, one types &lt;strong&gt;daw&lt;/strong&gt; (in &lt;em&gt;Normal&lt;/em&gt; mode), &lt;strong&gt;d&lt;/strong&gt;elete &lt;strong&gt;a&lt;/strong&gt; &lt;strong&gt;w&lt;/strong&gt;ord.
To delete the next 3 words, you can type &lt;strong&gt;3daw&lt;/strong&gt;. To edit the text between, for instance, &lt;strong&gt;()&lt;/strong&gt;
you would type &lt;strong&gt;ci(&lt;/strong&gt; (while in &lt;em&gt;Normal&lt;/em&gt; mode and anywhere between the braces
containing the text to edit), &lt;strong&gt;c&lt;/strong&gt;hange &lt;strong&gt;i&lt;/strong&gt;n &lt;strong&gt;(&lt;/strong&gt;. Same logic applies for &lt;strong&gt;ci[&lt;/strong&gt; for instance. Can you guess
what &lt;strong&gt;ciw&lt;/strong&gt; does? If you are in &lt;em&gt;Normal&lt;/em&gt; mode, and you want to change the word the cursor is on, this
command will erase the word and put you in &lt;em&gt;Insert&lt;/em&gt; mode so that you can write the new word.&lt;/p&gt;
&lt;p&gt;These are just basic reasons why vi (or its clones) are awesome. It is also possible to automate
very long and complex tasks using macros. One starts a macro by typing &lt;strong&gt;q&lt;/strong&gt; and then any letter of
the alphabet to name it, for instance &lt;strong&gt;a&lt;/strong&gt;. The user then performs the actions needed, types &lt;strong&gt;q&lt;/strong&gt;
again to stop the recording of the macro, and can then execute the macro with &lt;strong&gt;&lt;span class=&#34;citation&#34;&gt;@a&lt;/span&gt;&lt;/strong&gt;. If the user
needs to execute the macro say, 10 times, &lt;strong&gt;10@‌‌a&lt;/strong&gt; does the trick. It is possible to extend vi’s
functionalities by using plugins, but more on that down below.&lt;/p&gt;
&lt;p&gt;vi keybindings have inspired a lot of other programs. For instance, you can get extensions
for popular web browsers that mimick vi keybindings, such as
&lt;a href=&#34;https://github.com/tridactyl/tridactyl&#34;&gt;Tridayctl&lt;/a&gt; for Firefox, or &lt;a href=&#34;http://vimium.github.io/&#34;&gt;Vivium&lt;/a&gt;
for Chromium (or Google Chrome). There are even browsers that are built from scratch with support
for vi keybinds, such as my personal favorite, &lt;a href=&#34;http://qutebrowser.org/&#34;&gt;qutebrowser&lt;/a&gt;. You can even
go further and use a tiling window manager on GNU-Linux, for instance &lt;a href=&#34;https://i3wm.org/&#34;&gt;i3&lt;/a&gt;, which
I use, or &lt;a href=&#34;https://xmonad.org/&#34;&gt;xmonad&lt;/a&gt;. You might need to configure those to behave more like vi,
but it is possible. This means that by learning one set of keyboard shortcuts,
(and the logic behind chaining the keystrokes to achieve what you want), you can master several
different programs. This blog post only deals with the editor part, but as you can see, if you
go down the rabbit hole enough, a new exciting world opens up.&lt;/p&gt;
&lt;p&gt;I will show some common vi operations below, but before that let’s discuss Emacs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/emacs.png&#34; width=&#34;80%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I am not really familiar with Emacs; I know that Emacs users only swear by it (just like vi users
only swear by vi), and that Emacs is not a modal editor. However, it contains a lot of functions
that you can use by pressing &lt;strong&gt;ESC&lt;/strong&gt;, &lt;strong&gt;CTRL&lt;/strong&gt;, &lt;strong&gt;ALT&lt;/strong&gt; or &lt;strong&gt;META&lt;/strong&gt; (&lt;strong&gt;META&lt;/strong&gt; is the Windows key on a
regular PC keyboard) followed by regular keys. So the approach is different, but it is widely
accepted that productivity of proficient Emacs users is very high too. Emacs was started in 1985,
and the most popular clone is GNU Emacs. Emacs also features modes, but not in the same sense as vi.
There are major and minor modes.
For instance, if you’re editing a Python script, Emacs will be in Python mode, or if editing a Markdown
file Emacs will be in Markdown mode. This will change the available functions to the user, as well
as provide other niceties, such as auto-completion.
Emacs is also easily extensible, which is another reason why it is so popular.
Users can install packages for Emacs, just like R users would do for R, to extend Emacs’ capabilities.
For instance, a very important package if you plan to use Emacs for statistics or data science is
&lt;code&gt;ESS&lt;/code&gt;, &lt;code&gt;E&lt;/code&gt;macs &lt;code&gt;S&lt;/code&gt;peaks &lt;code&gt;S&lt;/code&gt;tatistics. Emacs contains other very high quality packages, and it seems
to me (but don’t quote me on that) that Emacs’ packages are more mature and feature-rich than vi’s
plugins. However, vi keybindings are really awesome. This is, I believe, what
&lt;a href=&#34;https://twitter.com/syl20bnr&#34;&gt;Sylvain Benner&lt;/a&gt; was thinking when he developed Spacemacs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spacemacs.png&#34; width=&#34;30%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Spacemacs’ motto is that &lt;em&gt;The best editor is neither Emacs nor Vim, it’s Emacs and Vim!&lt;/em&gt;.
Spacemacs is a version, or distribution of Emacs, that has a very specific way of doing things. However,
since it’s built on top of Emacs, all of Emacs’ packages are available to the user, notably &lt;em&gt;Evil&lt;/em&gt;,
which is a package that makes Emacs mimick vi’s modal mode and keybindings (the name of this
package tells you everything you need to know about what Emacs users think of vi users 😀)&lt;/p&gt;
&lt;p&gt;Not only does Spacemacs support Emacs packages, but Spacemacs also features so-called &lt;em&gt;layers&lt;/em&gt;, which
are configuration files that integrate one, or several packages, seamlessly into Spacemacs particular
workflow. This particular workflow is what gave Spacemacs its name.
Instead of relying on &lt;strong&gt;ESC&lt;/strong&gt;, &lt;strong&gt;CTRL&lt;/strong&gt;, &lt;strong&gt;ALT&lt;/strong&gt; or &lt;strong&gt;META&lt;/strong&gt; like Emacs, users can launch functions
by typing &lt;strong&gt;Space&lt;/strong&gt; in &lt;em&gt;Normal&lt;/em&gt; mode and then a sequence of letters. For instance, &lt;strong&gt;Spaceqr&lt;/strong&gt; restarts Spacemacs.
And what’s more, you don’t actually need to learn these new key sequences. When you type &lt;strong&gt;Space&lt;/strong&gt;,
the minibuffer, a little popup window at the bottom of Spacemacs, appears and shows you all the options
that you can type. For instance, typing &lt;strong&gt;b&lt;/strong&gt; after &lt;strong&gt;Space&lt;/strong&gt; opens up the buffer menu. Buffers are
what could be called tabs in Rstudio. Here you can chose to &lt;em&gt;delete&lt;/em&gt; a buffer, with &lt;strong&gt;d&lt;/strong&gt;, create
a new buffer with &lt;strong&gt;N&lt;/strong&gt;, and many more options.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/minibuffer.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Enough text, let’s get into the videos. But keep in mind the following: the videos below show the
keystrokes I am typing to perform the actions. However, because I use the BÉPO keyboard layout,
which is the french equivalent of the DVORAK layout, the keystrokes will be different than those
in a regular vi guide, which are mainly written for the QWERTY layout.
Also, to use Spacemacs for R, you need to enable the &lt;strong&gt;ESS&lt;/strong&gt; layer, which I show how to do at the end.
Enabling this layer will turn on auto-completion, as well as provide documentation in real time
for your function in the minibuffer:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spacemacs_autocompletion.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/spacemacs_doc.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first video shows Spacemacs divided into two windows. On the left, I am navigating around code
using the &lt;strong&gt;T&lt;/strong&gt; (move down) and &lt;strong&gt;S&lt;/strong&gt; (move up) keys. To execute a region that I select, I type
&lt;strong&gt;Spacemrr&lt;/strong&gt; (this stands for &lt;strong&gt;M&lt;/strong&gt;ajor mode &lt;strong&gt;R&lt;/strong&gt;un &lt;strong&gt;R&lt;/strong&gt;egion). Then around second 5, I key &lt;strong&gt;O&lt;/strong&gt;
which switches to &lt;em&gt;Insert&lt;/em&gt; mode one line below the line I was, type &lt;code&gt;head(mtcars)&lt;/code&gt; and then
&lt;strong&gt;ESC&lt;/strong&gt; to switch back to &lt;em&gt;Normal&lt;/em&gt; mode and run the line with
&lt;strong&gt;Spacemrl&lt;/strong&gt; (&lt;strong&gt;M&lt;/strong&gt;ajor mode &lt;strong&gt;R&lt;/strong&gt;un &lt;strong&gt;L&lt;/strong&gt;ine).&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_01_running_lines.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;In this video, I show you how to switch between windows. Type &lt;strong&gt;SpaceN&lt;/strong&gt; to switch to window N. At
the end, I key &lt;strong&gt;dd&lt;/strong&gt; which deletes a whole line.&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_02_switching_windows.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;In the video below, I show how to use the pipe operator with &lt;strong&gt;Spacemm&lt;/strong&gt;. This is a keyboard shortcut
that I have defined myself. You can also spot the auto-completion at work in this video. To run
the code, I first select it with &lt;strong&gt;V&lt;/strong&gt;, which selects the whole line the cursor is currently at
and enters &lt;em&gt;Visual&lt;/em&gt; mode. I then select the lines below with &lt;strong&gt;T&lt;/strong&gt; and run the region with &lt;strong&gt;Spacemrr&lt;/strong&gt;.&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_03_pipe.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Here I show how plotting behaves. When a plot is created, a new window is opened with the plot. This
is a major shortcoming of using Spacemacs for R programming; there is not a dedicated buffer for
plots, and it only shows the very last one created, so there is no way to keep all the plots created in
the current session in a neat, dedicated buffer. It seems to be possible using
&lt;a href=&#34;https://github.com/erikriverson/org-mode-R-tutorial/blob/master/org-mode-R-tutorial.org&#34;&gt;Org-mode&lt;/a&gt;,
which is an Emacs mode for writing notes, todos, and authoring documents. But I haven’t explored
this option yet, mainly because in my case, only looking at one plot at a time is ok.&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_04_ggplot.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Here I show how to quickly add text to the top of the document when at the cursor is at the bottom: I try to use
the &lt;code&gt;tabyl()&lt;/code&gt; function found in the &lt;code&gt;{janitor}&lt;/code&gt; package, which I forgot to load. I quickly go all the
way up with &lt;strong&gt;gg&lt;/strong&gt;, then key &lt;strong&gt;yy&lt;/strong&gt; to copy the first line, then &lt;strong&gt;P&lt;/strong&gt; to paste it on the line below
(&lt;strong&gt;p&lt;/strong&gt; would paste it on the same line), type &lt;strong&gt;fv&lt;/strong&gt;, to &lt;strong&gt;f&lt;/strong&gt;ind the letter v from the word “tidyverse”,
then type &lt;strong&gt;liw&lt;/strong&gt; (which is the BÉPO equivalent of &lt;strong&gt;ciw&lt;/strong&gt; for &lt;strong&gt;C&lt;/strong&gt;hange &lt;strong&gt;I&lt;/strong&gt;n &lt;strong&gt;W&lt;/strong&gt;ord) and
finally change “tidyverse” to “janitor”. This seems overly complex, but once you get used to this
way of working, you will wonder why you hadn’t tried vi sooner.&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_05_janitor.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Here I show how to do block comment. &lt;strong&gt;8gg&lt;/strong&gt; jumps to the 8th line, &lt;strong&gt;CTRLv&lt;/strong&gt; starts block visual
mode, which allows me to select a block of text. I select the first column of the text, &lt;strong&gt;G&lt;/strong&gt; to
jump all the way down, then &lt;strong&gt;A&lt;/strong&gt; to enter insert mode at the end of the selection (actually, it
would have been more logical to use &lt;strong&gt;I&lt;/strong&gt;, which enters insert mode at the beginning of the selection)
of the line and then add “#” to comment.&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_06_block_comment.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Here I show how to delete a block of text:&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_07_block_delete.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Search and replace, by entering &lt;em&gt;command-line&lt;/em&gt; mode (look at the very bottom of the window):&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_08_search_replace_undo.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;I forgot to add “,” characters on a bunch of lines. I add the first “,” to the first line, go down
and press &lt;strong&gt;ESC&lt;/strong&gt; to exit &lt;em&gt;Insert&lt;/em&gt; mode. Now in &lt;strong&gt;Normal&lt;/strong&gt; mode, I type &lt;strong&gt;.&lt;/strong&gt; to execute the last
command, which is &lt;em&gt;inserting a “,” character and going down a line&lt;/em&gt;. This &lt;em&gt;dot command&lt;/em&gt; is a feature
of vi, and it will always redo the last performed change.&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_09_dot.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;But instead of typing &lt;strong&gt;.&lt;/strong&gt; six times, just type &lt;strong&gt;6.&lt;/strong&gt; and be done with it:&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_09b_repeated_dot.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;What if you want to do something more complex, involving several commands? Here the &lt;em&gt;dot command&lt;/em&gt;
won’t be enough, since it only replicates the last command, not more. For this you
can define macros with **&lt;span class=&#34;citation&#34;&gt;@*&lt;/span&gt;*. I look for the “,” character, twice, and put the rest of the characters
in the next line with enter. I then repeat this operation by executing the macro using &lt;strong&gt;@‌‌a&lt;/strong&gt;
repeatedly (&lt;strong&gt;@‌‌a&lt;/strong&gt; because I saved the actions in &lt;strong&gt;a&lt;/strong&gt;, but it could have been any other letter).
I then undo my changes and execute the macro 5 times with &lt;strong&gt;5@‌‌a&lt;/strong&gt;.&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_10_macros.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Here I show the undo tree (by typing &lt;strong&gt;Spaceua&lt;/strong&gt;), which is a feature Spacemacs inherited from
Emacs: it makes undoing changes and going back to a previous version of your script very easily:&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_11_undo_tree.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;Finally, I show my Spacemacs configuration file. I show where one needs to specify the layers one wishes
to use. For R, the ESS layer (which is a configuration file for the ESS Emacs package) is mandatory.
As I explained above, it is also possible to use Emacs packages for which no layer is available.
These are the packages under &lt;code&gt;dotspacemacs-additional-packages&lt;/code&gt;. In my case I use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dotspacemacs-additional-packages &amp;#39;(polymode
                                  poly-R
                                  poly-noweb
                                  poly-markdown)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which makes working with RMarkdown possible. &lt;code&gt;polymode&lt;/code&gt; enables simultaneous Major modes, which is
needed for RMarkdown (because RMarkdown files mix Markdown and R).&lt;/p&gt;
&lt;div&gt;
&lt;video width=&#34;838&#34; height=&#34;436&#34; src=&#34;/img/spacemacs_12_config.mp4&#34; controls=&#34;true&#34; /&gt;
&lt;/div&gt;
&lt;p&gt;That’s the end of this long post. Spacemacs is really a joy to use, but the learning curve is quite
steep. However, it is definitely worth it. There are so many packages available for Emacs (and hence
Spacemacs) that allow you to browse the web, play games, listen to music, send and read emails…
that a recurrent joke is that Emacs is &lt;em&gt;a very nice operating system, but it lacks
a decent editor&lt;/em&gt;. If that’s the case, Spacemacs is the perfect operating system, because it includes
the greatest editor, vi.&lt;/p&gt;
&lt;p&gt;If you’re interested and and want to learn more about vi, I advise you to read the following book
&lt;a href=&#34;https://www.ossblog.org/wp-content/uploads/2017/06/vim-recipes.pdf&#34;&gt;Vim Recipes&lt;/a&gt; (pdf warning, free)
or &lt;a href=&#34;https://pragprog.com/book/dnvim2/practical-vim-second-edition&#34;&gt;Practical Vim, Edit Text at the Speed of thought&lt;/a&gt;
(not free, but worth every cent), and &lt;a href=&#34;https://leanpub.com/VimLikeAPro&#34;&gt;Use Vim Like a Pro&lt;/a&gt;, which
I have not read, but it looks quite good, and is free too if you want. Now this only covers the
vi part, not the Emacs aspects of Spacemacs, but you don’t really need to know about Emacs to
use Spacemacs. I had 0 experience with Emacs, and still have 0 experience with it. I only learned
how to configure Spacemacs, which does not require any previous experience. To find the packages
you need, as usual, use any search engine of your liking.&lt;/p&gt;
&lt;p&gt;The last point I want to address is the built-in Vim mode of Rstudio. While it works, it does not
work 100% as regular Vim, and worst of all, does not support, as far as I know, any other keyboard
layout than QWERTY, which is a nogo for me.&lt;/p&gt;
&lt;p&gt;In any case, if you’re looking to learn something new that you can use for many programs, including
Rstudio, learn Vim, and then give Spacemacs a try. Chaining keystrokes to edit text gets addictive
very quickly.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For reference, here is my &lt;code&gt;dotspacemacs/user-config&lt;/code&gt;, which is where I defined the shortcut for
the &lt;code&gt;%&amp;gt;%&lt;/code&gt; operator.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(defun dotspacemacs/user-config ()
  &amp;quot;Configuration for user code:
This function is called at the very end of Spacemacs startup, after layer
configuration.
Put your configuration code here, except for variables that should be set
before packages are loaded.&amp;quot;
;;; R modes
  (add-to-list &amp;#39;auto-mode-alist &amp;#39;(&amp;quot;\\.md&amp;quot; . poly-markdown-mode))
  (add-to-list &amp;#39;auto-mode-alist &amp;#39;(&amp;quot;\\.Snw&amp;quot; . poly-noweb+r-mode))
  (add-to-list &amp;#39;auto-mode-alist &amp;#39;(&amp;quot;\\.Rnw&amp;quot; . poly-noweb+r-mode))
  (add-to-list &amp;#39;auto-mode-alist &amp;#39;(&amp;quot;\\.Rmd&amp;quot; . poly-markdown+r-mode))

  ;; (require &amp;#39;poly-R)
  ;; (require &amp;#39;poly-markdown)
  ;; (add-to-list &amp;#39;auto-mode-alist &amp;#39;(&amp;quot;\\.Rmd&amp;quot; . poly-markdown+r-mode))

  (global-company-mode t)
  (global-hl-line-mode 1) ; Enable/Disable current line highlight
  (setq-default fill-column 99)
  (setq-default auto-fill-mode t)
  ;; ESS shortcuts
  (spacemacs/set-leader-keys &amp;quot;mdt&amp;quot; &amp;#39;ess-r-devtools-test-package)
  (spacemacs/set-leader-keys &amp;quot;mrl&amp;quot; &amp;#39;ess-eval-line)
  (spacemacs/set-leader-keys &amp;quot;mrr&amp;quot; &amp;#39;ess-eval-region)
  (spacemacs/set-leader-keys &amp;quot;mdb&amp;quot; &amp;#39;ess-r-devtools-build-package)
  (spacemacs/set-leader-keys &amp;quot;mdd&amp;quot; &amp;#39;ess-r-devtools-document-package)
  (spacemacs/set-leader-keys &amp;quot;mdl&amp;quot; &amp;#39;ess-r-devtools-load-package)
  (spacemacs/set-leader-keys &amp;quot;mdc&amp;quot; &amp;#39;ess-r-devtools-check-package)
  (spacemacs/set-leader-keys &amp;quot;mdp&amp;quot; &amp;#39;ess-r-package-mode)
  (add-hook &amp;#39;ess-mode-hook
            (lambda ()
              (ess-toggle-underscore nil)))
  (define-key evil-normal-state-map (kbd &amp;quot;SPC mm&amp;quot;)
            (lambda ()
              (interactive)
              (insert &amp;quot; %&amp;gt;% &amp;quot;)
              (evil-insert-state)
              ))
  ;; Move lines around
  (spacemacs/set-leader-keys &amp;quot;MS&amp;quot; &amp;#39;move-text-line-up)
  (spacemacs/set-leader-keys &amp;quot;MT&amp;quot; &amp;#39;move-text-line-down)
  (setq-default whitespace-mode t)
  (setq-default whitespace-style (quote (spaces tabs newline space-mark tab-mark newline-mark)))
  (setq-default whitespace-display-mappings
        ;; all numbers are Unicode codepoint in decimal. try (insert-char 182 ) to see it
        &amp;#39;(
          (space-mark 32 [183] [46]) ; 32 SPACE, 183 MIDDLE DOT 「·」, 46 FULL STOP 「.」
          (newline-mark 10 [9226 10]) ; 10 LINE FEED
          (tab-mark 9 [9655 9] [92 9]) ; 9 TAB, 9655 WHITE RIGHT-POINTING TRIANGLE 「▷」
          ))
  (setq-default TeX-view-program-selection
         &amp;#39;((output-pdf &amp;quot;PDF Viewer&amp;quot;)))
  (setq-default TeX-view-program-list
        &amp;#39;((&amp;quot;PDF Viewer&amp;quot; &amp;quot;okular %o&amp;quot;)))
  (setq-default indent-tabs-mode nil)
  (setq-default tab-width 2)
   ;; (setq org-default-notes-file (concat org-directory &amp;quot;/agenda/notes.org&amp;quot;))
   (add-hook &amp;#39;prog-mode-hook &amp;#39;spacemacs/toggle-fill-column-indicator-on)
   (add-hook &amp;#39;text-mode-hook &amp;#39;spacemacs/toggle-fill-column-indicator-on)
   (add-hook &amp;#39;markdown-mode-hook &amp;#39;spacemacs/toggle-fill-column-indicator-on)
  )&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>For posterity: install {xml2} on GNU/Linux distros</title>
      <link>/blog/2019-05-18-xml2/</link>
      <pubDate>Sat, 18 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-05-18-xml2/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Death_mask&#34;&gt;
&lt;img src=&#34;/img/napoleon_death_mask.jpg&#34; title = &#34;I will probably be the only reader of this blog post&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Today I’ve removed my system’s R package and installed MRO instead. While re-installing all packages,
I’ve encountered one of the most frustrating error message for someone installing packages from
source:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error : /tmp/Rtmpw60aCp/R.INSTALL7819efef27e/xml2/man/read_xml.Rd:47: unable to load shared object
&amp;#39;/usr/lib64/R/library/xml2/libs/xml2.so&amp;#39;: 
libicui18n.so.58: cannot open shared object file: No such file or directory ERROR: 
installing Rd objects failed for package ‘xml2’ &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This library, &lt;code&gt;libicui18n.so.58&lt;/code&gt; is a pain in the butt. However, you can easily install it if you
install miniconda. After installing miniconda, you can look for it with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[19-05-18 18:26] cbrunos in ~/ ➤ locate libicui18n.so.58

/home/cbrunos/miniconda3/lib/libicui18n.so.58
/home/cbrunos/miniconda3/lib/libicui18n.so.58.2
/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58
/home/cbrunos/miniconda3/pkgs/icu-58.2-h9c2bf20_1/lib/libicui18n.so.58.2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now you need to tell R where to look for this library. The
&lt;a href=&#34;https://stackoverflow.com/a/47851648&#34;&gt;following Stackoverflow&lt;/a&gt; answer saved the day. Add the following
lines to &lt;code&gt;R_HOME/etc/ldpaths&lt;/code&gt; (in my case, it was in &lt;code&gt;/opt/microsoft/ropen/3.5.2/lib64/R/etc/&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/miniconda3/lib/
export LD_LIBRARY_PATH&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and try to install &lt;code&gt;xml2&lt;/code&gt; again, and it should work! If not, just abandon the idea of using R and
switch to doing data science with VBA, it’ll be less frustrating.&lt;/p&gt;
&lt;p&gt;Something else, if you install Microsoft R Open, you’ll be stuck with some older packages, because
by default MRO uses a snapshot of CRAN from a given day as a mirror. To get the freshest packages,
add the following line to your &lt;code&gt;.Rprofile&lt;/code&gt; file (which should be located in your &lt;code&gt;HOME&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;options(repos = c(CRAN = &amp;quot;http://cran.rstudio.com/&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to finish this short blog post, add the following line to your &lt;code&gt;.Rprofile&lt;/code&gt;
if you get the following error messages when trying to install a package from github:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;remotes::install_github(&amp;#39;rstudio/DT&amp;#39;) Downloading GitHub repo rstudio/DT@master tar: 
This does not look like a tar archive gzip: stdin: unexpected end of file tar: Child returned 
status 1 tar: Error is not recoverable: exiting now tar: This does not look like a tar archive 
gzip: stdin: unexpected end of file tar: Child returned status 1 tar: Error is not recoverable: 
exiting now Error in getrootdir(untar(src, list = TRUE)) : length(file_list) &amp;gt; 0 is not TRUE Calls: 
&amp;lt;Anonymous&amp;gt; ... source_pkg -&amp;gt; decompress -&amp;gt; getrootdir -&amp;gt; stopifnot In addition: Warning messages: 1: 
In utils::untar(tarfile, ...) : ‘tar -xf &amp;#39;/tmp/RtmpitCFRe/file2677442609b8.tar.gz&amp;#39; -C 
&amp;#39;/tmp/RtmpitCFRe/remotes267752f2629f&amp;#39;’ returned error code 2 2: 
In system(cmd, intern = TRUE) : running command &amp;#39;tar -tf &amp;#39;/tmp/RtmpitCFRe/file2677442609b8.tar.gz&amp;#39;&amp;#39; 
had status 2 Execution halted&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The solution, which can found &lt;a href=&#34;https://github.com/r-lib/remotes/issues/350#issuecomment-493649792&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;options(&amp;quot;download.file.method&amp;quot; = &amp;quot;libcurl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fast food, causality and R packages, part 2</title>
      <link>/blog/2019-05-04-diffindiff_part2/</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-05-04-diffindiff_part2/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Joke&#34;&gt;
&lt;img src=&#34;/img/distracted_economist.jpg&#34; title = &#34;Soon, humanity will only communicate in memes&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I am currently working on a package for the R programming language; its initial goal was to simply
distribute the data used in the Card and Krueger 1994 paper that you can read
&lt;a href=&#34;http://davidcard.berkeley.edu/papers/njmin-aer.pdf&#34;&gt;here&lt;/a&gt; (PDF warning). However, I decided that I
would add code to perform diff-in-diff.&lt;/p&gt;
&lt;p&gt;In my &lt;a href=&#34;https://www.brodrigues.co/blog/2019-04-28-diffindiff_part1/&#34;&gt;previous blog post&lt;/a&gt; I showed
how to set up the structure of your new package. In this blog post, I will only focus on getting
Card and Krueger’s data and prepare it for distribution. The next blog posts will focus on writing
a function to perform difference-in-differences.&lt;/p&gt;
&lt;p&gt;If you want to distribute data through a package, you first need to use the &lt;code&gt;usethis::use_data_raw()&lt;/code&gt;
function (as shown in part 1).&lt;/p&gt;
&lt;p&gt;This creates a &lt;code&gt;data-raw&lt;/code&gt; folder, and inside you will find the &lt;code&gt;DATASET.R&lt;/code&gt; script. You can edit this
script to prepare the data.&lt;/p&gt;
&lt;p&gt;First, let’s download the data from Card’s website, unzip it and load the data into R. All these
operations will be performed from R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

tempfile_path &amp;lt;- tempfile()

download.file(&amp;quot;http://davidcard.berkeley.edu/data_sets/njmin.zip&amp;quot;, destfile = tempfile_path)

tempdir_path &amp;lt;- tempdir()

unzip(tempfile_path, exdir = tempdir_path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download and unzip a file from R, first, you need to define where you want to save the file. Because
I am not interested in keeping the downloaded file, I use the &lt;code&gt;tempfile()&lt;/code&gt; function to get a temporary
file in my &lt;code&gt;/tmp/&lt;/code&gt; folder (which is the folder that contains temporary files and folders in a GNU+Linux
system). Then, using &lt;code&gt;download.file()&lt;/code&gt; I download the file, and save it in my temporary file. I then
create a temporary directory using &lt;code&gt;tempdir()&lt;/code&gt; (the idea is the same as with &lt;code&gt;tempfile()&lt;/code&gt;), and use
this folder to save the files that I will unzip, using the &lt;code&gt;unzip()&lt;/code&gt; function. This folder now contains
several files:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;check.sas
codebook
public.csv
read.me
survey1.nj
survey2.nj&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;check.sas&lt;/code&gt; is the SAS script Card and Krueger used. It’s interesting, because it is quite simple,
quite short (170 lines long) and yet the impact of Card and Krueger’s research was and has been
very important for the field of econometrics. This script will help me define my own functions.
&lt;code&gt;codebook&lt;/code&gt;, you guessed it, contains the variables’ descriptions. I will use this to name the columns
of the data and to write the dataset’s documentation.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;public.csv&lt;/code&gt; is the data. It does not contain any column names:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 46 1 0 0 0 0 0 1 0 0  0 30.00 15.00  3.00   .    19.0   .   1    .  2  6.50 16.50  1.03  1.03  0.52  3  3 1 1 111792  1  3.50 35.00  3.00  4.30  26.0  0.08 1 2  6.50 16.50  1.03   .    0.94  4  4    
 49 2 0 0 0 0 0 1 0 0  0  6.50  6.50  4.00   .    26.0   .   0    .  2 10.00 13.00  1.01  0.90  2.35  4  3 1 1 111292  .  0.00 15.00  4.00  4.45  13.0  0.05 0 2 10.00 13.00  1.01  0.89  2.35  4  4    
506 2 1 0 0 0 0 1 0 0  0  3.00  7.00  2.00   .    13.0  0.37 0  30.0 2 11.00 10.00  0.95  0.74  2.33  3  3 1 1 111292  .  3.00  7.00  4.00  5.00  19.0  0.25 . 1 11.00 11.00  0.95  0.74  2.33  4  3    
 56 4 1 0 0 0 0 1 0 0  0 20.00 20.00  4.00  5.00  26.0  0.10 1   0.0 2 10.00 12.00  0.87  0.82  1.79  2  2 1 1 111492  .  0.00 36.00  2.00  5.25  26.0  0.15 0 2 10.00 12.00  0.92  0.79  0.87  2  2    
 61 4 1 0 0 0 0 1 0 0  0  6.00 26.00  5.00  5.50  52.0  0.15 1   0.0 3 10.00 12.00  0.87  0.77  1.65  2  2 1 1 111492  . 28.00  3.00  6.00  4.75  13.0  0.15 0 2 10.00 12.00  1.01  0.84  0.95  2  2    
 62 4 1 0 0 0 0 1 0 0  2  0.00 31.00  5.00  5.00  26.0  0.07 0  45.0 2 10.00 12.00  0.87  0.77  0.95  2  2 1 1 111492  .   .     .     .     .    26.0   .   0 2 10.00 12.00   .    0.84  1.79  3  3    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Missing data is defined by &lt;code&gt;.&lt;/code&gt; and the delimiter is the space character. &lt;code&gt;read.me&lt;/code&gt; is a README file.
Finally, &lt;code&gt;survey1.nj&lt;/code&gt; and &lt;code&gt;survey2.nj&lt;/code&gt; are the surveys that were administered to the fast food
restaurants’ managers; one in February (before the raise) and the second one in November
(after the minimum wage raise).&lt;/p&gt;
&lt;p&gt;The next lines import the codebook:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codebook &amp;lt;- read_lines(file = paste0(tempdir_path, &amp;quot;/codebook&amp;quot;))

variable_names &amp;lt;- codebook %&amp;gt;%
    `[`(8:59) %&amp;gt;%
    `[`(-c(5, 6, 13, 14, 32, 33)) %&amp;gt;%
    str_sub(1, 13) %&amp;gt;%
    str_squish() %&amp;gt;%
    str_to_lower()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once I import the codebook, I select lines 8 to 59 using the &lt;code&gt;`[`()&lt;/code&gt; function.
If you’re not familiar with this notation, try the following in a console:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 100)[1:10]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and compare:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 100) %&amp;gt;% 
  `[`(., 1:10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;both are equivalent, as you can see. You can also try the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 + 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 %&amp;gt;% 
  `+`(., 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the same trick, I remove lines that I do not need, and then using &lt;code&gt;stringr::str_sub(1, 13)&lt;/code&gt;
I only keep the first 13 characters (which are the variable names, plus some white space characters)
and then, to remove all the unneeded white space characters I use &lt;code&gt;stringr::squish()&lt;/code&gt;, and then
change the column names to lowercase.&lt;/p&gt;
&lt;p&gt;I then load the data, and add the column names that I extracted before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- read_table2(paste0(tempdir_path, &amp;quot;/public.dat&amp;quot;),
                      col_names = FALSE)

dataset &amp;lt;- dataset %&amp;gt;%
    select(-X47) %&amp;gt;%
    `colnames&amp;lt;-`(., variable_names) %&amp;gt;%
    mutate_all(as.numeric) %&amp;gt;%
    mutate(sheet = as.character(sheet))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I use the same trick as before. I rename the 47th column, which is empty,
I name the columns with &lt;code&gt;`colnames&amp;lt;-`()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After this, I perform some data cleaning. It’s mostly renaming categories of categorical variables,
and creating a “true” panel format. Several variables were measured at several points in time. Variables
that were measured a second time have a “2” at the end of their name. I remove these variables,
and add an observation data variable. So my data as twice as many rows as the original data, but
that format makes it way easier to work with. Below you can read the full code:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset &amp;lt;- dataset %&amp;gt;%
    mutate(chain = case_when(chain == 1 ~ &amp;quot;bk&amp;quot;,
                             chain == 2 ~ &amp;quot;kfc&amp;quot;,
                             chain == 3 ~ &amp;quot;roys&amp;quot;,
                             chain == 4 ~ &amp;quot;wendys&amp;quot;)) %&amp;gt;%
    mutate(state = case_when(state == 1 ~ &amp;quot;New Jersey&amp;quot;,
                             state == 0 ~ &amp;quot;Pennsylvania&amp;quot;)) %&amp;gt;%
    mutate(region = case_when(southj == 1 ~ &amp;quot;southj&amp;quot;,
              centralj == 1 ~ &amp;quot;centralj&amp;quot;,
              northj == 1 ~ &amp;quot;northj&amp;quot;,
              shore == 1 ~ &amp;quot;shorej&amp;quot;,
              pa1 == 1 ~ &amp;quot;pa1&amp;quot;,
              pa2 == 1 ~ &amp;quot;pa2&amp;quot;)) %&amp;gt;%
    mutate(meals = case_when(meals == 0 ~ &amp;quot;None&amp;quot;,
                             meals == 1 ~ &amp;quot;Free meals&amp;quot;,
                             meals == 2 ~ &amp;quot;Reduced price meals&amp;quot;,
                             meals == 3 ~ &amp;quot;Both free and reduced price meals&amp;quot;)) %&amp;gt;%
    mutate(meals2 = case_when(meals2 == 0 ~ &amp;quot;None&amp;quot;,
                             meals2 == 1 ~ &amp;quot;Free meals&amp;quot;,
                             meals2 == 2 ~ &amp;quot;Reduced price meals&amp;quot;,
                             meals2 == 3 ~ &amp;quot;Both free and reduced price meals&amp;quot;)) %&amp;gt;%
    mutate(status2 = case_when(status2 == 0 ~ &amp;quot;Refused 2nd interview&amp;quot;,
                               status2 == 1 ~ &amp;quot;Answered 2nd interview&amp;quot;,
                               status2 == 2 ~ &amp;quot;Closed for renovations&amp;quot;,
                               status2 == 3 ~ &amp;quot;Closed permanently&amp;quot;,
                               status2 == 4 ~ &amp;quot;Closed for highway construction&amp;quot;,
                               status2 == 5 ~ &amp;quot;Closed due to Mall fire&amp;quot;)) %&amp;gt;%
    mutate(co_owned = if_else(co_owned == 1, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)) %&amp;gt;%
    mutate(bonus = if_else(bonus == 1, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)) %&amp;gt;%
    mutate(special2 = if_else(special2 == 1, &amp;quot;Yes&amp;quot;, &amp;quot;No&amp;quot;)) %&amp;gt;%
    mutate(type2 = if_else(type2 == 1, &amp;quot;Phone&amp;quot;, &amp;quot;Personal&amp;quot;)) %&amp;gt;%
    select(sheet, chain, co_owned, state, region, everything()) %&amp;gt;%
    select(-southj, -centralj, -northj, -shore, -pa1, -pa2) %&amp;gt;%
    mutate(date2 = lubridate::mdy(date2)) %&amp;gt;%
    rename(open2 = open2r) %&amp;gt;%
    rename(firstinc2 = firstin2)

dataset1 &amp;lt;- dataset %&amp;gt;%
    select(-ends_with(&amp;quot;2&amp;quot;), -sheet, -chain, -co_owned, -state, -region, -bonus) %&amp;gt;%
    mutate(type = NA_character_,
           status = NA_character_,
           date = NA)

dataset2 &amp;lt;- dataset %&amp;gt;%
    select(ends_with(&amp;quot;2&amp;quot;)) %&amp;gt;%
    #mutate(bonus = NA_character_) %&amp;gt;%
    rename_all(~str_remove(., &amp;quot;2&amp;quot;))

other_cols &amp;lt;- dataset %&amp;gt;%
    select(sheet, chain, co_owned, state, region, bonus)

other_cols_1 &amp;lt;- other_cols %&amp;gt;%
    mutate(observation = &amp;quot;February 1992&amp;quot;)

other_cols_2 &amp;lt;- other_cols %&amp;gt;%
    mutate(observation = &amp;quot;November 1992&amp;quot;)

dataset1 &amp;lt;- bind_cols(other_cols_1, dataset1)
dataset2 &amp;lt;- bind_cols(other_cols_2, dataset2)

njmin &amp;lt;- bind_rows(dataset1, dataset2) %&amp;gt;%
    select(sheet, chain, state, region, observation, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;The line I would like to comment is the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset %&amp;gt;%
    select(-ends_with(&amp;quot;2&amp;quot;), -sheet, -chain, -co_owned, -state, -region, -bonus)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This select removes every column that ends with the character “2” (among others). I split the data
in two, to then bind the rows together and thus create my long dataset. I then save the data
into the &lt;code&gt;data/&lt;/code&gt; folder:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;usethis::use_data(njmin, overwrite = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This saves the data as an &lt;code&gt;.rda&lt;/code&gt; file. To enable users to read the data by typing &lt;code&gt;data(&#34;njmin&#34;)&lt;/code&gt;,
you need to create a &lt;code&gt;data.R&lt;/code&gt; script in the &lt;code&gt;R/&lt;/code&gt; folder. You can read my &lt;code&gt;data.R&lt;/code&gt; script below:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#&amp;#39; Data from the Card and Krueger 1994 paper *Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania*
#&amp;#39;
#&amp;#39; This dataset was downloaded and distributed with the permission of David Card. The original
#&amp;#39; data contains 410 observations and 46 variables. The data distributed in this package is
#&amp;#39; exactly the same, but was changed from a wide to a long dataset, which is better suited for
#&amp;#39; manipulation with *tidyverse* functions.
#&amp;#39;
#&amp;#39; @format A data frame with 820 rows and 28 variables:
#&amp;#39; \describe{
#&amp;#39;   \item{\code{sheet}}{Sheet number (unique store id).}
#&amp;#39;   \item{\code{chain}}{The fastfood chain: bk is Burger King, kfc is Kentucky Fried Chicken, wendys is Wendy&amp;#39;s, roys is Roy Rogers.}
#&amp;#39;   \item{\code{state}}{State where the restaurant is located.}
#&amp;#39;   \item{\code{region}}{pa1 is northeast suburbs of Phila, pa2 is Easton etc, centralj is central NJ, northj is northern NJ, southj is south NJ.}
#&amp;#39;   \item{\code{observation}}{Date of first (February 1992) and second (November 1992) observation.}
#&amp;#39;   \item{\code{co_owned}}{&amp;quot;Yes&amp;quot; if company owned.}
#&amp;#39;   \item{\code{ncalls}}{Number of call-backs. Is 0 if contacted on first call.}
#&amp;#39;   \item{\code{empft}}{Number full-time employees.}
#&amp;#39;   \item{\code{emppt}}{Number part-time employees.}
#&amp;#39;   \item{\code{nmgrs}}{Number of managers/assistant managers.}
#&amp;#39;   \item{\code{wage_st}}{Starting wage ($/hr).}
#&amp;#39;   \item{\code{inctime}}{Months to usual first raise.}
#&amp;#39;   \item{\code{firstinc}}{Usual amount of first raise (\$/hr).}
#&amp;#39;   \item{\code{bonus}}{&amp;quot;Yes&amp;quot; if cash bounty for new workers.}
#&amp;#39;   \item{\code{pctaff}}{\% of employees affected by new minimum.}
#&amp;#39;   \item{\code{meals}}{Free/reduced priced code.}
#&amp;#39;   \item{\code{open}}{Hour of opening.}
#&amp;#39;   \item{\code{hrsopen}}{Number of hours open per day.}
#&amp;#39;   \item{\code{psode}}{Price of medium soda, including tax.}
#&amp;#39;   \item{\code{pfry}}{Price of small fries, including tax.}
#&amp;#39;   \item{\code{pentree}}{Price of entree, including tax.}
#&amp;#39;   \item{\code{nregs}}{Number of cash registers in store.}
#&amp;#39;   \item{\code{nregs11}}{Number of registers open at 11:00 pm.}
#&amp;#39;   \item{\code{type}}{Type of 2nd interview.}
#&amp;#39;   \item{\code{status}}{Status of 2nd interview.}
#&amp;#39;   \item{\code{date}}{Date of 2nd interview.}
#&amp;#39;   \item{\code{nregs11}}{&amp;quot;Yes&amp;quot; if special program for new workers.}
#&amp;#39; }
#&amp;#39; @source \url{http://davidcard.berkeley.edu/data_sets.html}
&amp;quot;njmin&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;I have documented the data, and using &lt;code&gt;roxygen2::royxgenise()&lt;/code&gt; to create the dataset’s documentation.&lt;/p&gt;
&lt;p&gt;The data can now be used to create some nifty plots:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(njmin, aes(wage_st)) + geom_density(aes(fill = state), alpha = 0.3) +
    facet_wrap(vars(observation)) + theme_blog() +
    theme(legend.title = element_blank(), plot.caption = element_text(colour = &amp;quot;white&amp;quot;)) +
    labs(title = &amp;quot;Distribution of starting wage rates in fast food restaurants&amp;quot;,
         caption = &amp;quot;On April 1st, 1992, New Jersey&amp;#39;s minimum wage rose from $4.25 to $5.05. Source: Card and Krueger (1994)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 41 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-05-04-diffindiff_part2_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the next blog post, I am going to write a first function to perform diff and diff, and we will
learn how to make it available to users, document and test it!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fast food, causality and R packages, part 1</title>
      <link>/blog/2019-04-28-diffindiff_part1/</link>
      <pubDate>Sun, 28 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-04-28-diffindiff_part1/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Joke&#34;&gt;
&lt;img src=&#34;/img/distracted_economist.jpg&#34; title = &#34;Soon, humanity will only communicate in memes&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I am currently working on a package for the R programming language; its initial goal was to simply
distribute the data used in the Card and Krueger 1994 paper that you can read
&lt;a href=&#34;http://davidcard.berkeley.edu/papers/njmin-aer.pdf&#34;&gt;here&lt;/a&gt; (PDF warning).&lt;/p&gt;
&lt;p&gt;The gist of the paper is to try to answer the following question: &lt;em&gt;Do increases in minimum wages reduce employment?&lt;/em&gt;
According to Card and Krueger’s paper from 1994, no.
The authors studied a change in legislation in New Jersey which increased the minimum wage from $4.25 an hour to
$5.05 an hour. The neighbourghing state of Pennsylvania did not introduce such an increase. The authors thus used
the State of Pennsylvania as a control for the State of New Jersey and studied how the increase in minimum wage impacted
the employment in fast food restaurants and found, against what economic theory predicted, an
increase and not a decrease in employment.
The authors used a method called difference-in-differences to asses the impact of the minimum wage increase.&lt;/p&gt;
&lt;p&gt;This result was and still is controversial, with subsequent studies finding subtler results.
For instance, showing that there is a reduction in employment following an increase in minimum wage,
but only for large restaurants (see Ropponen and Olli, 2011).&lt;/p&gt;
&lt;p&gt;Anyways, this blog post will discuss how to create a package using to distribute the data. In a future
blog post, I will discuss preparing the data to make it available as a demo dataset inside the
package, and then writing and documenting functions.&lt;/p&gt;
&lt;p&gt;The first step to create a package, is to create a new project:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/package_01.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Select “New Directory”:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/package_02.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Then “R package”:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/package_03.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;and on the window that appears, you can choose the name of the package, as well as already some
starting source files:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/package_04.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Also, I’d highly recommend you click on the “Create a git repository” box and use git within your
project for reproducibility and sharing your code more easily. If you do not know git, there’s a lot of
online resources to get you started. It’s not super difficult, but it does require making some new
habits, which can take some time.&lt;/p&gt;
&lt;p&gt;I called my package &lt;code&gt;{diffindiff}&lt;/code&gt;, and clicked on “Create Project”. This opens up a new project
with a &lt;code&gt;hello.R&lt;/code&gt; script, which gives you some pointers:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Hello, world!
#
# This is an example function named &amp;#39;hello&amp;#39; 
# which prints &amp;#39;Hello, world!&amp;#39;.
#
# You can learn more about package authoring with RStudio at:
#
#   http://r-pkgs.had.co.nz/
#
# Some useful keyboard shortcuts for package authoring:
#
#   Install Package:           &amp;#39;Ctrl + Shift + B&amp;#39;
#   Check Package:             &amp;#39;Ctrl + Shift + E&amp;#39;
#   Test Package:              &amp;#39;Ctrl + Shift + T&amp;#39;

hello &amp;lt;- function() {
  print(&amp;quot;Hello, world!&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to simplify the creation of your package, I highly recommend you use the &lt;code&gt;{usethis}&lt;/code&gt; package.
&lt;code&gt;{usethis}&lt;/code&gt; removes a lot of the pain involved in creating packages.&lt;/p&gt;
&lt;p&gt;For instance, want to start by adding a README file? Simply run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;usethis::use_readme_md()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;✔ Setting active project to &amp;#39;/path/to/your/package/diffindiff&amp;#39;
✔ Writing &amp;#39;README.md&amp;#39;
● Modify &amp;#39;README.md&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a &lt;code&gt;README.md&lt;/code&gt; file in the root directory of your package. Simply change that file, and that’s it.&lt;/p&gt;
&lt;p&gt;The next step could be setting up your package to work with &lt;code&gt;{roxygen2}&lt;/code&gt;, which is very useful for
writing documentation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;usethis::use_roxygen_md()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;✔ Setting Roxygen field in DESCRIPTION to &amp;#39;list(markdown = TRUE)&amp;#39;
✔ Setting RoxygenNote field in DESCRIPTION to &amp;#39;6.1.1&amp;#39;
● Run `devtools::document()`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See how the output tells you to run &lt;code&gt;devtools::document()&lt;/code&gt;? This function will document your package,
transforming the comments you write to describe your functions to documentation and managing the NAMESPACE
file. Let’s run this function too:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::document()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Updating diffindiff documentation
First time using roxygen2. Upgrading automatically...
Loading diffindiff
Warning: The existing &amp;#39;NAMESPACE&amp;#39; file was not generated by roxygen2, and will not be overwritten.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might have a similar message than me, telling you that the NAMESPACE file was not generated by
&lt;code&gt;{roxygen2}&lt;/code&gt;, and will thus not be overwritten. Simply remove the file and run &lt;code&gt;devtools::document()&lt;/code&gt;
again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::document()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Updating diffindiff documentation
First time using roxygen2. Upgrading automatically...
Writing NAMESPACE
Loading diffindiff&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But what is actually the NAMESPACE file? This file is quite important, as it details where your
package’s functions have to look for in order to use other functions. This means that if your package needs function
&lt;code&gt;foo()&lt;/code&gt; from package &lt;code&gt;{bar}&lt;/code&gt;, it will consistently look for &lt;code&gt;foo()&lt;/code&gt; inside &lt;code&gt;{bar}&lt;/code&gt; and not confuse
it with, say, the &lt;code&gt;foo()&lt;/code&gt; function from the &lt;code&gt;{barley}&lt;/code&gt; package, even if you load &lt;code&gt;{barley}&lt;/code&gt; after
&lt;code&gt;{bar}&lt;/code&gt; in your interactive session. This can seem confusing now, but in the next blog posts I will
detail this, and you will see that it’s not that difficult. Just know that it is an important file,
and that you do not have to edit it by hand.&lt;/p&gt;
&lt;p&gt;Next, I like to run the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;usethis::use_pipe()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;✔ Adding &amp;#39;magrittr&amp;#39; to Imports field in DESCRIPTION
✔ Writing &amp;#39;R/utils-pipe.R&amp;#39;
● Run `devtools::document()`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This makes the now famous &lt;code&gt;%&amp;gt;%&lt;/code&gt; function available internally to your package (so you can use it
to write the functions that will be included in your package) but also available to the users that
will load the package.&lt;/p&gt;
&lt;p&gt;Your package is still missing a license. If you plan on writing a package for your own personal use,
for instance, a collection of functions, there is no need to think about licenses. But if you’re making
your package available through CRAN, then you definitely need to think about it. For this package,
I’ll be using the MIT license, because the package will distribute data which I do not own (I’ve got permission
from Card to re-distribute it) and thus I think it would be better to use a permissive license (I don’t know
if the GPL, another license, which is stricter in terms of redistribution, could be used in this case).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;usethis::use_mit_license()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;✔ Setting License field in DESCRIPTION to &amp;#39;MIT + file LICENSE&amp;#39;
✔ Writing &amp;#39;LICENSE.md&amp;#39;
✔ Adding &amp;#39;^LICENSE\\.md$&amp;#39; to &amp;#39;.Rbuildignore&amp;#39;
✔ Writing &amp;#39;LICENSE&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re almost done setting up the structure of the package. If we forget something though, it’s not an issue,
we’ll just have to run the right &lt;code&gt;use_*&lt;/code&gt; function later on. Let’s finish by preparing the folder
that will contains the script to prepare the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;usethis::use_data_raw()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;✔ Creating &amp;#39;data-raw/&amp;#39;
✔ Adding &amp;#39;^data-raw$&amp;#39; to &amp;#39;.Rbuildignore&amp;#39;
✔ Writing &amp;#39;data-raw/DATASET.R&amp;#39;
● Modify &amp;#39;data-raw/DATASET.R&amp;#39;
● Finish the data preparation script in &amp;#39;data-raw/DATASET.R&amp;#39;
● Use `usethis::use_data()` to add prepared data to package&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates the &lt;code&gt;data-raw&lt;/code&gt; folder with the &lt;code&gt;DATASET.R&lt;/code&gt; script inside. This is the script that will
contain the code to download and prepare datasets that you want to include in your package. This will
be the subject of the next blog post.&lt;/p&gt;
&lt;p&gt;Let’s now finish by documenting the package, and pushing everything to Github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::document()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following lines will only work if you set up the Github repo:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git add .
git commit -am &amp;quot;first commit&amp;quot;
git push origin master&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Historical newspaper scraping with {tesseract} and R</title>
      <link>/blog/2019-04-07-historical_newspaper_scraping_tesseract/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-04-07-historical_newspaper_scraping_tesseract/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cliometrics&#34;&gt;
&lt;img src=&#34;/img/clio.jpg&#34; title = &#34;Historical newspapers as a source to practice cliometrics?&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I have been playing around with historical newspapers data for some months now. The “obvious” type of analysis
to do is NLP, but there is also a lot of numerical data inside historical newspapers.
For instance, you can find these tables that show the market prices of the day in the &lt;em&gt;L’Indépendance Luxembourgeoise&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/market_price_table.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;I wanted to see how easy it was to extract these tables from the newspapers and then make it available.
It was a bit more complicated than anticipated.&lt;/p&gt;
&lt;div id=&#34;download-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Download data&lt;/h2&gt;
&lt;p&gt;The first step is to download the data. For this, I have used the code &lt;a href=&#34;https://twitter.com/yvesmaurer&#34;&gt;&lt;code&gt;@yvesmaurer&lt;/code&gt;&lt;/a&gt; which you
can find &lt;a href=&#34;https://github.com/ymaurer/eluxemburgensia-opendata-ark&#34;&gt;here&lt;/a&gt;. This code makes it easy to download individual
pages of certain newspapers,
for instance &lt;a href=&#34;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F1/full/full/0/default.jpg&#34;&gt;this one&lt;/a&gt;. The
pages I am interested in are pages 3, which contain the tables I need, for example
&lt;a href=&#34;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg&#34;&gt;here&lt;/a&gt;.
&lt;a href=&#34;https://twitter.com/yvesmaurer&#34;&gt;&lt;code&gt;@yvesmaurer&lt;/code&gt;&lt;/a&gt;’s code makes it easy to find the download links, which look like
this: &lt;code&gt;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F1drtkj%2Fpages%2F3/full/full/0/default.jpg&lt;/code&gt;. It is also possible to
crop the image by changing some parameters &lt;a href=&#34;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwsvhwh%2Fpages%2F3/pct:74,0,100,100/full/0/default.jpg&#34;&gt;like so&lt;/a&gt;.
This is helpful, because it makes the image smaller. The tables I’m interested in are always in the last column, so I can can use
this feature to get smaller images. However, not every issue contains these tables, and I only want to download the ones
that have these tables. So I wrote the following code to download the images I’m interested in:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(magick)
library(tesseract)
library(furrr)

download_image &amp;lt;- function(link){

    print(link)

    isok &amp;lt;- image_read(link) %&amp;gt;%
        ocr(engine = &amp;quot;fra&amp;quot;) %&amp;gt;%
        str_to_lower() %&amp;gt;%
        str_detect(&amp;quot;marché de luxembourg&amp;quot;)

    if(isok){
        date_link &amp;lt;- link %&amp;gt;%
            str_replace(&amp;quot;pages%2f3&amp;quot;, &amp;quot;pages%2f1&amp;quot;) %&amp;gt;%
            str_replace(&amp;quot;pct:74,0,100,100&amp;quot;, &amp;quot;pct:76,1,17,5&amp;quot;)

        paper_date &amp;lt;- image_read(date_link) %&amp;gt;%
            ocr(engine = &amp;quot;fra&amp;quot;) %&amp;gt;%
            str_squish() %&amp;gt;%
            str_remove(&amp;quot;%&amp;quot;) %&amp;gt;%
            str_remove(&amp;quot;&amp;amp;&amp;quot;) %&amp;gt;%
            str_remove(&amp;quot;/&amp;quot;)

        ark &amp;lt;- link %&amp;gt;%
            str_sub(53, 60)

        download.file(link, paste0(&amp;quot;indep_pages/&amp;quot;, ark, &amp;quot;-&amp;quot;, paper_date, &amp;quot;.jpg&amp;quot;))
    } else {
        NULL
        }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code only downloads
an image if the &lt;code&gt;ocr()&lt;/code&gt; from the {tesseract} (which does, you guessed it, OCR) detects the string “marché de luxembourg” which
is the title of the tables. This is a bit extreme, because if a single letter cannot be correctly detected by the OCR, the page will not
be downloaded. But I figured that if this string could not be easily recognized, this would be a canary telling me that the text
inside the table would also not be easily recognized. So it might be extreme, but my hope was that it would make detecting
the table itself easier. Turned out it wasn’t so easy, but more on this later.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-images&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparing images&lt;/h2&gt;
&lt;p&gt;Now that I have the images, I will prepare them to make character recognition easier. To do this, I’m using the &lt;code&gt;{magick}&lt;/code&gt;
package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(magick)
library(tesseract)
library(furrr)

prepare_image &amp;lt;- function(image_path){
    image &amp;lt;- image_read(image_path)

    image &amp;lt;- image %&amp;gt;%
        image_modulate(brightness = 150) %&amp;gt;%
        image_convolve(&amp;#39;DoG:0,0,2&amp;#39;, scaling = &amp;#39;1000, 100%&amp;#39;) %&amp;gt;%
        image_despeckle(times = 10)

    image_write(image, paste0(getwd(), &amp;quot;/edited/&amp;quot;, str_remove(image_path, &amp;quot;.jpg&amp;quot;), &amp;quot;edited.jpg&amp;quot;))
}


image_paths &amp;lt;- dir(path = &amp;quot;indep_pages&amp;quot;, pattern = &amp;quot;*.jpg&amp;quot;, full.names = TRUE)

plan(multiprocess, workers = 8)

image_paths %&amp;gt;%
    future_map(prepare_image)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The picture below shows the result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/table_and_edit.jpg&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Now comes the complicated part, which is going from the image above, to the dataset below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;good_fr,good_en,unit,market_date,price,source_url
Froment,Wheat,hectolitre,1875-08-28,23,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Métail,Meslin,hectolitre,1875-08-28,21,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Seigle,Rye,hectolitre,1875-08-28,15,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge,Barley,hectolitre,1875-08-28,16,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge mondé,Pot Barley,kilogram,1875-08-28,0.85,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Orge perlé,Pearl barley,kilogram,1875-08-28,0.8,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Avoine,Oats,hectolitre,1875-08-28,8.5,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg
Pois,Peas,hectolitre,1875-08-28,NA,https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F02grxj%2Fpages%2F1/full/full/0/default.jpg&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ocr-with-tesseract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;OCR with {tesseract}&lt;/h2&gt;
&lt;p&gt;The first step was to get the date. For this, I have used the following function, which will then
be used inside another function, which will extract the data and prices.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(magick)
library(tesseract)
library(furrr)
library(janitor)

is_empty_line &amp;lt;- function(line){
    ifelse(line == &amp;quot;&amp;quot;, TRUE, FALSE)
}

Sys.setlocale(&amp;#39;LC_TIME&amp;#39;, &amp;quot;fr_FR&amp;quot;)

get_date &amp;lt;- function(string, annee){

    liste_mois &amp;lt;- c(&amp;quot;janvier&amp;quot;, &amp;quot;février&amp;quot;, &amp;quot;mars&amp;quot;, &amp;quot;avril&amp;quot;, &amp;quot;mai&amp;quot;, &amp;quot;juin&amp;quot;, &amp;quot;juillet&amp;quot;,
                    &amp;quot;août&amp;quot;, &amp;quot;septembre&amp;quot;, &amp;quot;octobre&amp;quot;, &amp;quot;novembre&amp;quot;, &amp;quot;décembre&amp;quot;)

    raw_date &amp;lt;- string %&amp;gt;%
      str_to_lower() %&amp;gt;%
        str_remove_all(&amp;quot;\\.&amp;quot;) %&amp;gt;%
        str_extract(&amp;quot;\\d{1,2} .{3,9}(\\s+)?\\d{0,4}&amp;quot;) %&amp;gt;%
        str_split(&amp;quot;\\s+&amp;quot;, simplify = TRUE)

    if(ncol(raw_date) == 2){
        raw_date &amp;lt;- cbind(raw_date, &amp;quot;annee&amp;quot;)
    }

    raw_date[1, 3] &amp;lt;- annee

    raw_date &amp;lt;- str_to_lower(raw_date[1:1, 1:3])

    long_month &amp;lt;- case_when(
      raw_date[2] == &amp;quot;janv&amp;quot; ~ &amp;quot;janvier&amp;quot;,
      raw_date[2] == &amp;quot;févr&amp;quot; ~ &amp;quot;février&amp;quot;,
      raw_date[2] == &amp;quot;sept&amp;quot; ~ &amp;quot;septembre&amp;quot;,
      raw_date[2] == &amp;quot;oct&amp;quot; ~ &amp;quot;octobre&amp;quot;,
      raw_date[2] == &amp;quot;nov&amp;quot; ~ &amp;quot;novembre&amp;quot;,
      raw_date[2] == &amp;quot;dec&amp;quot; ~ &amp;quot;décembre&amp;quot;,
      TRUE ~ as.character(raw_date[2]))

    raw_date[2] &amp;lt;- long_month

    is_it_date &amp;lt;- as.Date(paste0(raw_date, collapse = &amp;quot;-&amp;quot;), format = &amp;quot;%d-%b-%Y&amp;quot;) %&amp;gt;%
        is.na() %&amp;gt;% `!`()

    if(is_it_date){
        return(as.Date(paste0(raw_date, collapse = &amp;quot;-&amp;quot;), format = &amp;quot;%d-%b-%Y&amp;quot;))
    } else {
        if(!(raw_date[2] %in% liste_mois)){
            raw_date[2] &amp;lt;- liste_mois[stringdist::amatch(raw_date[2], liste_mois, maxDist = 2)]
            return(as.Date(paste0(raw_date, collapse = &amp;quot;-&amp;quot;), format = &amp;quot;%d-%b-%Y&amp;quot;))
        }
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is more complicated than I had hoped. This is because dates come in different formats.
For example, there are dates written like this “21 Janvier 1872”, or “12 Septembre” or “12 sept.”.
The biggest problem here is that sometimes the year is missing. I deal with this in the next
function, which is again, more complicated than what I had hoped. I won’t go into details and
explain every step of the function above, but the idea is to extract the data from the raw text,
replace abbreviated months with the full month name if needed, and then check if I get a valid date.
If not, I try my luck with &lt;code&gt;stringdist::amatch()&lt;/code&gt;, to try to match, say “jonvier” with “janvier”.
This is in case the OCR made a mistake. I am not very happy with this solution, because it is very
approximative, but oh well.&lt;/p&gt;
&lt;p&gt;The second step is to get the data. I noticed that the rows stay consistent, but do change
after June 1st 1876. So I simply hardcoded the goods names, and was only concerned with extracting
the prices. I also apply some manual corrections inside the function; mainly dates that were
wrongly recognized by the OCR engine, and which were causing problems. Again, not an optimal solution,
the other alternative was to simply drop this data, which I did not want to do. Here is the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_table &amp;lt;- function(image_path){

  image &amp;lt;- image_read(image_path)

  annee &amp;lt;- image_path %&amp;gt;%
    str_extract(&amp;quot;187\\d&amp;quot;)

  ark &amp;lt;- image_path %&amp;gt;%
    str_sub(22, 27)

  source_url &amp;lt;- str_glue(&amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F{ark}%2Fpages%2F1/full/full/0/default.jpg&amp;quot;,
                         ark = ark)

  text &amp;lt;- ocr(image, engine = &amp;quot;fra&amp;quot;)

    text &amp;lt;- text %&amp;gt;%
      str_split(&amp;quot;\n&amp;quot;) %&amp;gt;%
      unlist %&amp;gt;%
      str_squish() %&amp;gt;%
      str_remove_all(&amp;quot;^.{1,10}$&amp;quot;) %&amp;gt;%
      discard(is_empty_line) %&amp;gt;%
      str_replace(&amp;quot;Mercuriale du \\+ Nov. 1831.&amp;quot;, &amp;quot;Mercuriale du 4 Nov. 1831.&amp;quot;) %&amp;gt;%
      str_replace(&amp;quot;….u .T juillet.&amp;quot;, &amp;quot;du 7 juillet&amp;quot;) %&amp;gt;%
      str_replace(&amp;quot;octobré&amp;quot;, &amp;quot;octobre&amp;quot;) %&amp;gt;%
      str_replace(&amp;quot;AT octobre&amp;quot;, &amp;quot;17 octobre&amp;quot;) %&amp;gt;% # correction for &amp;quot;f8g6kq8-18  LUNDI 19 OCTOBRÉ 1874. BUREAUX de fa RÉDACTIGedited.jpg&amp;quot;
      str_replace(&amp;quot;T norembre&amp;quot;, &amp;quot;7 novembre&amp;quot;) %&amp;gt;%  # correction for fcrhrn5-LE 8  LUNDI 9 NOVEMBRE 1874 BUREAUX de la RÉDedited.jpg
      str_replace(&amp;quot;À oc demain 5&amp;quot;, &amp;quot;27 mai&amp;quot;) %&amp;gt;% # correction for fd61vzp-MARDI 50. MAI 1876 BUREAUX de la. RED, n VE DE L’ADMINISTRAedited.jpg
      str_replace(&amp;quot;G&amp;quot;, &amp;quot;6&amp;quot;) %&amp;gt;%
      str_replace(&amp;quot;Hercariale du 80 nov. 1872,&amp;quot;, &amp;quot;du 30 novembre 1872&amp;quot;) %&amp;gt;%
      str_replace(&amp;quot;….u .T juillet.&amp;quot;, &amp;quot;du 7 juillet&amp;quot;) %&amp;gt;%
      str_replace(&amp;quot;Rs ne its du 28-octobré.: :!: :&amp;quot;, &amp;quot;28 octobre&amp;quot;) %&amp;gt;%
      str_replace(&amp;quot;De routes due 98-juilléle. à eat&amp;quot;, &amp;quot;28 juillet&amp;quot;) %&amp;gt;%
      str_replace(&amp;quot;\\| Mereariale dn 14 dre. 1872,&amp;quot;, &amp;quot;14 décembre 1872&amp;quot;)


  start &amp;lt;- text %&amp;gt;%
    str_which(&amp;quot;MARCH(É|E).*D(E|É).*LUXEMBOUR(G|6)&amp;quot;) + 2

  start &amp;lt;- ifelse(is_empty(start), str_which(text, &amp;quot;.*D.*UXEM.*&amp;quot;) + 2, start)

  end &amp;lt;- start + 40

  pricing_date &amp;lt;- text[start - 1] %&amp;gt;%
    str_remove(&amp;quot;%&amp;quot;) %&amp;gt;%
    str_remove(&amp;quot;er&amp;quot;) %&amp;gt;%
    str_remove(&amp;quot;\\.+&amp;quot;) %&amp;gt;%
    str_remove(&amp;quot;\\*&amp;quot;) %&amp;gt;%
    str_remove(&amp;quot;®&amp;quot;) %&amp;gt;%
    str_remove(&amp;quot;:&amp;quot;) %&amp;gt;%
    str_remove(&amp;quot;\\?&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;\\$&amp;quot;, &amp;quot;9&amp;quot;) %&amp;gt;%
    str_remove(&amp;quot;°&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;‘du 14août.. - ; En&amp;quot;, &amp;quot;14 août&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;OP PE CN AP PP&amp;quot;, &amp;quot;du 28 juin&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;‘ du 81 janvi Le&amp;quot;, &amp;quot;31 janvier&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;\\| \\| du AT août&amp;quot;, &amp;quot;17 août&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;Su”  du 81 juillet. L&amp;quot;, &amp;quot;31 juillet&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;0 du 29 avril \&amp;quot; \\|&amp;quot;, &amp;quot;29 avril&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;LU 0 du 28 ail&amp;quot;, &amp;quot;28 avril&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;Rs ne its du 28-octobre :!: :&amp;quot;, &amp;quot;23 octobre&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;7 F \\|  du 13 octobre LA LOTS&amp;quot;, &amp;quot;13 octobre&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;À. du 18 juin UT ET&amp;quot;, &amp;quot;13 juin&amp;quot;)


  market_date &amp;lt;- get_date(pricing_date, annee)

  items &amp;lt;- c(&amp;quot;Froment&amp;quot;, &amp;quot;Métail&amp;quot;, &amp;quot;Seigle&amp;quot;, &amp;quot;Orge&amp;quot;, &amp;quot;Orge mondé&amp;quot;, &amp;quot;Orge perlé&amp;quot;, &amp;quot;Avoine&amp;quot;, &amp;quot;Pois&amp;quot;, &amp;quot;Haricots&amp;quot;,
             &amp;quot;Lentilles&amp;quot;, &amp;quot;Pommes de terre&amp;quot;, &amp;quot;Bois de hêtre&amp;quot;, &amp;quot;Bois de chêne&amp;quot;, &amp;quot;Beurre&amp;quot;, &amp;quot;Oeufs&amp;quot;, &amp;quot;Foin&amp;quot;,
             &amp;quot;Paille&amp;quot;, &amp;quot;Viande de boeuf&amp;quot;, &amp;quot;Viande de vache&amp;quot;, &amp;quot;Viande de veau&amp;quot;, &amp;quot;Viande de mouton&amp;quot;,
             &amp;quot;Viande fraîche de cochon&amp;quot;, &amp;quot;Viande fumée de cochon&amp;quot;, &amp;quot;Haricots&amp;quot;, &amp;quot;Pois&amp;quot;, &amp;quot;Lentilles&amp;quot;,
             &amp;quot;Farines de froment&amp;quot;, &amp;quot;Farines de méteil&amp;quot;, &amp;quot;Farines de seigle&amp;quot;)

  items_en &amp;lt;- c(&amp;quot;Wheat&amp;quot;, &amp;quot;Meslin&amp;quot;, &amp;quot;Rye&amp;quot;, &amp;quot;Barley&amp;quot;, &amp;quot;Pot Barley&amp;quot;, &amp;quot;Pearl barley&amp;quot;, &amp;quot;Oats&amp;quot;, &amp;quot;Peas&amp;quot;, &amp;quot;Beans&amp;quot;,
    &amp;quot;Lentils&amp;quot;, &amp;quot;Potatoes&amp;quot;, &amp;quot;Beech wood&amp;quot;, &amp;quot;Oak wood&amp;quot;, &amp;quot;Butter&amp;quot;, &amp;quot;Eggs&amp;quot;, &amp;quot;Hay&amp;quot;, &amp;quot;Straw&amp;quot;, &amp;quot;Beef meat&amp;quot;,
    &amp;quot;Cow meat&amp;quot;, &amp;quot;Veal meat&amp;quot;, &amp;quot;Sheep meat&amp;quot;, &amp;quot;Fresh pig meat&amp;quot;, &amp;quot;Smoked pig meat&amp;quot;, &amp;quot;Beans&amp;quot;, &amp;quot;Peas&amp;quot;,
    &amp;quot;Lentils&amp;quot;, &amp;quot;Wheat flours&amp;quot;, &amp;quot;Meslin flours&amp;quot;, &amp;quot;Rye flours&amp;quot;)


  unit &amp;lt;- c(&amp;quot;hectolitre&amp;quot;, &amp;quot;hectolitre&amp;quot;, &amp;quot;hectolitre&amp;quot;, &amp;quot;hectolitre&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;hectolitre&amp;quot;,
            &amp;quot;hectolitre&amp;quot;, &amp;quot;hectolitre&amp;quot;, &amp;quot;hectolitre&amp;quot;, &amp;quot;hectolitre&amp;quot;, &amp;quot;stere&amp;quot;, &amp;quot;stere&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;dozen&amp;quot;,
            &amp;quot;500 kilogram&amp;quot;, &amp;quot;500 kilogram&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;kilogram&amp;quot;,
            &amp;quot;kilogram&amp;quot;, &amp;quot;litre&amp;quot;, &amp;quot;litre&amp;quot;, &amp;quot;litre&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;kilogram&amp;quot;, &amp;quot;kilogram&amp;quot;)

  # starting with june 1876, the order of the items changes
  items_06_1876 &amp;lt;- c(&amp;quot;Froment&amp;quot;, &amp;quot;Métail&amp;quot;, &amp;quot;Seigle&amp;quot;, &amp;quot;Orge&amp;quot;, &amp;quot;Avoine&amp;quot;, &amp;quot;Pois&amp;quot;, &amp;quot;Haricots&amp;quot;, &amp;quot;Lentilles&amp;quot;,
                     &amp;quot;Pommes de terre&amp;quot;, &amp;quot;Farines de froment&amp;quot;, &amp;quot;Farines de méteil&amp;quot;, &amp;quot;Farines de seigle&amp;quot;, &amp;quot;Orge mondé&amp;quot;,
                     &amp;quot;Beurre&amp;quot;, &amp;quot;Oeufs&amp;quot;, &amp;quot;Foins&amp;quot;, &amp;quot;Paille&amp;quot;, &amp;quot;Bois de hêtre&amp;quot;, &amp;quot;Bois de chêne&amp;quot;, &amp;quot;Viande de boeuf&amp;quot;, &amp;quot;Viande de vache&amp;quot;,
                     &amp;quot;Viande de veau&amp;quot;, &amp;quot;Viande de mouton&amp;quot;, &amp;quot;Viande fraîche de cochon&amp;quot;, &amp;quot;Viande fumée de cochon&amp;quot;)

  items_06_1876_en &amp;lt;- c(&amp;quot;Wheat&amp;quot;, &amp;quot;Meslin&amp;quot;, &amp;quot;Rye&amp;quot;, &amp;quot;Barley&amp;quot;, &amp;quot;Oats&amp;quot;, &amp;quot;Peas&amp;quot;, &amp;quot;Beans&amp;quot;, &amp;quot;Lentils&amp;quot;,
                        &amp;quot;Potatoes&amp;quot;, &amp;quot;Wheat flours&amp;quot;, &amp;quot;Meslin flours&amp;quot;, &amp;quot;Rye flours&amp;quot;, &amp;quot;Pot barley&amp;quot;,
                        &amp;quot;Butter&amp;quot;, &amp;quot;Eggs&amp;quot;, &amp;quot;Hay&amp;quot;, &amp;quot;Straw&amp;quot;, &amp;quot;Beechwood&amp;quot;, &amp;quot;Oakwood&amp;quot;, &amp;quot;Beef meat&amp;quot;, &amp;quot;Cow meat&amp;quot;,
                        &amp;quot;Veal meat&amp;quot;, &amp;quot;Sheep meat&amp;quot;, &amp;quot;Fresh pig meat&amp;quot;, &amp;quot;Smoked pig meat&amp;quot;)

  units_06_1876 &amp;lt;- c(rep(&amp;quot;hectolitre&amp;quot;, 9), rep(&amp;quot;kilogram&amp;quot;, 5), &amp;quot;douzaine&amp;quot;, rep(&amp;quot;500 kilogram&amp;quot;, 2),
                     &amp;quot;stere&amp;quot;, &amp;quot;stere&amp;quot;, rep(&amp;quot;kilogram&amp;quot;, 6))

  raw_data &amp;lt;- text[start:end]

  prices &amp;lt;- raw_data %&amp;gt;%
    str_replace_all(&amp;quot;©&amp;quot;, &amp;quot;0&amp;quot;) %&amp;gt;%
    str_extract(&amp;quot;\\d{1,2}\\s\\d{2}&amp;quot;) %&amp;gt;%
    str_replace(&amp;quot;\\s&amp;quot;, &amp;quot;\\.&amp;quot;) %&amp;gt;%
    as.numeric

  if(is.na(prices[1])){
    prices &amp;lt;- tail(prices, -1)
  } else {
    prices &amp;lt;- prices
  }

  if(market_date &amp;lt; as.Date(&amp;quot;01-06-1876&amp;quot;, format = &amp;quot;%d-%m-%Y&amp;quot;)){
    prices &amp;lt;- prices[1:length(items)]
    tibble(&amp;quot;good_fr&amp;quot; = items, &amp;quot;good_en&amp;quot; = items_en, &amp;quot;unit&amp;quot; = unit, &amp;quot;market_date&amp;quot; = market_date,
           &amp;quot;price&amp;quot; = prices, &amp;quot;source_url&amp;quot; = source_url)
  } else {
    prices &amp;lt;- prices[1:length(items_06_1876_en)]
    tibble(&amp;quot;good_fr&amp;quot; = items_06_1876, &amp;quot;good_en&amp;quot; = items_06_1876_en, &amp;quot;unit&amp;quot; = units_06_1876,
           &amp;quot;market_date&amp;quot; = market_date, &amp;quot;price&amp;quot; = prices, &amp;quot;source_url&amp;quot; = source_url)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I wrote previously, I had to deal with the missing year in the date inside this function. To do
that, I extracted the year from the name of the file, and pasted it then into the date. The file
name contains the data because the function in the function that downloads the files I also performed
OCR on the first page, to get the date of the newspaper issue. The sole purpose of this was to
get the year. Again, the function is more complex than what I hoped, but it did work well overall.
There are still mistakes in the data, for example sometimes the prices are in the wrong order;
meaning that they’re “shifted”, for example instead of the prices for eggs, I have the prices of the
good that comes next. So obviously be careful if you decide to analyze the data, and double-check
if something seems weird. I have made the data available on Luxembourg Open Data Portal,
&lt;a href=&#34;https://data.public.lu/fr/datasets/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/#resource-community-27293c42-22e5-4811-aee8-89d6f7fa9533&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analyzing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analyzing the data&lt;/h2&gt;
&lt;p&gt;And now, to the fun part. I want to know what was the price of smoked pig meat, and how it varied
through time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ggplot2)
library(brotools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price &amp;lt;- read_csv(&amp;quot;https://download.data.public.lu/resources/digitised-luxembourg-historical-newspapers-journaux-historiques-luxembourgeois-numerises/20190407-183605/market-price.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   good_fr = col_character(),
##   good_en = col_character(),
##   unit = col_character(),
##   market_date = col_date(format = &amp;quot;&amp;quot;),
##   price = col_double(),
##   source_url = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;%
    filter(good_en == &amp;quot;Smoked pig meat&amp;quot;) %&amp;gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog() + 
    labs(title = &amp;quot;Prices of smoked pig meat at the Luxembourg-City market in the 19th century&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, there is a huge spike somewhere in 1874. Maybe there was a very severe smoked pig
meat shortage that caused the prices to increase dramatically, but the more likely explanation is
that there was some sort of mistake, either in the OCR step, or when I extracted the prices, and somehow
that particular price of smoked pig meat is actually the price of another, more expensive good.&lt;/p&gt;
&lt;p&gt;So let’s only consider prices that are below, say, 20 franks, which is already very high:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;%
    filter(good_en == &amp;quot;Smoked pig meat&amp;quot;) %&amp;gt;%
    filter(price &amp;lt; 20) %&amp;gt;% 
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog() + 
    labs(title = &amp;quot;Prices of smoked pig meat at the Luxembourg-City market in the 1870s&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, some prices are very high. Let’s check if it’s a mistake:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;% 
    filter(good_en == &amp;quot;Smoked pig meat&amp;quot;) %&amp;gt;% 
    filter(between(price, 5, 20)) %&amp;gt;% 
    pull(source_url)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fbs2fs6%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [2] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fd61vzp%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [3] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fjdwb6m%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [4] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fng14m3%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [5] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fw9jdrb%2Fpages%2F1/full/full/0/default.jpg&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you go to the first url, you will land on the first page of the newspaper. To check the table,
you need to check the third page, by changing this part of the url “pages%2F1” to this “pages%2F3”.&lt;/p&gt;
&lt;p&gt;You will then find the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/price_smoked_pig.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the price was 2.5, but the OCR returned 7.5. This is a problem that is unavoidable
with OCR; there is no way of knowing a priori if characters were not well recognized. It is actually
quite interesting how the price for smoked pig meat stayed constant through all these years.
A density plot shows that most prices were around 2.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;% 
    filter(good_en == &amp;quot;Smoked pig meat&amp;quot;) %&amp;gt;% 
    filter(price &amp;lt; 20) %&amp;gt;% 
    ggplot() + 
    geom_density(aes(price), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What about another good, say, barley?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;%
    filter(good_en == &amp;quot;Barley&amp;quot;) %&amp;gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog() + 
    labs(title = &amp;quot;Prices of barley at the Luxembourg-City market in the 1870s&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here again, we see some very high spikes, most likely due to errors. Let’s try to limit the prices
to likely values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;%
    filter(good_en == &amp;quot;Barley&amp;quot;) %&amp;gt;%
    filter(between(price, 10, 40)) %&amp;gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog() + 
    labs(title = &amp;quot;Prices of barley at the Luxembourg-City market in the 1870s&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;% 
    filter(good_en == &amp;quot;Barley&amp;quot;) %&amp;gt;% 
    ggplot() + 
    geom_density(aes(price), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 39 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s finish this with one of my favourite legume, lentils:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;%
    filter(good_en == &amp;quot;Lentils&amp;quot;) %&amp;gt;%
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog() + 
    labs(title = &amp;quot;Prices of lentils at the Luxembourg-City market in the 1870s&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;% 
    filter(good_en == &amp;quot;Lentils&amp;quot;) %&amp;gt;% 
    ggplot() + 
    geom_density(aes(price), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 79 rows containing non-finite values (stat_density).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All these 0’s might be surprising, but in most cases, they are actually true zeros! For example,
you can check this
&lt;a href=&#34;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwsvhwh%2Fpages%2F3/pct:74,0,100,100/full/0/default.jpg&#34;&gt;issue&lt;/a&gt;.
This very likely means that no lentils were available that day at the market.
Let’s get rid of the 0s and other extreme values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;%
    filter(good_en == &amp;quot;Lentils&amp;quot;) %&amp;gt;%
    filter(between(price, 1, 40)) %&amp;gt;% 
    ggplot(aes(x = market_date, y = price)) +
    geom_line(aes(group = 1), colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog() + 
    labs(title = &amp;quot;Prices of lentils at the Luxembourg-City market in the 1870s&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-04-07-historical_newspaper_scraping_tesseract_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I would like to see if the spikes above 30 are errors or not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;% 
    filter(good_en == &amp;quot;Lentils&amp;quot;) %&amp;gt;% 
    filter(between(price, 30, 40)) %&amp;gt;% 
    pull(source_url)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [2] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fb8zp31%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [3] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fkzrj53%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [4] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fs8sw2v%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [5] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fsjptsk%2Fpages%2F1/full/full/0/default.jpg&amp;quot;
## [6] &amp;quot;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2Fwk65b6%2Fpages%2F1/full/full/0/default.jpg&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The price was recognized as being 35, and turns out it was correct as you can see
&lt;a href=&#34;https://iiif.eluxemburgensia.lu/iiif/2/ark:%2F70795%2F04mb5t%2Fpages%2F3/full/full/0/default.jpg&#34;&gt;here&lt;/a&gt;.
This is quite interesting, because the average price was way lower than that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_price %&amp;gt;%
    filter(good_en == &amp;quot;Lentils&amp;quot;) %&amp;gt;%
    filter(between(price, 1, 40)) %&amp;gt;% 
    summarise(mean_price = mean(price), 
              sd_price = sd(price))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   mean_price sd_price
##        &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1       20.8     5.82&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m going to finish here; it was an interesting project, and I can’t wait for more newspapers to be
digitized and OCR to work even better. There is a lot more historical data trapped in these newspapers
that could provide a lot insights on Luxembourg’s society in the 19th century.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Get text from pdfs or images using OCR: a tutorial with {tesseract} and {magick}</title>
      <link>/blog/2019-03-31-tesseract/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-03-31-tesseract/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Michel_Rodange&#34;&gt;
&lt;img src=&#34;/img/michelrodange.jpg&#34; title = &#34;The high school I attended was named after this gentleman&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this blog post I’m going to show you how you can extract text from scanned pdf files, or pdf files
where no text recognition was performed. (For pdfs where text recognition was performed, you can
read my &lt;a href=&#34;https://www.brodrigues.co/blog/2018-06-10-scraping_pdfs/&#34;&gt;other blog post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The pdf I’m going to use can be downloaded from &lt;a href=&#34;http://www.luxemburgensia.bnl.lu/cgi/getPdf1_2.pl?mode=item&amp;amp;id=7110&#34;&gt;here&lt;/a&gt;.
It’s a poem titled, &lt;em&gt;D’Léierchen (Dem Léiweckerche säi Lidd)&lt;/em&gt;,
written by Michel Rodange, arguably Luxembourg’s most well known writer and poet. Michel Rodange is
mostly known for his fable, &lt;em&gt;Renert oder De Fuuß am Frack an a Ma’nsgrëßt&lt;/em&gt;, starring a central European
&lt;a href=&#34;https://en.wikipedia.org/wiki/Reynard_the_Fox&#34;&gt;trickster anthropomorphic red fox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/d/d4/Reynard-the-fox.jpg&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Anyway, back to the point of this blog post. How can we get data from a pdf where no text recognition
was performed (or, how can we get text from an image)? The pdf we need the text from looks like
this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/dleierchen_03.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;To get the text from the pdf, we can use the &lt;code&gt;{tesseract}&lt;/code&gt; package, which provides bindings to the &lt;code&gt;tesseract&lt;/code&gt; program.
&lt;code&gt;tesseract&lt;/code&gt; is an open source OCR engine developed by Google. This means that first you will need
to install the &lt;code&gt;tesseract&lt;/code&gt; program on your system. You can follow the intructions from &lt;code&gt;tesseract&lt;/code&gt;’s
github &lt;a href=&#34;https://github.com/tesseract-ocr/tesseract&#34;&gt;page&lt;/a&gt;. &lt;code&gt;tesseract&lt;/code&gt; is currently at version 4.&lt;/p&gt;
&lt;p&gt;Before applying OCR to a pdf, let’s first use the &lt;code&gt;{pdftools}&lt;/code&gt; package to convert the pdf to png.
This is because &lt;code&gt;{tesseract}&lt;/code&gt; requires images as input (if you provide a pdf file, it will
converted on the fly). Let’s first load the needed packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tesseract)
library(pdftools)
library(magick)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now let’s convert the pdf to png files (in plural, because we’ll get one image per page of the pdf):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pngfile &amp;lt;- pdftools::pdf_convert(&amp;quot;path/to/pdf&amp;quot;, dpi = 600)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will generate 14 png files. I erase the ones that are not needed, such as the title page. Now,
let’s read in all the image files:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path &amp;lt;- dir(path = &amp;quot;path/to/pngs&amp;quot;, pattern = &amp;quot;*.png&amp;quot;, full.names = TRUE)

images &amp;lt;- map(path, magick::image_read)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;images&lt;/code&gt; object is a list of &lt;code&gt;magick-image&lt;/code&gt;s, which we can parse. BUUUUUT! There’s a problem.
The text is laid out in two columns. Which means that the first line after performing OCR will be
the first line of the first column, and the first line of the second column joined together. Same
for the other lines of course. So ideally, I’d need to split the file in the middle, and then
perform OCR. This is easily done with the &lt;code&gt;{magick}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first_half &amp;lt;- map(images, ~image_crop(., geometry = &amp;quot;2307x6462&amp;quot;))

second_half &amp;lt;- map(images, ~image_crop(., geometry = &amp;quot;2307x6462+2307+0&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because the pngs are 4614 by 6962 pixels, I can get the first half of the png by cropping at
“2307x6462” (I decrease the height a bit to get rid of the page number), and the second half by
applying the same logic, but starting the cropping at the “2307+0” position. The result looks like
this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/dleierchen_cropped.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Much better! Now I need to join these two lists together. I cannot simply join them. Consider
the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one &amp;lt;- list(1, 3, 5)

two &amp;lt;- list(2, 4, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the setup I currently have; &lt;code&gt;first_half&lt;/code&gt; contains odd pages, and &lt;code&gt;second_half&lt;/code&gt; contains
even pages. The result I want would look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list(1, 2, 3, 4, 5, 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 2
## 
## [[3]]
## [1] 3
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 5
## 
## [[6]]
## [1] 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a very elegant solution, with &lt;code&gt;reduce2()&lt;/code&gt; from the &lt;code&gt;{purrr}&lt;/code&gt; package. &lt;code&gt;reduce()&lt;/code&gt; takes one
list and a function, and … &lt;em&gt;reduces&lt;/em&gt; the list to a single element. For instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reduce(list(1, 2, 3), paste)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1 2 3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;reduce2()&lt;/code&gt; is very similar, but takes in two lists, but the second list must be one element shorter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reduce2(list(1, 2, 3), list(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;), paste)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1 2 a 3 b&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we cannot simply use &lt;code&gt;reduce2()&lt;/code&gt; on lists &lt;code&gt;one&lt;/code&gt; and &lt;code&gt;two&lt;/code&gt;, because they’re the same length. So let’s
prepend a value to &lt;code&gt;one&lt;/code&gt;, using the &lt;code&gt;prepend()&lt;/code&gt; function of &lt;code&gt;{purrr}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prepend(one, 0) %&amp;gt;% 
    reduce2(two, c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0 1 2 3 4 5 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Exactly what we need! Let’s apply this trick to our lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;merged_list &amp;lt;- prepend(first_half, NA) %&amp;gt;% 
    reduce2(second_half, c) %&amp;gt;% 
    discard(is.na)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ve prepended &lt;code&gt;NA&lt;/code&gt; to the first list, and then used &lt;code&gt;reduce2()&lt;/code&gt; and then used &lt;code&gt;discard(is.na)&lt;/code&gt; to
remove the &lt;code&gt;NA&lt;/code&gt; I’ve added at the start. Now, we can use OCR to get the text:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text_list &amp;lt;- map(merged_list, ocr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ocr()&lt;/code&gt; uses a model trained on English by default, and even though there is a model trained on
Luxembourguish, the one trained on English works better! Very likely because the English model was trained
on a lot more data than the Luxembourguish one. I was worried the English model was not going to
recognize characters such as &lt;code&gt;é&lt;/code&gt;, but no, it worked quite well.&lt;/p&gt;
&lt;p&gt;This is how it looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text_list

[[1]]
[1] &amp;quot;Lhe\n| Kaum huet d’Feld dat fréndlecht Feier\nVun der Aussentssonn gesunn\nAs mam Plou aus Stall a Scheier\n* D’lescht e Bauer ausgezunn.\nFir de Plou em nach ze dreiwen\nWar sai Jéngelchen alaert,\nDeen nét wéllt doheem méi bleiwen\n8 An esouz um viischte Paerd.\nOp der Schéllche stoung ze denken\nD’Léierche mam Hierz voll Lidder\nFir de Béifchen nach ze zanken\n12 Duckelt s’an de Som sech nidder.\nBis e laascht war, an du stémmt se\nUn e Liddchen, datt et kraacht\nOp der Nouteleder klémmt se\n16 Datt dem Béifchen d’Haerz alt laacht.\nAn du sot en: Papp, ech mengen\nBal de Vull dee kénnt och schwatzen.\nLauschter, sot de Papp zum Klengen,\n20 Ech kann d’Liddchen iwersetzen.\nI\nBas de do, mii léiwe Fréndchen\nMa de Wanter dee war laang!\nKuck, ech hat keng fréilech Sténnchen\n24 *T war fir dech a mech mer baang.\nAn du koum ech dech besichen\nWell du goungs nét méi eraus\nMann wat hues jo du eng Kichen\n28 Wat eng Scheier wat en Haus.\nWi zerguttster, a wat Saachen!\nAn déng Frache gouf mer Brout.\nAn déng Kanner, wi se laachen,\n32, An hir Backelcher, wi rout!\nJo, bei dir as Rot nét deier!\nJo a kuck mer wat eng Méscht.\nDat gét Saache fir an d’Scheier\n36 An och Sué fir an d’Késcht.\nMuerges waars de schuns um Dreschen\nIr der Daudes d’Schung sech stréckt\nBas am Do duurch Wis a Paschen\n40 Laascht all Waassergruef geschréckt.\n&amp;quot;
....
....&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still need to split at the &lt;code&gt;&#34;\n&#34;&lt;/code&gt; character:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text_list &amp;lt;- text_list %&amp;gt;% 
    map(., ~str_split(., &amp;quot;\n&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The end result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text_list

[[1]]
[[1]][[1]]
 [1] &amp;quot;Lhe&amp;quot;                                      &amp;quot;| Kaum huet d’Feld dat fréndlecht Feier&amp;quot; 
 [3] &amp;quot;Vun der Aussentssonn gesunn&amp;quot;              &amp;quot;As mam Plou aus Stall a Scheier&amp;quot;         
 [5] &amp;quot;* D’lescht e Bauer ausgezunn.&amp;quot;            &amp;quot;Fir de Plou em nach ze dreiwen&amp;quot;          
 [7] &amp;quot;War sai Jéngelchen alaert,&amp;quot;               &amp;quot;Deen nét wéllt doheem méi bleiwen&amp;quot;       
 [9] &amp;quot;8 An esouz um viischte Paerd.&amp;quot;            &amp;quot;Op der Schéllche stoung ze denken&amp;quot;       
[11] &amp;quot;D’Léierche mam Hierz voll Lidder&amp;quot;         &amp;quot;Fir de Béifchen nach ze zanken&amp;quot;          
[13] &amp;quot;12 Duckelt s’an de Som sech nidder.&amp;quot;      &amp;quot;Bis e laascht war, an du stémmt se&amp;quot;      
[15] &amp;quot;Un e Liddchen, datt et kraacht&amp;quot;           &amp;quot;Op der Nouteleder klémmt se&amp;quot;             
[17] &amp;quot;16 Datt dem Béifchen d’Haerz alt laacht.&amp;quot; &amp;quot;An du sot en: Papp, ech mengen&amp;quot;          
[19] &amp;quot;Bal de Vull dee kénnt och schwatzen.&amp;quot;     &amp;quot;Lauschter, sot de Papp zum Klengen,&amp;quot;     
[21] &amp;quot;20 Ech kann d’Liddchen iwersetzen.&amp;quot;       &amp;quot;I&amp;quot;                                       
[23] &amp;quot;Bas de do, mii léiwe Fréndchen&amp;quot;           &amp;quot;Ma de Wanter dee war laang!&amp;quot;             
[25] &amp;quot;Kuck, ech hat keng fréilech Sténnchen&amp;quot;    &amp;quot;24 *T war fir dech a mech mer baang.&amp;quot;    
[27] &amp;quot;An du koum ech dech besichen&amp;quot;             &amp;quot;Well du goungs nét méi eraus&amp;quot;            
[29] &amp;quot;Mann wat hues jo du eng Kichen&amp;quot;           &amp;quot;28 Wat eng Scheier wat en Haus.&amp;quot;         
[31] &amp;quot;Wi zerguttster, a wat Saachen!&amp;quot;           &amp;quot;An déng Frache gouf mer Brout.&amp;quot;          
[33] &amp;quot;An déng Kanner, wi se laachen,&amp;quot;           &amp;quot;32, An hir Backelcher, wi rout!&amp;quot;         
[35] &amp;quot;Jo, bei dir as Rot nét deier!&amp;quot;            &amp;quot;Jo a kuck mer wat eng Méscht.&amp;quot;           
[37] &amp;quot;Dat gét Saache fir an d’Scheier&amp;quot;          &amp;quot;36 An och Sué fir an d’Késcht.&amp;quot;          
[39] &amp;quot;Muerges waars de schuns um Dreschen&amp;quot;      &amp;quot;Ir der Daudes d’Schung sech stréckt&amp;quot;     
[41] &amp;quot;Bas am Do duurch Wis a Paschen&amp;quot;           &amp;quot;40 Laascht all Waassergruef geschréckt.&amp;quot; 
[43] &amp;quot;&amp;quot;  
...
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect! Some more cleaning would be needed though. For example, I need to remove the little
annotations that are included:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/dleierchen_anot.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;I don’t know yet how I’m going to do that.I also need to remove the line numbers at the beginning
of every fourth line, but this is easily done with a simple regular expression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str_remove_all(c(&amp;quot;12 bla&amp;quot;, &amp;quot;blb&amp;quot;, &amp;quot;123 blc&amp;quot;), &amp;quot;^\\d{1,}\\s+&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;bla&amp;quot; &amp;quot;blb&amp;quot; &amp;quot;blc&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this will be left for a future blog post!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pivoting data frames just got easier thanks to `pivot_wide()` and `pivot_long()`</title>
      <link>/blog/2019-03-20-pivot/</link>
      <pubDate>Wed, 20 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-03-20-pivot/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/R2u0sN9stbA?t=69&#34;&gt;
&lt;img src=&#34;/img/pivot.jpg&#34; title = &#34;You know where this leads&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There’s a lot going on in the development version of &lt;code&gt;{tidyr}&lt;/code&gt;. New functions for pivoting data
frames, &lt;code&gt;pivot_wide()&lt;/code&gt; and &lt;code&gt;pivot_long()&lt;/code&gt; are coming, and will replace the current functions,
&lt;code&gt;spread()&lt;/code&gt; and &lt;code&gt;gather()&lt;/code&gt;. &lt;code&gt;spread()&lt;/code&gt; and &lt;code&gt;gather()&lt;/code&gt; will remain in the package though:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;You may have heard a rumour that gather/spread are going away. This is simply not true (they’ll stay around forever) but I am working on better replacements which you can learn about at &lt;a href=&#34;https://t.co/sU2GzWeBaf&#34;&gt;https://t.co/sU2GzWeBaf&lt;/a&gt;. Now is a great time for feedback! &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt;&lt;/p&gt;&amp;mdash; Hadley Wickham (@hadleywickham) &lt;a href=&#34;https://twitter.com/hadleywickham/status/1108107722128613377?ref_src=twsrc%5Etfw&#34;&gt;March 19, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;If you want to try out these new functions, you need to install the development version of &lt;code&gt;{tidyr}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;tidyverse/tidyr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and you can read the vignette &lt;a href=&#34;https://tidyr.tidyverse.org/dev/articles/pivot.html#many-variables-in-column-names&#34;&gt;here&lt;/a&gt;.
Because these functions are still being developed, some more changes might be introduced, but I guess
that the main functionality will not change much.&lt;/p&gt;
&lt;p&gt;Let’s play around with these functions and the &lt;code&gt;mtcars&lt;/code&gt; data set. First let’s load the packages and
the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
data(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let’s create a wide dataset, by &lt;em&gt;spreading&lt;/em&gt; the levels of the “am” column to two new columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide1 &amp;lt;- mtcars %&amp;gt;% 
    pivot_wide(names_from = &amp;quot;am&amp;quot;, values_from = &amp;quot;mpg&amp;quot;) 

mtcars_wide1 %&amp;gt;% 
    select(`0`, `1`, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 11
##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4
##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4
##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1
##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2
##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1
##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4
##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2
##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2
## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;pivot_wide()&lt;/code&gt;’s arguments are quite explicit: &lt;code&gt;names_from =&lt;/code&gt; is where you specify the column that
will be spread across the data frame, meaning, the levels of this column will become new columns.
&lt;code&gt;values_from =&lt;/code&gt; is where you specify the column that will fill in the values of the new columns.&lt;/p&gt;
&lt;p&gt;“0” and “1” are the new columns (“am” had two levels, &lt;code&gt;0&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;), which contain the miles per
gallon for manual and automatic cars respectively. Let’s also take a look at the data frame itself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide1 %&amp;gt;% 
    select(`0`, `1`, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 11
##      `0`   `1`   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  NA    21       6  160    110  3.9   2.62  16.5     0     4     4
##  2  NA    21       6  160    110  3.9   2.88  17.0     0     4     4
##  3  NA    22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4  21.4  NA       6  258    110  3.08  3.22  19.4     1     3     1
##  5  18.7  NA       8  360    175  3.15  3.44  17.0     0     3     2
##  6  18.1  NA       6  225    105  2.76  3.46  20.2     1     3     1
##  7  14.3  NA       8  360    245  3.21  3.57  15.8     0     3     4
##  8  24.4  NA       4  147.    62  3.69  3.19  20       1     4     2
##  9  22.8  NA       4  141.    95  3.92  3.15  22.9     1     4     2
## 10  19.2  NA       6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now suppose that we want to spread the values of “am” times “cyl”, and filling the data with the
values of “mpg”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide2 &amp;lt;- mtcars %&amp;gt;% 
    pivot_wide(names_from = c(&amp;quot;am&amp;quot;, &amp;quot;cyl&amp;quot;), values_from = &amp;quot;mpg&amp;quot;) 

mtcars_wide2 %&amp;gt;% 
    select(matches(&amp;quot;^0|1&amp;quot;), everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 14
##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0
##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0
##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1
##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1
##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0
##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1
##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0
##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1
##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1
## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &amp;lt;dbl&amp;gt;, carb &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this is easily achieved by simply providing more columns to &lt;code&gt;names_from =&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, it is also possible to use an optional data set which contains the specifications of the
new columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_spec &amp;lt;- mtcars %&amp;gt;% 
    expand(am, cyl, .value = &amp;quot;mpg&amp;quot;) %&amp;gt;%
    unite(&amp;quot;.name&amp;quot;, am, cyl, remove = FALSE)

mtcars_spec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   .name    am   cyl .value
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; 
## 1 0_4       0     4 mpg   
## 2 0_6       0     6 mpg   
## 3 0_8       0     8 mpg   
## 4 1_4       1     4 mpg   
## 5 1_6       1     6 mpg   
## 6 1_8       1     8 mpg&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This optional data set defines how the columns “0_4”, “0_6” etc are constructed, and also the
value that shall be used to fill in the values. “am” and “cyl” will be used to create the “.name”
and the “mpg” column will be used for the “.value”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
    pivot_wide(spec = mtcars_spec) %&amp;gt;% 
    select(matches(&amp;quot;^0|1&amp;quot;), everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 14
##    `0_4` `0_6` `0_8` `1_4` `1_6` `1_8`  disp    hp  drat    wt  qsec    vs
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  NA    NA    NA    NA      21    NA  160    110  3.9   2.62  16.5     0
##  2  NA    NA    NA    NA      21    NA  160    110  3.9   2.88  17.0     0
##  3  NA    NA    NA    22.8    NA    NA  108     93  3.85  2.32  18.6     1
##  4  NA    21.4  NA    NA      NA    NA  258    110  3.08  3.22  19.4     1
##  5  NA    NA    18.7  NA      NA    NA  360    175  3.15  3.44  17.0     0
##  6  NA    18.1  NA    NA      NA    NA  225    105  2.76  3.46  20.2     1
##  7  NA    NA    14.3  NA      NA    NA  360    245  3.21  3.57  15.8     0
##  8  24.4  NA    NA    NA      NA    NA  147.    62  3.69  3.19  20       1
##  9  22.8  NA    NA    NA      NA    NA  141.    95  3.92  3.15  22.9     1
## 10  NA    19.2  NA    NA      NA    NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &amp;lt;dbl&amp;gt;, carb &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using a spec is especially useful if you need to make new levels that are not in the data.
For instance, suppose that there are actually 10-cylinder cars too, but they do not appear in our
sample. We would like to make the fact that they’re missing explicit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_spec2 &amp;lt;- mtcars %&amp;gt;% 
    expand(am, &amp;quot;cyl&amp;quot; = c(cyl, 10), .value = &amp;quot;mpg&amp;quot;) %&amp;gt;%
    unite(&amp;quot;.name&amp;quot;, am, cyl, remove = FALSE)

mtcars_spec2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 4
##   .name    am   cyl .value
##   &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; 
## 1 0_4       0     4 mpg   
## 2 0_6       0     6 mpg   
## 3 0_8       0     8 mpg   
## 4 0_10      0    10 mpg   
## 5 1_4       1     4 mpg   
## 6 1_6       1     6 mpg   
## 7 1_8       1     8 mpg   
## 8 1_10      1    10 mpg&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
    pivot_wide(spec = mtcars_spec2) %&amp;gt;% 
    select(matches(&amp;quot;^0|1&amp;quot;), everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 16
##    `0_4` `0_6` `0_8` `0_10` `1_4` `1_6` `1_8` `1_10`  disp    hp  drat
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 
##  2  NA    NA    NA       NA  NA      21    NA     NA  160    110  3.9 
##  3  NA    NA    NA       NA  22.8    NA    NA     NA  108     93  3.85
##  4  NA    21.4  NA       NA  NA      NA    NA     NA  258    110  3.08
##  5  NA    NA    18.7     NA  NA      NA    NA     NA  360    175  3.15
##  6  NA    18.1  NA       NA  NA      NA    NA     NA  225    105  2.76
##  7  NA    NA    14.3     NA  NA      NA    NA     NA  360    245  3.21
##  8  24.4  NA    NA       NA  NA      NA    NA     NA  147.    62  3.69
##  9  22.8  NA    NA       NA  NA      NA    NA     NA  141.    95  3.92
## 10  NA    19.2  NA       NA  NA      NA    NA     NA  168.   123  3.92
## # … with 22 more rows, and 5 more variables: wt &amp;lt;dbl&amp;gt;, qsec &amp;lt;dbl&amp;gt;,
## #   vs &amp;lt;dbl&amp;gt;, gear &amp;lt;dbl&amp;gt;, carb &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we now have two more columns have been added, and they are full of NA’s.&lt;/p&gt;
&lt;p&gt;Now, let’s try to go from wide to long data sets, using &lt;code&gt;pivot_long()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide1 %&amp;gt;% 
  pivot_long(cols = c(`1`, `0`), names_to = &amp;quot;am&amp;quot;, values_to = &amp;quot;mpg&amp;quot;) %&amp;gt;% 
  select(am, mpg, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 64 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4
##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4
##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4
##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4
##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1
##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1
##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1
##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1
##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2
## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The arguments of &lt;code&gt;pivot_long()&lt;/code&gt; are quite explicit too, and similar to the ones in &lt;code&gt;pivot_wide()&lt;/code&gt;.
&lt;code&gt;cols =&lt;/code&gt; is where the user specifies the columns that need to be pivoted. &lt;code&gt;names_to =&lt;/code&gt; is where
the user can specify the name of the new columns, whose levels will be exactly the ones specified
to &lt;code&gt;cols =&lt;/code&gt;. &lt;code&gt;values_to =&lt;/code&gt; is where the user specifies the column name of the new column that
will contain the values.&lt;/p&gt;
&lt;p&gt;It is also possible to specify the columns that should not be transformed, by using &lt;code&gt;-&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide1 %&amp;gt;% 
  pivot_long(cols = -matches(&amp;quot;^[[:alpha:]]&amp;quot;), names_to = &amp;quot;am&amp;quot;, values_to = &amp;quot;mpg&amp;quot;) %&amp;gt;% 
  select(am, mpg, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 64 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 1      21       6   160   110  3.9   2.62  16.5     0     4     4
##  2 0      NA       6   160   110  3.9   2.62  16.5     0     4     4
##  3 1      21       6   160   110  3.9   2.88  17.0     0     4     4
##  4 0      NA       6   160   110  3.9   2.88  17.0     0     4     4
##  5 1      22.8     4   108    93  3.85  2.32  18.6     1     4     1
##  6 0      NA       4   108    93  3.85  2.32  18.6     1     4     1
##  7 1      NA       6   258   110  3.08  3.22  19.4     1     3     1
##  8 0      21.4     6   258   110  3.08  3.22  19.4     1     3     1
##  9 1      NA       8   360   175  3.15  3.44  17.0     0     3     2
## 10 0      18.7     8   360   175  3.15  3.44  17.0     0     3     2
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here the columns that should not be modified are all those that start with a letter, hence the “&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;”
regular expression. It is also possible to remove all the &lt;code&gt;NA&lt;/code&gt;’s from the data frame, with &lt;code&gt;na.rm =&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide1 %&amp;gt;% 
  pivot_long(cols = c(`1`, `0`), names_to = &amp;quot;am&amp;quot;, values_to = &amp;quot;mpg&amp;quot;, na.rm = TRUE) %&amp;gt;% 
  select(am, mpg, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 11
##    am      mpg   cyl  disp    hp  drat    wt  qsec    vs  gear  carb
##    &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 1      21       6  160    110  3.9   2.62  16.5     0     4     4
##  2 1      21       6  160    110  3.9   2.88  17.0     0     4     4
##  3 1      22.8     4  108     93  3.85  2.32  18.6     1     4     1
##  4 0      21.4     6  258    110  3.08  3.22  19.4     1     3     1
##  5 0      18.7     8  360    175  3.15  3.44  17.0     0     3     2
##  6 0      18.1     6  225    105  2.76  3.46  20.2     1     3     1
##  7 0      14.3     8  360    245  3.21  3.57  15.8     0     3     4
##  8 0      24.4     4  147.    62  3.69  3.19  20       1     4     2
##  9 0      22.8     4  141.    95  3.92  3.15  22.9     1     4     2
## 10 0      19.2     6  168.   123  3.92  3.44  18.3     1     4     4
## # … with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also pivot data frames where the names of the columns are made of two or more variables,
for example in our &lt;code&gt;mtcars_wide2&lt;/code&gt; data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide2 %&amp;gt;% 
    select(matches(&amp;quot;^0|1&amp;quot;), everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 14
##    `1_6` `1_4` `0_6` `0_8` `0_4` `1_8`  disp    hp  drat    wt  qsec    vs
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1    21  NA    NA    NA    NA      NA  160    110  3.9   2.62  16.5     0
##  2    21  NA    NA    NA    NA      NA  160    110  3.9   2.88  17.0     0
##  3    NA  22.8  NA    NA    NA      NA  108     93  3.85  2.32  18.6     1
##  4    NA  NA    21.4  NA    NA      NA  258    110  3.08  3.22  19.4     1
##  5    NA  NA    NA    18.7  NA      NA  360    175  3.15  3.44  17.0     0
##  6    NA  NA    18.1  NA    NA      NA  225    105  2.76  3.46  20.2     1
##  7    NA  NA    NA    14.3  NA      NA  360    245  3.21  3.57  15.8     0
##  8    NA  NA    NA    NA    24.4    NA  147.    62  3.69  3.19  20       1
##  9    NA  NA    NA    NA    22.8    NA  141.    95  3.92  3.15  22.9     1
## 10    NA  NA    19.2  NA    NA      NA  168.   123  3.92  3.44  18.3     1
## # … with 22 more rows, and 2 more variables: gear &amp;lt;dbl&amp;gt;, carb &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All the columns that start with either “0” or “1” must be pivoted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide2 %&amp;gt;% 
  pivot_long(cols = matches(&amp;quot;0|1&amp;quot;), names_to = &amp;quot;am_cyl&amp;quot;, values_to = &amp;quot;mpg&amp;quot;, na.rm = TRUE) %&amp;gt;% 
  select(am_cyl, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 10
##    am_cyl  disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 1_6     160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1_6     160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1_4     108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0_6     258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0_8     360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0_6     225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0_8     360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0_4     147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0_4     141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0_6     168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, there is one new column, “am_cyl” which must still be transformed by separating “am_cyl” into two new columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide2 %&amp;gt;% 
  pivot_long(cols = matches(&amp;quot;0|1&amp;quot;), names_to = &amp;quot;am_cyl&amp;quot;, values_to = &amp;quot;mpg&amp;quot;, na.rm = TRUE) %&amp;gt;% 
  separate(am_cyl, into = c(&amp;quot;am&amp;quot;, &amp;quot;cyl&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;% 
  select(am, cyl, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 11
##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to achieve this using a data frame with the specification of what you need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_spec_long &amp;lt;- mtcars_wide2 %&amp;gt;% 
  pivot_long_spec(matches(&amp;quot;0|1&amp;quot;), values_to = &amp;quot;mpg&amp;quot;) %&amp;gt;% 
  separate(name, c(&amp;quot;am&amp;quot;, &amp;quot;cyl&amp;quot;), sep = &amp;quot;_&amp;quot;)

mtcars_spec_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##   .name .value am    cyl  
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
## 1 1_6   mpg    1     6    
## 2 1_4   mpg    1     4    
## 3 0_6   mpg    0     6    
## 4 0_8   mpg    0     8    
## 5 0_4   mpg    0     4    
## 6 1_8   mpg    1     8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Providing this spec to &lt;code&gt;pivot_long()&lt;/code&gt; solves the issue:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars_wide2 %&amp;gt;% 
  pivot_long(spec = mtcars_spec_long, na.rm = TRUE) %&amp;gt;% 
  select(am, cyl, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 32 x 11
##    am    cyl    disp    hp  drat    wt  qsec    vs  gear  carb   mpg
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 1     6      160    110  3.9   2.62  16.5     0     4     4  21  
##  2 1     6      160    110  3.9   2.88  17.0     0     4     4  21  
##  3 1     4      108     93  3.85  2.32  18.6     1     4     1  22.8
##  4 0     6      258    110  3.08  3.22  19.4     1     3     1  21.4
##  5 0     8      360    175  3.15  3.44  17.0     0     3     2  18.7
##  6 0     6      225    105  2.76  3.46  20.2     1     3     1  18.1
##  7 0     8      360    245  3.21  3.57  15.8     0     3     4  14.3
##  8 0     4      147.    62  3.69  3.19  20       1     4     2  24.4
##  9 0     4      141.    95  3.92  3.15  22.9     1     4     2  22.8
## 10 0     6      168.   123  3.92  3.44  18.3     1     4     4  19.2
## # … with 22 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Stay tuned to Hadley Wickham’s &lt;a href=&#34;https://twitter.com/hadleywickham&#34;&gt;twitter&lt;/a&gt; as there will definitely
be announcements soon!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;[:alpha:]&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 2</title>
      <link>/blog/2019-03-05-historical_vowpal_part2/</link>
      <pubDate>Tue, 05 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-03-05-historical_vowpal_part2/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/BilPXIt0R2w?t=41&#34;&gt;
&lt;img src=&#34;/img/wabbit_reading.jpg&#34; title = &#34;Vowpal Wabbit is fast as heck&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.brodrigues.co/blog/2019-03-03-historical_vowpal/&#34;&gt;part 1&lt;/a&gt; of this series I set
up Vowpal Wabbit to classify newspapers content. Now, let’s use the model to make predictions and
see how and if we can improve the model. Then, let’s train the model on the whole data.&lt;/p&gt;
&lt;div id=&#34;step-1-prepare-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: prepare the data&lt;/h2&gt;
&lt;p&gt;The first step consists in importing the test data and preparing it. The test data need not be large
and thus can be imported and worked on in R.&lt;/p&gt;
&lt;p&gt;I need to remove the target column from the test set, or else it will be used to make predictions.
If you do not remove this column the accuracy of the model will be very high, but it will be wrong
since, of course, you do not have the target column at running time… because it is the column
that you want to predict!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;yardstick&amp;quot;)

small_test &amp;lt;- read_delim(&amp;quot;data_split/small_test.txt&amp;quot;, &amp;quot;|&amp;quot;,
                      escape_double = FALSE, col_names = FALSE,
                      trim_ws = TRUE)

small_test %&amp;gt;%
    mutate(X1= &amp;quot; &amp;quot;) %&amp;gt;%
    write_delim(&amp;quot;data_split/small_test2.txt&amp;quot;, col_names = FALSE, delim = &amp;quot;|&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I wrote the data in a file called &lt;code&gt;small_test2.txt&lt;/code&gt; and can now use my model to make predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;/home/cbrunos/miniconda3/bin/vw&amp;quot;, args = &amp;quot;-t -i vw_models/small_oaa.model data_split/small_test2.txt -p data_split/small_oaa.predict&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The predictions get saved in the file &lt;code&gt;small_oaa.predict&lt;/code&gt;, which is a plain text file. Let’s add these
predictions to the original test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_predictions &amp;lt;- read_delim(&amp;quot;data_split/small_oaa.predict&amp;quot;, &amp;quot;|&amp;quot;,
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

small_test &amp;lt;- small_test %&amp;gt;%
    rename(truth = X1) %&amp;gt;%
    mutate(truth = factor(truth, levels = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;)))

small_predictions &amp;lt;- small_predictions %&amp;gt;%
    rename(predictions = X1) %&amp;gt;%
    mutate(predictions = factor(predictions, levels = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;)))

small_test &amp;lt;- small_test %&amp;gt;%
    bind_cols(small_predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-use-the-model-and-test-data-to-evaluate-performance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: use the model and test data to evaluate performance&lt;/h2&gt;
&lt;p&gt;We can use the several metrics included in &lt;code&gt;{yardstick}&lt;/code&gt; to evaluate the model’s performance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat(small_test, truth = truth, estimate = predictions)

accuracy(small_test, truth = truth, estimate = predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          Truth
Prediction  1  2  3  4  5
         1 51 15  2 10  1
         2 11  6  3  1  0
         3  0  0  0  0  0
         4  0  0  0  0  0
         5  0  0  0  0  0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric  .estimator .estimate
  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 accuracy multiclass     0.570&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the model never predicted class &lt;code&gt;3&lt;/code&gt;, &lt;code&gt;4&lt;/code&gt; or &lt;code&gt;5&lt;/code&gt;. Can we improve by adding some
regularization? Let’s find out!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-adding-regularization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: adding regularization&lt;/h2&gt;
&lt;p&gt;Before trying regularization, let’s try changing the cost function from the logistic function to the
hinge function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Train the model
hinge_oaa_fit &amp;lt;- system2(&amp;quot;/home/cbrunos/miniconda3/bin/vw&amp;quot;, args = &amp;quot;--oaa 5 -d data_split/small_train.txt --loss_function hinge -f vw_models/hinge_oaa.model&amp;quot;, stderr = TRUE)

system2(&amp;quot;/home/cbrunos/miniconda3/bin/vw&amp;quot;, args = &amp;quot;-i vw_models/hinge_oaa.model -t -d data_split/small_test2.txt -p data_split/hinge_oaa.predict&amp;quot;)


predictions &amp;lt;- read_delim(&amp;quot;data_split/hinge_oaa.predict&amp;quot;, &amp;quot;|&amp;quot;,
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

test &amp;lt;- test %&amp;gt;%
    select(-predictions)

predictions &amp;lt;- predictions %&amp;gt;%
    rename(predictions = X1) %&amp;gt;%
    mutate(predictions = factor(predictions, levels = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;)))

test &amp;lt;- test %&amp;gt;%
    bind_cols(predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          Truth
Prediction   1   2   3   4   5
         1 411 120  45  92   1
         2 355 189  12  17   0
         3  11   2   0   0   0
         4  36   4   0   1   0
         5   3   0   3   0   0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric  .estimator .estimate
  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 accuracy multiclass     0.462&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, didn’t work out so well, but at least we now know how to change the loss function. Let’s go
back to the logistic loss and add some regularization. First, let’s train the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regul_oaa_fit &amp;lt;- system2(&amp;quot;/home/cbrunos/miniconda3/bin/vw&amp;quot;, args = &amp;quot;--oaa 5 --l1 0.005 --l2 0.005 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model&amp;quot;, stderr = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can use it for prediction:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;/home/cbrunos/miniconda3/bin/vw&amp;quot;, args = &amp;quot;-i vw_models/small_regul_oaa.model -t -d data_split/test2.txt -p data_split/small_regul_oaa.predict&amp;quot;)


predictions &amp;lt;- read_delim(&amp;quot;data_split/small_regul_oaa.predict&amp;quot;, &amp;quot;|&amp;quot;,
                          escape_double = FALSE, col_names = FALSE,
                          trim_ws = TRUE)

test &amp;lt;- test %&amp;gt;%
    select(-predictions)

predictions &amp;lt;- predictions %&amp;gt;%
    rename(predictions = X1) %&amp;gt;%
    mutate(predictions = factor(predictions, levels = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;)))

test &amp;lt;- test %&amp;gt;%
    bind_cols(predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now use it for predictions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          Truth
Prediction   1   2   3   4   5
         1 816 315  60 110   1
         2   0   0   0   0   0
         3   0   0   0   0   0
         4   0   0   0   0   0
         5   0   0   0   0   0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric  .estimator .estimate
  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 accuracy multiclass     0.627&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So accuracy improved, but the model only predicts class 1 now… let’s try with other hyper-parameters values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regul_oaa_fit &amp;lt;- system2(&amp;quot;/home/cbrunos/miniconda3/bin/vw&amp;quot;, args = &amp;quot;--oaa 5 --l1 0.00015 --l2 0.00015 -d data_split/small_train.txt -f vw_models/small_regul_oaa.model&amp;quot;, stderr = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          Truth
Prediction   1   2   3   4   5
         1 784 300  57 108   1
         2  32  14   3   2   0
         3   0   1   0   0   0
         4   0   0   0   0   0
         5   0   0   0   0   0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric  .estimator .estimate
  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 accuracy multiclass     0.613&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So accuracy is lower than previously, but at least more categories get correctly predicted. Depending
on your needs, you should consider different metrics. Especially for classification problems, you might
not be interested in accuracy, in particular if the data is severely unbalanced.&lt;/p&gt;
&lt;p&gt;Anyhow, to finish this blog post, let’s train the model on the whole data and measure the time it
takes to run the full model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-training-on-the-whole-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4: Training on the whole data&lt;/h2&gt;
&lt;p&gt;Let’s first split the whole data into a training and a testing set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nb_lines &amp;lt;- system2(&amp;quot;cat&amp;quot;, args = &amp;quot;text_fr.txt | wc -l&amp;quot;, stdout = TRUE)

system2(&amp;quot;split&amp;quot;, args = paste0(&amp;quot;-l&amp;quot;, floor(as.numeric(nb_lines)*0.995), &amp;quot; text_fr.txt data_split/&amp;quot;))

system2(&amp;quot;mv&amp;quot;, args = &amp;quot;data_split/aa data_split/train.txt&amp;quot;)
system2(&amp;quot;mv&amp;quot;, args = &amp;quot;data_split/ab data_split/test.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The whole data contains 260247 lines, and the training set weighs 667MB, which is quite large. Let’s train
the simple multiple classifier on the data and see how long it takes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic &amp;lt;- Sys.time()
oaa_fit &amp;lt;- system2(&amp;quot;/home/cbrunos/miniconda3/bin/vw&amp;quot;, args = &amp;quot;--oaa 5 -d data_split/train.txt -f vw_models/oaa.model&amp;quot;, stderr = TRUE)
Sys.time() - tic&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Time difference of 4.73266 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yep, you read that right. Training the classifier on 667MB of data took less than 5 seconds!&lt;/p&gt;
&lt;p&gt;Let’s take a look at the final object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oaa_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; [1] &amp;quot;final_regressor = vw_models/oaa.model&amp;quot;                                   
 [2] &amp;quot;Num weight bits = 18&amp;quot;                                                    
 [3] &amp;quot;learning rate = 0.5&amp;quot;                                                     
 [4] &amp;quot;initial_t = 0&amp;quot;                                                           
 [5] &amp;quot;power_t = 0.5&amp;quot;                                                           
 [6] &amp;quot;using no cache&amp;quot;                                                          
 [7] &amp;quot;Reading datafile = data_split/train.txt&amp;quot;                                 
 [8] &amp;quot;num sources = 1&amp;quot;                                                         
 [9] &amp;quot;average  since         example        example  current  current  current&amp;quot;
[10] &amp;quot;loss     last          counter         weight    label  predict features&amp;quot;
[11] &amp;quot;1.000000 1.000000            1            1.0        2        1      253&amp;quot;
[12] &amp;quot;0.500000 0.000000            2            2.0        2        2      499&amp;quot;
[13] &amp;quot;0.250000 0.000000            4            4.0        2        2        6&amp;quot;
[14] &amp;quot;0.250000 0.250000            8            8.0        1        1     2268&amp;quot;
[15] &amp;quot;0.312500 0.375000           16           16.0        1        1      237&amp;quot;
[16] &amp;quot;0.250000 0.187500           32           32.0        1        1      557&amp;quot;
[17] &amp;quot;0.171875 0.093750           64           64.0        1        1      689&amp;quot;
[18] &amp;quot;0.179688 0.187500          128          128.0        2        2      208&amp;quot;
[19] &amp;quot;0.144531 0.109375          256          256.0        1        1      856&amp;quot;
[20] &amp;quot;0.136719 0.128906          512          512.0        4        4        4&amp;quot;
[21] &amp;quot;0.122070 0.107422         1024         1024.0        1        1     1353&amp;quot;
[22] &amp;quot;0.106934 0.091797         2048         2048.0        1        1      571&amp;quot;
[23] &amp;quot;0.098633 0.090332         4096         4096.0        1        1       43&amp;quot;
[24] &amp;quot;0.080566 0.062500         8192         8192.0        1        1      885&amp;quot;
[25] &amp;quot;0.069336 0.058105        16384        16384.0        1        1      810&amp;quot;
[26] &amp;quot;0.062683 0.056030        32768        32768.0        2        2      467&amp;quot;
[27] &amp;quot;0.058167 0.053650        65536        65536.0        1        1       47&amp;quot;
[28] &amp;quot;0.056061 0.053955       131072       131072.0        1        1      495&amp;quot;
[29] &amp;quot;&amp;quot;                                                                        
[30] &amp;quot;finished run&amp;quot;                                                            
[31] &amp;quot;number of examples = 258945&amp;quot;                                             
[32] &amp;quot;weighted example sum = 258945.000000&amp;quot;                                    
[33] &amp;quot;weighted label sum = 0.000000&amp;quot;                                           
[34] &amp;quot;average loss = 0.054467&amp;quot;                                                 
[35] &amp;quot;total feature number = 116335486&amp;quot;  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use the test set and see how the model fares:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat(test, truth = truth, estimate = predictions)

accuracy(test, truth = truth, estimate = predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          Truth
Prediction   1   2   3   4   5
         1 537 175  52 100   1
         2 271 140   8   9   0
         3   1   0   0   0   0
         4   7   0   0   1   0
         5   0   0   0   0   0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A tibble: 1 x 3
  .metric  .estimator .estimate
  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
1 accuracy multiclass     0.521&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Better accuracy can certainly be achieved with hyper-parameter tuning… maybe the subject for a
future blog post? In any case I am very impressed with Vowpal Wabbit and am certainly looking forward
to future developments of &lt;code&gt;{RVowpalWabbit}&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Classification of historical newspapers content: a tutorial combining R, bash and Vowpal Wabbit, part 1</title>
      <link>/blog/2019-03-03-historical_vowpal/</link>
      <pubDate>Sun, 03 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-03-03-historical_vowpal/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/BilPXIt0R2w?t=41&#34;&gt;
&lt;img src=&#34;/img/wabbit_reading.jpg&#34; title = &#34;Vowpal Wabbit is fast as heck&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Can I get enough of historical newspapers data? Seems like I don’t. I already wrote four
(&lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-04-newspapers/&#34;&gt;1&lt;/a&gt;,
&lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/&#34;&gt;2&lt;/a&gt;,
&lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-31-newspapers_shiny_app/&#34;&gt;3&lt;/a&gt; and
&lt;a href=&#34;https://www.brodrigues.co/blog/2019-02-04-newspapers_shiny_app_tutorial/&#34;&gt;4&lt;/a&gt;) blog posts, but
there’s still a lot to explore. This blog post uses a new batch of data announced on twitter:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;For all who love to analyse text, the BnL released half a million of processed newspaper articles. Historical news from 1841-1878. They directly contain the full text as well as other metadata. It’s really easy to work with! &lt;a href=&#34;https://twitter.com/hashtag/digitsation?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#digitsation&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/historicalnews?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#historicalnews&lt;/a&gt; &lt;a href=&#34;https://t.co/f5mv4bigQ2&#34;&gt;https://t.co/f5mv4bigQ2&lt;/a&gt; &lt;a href=&#34;https://t.co/cykjx9vstX&#34;&gt;pic.twitter.com/cykjx9vstX&lt;/a&gt;&lt;/p&gt;&amp;mdash; Marschall Ralph (@RalphMarschall) &lt;a href=&#34;https://twitter.com/RalphMarschall/status/1101432342214782981?ref_src=twsrc%5Etfw&#34;&gt;March 1, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;and this data could not have arrived at a better moment, since something else got announced via Twitter
recently:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;RVowpalWabbit 0.0.13: Keeping CRAN happy&lt;br&gt;R Interface to the &amp;#39;Vowpal Wabbit&amp;#39; fast out-of-core learner&lt;a href=&#34;https://t.co/XXXbfMqrgT&#34;&gt;https://t.co/XXXbfMqrgT&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rcpp?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rcpp&lt;/a&gt; &lt;a href=&#34;https://t.co/XQwNdpK2dc&#34;&gt;pic.twitter.com/XQwNdpK2dc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Dirk Eddelbuettel (@eddelbuettel) &lt;a href=&#34;https://twitter.com/eddelbuettel/status/1098941963527700480?ref_src=twsrc%5Etfw&#34;&gt;February 22, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;I wanted to try using &lt;a href=&#34;https://github.com/VowpalWabbit/vowpal_wabbit&#34;&gt;Vowpal Wabbit&lt;/a&gt;
for a couple of weeks now because it seems to be the perfect
tool for when you’re dealing with what I call &lt;em&gt;big-ish&lt;/em&gt; data: data that is not big data, and might
fit in your RAM, but is still a PITA to deal with. It can be data that is large enough to take 30
seconds to be imported into R, and then every operation on it lasts for minutes, and estimating/training
a model on it might eat up all your RAM. Vowpal Wabbit avoids all this because it’s an online-learning
system. Vowpal Wabbit is capable of training a model with data that it sees on the fly, which means
VW can be used for real-time machine learning, but also for when the training data is very large.
Each row of the data gets streamed into VW which updates the estimated parameters of the model
(or weights) in real time. So no need to first import all the data into R!&lt;/p&gt;
&lt;p&gt;The goal of this blog post is to get started with VW, and build a very simple logistic model
to classify documents using the historical newspapers data from the National Library of Luxembourg,
which you can download &lt;a href=&#34;https://data.bnl.lu/data/historical-newspapers/&#34;&gt;here&lt;/a&gt; (scroll down and
download the &lt;em&gt;Text Analysis Pack&lt;/em&gt;). The goal is not to build the best model, but &lt;em&gt;a&lt;/em&gt; model. Several
steps are needed for this: prepare the data, install VW and train a model using &lt;code&gt;{RVowpalWabbit}&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;step-1-preparing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Preparing the data&lt;/h2&gt;
&lt;p&gt;The data is in a neat &lt;code&gt;.xml&lt;/code&gt; format, and extracting what I need will be easy. However, the input
format for VW is a bit unusual; it resembles &lt;em&gt;.psv&lt;/em&gt; files (&lt;strong&gt;P&lt;/strong&gt;ipe &lt;strong&gt;S&lt;/strong&gt;eparated &lt;strong&gt;V&lt;/strong&gt;alues) but
allows for more flexibility. I will not dwell much into it, but for our purposes, the file must
look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1 | this is the first observation, which in our case will be free text
2 | this is another observation, its label, or class, equals 2
4 | this is another observation, of class 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first column, before the “|” is the target class we want to predict, and the second column
contains free text.&lt;/p&gt;
&lt;p&gt;The raw data looks like this:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the raw data&lt;/summary&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;OAI-PMH xmlns=&amp;quot;http://www.openarchives.org/OAI/2.0/&amp;quot; xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; xsi:schemaLocation=&amp;quot;http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd&amp;quot;&amp;gt;
&amp;lt;responseDate&amp;gt;2019-02-28T11:13:01&amp;lt;/responseDate&amp;gt;
&amp;lt;request&amp;gt;http://www.eluxemburgensia.lu/OAI&amp;lt;/request&amp;gt;
&amp;lt;ListRecords&amp;gt;
&amp;lt;record&amp;gt;
&amp;lt;header&amp;gt;
&amp;lt;identifier&amp;gt;digitool-publish:3026998-DTL45&amp;lt;/identifier&amp;gt;
&amp;lt;datestamp&amp;gt;2019-02-28T11:13:01Z&amp;lt;/datestamp&amp;gt;
&amp;lt;/header&amp;gt;
&amp;lt;metadata&amp;gt;
&amp;lt;oai_dc:dc xmlns:oai_dc=&amp;quot;http://www.openarchives.org/OAI/2.0/oai_dc/&amp;quot; xmlns:dc=&amp;quot;http://purl.org/dc/elements/1.1/&amp;quot; xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; xmlns:dcterms=&amp;quot;http://purl.org/dc/terms/&amp;quot; xsi:schemaLocation=&amp;quot;http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd&amp;quot;&amp;gt;
&amp;lt;dc:identifier&amp;gt;
https://persist.lu/ark:/70795/6gq1q1/articles/DTL45
&amp;lt;/dc:identifier&amp;gt;
&amp;lt;dc:source&amp;gt;newspaper/indeplux/1871-12-29_01&amp;lt;/dc:source&amp;gt;
&amp;lt;dcterms:isPartOf&amp;gt;L&amp;#39;indépendance luxembourgeoise&amp;lt;/dcterms:isPartOf&amp;gt;
&amp;lt;dcterms:isReferencedBy&amp;gt;
issue:newspaper/indeplux/1871-12-29_01/article:DTL45
&amp;lt;/dcterms:isReferencedBy&amp;gt;
&amp;lt;dc:date&amp;gt;1871-12-29&amp;lt;/dc:date&amp;gt;
&amp;lt;dc:publisher&amp;gt;Jean Joris&amp;lt;/dc:publisher&amp;gt;
&amp;lt;dc:relation&amp;gt;3026998&amp;lt;/dc:relation&amp;gt;
&amp;lt;dcterms:hasVersion&amp;gt;
http://www.eluxemburgensia.lu/webclient/DeliveryManager?pid=3026998#panel:pp|issue:3026998|article:DTL45
&amp;lt;/dcterms:hasVersion&amp;gt;
&amp;lt;dc:description&amp;gt;
CONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.) Art. 6. Glacière communale. M. le Bourgmcstr ¦ . Le collège échevinal propose un autro mode de se procurer de la glace. Nous avons dépensé 250 fr. cha- que année pour distribuer 30 kilos do glace; c’est une trop forte somme pour un résultat si minime. Nous aurions voulu nous aboucher avec des fabricants de bière ou autres industriels qui nous auraient fourni de la glace en cas de besoin. L’architecte qui été chargé de passer un contrat, a été trouver des négociants, mais ses démarches n’ont pas abouti. 
&amp;lt;/dc:description&amp;gt;
&amp;lt;dc:title&amp;gt;
CONSEIL COMMUNAL de la ville de Luxembourg. Séance du 23 décembre 1871. (Suite.)
&amp;lt;/dc:title&amp;gt;
&amp;lt;dc:type&amp;gt;ARTICLE&amp;lt;/dc:type&amp;gt;
&amp;lt;dc:language&amp;gt;fr&amp;lt;/dc:language&amp;gt;
&amp;lt;dcterms:extent&amp;gt;863&amp;lt;/dcterms:extent&amp;gt;
&amp;lt;/oai_dc:dc&amp;gt;
&amp;lt;/metadata&amp;gt;
&amp;lt;/record&amp;gt;
&amp;lt;/ListRecords&amp;gt;
&amp;lt;/OAI-PMH&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;I need several things from this file:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The title of the newspaper: &lt;code&gt;&amp;lt;dcterms:isPartOf&amp;gt;L&#39;indépendance luxembourgeoise&amp;lt;/dcterms:isPartOf&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The type of the article: &lt;code&gt;&amp;lt;dc:type&amp;gt;ARTICLE&amp;lt;/dc:type&amp;gt;&lt;/code&gt;. Can be Article, Advertisement, Issue, Section or Other.&lt;/li&gt;
&lt;li&gt;The contents: &lt;code&gt;&amp;lt;dc:description&amp;gt;CONSEIL COMMUNAL de la ville de Luxembourg. Séance du ....&amp;lt;/dc:description&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will only focus on newspapers in French, even though newspapers in German also had articles in French.
This is because the tag &lt;code&gt;&amp;lt;dc:language&amp;gt;fr&amp;lt;/dc:language&amp;gt;&lt;/code&gt; is not always available. If it were, I could
simply look for it and extract all the content in French easily, but unfortunately this is not the case.&lt;/p&gt;
&lt;p&gt;First of all, let’s get the data into R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;xml2&amp;quot;)
library(&amp;quot;furrr&amp;quot;)

files &amp;lt;- list.files(path = &amp;quot;export01-newspapers1841-1878/&amp;quot;, all.files = TRUE, recursive = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in a character vector with the path to all the files:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(files)
[1] &amp;quot;000/1400000/1400000-ADVERTISEMENT-DTL78.xml&amp;quot;   &amp;quot;000/1400000/1400000-ADVERTISEMENT-DTL79.xml&amp;quot;  
[3] &amp;quot;000/1400000/1400000-ADVERTISEMENT-DTL80.xml&amp;quot;   &amp;quot;000/1400000/1400000-ADVERTISEMENT-DTL81.xml&amp;quot;  
[5] &amp;quot;000/1400000/1400000-MODSMD_ARTICLE1-DTL34.xml&amp;quot; &amp;quot;000/1400000/1400000-MODSMD_ARTICLE2-DTL35.xml&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I write a function that does the needed data preparation steps. I describe what the function
does in the comments inside:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;to_vw &amp;lt;- function(xml_file){

    # read in the xml file
    file &amp;lt;- read_xml(paste0(&amp;quot;export01-newspapers1841-1878/&amp;quot;, xml_file))

    # Get the newspaper
    newspaper &amp;lt;- xml_find_all(file, &amp;quot;.//dcterms:isPartOf&amp;quot;) %&amp;gt;% xml_text()

    # Only keep the newspapers written in French
    if(!(newspaper %in% c(&amp;quot;L&amp;#39;UNION.&amp;quot;,
                          &amp;quot;L&amp;#39;indépendance luxembourgeoise&amp;quot;,
                          &amp;quot;COURRIER DU GRAND-DUCHÉ DE LUXEMBOURG.&amp;quot;,
                          &amp;quot;JOURNAL DE LUXEMBOURG.&amp;quot;,
                          &amp;quot;L&amp;#39;AVENIR&amp;quot;,
                          &amp;quot;L’Arlequin&amp;quot;,
                          &amp;quot;La Gazette du Grand-Duché de Luxembourg&amp;quot;,
                          &amp;quot;L&amp;#39;AVENIR DE LUXEMBOURG&amp;quot;,
                          &amp;quot;L&amp;#39;AVENIR DU GRAND-DUCHE DE LUXEMBOURG.&amp;quot;,
                          &amp;quot;L&amp;#39;AVENIR DU GRAND-DUCHÉ DE LUXEMBOURG.&amp;quot;,
                          &amp;quot;Le gratis luxembourgeois&amp;quot;,
                          &amp;quot;Luxemburger Zeitung – Journal de Luxembourg&amp;quot;,
                          &amp;quot;Recueil des mémoires et des travaux publiés par la Société de Botanique du Grand-Duché de Luxembourg&amp;quot;))){
        return(NULL)
    } else {
        # Get the type of the content. Can be article, advert, issue, section or other
        type &amp;lt;- xml_find_all(file, &amp;quot;.//dc:type&amp;quot;) %&amp;gt;% xml_text()

        type &amp;lt;- case_when(type == &amp;quot;ARTICLE&amp;quot; ~ &amp;quot;1&amp;quot;,
                          type == &amp;quot;ADVERTISEMENT&amp;quot; ~ &amp;quot;2&amp;quot;,
                          type == &amp;quot;ISSUE&amp;quot; ~ &amp;quot;3&amp;quot;,
                          type == &amp;quot;SECTION&amp;quot; ~ &amp;quot;4&amp;quot;,
                          TRUE ~ &amp;quot;5&amp;quot;
        )

        # Get the content itself. Only keep alphanumeric characters, and remove any line returns or 
        # carriage returns
        description &amp;lt;- xml_find_all(file, &amp;quot;.//dc:description&amp;quot;) %&amp;gt;%
            xml_text() %&amp;gt;%
            str_replace_all(pattern = &amp;quot;[^[:alnum:][:space:]]&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
            str_to_lower() %&amp;gt;%
            str_replace_all(&amp;quot;\r?\n|\r|\n&amp;quot;, &amp;quot; &amp;quot;)

        # Return the final object: one line that looks like this
        # 1 | bla bla
        paste(type, &amp;quot;|&amp;quot;, description)
    }

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now run this code to parse all the files, and I do so in parallel, thanks to the &lt;code&gt;{furrr}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plan(multiprocess, workers = 12)

text_fr &amp;lt;- files %&amp;gt;%
    future_map(to_vw)

text_fr &amp;lt;- text_fr %&amp;gt;%
    discard(is.null)

write_lines(text_fr, &amp;quot;text_fr.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-install-vowpal-wabbit&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Install Vowpal Wabbit&lt;/h2&gt;
&lt;p&gt;To easiest way to install VW must be using Anaconda, and more specifically the conda package manager.
Anaconda is a Python (and R) distribution for scientific computing and it comes with a package manager
called conda which makes installing Python (or R) packages very easy. While VW is a standalone
piece of software, it can also be installed by conda or pip. Instead of installing the full Anaconda distribution,
you can install Miniconda, which only comes with the bare minimum: a Python executable and the
conda package manager. You can find Miniconda &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;here&lt;/a&gt;
and once it’s installed, you can install VW with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;conda install -c gwerbin vowpal-wabbit &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to install VW with pip, as detailed &lt;a href=&#34;https://pypi.org/project/vowpalwabbit/&#34;&gt;here&lt;/a&gt;,
but in my experience, managing Python packages with pip is not super. It is better to manage your
Python distribution through conda, because it creates environments in your home folder which are
independent of the system’s Python installation, which is often out-of-date.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-building-a-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Building &lt;em&gt;a&lt;/em&gt; model&lt;/h2&gt;
&lt;p&gt;Vowpal Wabbit can be used from the command line, but there are interfaces for Python and since a
few weeks, for R. The R interface is quite crude for now, as it’s still in very early stages. I’m
sure it will evolve, and perhaps a Vowpal Wabbit engine will be added to &lt;code&gt;{parsnip}&lt;/code&gt;, which would
make modeling with VW really easy.&lt;/p&gt;
&lt;p&gt;For now, let’s only use 10000 lines for prototyping purposes before running the model on the whole file. Because
the data is quite large, I do not want to import it into R. So I use command line tools to manipulate
this data directly from my hard drive:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prepare data
system2(&amp;quot;shuf&amp;quot;, args = &amp;quot;-n 10000 text_fr.txt &amp;gt; small.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;shuf&lt;/code&gt; is a Unix command, and as such the above code should work on GNU/Linux systems, and most
likely macOS too. &lt;code&gt;shuf&lt;/code&gt; generates random permutations of a given file to standard output. I use &lt;code&gt;&amp;gt;&lt;/code&gt;
to direct this output to another file, which I called &lt;code&gt;small.txt&lt;/code&gt;. The &lt;code&gt;-n 10000&lt;/code&gt; options simply
means that I want 10000 lines.&lt;/p&gt;
&lt;p&gt;I then split this small file into a training and a testing set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Adapted from http://bitsearch.blogspot.com/2009/03/bash-script-to-split-train-and-test.html

# The command below counts the lines in small.txt. This is not really needed, since I know that the 
# file only has 10000 lines, but I kept it here for future reference
# notice the stdout = TRUE option. This is needed because the output simply gets shown in R&amp;#39;s
# command line and does get saved into a variable.
nb_lines &amp;lt;- system2(&amp;quot;cat&amp;quot;, args = &amp;quot;small.txt | wc -l&amp;quot;, stdout = TRUE)

system2(&amp;quot;split&amp;quot;, args = paste0(&amp;quot;-l&amp;quot;, as.numeric(nb_lines)*0.99, &amp;quot; small.txt data_split/&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;split&lt;/code&gt; is the Unix command that does the splitting. I keep 99% of the lines in the training set and
1% in the test set. This creates two files, &lt;code&gt;aa&lt;/code&gt; and &lt;code&gt;ab&lt;/code&gt;. I rename them using the &lt;code&gt;mv&lt;/code&gt; Unix command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system2(&amp;quot;mv&amp;quot;, args = &amp;quot;data_split/aa data_split/small_train.txt&amp;quot;)
system2(&amp;quot;mv&amp;quot;, args = &amp;quot;data_split/ab data_split/small_test.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, now let’s run a model using the VW command line utility from R, using &lt;code&gt;system2()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oaa_fit &amp;lt;- system2(&amp;quot;~/miniconda3/bin/vw&amp;quot;, args = &amp;quot;--oaa 5 -d data_split/small_train.txt -f small_oaa.model&amp;quot;, stderr = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I need to point &lt;code&gt;system2()&lt;/code&gt; to the &lt;code&gt;vw&lt;/code&gt; executable, and then add some options. &lt;code&gt;--oaa&lt;/code&gt; stands for
&lt;em&gt;one against all&lt;/em&gt; and is a way of doing multiclass classification; first, one class gets classified
by a logistic classifier against all the others, then the other class against all the others, then
the other…. The &lt;code&gt;5&lt;/code&gt; in the option means that there are 5 classes.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;-d data_split/train.txt&lt;/code&gt; specifies the path to the training data. &lt;code&gt;-f&lt;/code&gt; means “final regressor”
and specifies where you want to save the trained model.&lt;/p&gt;
&lt;p&gt;This is the output that get’s captured and saved into &lt;code&gt;oaa_fit&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;final_regressor = oaa.model&amp;quot;                                             
 [2] &amp;quot;Num weight bits = 18&amp;quot;                                                    
 [3] &amp;quot;learning rate = 0.5&amp;quot;                                                     
 [4] &amp;quot;initial_t = 0&amp;quot;                                                           
 [5] &amp;quot;power_t = 0.5&amp;quot;                                                           
 [6] &amp;quot;using no cache&amp;quot;                                                          
 [7] &amp;quot;Reading datafile = data_split/train.txt&amp;quot;                                 
 [8] &amp;quot;num sources = 1&amp;quot;                                                         
 [9] &amp;quot;average  since         example        example  current  current  current&amp;quot;
[10] &amp;quot;loss     last          counter         weight    label  predict features&amp;quot;
[11] &amp;quot;1.000000 1.000000            1            1.0        3        1       87&amp;quot;
[12] &amp;quot;1.000000 1.000000            2            2.0        1        3     2951&amp;quot;
[13] &amp;quot;1.000000 1.000000            4            4.0        1        3      506&amp;quot;
[14] &amp;quot;0.625000 0.250000            8            8.0        1        1      262&amp;quot;
[15] &amp;quot;0.625000 0.625000           16           16.0        1        2      926&amp;quot;
[16] &amp;quot;0.500000 0.375000           32           32.0        4        1        3&amp;quot;
[17] &amp;quot;0.375000 0.250000           64           64.0        1        1      436&amp;quot;
[18] &amp;quot;0.296875 0.218750          128          128.0        2        2      277&amp;quot;
[19] &amp;quot;0.238281 0.179688          256          256.0        2        2      118&amp;quot;
[20] &amp;quot;0.158203 0.078125          512          512.0        2        2       61&amp;quot;
[21] &amp;quot;0.125000 0.091797         1024         1024.0        2        2      258&amp;quot;
[22] &amp;quot;0.096191 0.067383         2048         2048.0        1        1       45&amp;quot;
[23] &amp;quot;0.085205 0.074219         4096         4096.0        1        1      318&amp;quot;
[24] &amp;quot;0.076172 0.067139         8192         8192.0        2        1      523&amp;quot;
[25] &amp;quot;&amp;quot;                                                                        
[26] &amp;quot;finished run&amp;quot;                                                            
[27] &amp;quot;number of examples = 9900&amp;quot;                                               
[28] &amp;quot;weighted example sum = 9900.000000&amp;quot;                                      
[29] &amp;quot;weighted label sum = 0.000000&amp;quot;                                           
[30] &amp;quot;average loss = 0.073434&amp;quot;                                                 
[31] &amp;quot;total feature number = 4456798&amp;quot;  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, when I try to run the same model using &lt;code&gt;RVowpalWabbit::vw()&lt;/code&gt; I get the following error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oaa_class &amp;lt;- c(&amp;quot;--oaa&amp;quot;, &amp;quot;5&amp;quot;,
               &amp;quot;-d&amp;quot;, &amp;quot;data_split/small_train.txt&amp;quot;,
               &amp;quot;-f&amp;quot;, &amp;quot;vw_models/small_oaa.model&amp;quot;)

result &amp;lt;- vw(oaa_class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in Rvw(args) : unrecognised option &amp;#39;--oaa&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I think the problem might be because I installed Vowpal Wabbit using conda, and the package
cannot find the executable. I’ll open an issue with reproducible code and we’ll see.&lt;/p&gt;
&lt;p&gt;In any case, that’s it for now! In the next blog post, we’ll see how to get the accuracy of this
very simple model, and see how to improve it!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Manipulating strings with the {stringr} package</title>
      <link>/blog/2019-02-10-stringr_package/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-02-10-stringr_package/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://b-rodrigues.github.io/modern_R/descriptive-statistics-and-data-manipulation.html#manipulate-strings-with-stringr&#34;&gt;
&lt;img src=&#34;/img/string.jpg&#34; title = &#34;Click here to go the ebook&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This blog post is an excerpt of my ebook Modern R with the tidyverse that you can read for
free &lt;a href=&#34;https://b-rodrigues.github.io/modern_R/&#34;&gt;here&lt;/a&gt;. This is taken from Chapter 4,
in which I introduce the &lt;code&gt;{stringr}&lt;/code&gt; package.&lt;/p&gt;
&lt;div id=&#34;manipulate-strings-with-stringr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manipulate strings with &lt;code&gt;{stringr}&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;{stringr}&lt;/code&gt; contains functions to manipulate strings. In Chapter 10, I will teach you about regular
expressions, but the functions contained in &lt;code&gt;{stringr}&lt;/code&gt; allow you to already do a lot of work on
strings, without needing to be a regular expression expert.&lt;/p&gt;
&lt;p&gt;I will discuss the most common string operations: detecting, locating, matching, searching and
replacing, and exctracting/removing strings.&lt;/p&gt;
&lt;p&gt;To introduce these operations, let us use an ALTO file of an issue of &lt;em&gt;The Winchester News&lt;/em&gt; from
October 31, 1910, which you can find on this
&lt;a href=&#34;https://gist.githubusercontent.com/b-rodrigues/5139560e7d0f2ecebe5da1df3629e015/raw/e3031d894ffb97217ddbad1ade1b307c9937d2c8/gistfile1.txt&#34;&gt;link&lt;/a&gt; (to see
how the newspaper looked like,
&lt;a href=&#34;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/&#34;&gt;click here&lt;/a&gt;). I re-hosted
the file on a public gist for archiving purposes. While working on the book, the original site went
down several times…&lt;/p&gt;
&lt;p&gt;ALTO is an XML schema for the description of text OCR and layout information of pages for digitzed
material, such as newspapers (source: &lt;a href=&#34;https://en.wikipedia.org/wiki/ALTO_(XML)&#34;&gt;ALTO Wikipedia page&lt;/a&gt;).
For more details, you can read my
&lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/&#34;&gt;blogpost&lt;/a&gt;
on the matter, but for our current purposes, it is enough to know that the file contains the text
of newspaper articles. The file looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;TextLine HEIGHT=&amp;quot;138.0&amp;quot; WIDTH=&amp;quot;2434.0&amp;quot; HPOS=&amp;quot;4056.0&amp;quot; VPOS=&amp;quot;5814.0&amp;quot;&amp;gt;
&amp;lt;String STYLEREFS=&amp;quot;ID7&amp;quot; HEIGHT=&amp;quot;108.0&amp;quot; WIDTH=&amp;quot;393.0&amp;quot; HPOS=&amp;quot;4056.0&amp;quot; VPOS=&amp;quot;5838.0&amp;quot; CONTENT=&amp;quot;timore&amp;quot; WC=&amp;quot;0.82539684&amp;quot;&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;timole&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;tlnldre&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;timor&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;insole&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;landed&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;/String&amp;gt;
&amp;lt;SP WIDTH=&amp;quot;74.0&amp;quot; HPOS=&amp;quot;4449.0&amp;quot; VPOS=&amp;quot;5838.0&amp;quot;/&amp;gt;
&amp;lt;String STYLEREFS=&amp;quot;ID7&amp;quot; HEIGHT=&amp;quot;105.0&amp;quot; WIDTH=&amp;quot;432.0&amp;quot; HPOS=&amp;quot;4524.0&amp;quot; VPOS=&amp;quot;5847.0&amp;quot; CONTENT=&amp;quot;market&amp;quot; WC=&amp;quot;0.95238096&amp;quot;/&amp;gt;
&amp;lt;SP WIDTH=&amp;quot;116.0&amp;quot; HPOS=&amp;quot;4956.0&amp;quot; VPOS=&amp;quot;5847.0&amp;quot;/&amp;gt;
&amp;lt;String STYLEREFS=&amp;quot;ID7&amp;quot; HEIGHT=&amp;quot;69.0&amp;quot; WIDTH=&amp;quot;138.0&amp;quot; HPOS=&amp;quot;5073.0&amp;quot; VPOS=&amp;quot;5883.0&amp;quot; CONTENT=&amp;quot;as&amp;quot; WC=&amp;quot;0.96825397&amp;quot;/&amp;gt;
&amp;lt;SP WIDTH=&amp;quot;74.0&amp;quot; HPOS=&amp;quot;5211.0&amp;quot; VPOS=&amp;quot;5883.0&amp;quot;/&amp;gt;
&amp;lt;String STYLEREFS=&amp;quot;ID7&amp;quot; HEIGHT=&amp;quot;69.0&amp;quot; WIDTH=&amp;quot;285.0&amp;quot; HPOS=&amp;quot;5286.0&amp;quot; VPOS=&amp;quot;5877.0&amp;quot; CONTENT=&amp;quot;were&amp;quot; WC=&amp;quot;1.0&amp;quot;&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;verc&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;veer&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;/String&amp;gt;
&amp;lt;SP WIDTH=&amp;quot;68.0&amp;quot; HPOS=&amp;quot;5571.0&amp;quot; VPOS=&amp;quot;5877.0&amp;quot;/&amp;gt;
&amp;lt;String STYLEREFS=&amp;quot;ID7&amp;quot; HEIGHT=&amp;quot;111.0&amp;quot; WIDTH=&amp;quot;147.0&amp;quot; HPOS=&amp;quot;5640.0&amp;quot; VPOS=&amp;quot;5838.0&amp;quot; CONTENT=&amp;quot;all&amp;quot; WC=&amp;quot;1.0&amp;quot;/&amp;gt;
&amp;lt;SP WIDTH=&amp;quot;83.0&amp;quot; HPOS=&amp;quot;5787.0&amp;quot; VPOS=&amp;quot;5838.0&amp;quot;/&amp;gt;
&amp;lt;String STYLEREFS=&amp;quot;ID7&amp;quot; HEIGHT=&amp;quot;111.0&amp;quot; WIDTH=&amp;quot;183.0&amp;quot; HPOS=&amp;quot;5871.0&amp;quot; VPOS=&amp;quot;5835.0&amp;quot; CONTENT=&amp;quot;the&amp;quot; WC=&amp;quot;0.95238096&amp;quot;&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;tll&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;Cu&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;ALTERNATIVE&amp;gt;tall&amp;lt;/ALTERNATIVE&amp;gt;
&amp;lt;/String&amp;gt;
&amp;lt;SP WIDTH=&amp;quot;75.0&amp;quot; HPOS=&amp;quot;6054.0&amp;quot; VPOS=&amp;quot;5835.0&amp;quot;/&amp;gt;
&amp;lt;String STYLEREFS=&amp;quot;ID3&amp;quot; HEIGHT=&amp;quot;132.0&amp;quot; WIDTH=&amp;quot;351.0&amp;quot; HPOS=&amp;quot;6129.0&amp;quot; VPOS=&amp;quot;5814.0&amp;quot; CONTENT=&amp;quot;cattle&amp;quot; WC=&amp;quot;0.95238096&amp;quot;/&amp;gt;
&amp;lt;/TextLine&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are interested in the strings after &lt;code&gt;CONTENT=&lt;/code&gt;. We are going to use functions from the &lt;code&gt;{stringr}&lt;/code&gt;
package to get the strings after &lt;code&gt;CONTENT=&lt;/code&gt;. In Chapter 10, we are going to explore this file
again, but using complex regular expressions to get all the content in one go.&lt;/p&gt;
&lt;div id=&#34;getting-text-data-into-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting text data into Rstudio&lt;/h3&gt;
&lt;p&gt;First of all, let us read in the file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester &amp;lt;- read_lines(&amp;quot;https://gist.githubusercontent.com/b-rodrigues/5139560e7d0f2ecebe5da1df3629e015/raw/e3031d894ffb97217ddbad1ade1b307c9937d2c8/gistfile1.txt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Even though the file is an XML file, I still read it in using &lt;code&gt;read_lines()&lt;/code&gt; and not &lt;code&gt;read_xml()&lt;/code&gt;
from the &lt;code&gt;{xml2}&lt;/code&gt; package. This is for the purposes of the current exercise, and also because I
always have trouble with XML files, and prefer to treat them as simple text files, and use regular
expressions to get what I need.&lt;/p&gt;
&lt;p&gt;Now that the ALTO file is read in and saved in the &lt;code&gt;winchester&lt;/code&gt; variable, you might want to print
the whole thing in the console. Before that, take a look at the structure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(winchester)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  chr [1:43] &amp;quot;&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the &lt;code&gt;winchester&lt;/code&gt; variable is a character atomic vector with 43 elements. So first, we need to
understand what these elements are. Let’s start with the first one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, so it seems like the first element is part of the header of the file. What about the second one?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester[2]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;&amp;lt;meta http-equiv=\&amp;quot;Content-Type\&amp;quot; content=\&amp;quot;text/html; charset=UTF-8\&amp;quot;&amp;gt;&amp;lt;base href=\&amp;quot;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\&amp;quot;&amp;gt;&amp;lt;style&amp;gt;body{margin-left:0;margin-right:0;margin-top:0}#bN015htcoyT__google-cache-hdr{background:#f5f5f5;font:13px arial,sans-serif;text-align:left;color:#202020;border:0;margin:0;border-bottom:1px solid #cecece;line-height:16px;padding:16px 28px 24px 28px}#bN015htcoyT__google-cache-hdr *{display:inline;font:inherit;text-align:inherit;color:inherit;line-height:inherit;background:none;border:0;margin:0;padding:0;letter-spacing:0}#bN015htcoyT__google-cache-hdr a{text-decoration:none;color:#1a0dab}#bN015htcoyT__google-cache-hdr a:hover{text-decoration:underline}#bN015htcoyT__google-cache-hdr a:visited{color:#609}#bN015htcoyT__google-cache-hdr div{display:block;margin-top:4px}#bN015htcoyT__google-cache-hdr b{font-weight:bold;display:inline-block;direction:ltr}&amp;lt;/style&amp;gt;&amp;lt;div id=\&amp;quot;bN015htcoyT__google-cache-hdr\&amp;quot;&amp;gt;&amp;lt;div&amp;gt;&amp;lt;span&amp;gt;This is Google&amp;#39;s cache of &amp;lt;a href=\&amp;quot;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\&amp;quot;&amp;gt;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;lt;/a&amp;gt;.&amp;lt;/span&amp;gt;&amp;amp;nbsp;&amp;lt;span&amp;gt;It is a snapshot of the page as it appeared on 21 Jan 2019 05:18:18 GMT.&amp;lt;/span&amp;gt;&amp;amp;nbsp;&amp;lt;span&amp;gt;The &amp;lt;a href=\&amp;quot;https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml\&amp;quot;&amp;gt;current page&amp;lt;/a&amp;gt; could have changed in the meantime.&amp;lt;/span&amp;gt;&amp;amp;nbsp;&amp;lt;a href=\&amp;quot;http://support.google.com/websearch/bin/answer.py?hl=en&amp;amp;amp;p=cached&amp;amp;amp;answer=1687222\&amp;quot;&amp;gt;&amp;lt;span&amp;gt;Learn more&amp;lt;/span&amp;gt;.&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;&amp;lt;span style=\&amp;quot;display:inline-block;margin-top:8px;margin-right:104px;white-space:nowrap\&amp;quot;&amp;gt;&amp;lt;span style=\&amp;quot;margin-right:28px\&amp;quot;&amp;gt;&amp;lt;span style=\&amp;quot;font-weight:bold\&amp;quot;&amp;gt;Full version&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;span style=\&amp;quot;margin-right:28px\&amp;quot;&amp;gt;&amp;lt;a href=\&amp;quot;http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;amp;amp;hl=en&amp;amp;amp;gl=lu&amp;amp;amp;strip=1&amp;amp;amp;vwsrc=0\&amp;quot;&amp;gt;&amp;lt;span&amp;gt;Text-only version&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;span style=\&amp;quot;margin-right:28px\&amp;quot;&amp;gt;&amp;lt;a href=\&amp;quot;http://webcache.googleusercontent.com/search?q=cache:2BVPV8QGj3oJ:https://chroniclingamerica.loc.gov/lccn/sn86069133/1910-10-31/ed-1/seq-1/ocr.xml&amp;amp;amp;hl=en&amp;amp;amp;gl=lu&amp;amp;amp;strip=0&amp;amp;amp;vwsrc=1\&amp;quot;&amp;gt;&amp;lt;span&amp;gt;View source&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;span style=\&amp;quot;display:inline-block;margin-top:8px;color:#717171\&amp;quot;&amp;gt;&amp;lt;span&amp;gt;Tip: To quickly find your search term on this page, press &amp;lt;b&amp;gt;Ctrl+F&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;⌘-F&amp;lt;/b&amp;gt; (Mac) and use the find bar.&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;div style=\&amp;quot;position:relative;\&amp;quot;&amp;gt;&amp;lt;?xml version=\&amp;quot;1.0\&amp;quot; encoding=\&amp;quot;UTF-8\&amp;quot;?&amp;gt;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Same. So where is the content? The file is very large, so if you print it in the console, it will
take quite some time to print, and you will not really be able to make out anything. The best
way would be to try to detect the string &lt;code&gt;CONTENT&lt;/code&gt; and work from there.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;detecting-getting-the-position-and-locating-strings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Detecting, getting the position and locating strings&lt;/h3&gt;
&lt;p&gt;When confronted to an atomic vector of strings, you might want to know inside which elements you
can find certain strings. For example, to know which elements of &lt;code&gt;winchester&lt;/code&gt; contain the string
&lt;code&gt;CONTENT&lt;/code&gt;, use &lt;code&gt;str_detect()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester %&amp;gt;%
  str_detect(&amp;quot;CONTENT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
## [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a boolean atomic vector of the same length as &lt;code&gt;winchester&lt;/code&gt;. If the string &lt;code&gt;CONTENT&lt;/code&gt; is
nowhere to be found, the result will equal &lt;code&gt;FALSE&lt;/code&gt;, if not it will equal &lt;code&gt;TRUE&lt;/code&gt;. Here it is easy to
see that the last element contains the string &lt;code&gt;CONTENT&lt;/code&gt;. But what if instead of having 43 elements,
the vector had 24192 elements? And hundreds would contain the string &lt;code&gt;CONTENT&lt;/code&gt;? It would be easier
to instead have the indices of the vector where one can find the word &lt;code&gt;CONTENT&lt;/code&gt;. This is possible
with &lt;code&gt;str_which()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester %&amp;gt;%
  str_which(&amp;quot;CONTENT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 43&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the result is 43, meaning that the 43rd element of &lt;code&gt;winchester&lt;/code&gt; contains the string &lt;code&gt;CONTENT&lt;/code&gt;
somewhere. If we need more precision, we can use &lt;code&gt;str_locate()&lt;/code&gt; and &lt;code&gt;str_locate_all()&lt;/code&gt;. To explain
how both these functions work, let’s create a very small example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers &amp;lt;- c(&amp;quot;aristotle&amp;quot;, &amp;quot;plato&amp;quot;, &amp;quot;epictetus&amp;quot;, &amp;quot;seneca the younger&amp;quot;, &amp;quot;epicurus&amp;quot;, &amp;quot;marcus aurelius&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now suppose I am interested in philosophers whose name ends in &lt;code&gt;us&lt;/code&gt;. Let us use &lt;code&gt;str_locate()&lt;/code&gt; first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_locate(&amp;quot;us&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      start end
## [1,]    NA  NA
## [2,]    NA  NA
## [3,]     8   9
## [4,]    NA  NA
## [5,]     7   8
## [6,]     5   6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can interpret the result as follows: in the rows, the index of the vector where the
string &lt;code&gt;us&lt;/code&gt; is found. So the 3rd, 5th and 6th philosopher have &lt;code&gt;us&lt;/code&gt; somewhere in their name.
The result also has two columns: &lt;code&gt;start&lt;/code&gt; and &lt;code&gt;end&lt;/code&gt;. These give the position of the string. So the
string &lt;code&gt;us&lt;/code&gt; can be found starting at position 8 of the 3rd element of the vector, and ends at position
9. Same goes for the other philisophers. However, consider Marcus Aurelius. He has two names, both
ending with &lt;code&gt;us&lt;/code&gt;. However, &lt;code&gt;str_locate()&lt;/code&gt; only shows the position of the &lt;code&gt;us&lt;/code&gt; in &lt;code&gt;Marcus&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To get both &lt;code&gt;us&lt;/code&gt; strings, you need to use &lt;code&gt;str_locate_all()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_locate_all(&amp;quot;us&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##      start end
## 
## [[2]]
##      start end
## 
## [[3]]
##      start end
## [1,]     8   9
## 
## [[4]]
##      start end
## 
## [[5]]
##      start end
## [1,]     7   8
## 
## [[6]]
##      start end
## [1,]     5   6
## [2,]    14  15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we get the position of the two &lt;code&gt;us&lt;/code&gt; in Marcus Aurelius. Doing this on the &lt;code&gt;winchester&lt;/code&gt; vector
will give use the position of the &lt;code&gt;CONTENT&lt;/code&gt; string, but this is not really important right now. What
matters is that you know how &lt;code&gt;str_locate()&lt;/code&gt; and &lt;code&gt;str_locate_all()&lt;/code&gt; work.&lt;/p&gt;
&lt;p&gt;So now that we know what interests us in the 43nd element of &lt;code&gt;winchester&lt;/code&gt;, let’s take a closer
look at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester[43]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, it’s a mess:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;TextLine HEIGHT=\&amp;quot;126.0\&amp;quot; WIDTH=\&amp;quot;1731.0\&amp;quot; HPOS=\&amp;quot;17160.0\&amp;quot; VPOS=\&amp;quot;21252.0\&amp;quot;&amp;gt;&amp;lt;String HEIGHT=\&amp;quot;114.0\&amp;quot; WIDTH=\&amp;quot;354.0\&amp;quot; HPOS=\&amp;quot;17160.0\&amp;quot; VPOS=\&amp;quot;21264.0\&amp;quot; CONTENT=\&amp;quot;0tV\&amp;quot; WC=\&amp;quot;0.8095238\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;131.0\&amp;quot; HPOS=\&amp;quot;17514.0\&amp;quot; VPOS=\&amp;quot;21264.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;111.0\&amp;quot; WIDTH=\&amp;quot;474.0\&amp;quot; HPOS=\&amp;quot;17646.0\&amp;quot; VPOS=\&amp;quot;21258.0\&amp;quot; CONTENT=\&amp;quot;BATES\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;140.0\&amp;quot; HPOS=\&amp;quot;18120.0\&amp;quot; VPOS=\&amp;quot;21258.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;114.0\&amp;quot; WIDTH=\&amp;quot;630.0\&amp;quot; HPOS=\&amp;quot;18261.0\&amp;quot; VPOS=\&amp;quot;21252.0\&amp;quot; CONTENT=\&amp;quot;President\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;&amp;gt;&amp;lt;ALTERNATIVE&amp;gt;Prcideht&amp;lt;/ALTERNATIVE&amp;gt;&amp;lt;ALTERNATIVE&amp;gt;Pride&amp;lt;/ALTERNATIVE&amp;gt;&amp;lt;/String&amp;gt;&amp;lt;/TextLine&amp;gt;&amp;lt;TextLine HEIGHT=\&amp;quot;153.0\&amp;quot; WIDTH=\&amp;quot;1689.0\&amp;quot; HPOS=\&amp;quot;17145.0\&amp;quot; VPOS=\&amp;quot;21417.0\&amp;quot;&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;105.0\&amp;quot; WIDTH=\&amp;quot;258.0\&amp;quot; HPOS=\&amp;quot;17145.0\&amp;quot; VPOS=\&amp;quot;21439.0\&amp;quot; CONTENT=\&amp;quot;WM\&amp;quot; WC=\&amp;quot;0.82539684\&amp;quot;&amp;gt;&amp;lt;TextLine HEIGHT=\&amp;quot;120.0\&amp;quot; WIDTH=\&amp;quot;2211.0\&amp;quot; HPOS=\&amp;quot;16788.0\&amp;quot; VPOS=\&amp;quot;21870.0\&amp;quot;&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;96.0\&amp;quot; WIDTH=\&amp;quot;102.0\&amp;quot; HPOS=\&amp;quot;16788.0\&amp;quot; VPOS=\&amp;quot;21894.0\&amp;quot; CONTENT=\&amp;quot;It\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;72.0\&amp;quot; HPOS=\&amp;quot;16890.0\&amp;quot; VPOS=\&amp;quot;21894.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;96.0\&amp;quot; WIDTH=\&amp;quot;93.0\&amp;quot; HPOS=\&amp;quot;16962.0\&amp;quot; VPOS=\&amp;quot;21885.0\&amp;quot; CONTENT=\&amp;quot;is\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;80.0\&amp;quot; HPOS=\&amp;quot;17055.0\&amp;quot; VPOS=\&amp;quot;21885.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;102.0\&amp;quot; WIDTH=\&amp;quot;417.0\&amp;quot; HPOS=\&amp;quot;17136.0\&amp;quot; VPOS=\&amp;quot;21879.0\&amp;quot; CONTENT=\&amp;quot;seldom\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;80.0\&amp;quot; HPOS=\&amp;quot;17553.0\&amp;quot; VPOS=\&amp;quot;21879.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;96.0\&amp;quot; WIDTH=\&amp;quot;267.0\&amp;quot; HPOS=\&amp;quot;17634.0\&amp;quot; VPOS=\&amp;quot;21873.0\&amp;quot; CONTENT=\&amp;quot;hard\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;81.0\&amp;quot; HPOS=\&amp;quot;17901.0\&amp;quot; VPOS=\&amp;quot;21873.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;87.0\&amp;quot; WIDTH=\&amp;quot;111.0\&amp;quot; HPOS=\&amp;quot;17982.0\&amp;quot; VPOS=\&amp;quot;21879.0\&amp;quot; CONTENT=\&amp;quot;to\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;81.0\&amp;quot; HPOS=\&amp;quot;18093.0\&amp;quot; VPOS=\&amp;quot;21879.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;96.0\&amp;quot; WIDTH=\&amp;quot;219.0\&amp;quot; HPOS=\&amp;quot;18174.0\&amp;quot; VPOS=\&amp;quot;21870.0\&amp;quot; CONTENT=\&amp;quot;find\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;77.0\&amp;quot; HPOS=\&amp;quot;18393.0\&amp;quot; VPOS=\&amp;quot;21870.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;69.0\&amp;quot; WIDTH=\&amp;quot;66.0\&amp;quot; HPOS=\&amp;quot;18471.0\&amp;quot; VPOS=\&amp;quot;21894.0\&amp;quot; CONTENT=\&amp;quot;a\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;77.0\&amp;quot; HPOS=\&amp;quot;18537.0\&amp;quot; VPOS=\&amp;quot;21894.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;78.0\&amp;quot; WIDTH=\&amp;quot;384.0\&amp;quot; HPOS=\&amp;quot;18615.0\&amp;quot; VPOS=\&amp;quot;21888.0\&amp;quot; CONTENT=\&amp;quot;succes\&amp;quot; WC=\&amp;quot;0.82539684\&amp;quot;&amp;gt;&amp;lt;ALTERNATIVE&amp;gt;success&amp;lt;/ALTERNATIVE&amp;gt;&amp;lt;/String&amp;gt;&amp;lt;/TextLine&amp;gt;&amp;lt;TextLine HEIGHT=\&amp;quot;126.0\&amp;quot; WIDTH=\&amp;quot;2316.0\&amp;quot; HPOS=\&amp;quot;16662.0\&amp;quot; VPOS=\&amp;quot;22008.0\&amp;quot;&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;75.0\&amp;quot; WIDTH=\&amp;quot;183.0\&amp;quot; HPOS=\&amp;quot;16662.0\&amp;quot; VPOS=\&amp;quot;22059.0\&amp;quot; CONTENT=\&amp;quot;sor\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;&amp;gt;&amp;lt;ALTERNATIVE&amp;gt;soar&amp;lt;/ALTERNATIVE&amp;gt;&amp;lt;/String&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;72.0\&amp;quot; HPOS=\&amp;quot;16845.0\&amp;quot; VPOS=\&amp;quot;22059.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;90.0\&amp;quot; WIDTH=\&amp;quot;168.0\&amp;quot; HPOS=\&amp;quot;16917.0\&amp;quot; VPOS=\&amp;quot;22035.0\&amp;quot; CONTENT=\&amp;quot;for\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;72.0\&amp;quot; HPOS=\&amp;quot;17085.0\&amp;quot; VPOS=\&amp;quot;22035.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;69.0\&amp;quot; WIDTH=\&amp;quot;267.0\&amp;quot; HPOS=\&amp;quot;17157.0\&amp;quot; VPOS=\&amp;quot;22050.0\&amp;quot; CONTENT=\&amp;quot;even\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;&amp;gt;&amp;lt;ALTERNATIVE&amp;gt;cen&amp;lt;/ALTERNATIVE&amp;gt;&amp;lt;ALTERNATIVE&amp;gt;cent&amp;lt;/ALTERNATIVE&amp;gt;&amp;lt;/String&amp;gt;&amp;lt;SP WIDTH=\&amp;quot;77.0\&amp;quot; HPOS=\&amp;quot;17434.0\&amp;quot; VPOS=\&amp;quot;22050.0\&amp;quot;/&amp;gt;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;66.0\&amp;quot; WIDTH=\&amp;quot;63.0\&amp;quot; HPOS=\&amp;quot;17502.0\&amp;quot; VPOS=\&amp;quot;22044.0\&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file was imported without any newlines. So we need to insert them ourselves, by splitting the
string in a clever way.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;splitting-strings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Splitting strings&lt;/h3&gt;
&lt;p&gt;There are two functions included in &lt;code&gt;{stringr}&lt;/code&gt; to split strings, &lt;code&gt;str_split()&lt;/code&gt; and &lt;code&gt;str_split_fixed()&lt;/code&gt;.
Let’s go back to our ancient philosophers. Two of them, Seneca the Younger and Marcus Aurelius have
something else in common than both being Roman Stoic philosophers. Their names are composed of several
words. If we want to split their names at the space character, we can use &lt;code&gt;str_split()&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_split(&amp;quot; &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;aristotle&amp;quot;
## 
## [[2]]
## [1] &amp;quot;plato&amp;quot;
## 
## [[3]]
## [1] &amp;quot;epictetus&amp;quot;
## 
## [[4]]
## [1] &amp;quot;seneca&amp;quot;  &amp;quot;the&amp;quot;     &amp;quot;younger&amp;quot;
## 
## [[5]]
## [1] &amp;quot;epicurus&amp;quot;
## 
## [[6]]
## [1] &amp;quot;marcus&amp;quot;   &amp;quot;aurelius&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;str_split()&lt;/code&gt; also has a &lt;code&gt;simplify = TRUE&lt;/code&gt; option:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_split(&amp;quot; &amp;quot;, simplify = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]        [,2]       [,3]     
## [1,] &amp;quot;aristotle&amp;quot; &amp;quot;&amp;quot;         &amp;quot;&amp;quot;       
## [2,] &amp;quot;plato&amp;quot;     &amp;quot;&amp;quot;         &amp;quot;&amp;quot;       
## [3,] &amp;quot;epictetus&amp;quot; &amp;quot;&amp;quot;         &amp;quot;&amp;quot;       
## [4,] &amp;quot;seneca&amp;quot;    &amp;quot;the&amp;quot;      &amp;quot;younger&amp;quot;
## [5,] &amp;quot;epicurus&amp;quot;  &amp;quot;&amp;quot;         &amp;quot;&amp;quot;       
## [6,] &amp;quot;marcus&amp;quot;    &amp;quot;aurelius&amp;quot; &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, the returned object is a matrix.&lt;/p&gt;
&lt;p&gt;What about &lt;code&gt;str_split_fixed()&lt;/code&gt;? The difference is that here you can specify the number of pieces
to return. For example, you could consider the name “Aurelius” to be the middle name of Marcus Aurelius,
and the “the younger” to be the middle name of Seneca the younger. This means that you would want
to split the name only at the first space character, and not at all of them. This is easily achieved
with &lt;code&gt;str_split_fixed()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_split_fixed(&amp;quot; &amp;quot;, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]        [,2]         
## [1,] &amp;quot;aristotle&amp;quot; &amp;quot;&amp;quot;           
## [2,] &amp;quot;plato&amp;quot;     &amp;quot;&amp;quot;           
## [3,] &amp;quot;epictetus&amp;quot; &amp;quot;&amp;quot;           
## [4,] &amp;quot;seneca&amp;quot;    &amp;quot;the younger&amp;quot;
## [5,] &amp;quot;epicurus&amp;quot;  &amp;quot;&amp;quot;           
## [6,] &amp;quot;marcus&amp;quot;    &amp;quot;aurelius&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives the expected result.&lt;/p&gt;
&lt;p&gt;So how does this help in our case? Well, if you look at how the ALTO file looks like, at the beginning
of this section, you will notice that every line ends with the “&amp;gt;” character. So let’s split at
that character!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_text &amp;lt;- winchester[43] %&amp;gt;%
  str_split(&amp;quot;&amp;gt;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a closer look at &lt;code&gt;winchester_text&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(winchester_text)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 1
##  $ : chr [1:19706] &amp;quot;&amp;lt;/processingStepSettings&amp;quot; &amp;quot;&amp;lt;processingSoftware&amp;quot; &amp;quot;&amp;lt;softwareCreator&amp;quot; &amp;quot;iArchives&amp;lt;/softwareCreator&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this is a list of length one, and the first, and only, element of that list is an atomic vector
with 19706 elements. Since this is a list of only one element, we can simplify it by saving the
atomic vector in a variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_text &amp;lt;- winchester_text[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now look at some lines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_text[1232:1245]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;&amp;lt;SP WIDTH=\&amp;quot;66.0\&amp;quot; HPOS=\&amp;quot;5763.0\&amp;quot; VPOS=\&amp;quot;9696.0\&amp;quot;/&amp;quot;                                                                         
##  [2] &amp;quot;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;108.0\&amp;quot; WIDTH=\&amp;quot;612.0\&amp;quot; HPOS=\&amp;quot;5829.0\&amp;quot; VPOS=\&amp;quot;9693.0\&amp;quot; CONTENT=\&amp;quot;Louisville\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;&amp;quot;
##  [3] &amp;quot;&amp;lt;ALTERNATIVE&amp;quot;                                                                                                                
##  [4] &amp;quot;Loniile&amp;lt;/ALTERNATIVE&amp;quot;                                                                                                        
##  [5] &amp;quot;&amp;lt;ALTERNATIVE&amp;quot;                                                                                                                
##  [6] &amp;quot;Lenities&amp;lt;/ALTERNATIVE&amp;quot;                                                                                                       
##  [7] &amp;quot;&amp;lt;/String&amp;quot;                                                                                                                    
##  [8] &amp;quot;&amp;lt;/TextLine&amp;quot;                                                                                                                  
##  [9] &amp;quot;&amp;lt;TextLine HEIGHT=\&amp;quot;150.0\&amp;quot; WIDTH=\&amp;quot;2520.0\&amp;quot; HPOS=\&amp;quot;4032.0\&amp;quot; VPOS=\&amp;quot;9849.0\&amp;quot;&amp;quot;                                                 
## [10] &amp;quot;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;108.0\&amp;quot; WIDTH=\&amp;quot;510.0\&amp;quot; HPOS=\&amp;quot;4032.0\&amp;quot; VPOS=\&amp;quot;9861.0\&amp;quot; CONTENT=\&amp;quot;Tobacco\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;/&amp;quot;  
## [11] &amp;quot;&amp;lt;SP WIDTH=\&amp;quot;113.0\&amp;quot; HPOS=\&amp;quot;4542.0\&amp;quot; VPOS=\&amp;quot;9861.0\&amp;quot;/&amp;quot;                                                                        
## [12] &amp;quot;&amp;lt;String STYLEREFS=\&amp;quot;ID7\&amp;quot; HEIGHT=\&amp;quot;105.0\&amp;quot; WIDTH=\&amp;quot;696.0\&amp;quot; HPOS=\&amp;quot;4656.0\&amp;quot; VPOS=\&amp;quot;9861.0\&amp;quot; CONTENT=\&amp;quot;Warehouse\&amp;quot; WC=\&amp;quot;1.0\&amp;quot;&amp;quot; 
## [13] &amp;quot;&amp;lt;ALTERNATIVE&amp;quot;                                                                                                                
## [14] &amp;quot;WHrchons&amp;lt;/ALTERNATIVE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This now looks easier to handle. We can narrow it down to the lines that only contain the string
we are interested in, “CONTENT”. First, let’s get the indices:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;content_winchester_index &amp;lt;- winchester_text %&amp;gt;%
  str_which(&amp;quot;CONTENT&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many lines contain the string “CONTENT”?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;length(content_winchester_index)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4462&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this reduces the amount of data we have to work with. Let us save this is a new
variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;content_winchester &amp;lt;- winchester_text[content_winchester_index]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;matching-strings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Matching strings&lt;/h3&gt;
&lt;p&gt;Matching strings is useful, but only in combination with regular expressions. As stated at the
beginning of this section, we are going to learn about regular expressions in Chapter 10, but in
order to make this section useful, we are going to learn the easiest, but perhaps the most useful
regular expression: &lt;code&gt;.*&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s go back to our ancient philosophers, and use &lt;code&gt;str_match()&lt;/code&gt; and see what happens. Let’s match
the “us” string:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_match(&amp;quot;us&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,] NA  
## [2,] NA  
## [3,] &amp;quot;us&amp;quot;
## [4,] NA  
## [5,] &amp;quot;us&amp;quot;
## [6,] &amp;quot;us&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not very useful, but what about the regular expression &lt;code&gt;.*&lt;/code&gt;? How could it help?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_match(&amp;quot;.*us&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]             
## [1,] NA               
## [2,] NA               
## [3,] &amp;quot;epictetus&amp;quot;      
## [4,] NA               
## [5,] &amp;quot;epicurus&amp;quot;       
## [6,] &amp;quot;marcus aurelius&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s already very interesting! So how does &lt;code&gt;.*&lt;/code&gt; work? To understand, let’s first start by using
&lt;code&gt;.&lt;/code&gt; alone:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_match(&amp;quot;.us&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] 
## [1,] NA   
## [2,] NA   
## [3,] &amp;quot;tus&amp;quot;
## [4,] NA   
## [5,] &amp;quot;rus&amp;quot;
## [6,] &amp;quot;cus&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This also matched whatever symbol comes just before the “u” from “us”. What if we use two &lt;code&gt;.&lt;/code&gt; instead?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_match(&amp;quot;..us&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]  
## [1,] NA    
## [2,] NA    
## [3,] &amp;quot;etus&amp;quot;
## [4,] NA    
## [5,] &amp;quot;urus&amp;quot;
## [6,] &amp;quot;rcus&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, we get the two symbols that immediately precede “us”. Instead of continuing like this
we now use the &lt;code&gt;*&lt;/code&gt;, which matches zero or more of &lt;code&gt;.&lt;/code&gt;. So by combining &lt;code&gt;*&lt;/code&gt; and &lt;code&gt;.&lt;/code&gt;, we can match
any symbol repeatedly, until there is nothing more to match. Note that there is also &lt;code&gt;+&lt;/code&gt;, which works
similarly to &lt;code&gt;*&lt;/code&gt;, but it matches one or more symbols.&lt;/p&gt;
&lt;p&gt;There is also a &lt;code&gt;str_match_all()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_match_all(&amp;quot;.*us&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##      [,1]
## 
## [[2]]
##      [,1]
## 
## [[3]]
##      [,1]       
## [1,] &amp;quot;epictetus&amp;quot;
## 
## [[4]]
##      [,1]
## 
## [[5]]
##      [,1]      
## [1,] &amp;quot;epicurus&amp;quot;
## 
## [[6]]
##      [,1]             
## [1,] &amp;quot;marcus aurelius&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this particular case it does not change the end result, but keep it in mind for cases like this one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(&amp;quot;haha&amp;quot;, &amp;quot;huhu&amp;quot;) %&amp;gt;%
  str_match(&amp;quot;ha&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]
## [1,] &amp;quot;ha&amp;quot;
## [2,] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(&amp;quot;haha&amp;quot;, &amp;quot;huhu&amp;quot;) %&amp;gt;%
  str_match_all(&amp;quot;ha&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##      [,1]
## [1,] &amp;quot;ha&amp;quot;
## [2,] &amp;quot;ha&amp;quot;
## 
## [[2]]
##      [,1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we want to match names containing the letter “t”? Easy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ancient_philosophers %&amp;gt;%
  str_match(&amp;quot;.*t.*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]                
## [1,] &amp;quot;aristotle&amp;quot;         
## [2,] &amp;quot;plato&amp;quot;             
## [3,] &amp;quot;epictetus&amp;quot;         
## [4,] &amp;quot;seneca the younger&amp;quot;
## [5,] NA                  
## [6,] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So how does this help us with our historical newspaper? Let’s try to get the strings that come
after “CONTENT”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_text %&amp;gt;%
  str_match(&amp;quot;CONTENT.*&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s use our faithful &lt;code&gt;str()&lt;/code&gt; function to take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content %&amp;gt;%
  str&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  chr [1:19706, 1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hum, there’s a lot of &lt;code&gt;NA&lt;/code&gt; values! This is because a lot of the lines from the file did not have the
string “CONTENT”, so there is no match possible. Let’s us remove all these &lt;code&gt;NA&lt;/code&gt;s. Because the
result is a matrix, we cannot use the &lt;code&gt;filter()&lt;/code&gt; function from &lt;code&gt;{dplyr}&lt;/code&gt;. So we need to convert it
to a tibble first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_content %&amp;gt;%
  as.tibble() %&amp;gt;%
  filter(!is.na(V1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because matrix columns do not have names, when a matrix gets converted into a tibble, the firt column
gets automatically called &lt;code&gt;V1&lt;/code&gt;. This is why I filter on this column. Let’s take a look at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(winchester_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##   V1                                  
##   &amp;lt;chr&amp;gt;                               
## 1 &amp;quot;CONTENT=\&amp;quot;J\&amp;quot; WC=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;   
## 2 &amp;quot;CONTENT=\&amp;quot;a\&amp;quot; WC=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;   
## 3 &amp;quot;CONTENT=\&amp;quot;Ira\&amp;quot; WC=\&amp;quot;0.95238096\&amp;quot;/&amp;quot;
## 4 &amp;quot;CONTENT=\&amp;quot;mj\&amp;quot; WC=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;  
## 5 &amp;quot;CONTENT=\&amp;quot;iI\&amp;quot; WC=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;  
## 6 &amp;quot;CONTENT=\&amp;quot;tE1r\&amp;quot; WC=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;searching-and-replacing-strings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Searching and replacing strings&lt;/h3&gt;
&lt;p&gt;We are getting close to the final result. We still need to do some cleaning however. Since our data
is inside a nice tibble, we might as well stick with it. So let’s first rename the column and
change all the strings to lowercase:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_content %&amp;gt;% 
  mutate(content = tolower(V1)) %&amp;gt;% 
  select(-V1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(winchester_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##   content                             
##   &amp;lt;chr&amp;gt;                               
## 1 &amp;quot;content=\&amp;quot;j\&amp;quot; wc=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;   
## 2 &amp;quot;content=\&amp;quot;a\&amp;quot; wc=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;   
## 3 &amp;quot;content=\&amp;quot;ira\&amp;quot; wc=\&amp;quot;0.95238096\&amp;quot;/&amp;quot;
## 4 &amp;quot;content=\&amp;quot;mj\&amp;quot; wc=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;  
## 5 &amp;quot;content=\&amp;quot;ii\&amp;quot; wc=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;  
## 6 &amp;quot;content=\&amp;quot;te1r\&amp;quot; wc=\&amp;quot;0.8095238\&amp;quot;/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second part of the string, “wc=….” is not really interesting. Let’s search and replace this
with an empty string, using &lt;code&gt;str_replace()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_content %&amp;gt;% 
  mutate(content = str_replace(content, &amp;quot;wc.*&amp;quot;, &amp;quot;&amp;quot;))

head(winchester_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##   content            
##   &amp;lt;chr&amp;gt;              
## 1 &amp;quot;content=\&amp;quot;j\&amp;quot; &amp;quot;   
## 2 &amp;quot;content=\&amp;quot;a\&amp;quot; &amp;quot;   
## 3 &amp;quot;content=\&amp;quot;ira\&amp;quot; &amp;quot; 
## 4 &amp;quot;content=\&amp;quot;mj\&amp;quot; &amp;quot;  
## 5 &amp;quot;content=\&amp;quot;ii\&amp;quot; &amp;quot;  
## 6 &amp;quot;content=\&amp;quot;te1r\&amp;quot; &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to use the regular expression from before to replace “wc” and every character that follows.
The same can be use to remove “content=”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_content %&amp;gt;% 
  mutate(content = str_replace(content, &amp;quot;content=&amp;quot;, &amp;quot;&amp;quot;))

head(winchester_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##   content    
##   &amp;lt;chr&amp;gt;      
## 1 &amp;quot;\&amp;quot;j\&amp;quot; &amp;quot;   
## 2 &amp;quot;\&amp;quot;a\&amp;quot; &amp;quot;   
## 3 &amp;quot;\&amp;quot;ira\&amp;quot; &amp;quot; 
## 4 &amp;quot;\&amp;quot;mj\&amp;quot; &amp;quot;  
## 5 &amp;quot;\&amp;quot;ii\&amp;quot; &amp;quot;  
## 6 &amp;quot;\&amp;quot;te1r\&amp;quot; &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are almost done, but some cleaning is still necessary:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exctracting-or-removing-strings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exctracting or removing strings&lt;/h3&gt;
&lt;p&gt;Now, because I now the ALTO spec, I know how to find words that are split between two sentences:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content %&amp;gt;% 
  filter(str_detect(content, &amp;quot;hyppart&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 64 x 1
##    content                                                               
##    &amp;lt;chr&amp;gt;                                                                 
##  1 &amp;quot;\&amp;quot;aver\&amp;quot; subs_type=\&amp;quot;hyppart1\&amp;quot; subs_content=\&amp;quot;average\&amp;quot; &amp;quot;           
##  2 &amp;quot;\&amp;quot;age\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;average\&amp;quot; &amp;quot;            
##  3 &amp;quot;\&amp;quot;considera\&amp;quot; subs_type=\&amp;quot;hyppart1\&amp;quot; subs_content=\&amp;quot;consideration\&amp;quot; &amp;quot;
##  4 &amp;quot;\&amp;quot;tion\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;consideration\&amp;quot; &amp;quot;     
##  5 &amp;quot;\&amp;quot;re\&amp;quot; subs_type=\&amp;quot;hyppart1\&amp;quot; subs_content=\&amp;quot;resigned\&amp;quot; &amp;quot;            
##  6 &amp;quot;\&amp;quot;signed\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;resigned\&amp;quot; &amp;quot;        
##  7 &amp;quot;\&amp;quot;install\&amp;quot; subs_type=\&amp;quot;hyppart1\&amp;quot; subs_content=\&amp;quot;installed\&amp;quot; &amp;quot;      
##  8 &amp;quot;\&amp;quot;ed\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;installed\&amp;quot; &amp;quot;           
##  9 &amp;quot;\&amp;quot;be\&amp;quot; subs_type=\&amp;quot;hyppart1\&amp;quot; subs_content=\&amp;quot;before\&amp;quot; &amp;quot;              
## 10 &amp;quot;\&amp;quot;fore\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;before\&amp;quot; &amp;quot;            
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For instance, the word “average” was split over two lines, the first part of the word, “aver” on the
first line, and the second part of the word, “age”, on the second line. We want to keep what comes
after “subs_content”. Let’s extract the word “average” using &lt;code&gt;str_extract()&lt;/code&gt;. However, because only
some words were split between two lines, we first need to detect where the string “hyppart1” is
located, and only then can we extract what comes after “subs_content”. Thus, we need to combine
&lt;code&gt;str_detect()&lt;/code&gt; to first detect the string, and then &lt;code&gt;str_extract()&lt;/code&gt; to extract what comes after
“subs_content”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_content %&amp;gt;% 
  mutate(content = if_else(str_detect(content, &amp;quot;hyppart1&amp;quot;), 
                           str_extract_all(content, &amp;quot;content=.*&amp;quot;, simplify = TRUE), 
                           content))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content %&amp;gt;% 
  filter(str_detect(content, &amp;quot;content&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 64 x 1
##    content                                                          
##    &amp;lt;chr&amp;gt;                                                            
##  1 &amp;quot;content=\&amp;quot;average\&amp;quot; &amp;quot;                                           
##  2 &amp;quot;\&amp;quot;age\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;average\&amp;quot; &amp;quot;       
##  3 &amp;quot;content=\&amp;quot;consideration\&amp;quot; &amp;quot;                                     
##  4 &amp;quot;\&amp;quot;tion\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;consideration\&amp;quot; &amp;quot;
##  5 &amp;quot;content=\&amp;quot;resigned\&amp;quot; &amp;quot;                                          
##  6 &amp;quot;\&amp;quot;signed\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;resigned\&amp;quot; &amp;quot;   
##  7 &amp;quot;content=\&amp;quot;installed\&amp;quot; &amp;quot;                                         
##  8 &amp;quot;\&amp;quot;ed\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;installed\&amp;quot; &amp;quot;      
##  9 &amp;quot;content=\&amp;quot;before\&amp;quot; &amp;quot;                                            
## 10 &amp;quot;\&amp;quot;fore\&amp;quot; subs_type=\&amp;quot;hyppart2\&amp;quot; subs_content=\&amp;quot;before\&amp;quot; &amp;quot;       
## # … with 54 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still need to get rid of the string “content=” and then of all the strings that contain “hyppart2”,
which are not needed now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_content %&amp;gt;% 
  mutate(content = str_replace(content, &amp;quot;content=&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;% 
  mutate(content = if_else(str_detect(content, &amp;quot;hyppart2&amp;quot;), NA_character_, content))

head(winchester_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##   content    
##   &amp;lt;chr&amp;gt;      
## 1 &amp;quot;\&amp;quot;j\&amp;quot; &amp;quot;   
## 2 &amp;quot;\&amp;quot;a\&amp;quot; &amp;quot;   
## 3 &amp;quot;\&amp;quot;ira\&amp;quot; &amp;quot; 
## 4 &amp;quot;\&amp;quot;mj\&amp;quot; &amp;quot;  
## 5 &amp;quot;\&amp;quot;ii\&amp;quot; &amp;quot;  
## 6 &amp;quot;\&amp;quot;te1r\&amp;quot; &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Almost done! We only need to remove the &lt;code&gt;&#34;&lt;/code&gt; characters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_content %&amp;gt;% 
  mutate(content = str_replace_all(content, &amp;quot;\&amp;quot;&amp;quot;, &amp;quot;&amp;quot;)) 

head(winchester_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##   content
##   &amp;lt;chr&amp;gt;  
## 1 &amp;quot;j &amp;quot;   
## 2 &amp;quot;a &amp;quot;   
## 3 &amp;quot;ira &amp;quot; 
## 4 &amp;quot;mj &amp;quot;  
## 5 &amp;quot;ii &amp;quot;  
## 6 &amp;quot;te1r &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s remove space characters with &lt;code&gt;str_trim()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;winchester_content &amp;lt;- winchester_content %&amp;gt;% 
  mutate(content = str_trim(content)) 

head(winchester_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##   content
##   &amp;lt;chr&amp;gt;  
## 1 j      
## 2 a      
## 3 ira    
## 4 mj     
## 5 ii     
## 6 te1r&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To finish off this section, let’s remove stop words (words that do not add any meaning to a sentence,
such as “as”, “and”…) and words that are composed of less than 3 characters. You can find a dataset
with stopwords inside the &lt;code&gt;{stopwords}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stopwords)

data(data_stopwords_stopwordsiso)

eng_stopwords &amp;lt;- tibble(&amp;quot;content&amp;quot; = data_stopwords_stopwordsiso$en)

winchester_content &amp;lt;- winchester_content %&amp;gt;% 
  anti_join(eng_stopwords) %&amp;gt;% 
  filter(nchar(content) &amp;gt; 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;content&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(winchester_content)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 1
##   content   
##   &amp;lt;chr&amp;gt;     
## 1 te1r      
## 2 jilas     
## 3 edition   
## 4 winchester
## 5 news      
## 6 injuries&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it for this section! You now know how to work with strings, but in Chapter 10 we are going
one step further by learning about regular expressions, which offer much more power.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building a shiny app to explore historical newspapers: a step-by-step guide</title>
      <link>/blog/2019-02-04-newspapers_shiny_app_tutorial/</link>
      <pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-02-04-newspapers_shiny_app_tutorial/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://brodriguesco.shinyapps.io/newspapers_app/&#34;&gt;
&lt;img src=&#34;/img/tf_idf.png&#34; title = &#34;Click here to go the app&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I started off this year by exploring a world that was unknown to me, the world of historical newspapers.
I did not know that historical newspapers data was a thing, and have been thoroughly enjoying myself
exploring the different datasets published by the National Library of Luxembourg. You can find
the data &lt;a href=&#34;https://data.bnl.lu/data/historical-newspapers/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In my &lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-04-newspapers/&#34;&gt;first blog post&lt;/a&gt;, I analyzed data from
&lt;em&gt;L’indépendence Luxembourgeoise&lt;/em&gt;. I focused on the ads, which were for the most part in the 4th and
last page of the newspaper. I did so by extracting the data from the ALTO files. ALTO files contain
the content of the newspapers, (basically, the words that make up the article). For this first
exercise, I disregarded the METS files, for two reasons. First, I simply wanted to have something
quick, and get used to the data. And second, I did not know about ALTO and METS files enough to
truly make something out of them. The problem of disregarding the METS file is that I only had a big
dump of words, and did not know which words came from which article, or ad in this case.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/&#34;&gt;second blog post&lt;/a&gt;), I extracted
data from the &lt;em&gt;L’Union&lt;/em&gt; newspaper, this time by using the metadata from the METS files too. By
combining the data from the ALTO files with the metadata from the METS files, I know which
words came from which article, which would make further analysis much more interesting.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-31-newspapers_shiny_app/&#34;&gt;third blog post&lt;/a&gt; of this series,
I built a Shiny app which makes it easy to explore the 10 years of publications of &lt;em&gt;L’Union&lt;/em&gt;. In this
blog post, I will explain in great detail how I created this app.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;part-1-getting-the-data-ready-for-the-shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 1: Getting the data ready for the Shiny app&lt;/h2&gt;
&lt;div id=&#34;step-1-extracting-the-needed-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 1: Extracting the needed data&lt;/h3&gt;
&lt;p&gt;If you want to follow along with a dataset from a single publication, you can download the following archive on
&lt;a href=&#34;https://www.dropbox.com/s/56ttqetz4cirsja/1533660_newspaper_lunion_1860-11-14.zip?dl=0&#34;&gt;dropbox&lt;/a&gt;.
Extract this archive, and you will find the data exactly as you would get it from the the big
archive you can download from the website of the National Library of Luxembourg. However, to keep
the size of the archive small, I removed the .pdf and .jpeg scans.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/&#34;&gt;second blog post&lt;/a&gt;) I wrote
some functions that made extracting the needed data from the files easy. However, after I wrote the
article, I noticed that in some cases these functions were not working exactly as intended. I
rewrote them a little bit to overcome these issues. You can find the code I used right below. I won’t
explain it too much, because you can read the details in the previous blog post. However, should
something be unclear, just drop me an email or a tweet!&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This functions will be used within the next functions to extract the relevant pieces

extractor &amp;lt;- function(string, regex, all = FALSE){
    if(all) {
        string %&amp;gt;%
            str_extract_all(regex) %&amp;gt;%
            flatten_chr() %&amp;gt;%
            str_remove_all(&amp;quot;=|\\\&amp;quot;&amp;quot;) %&amp;gt;%
            #str_extract_all(&amp;quot;[:alnum:]+|.|,|\\?|!&amp;quot;, simplify = FALSE) %&amp;gt;%
            map(paste, collapse = &amp;quot;&amp;quot;) %&amp;gt;%
            flatten_chr()
    } else {
        string %&amp;gt;%
            str_extract(regex) %&amp;gt;%
            str_remove_all(&amp;quot;=|\\\&amp;quot;&amp;quot;) %&amp;gt;%
            #str_extract_all(&amp;quot;[:alnum:]+|.|,|\\?|!&amp;quot;, simplify = TRUE) %&amp;gt;%
            paste(collapse = &amp;quot; &amp;quot;) %&amp;gt;%
            tolower()
    }
}

# This function extracts the data from the METS files, and returns a tibble:

extract_mets &amp;lt;- function(article){
    id &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=ID)(.*?)(?=LABEL)&amp;quot;)

    label &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=LABEL)(.*?)(?=TYPE)&amp;quot;)

    type &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=TYPE)(.*?)(?=&amp;gt;)&amp;quot;)

    begins &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=BEGIN)(.*?)(?=BETYPE)&amp;quot;, all = TRUE)

    tibble::tribble(~label, ~type, ~begins, ~id,
                    label, type, begins, id) %&amp;gt;%
        unnest()
}

# This function extracts the data from the ALTO files, and also returns a tibble:

extract_alto &amp;lt;- function(article){
    begins &amp;lt;- article[1] %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=^ID)(.*?)(?=HPOS)&amp;quot;, all = TRUE)

    content &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=CONTENT)(.*?)(?=WC)&amp;quot;, all = TRUE)

    tibble::tribble(~begins, ~content,
                    begins, content) %&amp;gt;%
        unnest()
}

# This function takes the path to a page as an argument, and extracts the data from 
# each article using the function defined above. It then writes a flat CSV to disk.

alto_csv &amp;lt;- function(page_path){

    page &amp;lt;- read_file(page_path)

    doc_name &amp;lt;- str_extract(page_path, &amp;quot;(?&amp;lt;=/text/).*&amp;quot;)

    alto_articles &amp;lt;- page %&amp;gt;%
        str_split(&amp;quot;TextBlock &amp;quot;) %&amp;gt;%
        flatten_chr()

    alto_df &amp;lt;- map_df(alto_articles, extract_alto)

    alto_df &amp;lt;- alto_df %&amp;gt;%
        mutate(document = doc_name)

    write_csv(alto_df, paste0(page_path, &amp;quot;.csv&amp;quot;))
}

# Same as above, but for the METS file:

mets_csv &amp;lt;- function(page_path){

    page &amp;lt;- read_file(page_path)

    doc_name &amp;lt;- str_extract(page_path, &amp;quot;(?&amp;lt;=/).*&amp;quot;)

    mets_articles &amp;lt;- page %&amp;gt;%
        str_split(&amp;quot;DMDID&amp;quot;) %&amp;gt;%
        flatten_chr()

    mets_df &amp;lt;- map_df(mets_articles, extract_mets)

    mets_df &amp;lt;- mets_df %&amp;gt;%
        mutate(document = doc_name)

    write_csv(mets_df, paste0(page_path, &amp;quot;.csv&amp;quot;))
}

# Time to use the above defined functions. First, let&amp;#39;s save the path of all the ALTO files
# into a list:

pages_alto &amp;lt;- str_match(list.files(path = &amp;quot;./&amp;quot;, all.files = TRUE, recursive = TRUE), &amp;quot;.*/text/.*.xml&amp;quot;) %&amp;gt;%
    discard(is.na)

# I use the {furrr} library to do the extraction in parallel, using 8 cores:

library(furrr)

plan(multiprocess, workers = 8)

tic &amp;lt;- Sys.time()
future_map(pages_alto, alto_csv)
toc &amp;lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins


# Same for the METS files:

pages_mets &amp;lt;- str_match(list.files(path = &amp;quot;./&amp;quot;, all.files = TRUE, recursive = TRUE), &amp;quot;.*mets.xml&amp;quot;) %&amp;gt;%
    discard(is.na)


library(furrr)

plan(multiprocess, workers = 8)

tic &amp;lt;- Sys.time()
future_map(pages_mets, mets_csv)
toc &amp;lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;If you want to try the above code for one ALTO and METS files, you can use the following lines
(use the download link in the beginning of the blog post to get the required data):&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mets &amp;lt;- read_file(&amp;quot;1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml&amp;quot;)

mets_articles2 &amp;lt;- mets %&amp;gt;%
    str_split(&amp;quot;DMDID&amp;quot;) %&amp;gt;%
    flatten_chr()


alto &amp;lt;- read_file(&amp;quot;1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml&amp;quot;)

alto_articles &amp;lt;- alto %&amp;gt;%
    str_split(&amp;quot;TextBlock &amp;quot;) %&amp;gt;%
    flatten_chr()

mets_df2 &amp;lt;- mets_articles2 %&amp;gt;%
    map_df(extract_mets)

# Same exercice for ALTO

alto_df &amp;lt;- alto_articles %&amp;gt;%
    map_df(extract_alto)&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-joining-the-data-and-the-metadata&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 2: Joining the data and the metadata&lt;/h3&gt;
&lt;p&gt;Now that I extracted the data from the ALTO files, and the metadata from the METS files, I still
need to join both data sets and do some cleaning. What is the goal of joining these two sources?
Remember, by doing this I will know which words come from which article, which will make things
much easier later on. I explain how the code works as comments in the code block below:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(udpipe)
library(textrank)
library(tidytext)

# First, I need the path to each folder that contains the ALTO and METS files. Each newspaper
# data is inside its own folder, one folder per publication. Inside, there&amp;#39;s `text` folder that
# contains the ALTO and METS files. This is also where I saved the .csv files from before.

pathdirs &amp;lt;- list.dirs(recursive = FALSE) %&amp;gt;%
    str_match(&amp;quot;.*lunion.*&amp;quot;) %&amp;gt;%
    discard(is.na)

# The following function imports the METS and the ALTO csv files, joins them, and does some 
# basic cleaning. I used a trick to detect German articles (even though L&amp;#39;Union is a French publication
# some articles are in German) and then remove them.

tidy_papers &amp;lt;- function(path){
    mets_path &amp;lt;- paste0(path, &amp;quot;/&amp;quot;, list.files(path, &amp;quot;.*.xml.csv&amp;quot;))
    mets_csv &amp;lt;- data.table::fread(mets_path)

    alto_path &amp;lt;- paste0(path, &amp;quot;/text/&amp;quot;, list.files(paste0(path, &amp;quot;/text/&amp;quot;), &amp;quot;.*.csv&amp;quot;))
    alto_csv &amp;lt;- map_dfr(alto_path, data.table::fread)

    final &amp;lt;- full_join(alto_csv, mets_csv, by = &amp;quot;begins&amp;quot;) %&amp;gt;%
        mutate(content = tolower(content)) %&amp;gt;%
        mutate(content = if_else(str_detect(content, &amp;quot;hyppart1&amp;quot;), str_extract_all(content, &amp;quot;(?&amp;lt;=CONTENT_).*&amp;quot;, simplify = TRUE), content)) %&amp;gt;%
        mutate(content = if_else(str_detect(content, &amp;quot;hyppart2&amp;quot;), NA_character_, content)) %&amp;gt;%
        # When words are separated by a hyphen and split over two lines, it looks like this in the data.
        # ex SUBS_TYPEHypPart1 SUBS_CONTENTexceptée
        # ceptée SUBS_TYPEHypPart2 SUBS_CONTENTexceptée
        # Here, the word `exceptée` is split over two lines, so using a regular expression, I keep
        # the string `exceptée`, which comes after the string `CONTENT`,  from the first line and 
        # replace the second line by an NA_character_
        mutate(content = if_else(str_detect(content, &amp;quot;superscript&amp;quot;), NA_character_, content)) %&amp;gt;%
        mutate(content = if_else(str_detect(content, &amp;quot;subscript&amp;quot;), NA_character_, content)) %&amp;gt;%
        filter(!is.na(content)) %&amp;gt;%
        filter(type == &amp;quot;article&amp;quot;) %&amp;gt;%
        group_by(id) %&amp;gt;%
        nest %&amp;gt;%
        # Below I create a list column with all the content of the article in a single string.
        mutate(article_text = map(data, ~paste(.$content, collapse = &amp;quot; &amp;quot;))) %&amp;gt;%
        mutate(article_text = as.character(article_text)) %&amp;gt;%
        # Detecting and removing german articles
        mutate(german = str_detect(article_text, &amp;quot;wenn|wird|und&amp;quot;)) %&amp;gt;%
        filter(german == FALSE) %&amp;gt;%
        select(-german) %&amp;gt;%
        # Finally, creating the label of the article (the title), and removing things that are 
        # not articles, such as the daily feuilleton.
        mutate(label = map(data, ~`[`(.$label, 1))) %&amp;gt;%
        filter(!str_detect(label, &amp;quot;embranchement|ligne|bourse|abonnés|feuilleton&amp;quot;)) %&amp;gt;%
        filter(label != &amp;quot;na&amp;quot;)

    # Save the data in the rds format, as it is not a flat file
    saveRDS(final, paste0(path, &amp;quot;/&amp;quot;, str_sub(path, 11, -1), &amp;quot;.rds&amp;quot;))
}

# Here again, I do this in parallel

library(furrr)

plan(multiprocess, workers = 8)

future_map(pathdirs, tidy_papers)&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;This is how one of these files looks like, after passing through this function:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/articles_rds.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;One line is one article. The first column is the id of the article, the second column contains
a data frame, the text of the article and finally the title of the article.
Let’s take a look at the content of the first element of the &lt;em&gt;data&lt;/em&gt; column:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/merged_alto_mets.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;This is the result of the merger of the METS and ALTO csv files. The first column is the id of the
article, the second column contains each individual word of the article, the &lt;em&gt;label&lt;/em&gt; column the
label, or title of the article.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-part-of-speech-annotation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 3: Part-of-speech annotation&lt;/h3&gt;
&lt;p&gt;Part-of-speech annotation is a technique with the aim of assigning to each word its part of speech.
Basically, Pos annotation tells us whether a word is a verb, a noun, an adjective… This will
be quite useful for the analysis. To perform Pos annotation, you need to install the &lt;code&gt;{udpipe}&lt;/code&gt;
package, and download the pre-trained model for the language you want to annotate, in my case French:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Only run this once. This downloads the model for French
udpipe_download_model(language = &amp;quot;french&amp;quot;)

# Load the model
udmodel_french &amp;lt;- udpipe_load_model(file = &amp;#39;french-gsd-ud-2.3-181115.udpipe&amp;#39;)

# Save the path of the files to annotate in a list:
pathrds &amp;lt;- list.files(path = &amp;quot;./&amp;quot;, all.files = TRUE, recursive = TRUE) %&amp;gt;% 
  str_match(&amp;quot;.*.rds&amp;quot;) %&amp;gt;%
  discard(is.na)

annotate_rds &amp;lt;- function(path, udmodel){

    newspaper &amp;lt;- readRDS(path)

    s &amp;lt;- udpipe_annotate(udmodel, newspaper$article_text, doc_id = newspaper$label)
    x &amp;lt;- data.frame(s)

    saveRDS(x, str_replace(path, &amp;quot;.rds&amp;quot;, &amp;quot;_annotated.rds&amp;quot;))
}

library(furrr)
plan(multiprocess, workers = 8)
tic &amp;lt;- Sys.time()
future_map(pathrds, annotate_rds, udmodel = udmodel_french)
toc &amp;lt;- Sys.time()
toc - tic&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;And here is the result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/pos_article.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;upos&lt;/em&gt; column contains the tags. Now I know which words are nouns, verbs, adjectives, stopwords…
Meaning that I can easily focus on the type of words that interest me. Plus, as an added benefit, I
can focus on the lemma of the words. For example, the word &lt;em&gt;viennent&lt;/em&gt;, is the
&lt;a href=&#34;https://en.wikipedia.org/wiki/French_conjugation&#34;&gt;conjugated&lt;/a&gt; form of the verb &lt;em&gt;venir&lt;/em&gt;. &lt;em&gt;venir&lt;/em&gt; is
thus the lemma of &lt;em&gt;viennent&lt;/em&gt;. This means that I can focus my analysis on lemmata. This is useful,
because if I compute the frequency of words, &lt;em&gt;viennent&lt;/em&gt; would be different from &lt;em&gt;venir&lt;/em&gt;, which is
not really what we want.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-tf-idf&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 4: tf-idf&lt;/h3&gt;
&lt;p&gt;Just like what I did in my &lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-04-newspapers/&#34;&gt;first blog post&lt;/a&gt;,
I compute the tf-idf of words. The difference, is that here the “document” is the article. This means
that I will get the most frequent words inside each article, but who are at the same time rare
in the other articles. Doing this ensures that I will only get very relevant words for each article.&lt;/p&gt;
&lt;p&gt;In the lines below, I prepare the data to then make the plots. The files that are created using
the code below are available in the following &lt;a href=&#34;https://github.com/b-rodrigues/newspapers_shinyapp/tree/master/tf_idf_data&#34;&gt;Github link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the Shiny app, I read the data directly from the repo. This way, I can keep the app small in size.&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path_annotatedrds &amp;lt;- list.files(path = &amp;quot;./&amp;quot;, all.files = TRUE, recursive = TRUE) %&amp;gt;% str_match(&amp;quot;.*_annotated.rds&amp;quot;) %&amp;gt;%
    discard(is.na)

prepare_tf_idf &amp;lt;- function(path){

    annotated_newspaper &amp;lt;- readRDS(path)

    tf_idf_data &amp;lt;- annotated_newspaper %&amp;gt;%
        filter(upos %in% c(&amp;quot;NOUN&amp;quot;, &amp;quot;VERB&amp;quot;, &amp;quot;ADJ&amp;quot;, &amp;quot;PROPN&amp;quot;)) %&amp;gt;%
        filter(nchar(lemma) &amp;gt; 3) %&amp;gt;%
        count(doc_id, lemma) %&amp;gt;%
        bind_tf_idf(lemma, doc_id, n) %&amp;gt;%
        arrange(desc(tf_idf)) %&amp;gt;%
        group_by(doc_id)

    name_tf_idf_data &amp;lt;- str_split(path, &amp;quot;/&amp;quot;, simplify = 1)[1] %&amp;gt;%
        paste0(&amp;quot;_tf_idf_data.rds&amp;quot;)  %&amp;gt;%
        str_sub(start = 9, -1)

    saveRDS(tf_idf_data, paste0(&amp;quot;tf_idf_data/&amp;quot;, name_tf_idf_data))
}

library(furrr)
plan(multiprocess, workers = 8)

future_map(path_annotatedrds, prepare_tf_idf)&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-summarizing-articles-by-extracting-the-most-relevant-sentences-using-textrank&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Step 5: Summarizing articles by extracting the most relevant sentences, using &lt;code&gt;{textrank}&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;The last step in data preparation is to extract the most relevant sentences of each articles, using
the &lt;code&gt;{textrank}&lt;/code&gt; package. This packages implements the &lt;em&gt;PageRank&lt;/em&gt; algorithm developed by Larry Page
and Sergey Brin in 1995. This algorithm ranks pages by the number of links that point to the pages;
the most popular and important pages are also the ones with more links to them. A similar approach
is used by the implementation of &lt;code&gt;{textrank}&lt;/code&gt;. The algorithm is explained in detail in the following
&lt;a href=&#34;https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, I cannot simply apply &lt;code&gt;{textrank}&lt;/code&gt; to the annotated data frame as it is. Because I have
several articles, I have to run the &lt;code&gt;textrank_sentences()&lt;/code&gt; function, which extracts the relevant
sentences, article by article. For this I still need to transform the data set and also need to
prepare the data in a way that makes it digestible by the function. I will not explain the code
below line by line, since the documentation of the package is quite straightforward. However,
keep in mind that I have to run the &lt;code&gt;textrank_sentences()&lt;/code&gt; function for each article, which explains
that as some point I use the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group_by(doc_id) %&amp;gt;%
    nest() %&amp;gt;%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which then makes it easy to work by article (&lt;em&gt;doc_id&lt;/em&gt; is the id of the articles). This part is
definitely the most complex, so if you’re interested in the methodology described here, really
take your time to understand this function. Let me know if I can clarify things!&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(textrank)
library(brotools)

path_annotatedrds &amp;lt;- list.files(path = &amp;quot;./&amp;quot;, all.files = TRUE, recursive = TRUE) %&amp;gt;% str_match(&amp;quot;.*_annotated.rds&amp;quot;) %&amp;gt;%
    discard(is.na)

prepare_textrank &amp;lt;- function(path){

    annotated_newspaper &amp;lt;- readRDS(path)

    # sentences summary
    x_text_rank &amp;lt;- annotated_newspaper %&amp;gt;%
        group_by(doc_id) %&amp;gt;%
        nest() %&amp;gt;%
        mutate(textrank_id = map(data, ~unique_identifier(., c(&amp;quot;paragraph_id&amp;quot;, &amp;quot;sentence_id&amp;quot;)))) %&amp;gt;%
        mutate(cleaned = map2(.x = data, .y = textrank_id, ~cbind(.x, &amp;quot;textrank_id&amp;quot; = .y))) %&amp;gt;%
        select(doc_id, cleaned)

    x_text_rank2 &amp;lt;- x_text_rank %&amp;gt;%
        mutate(sentences = map(cleaned, ~select(., textrank_id, sentence))) %&amp;gt;%
        # one_row() is a function from my own package, which eliminates duplicates rows
        # from a data frame
        mutate(sentences = map(sentences, ~one_row(., c(&amp;quot;textrank_id&amp;quot;, &amp;quot;sentence&amp;quot;))))

    x_terminology &amp;lt;- x_text_rank %&amp;gt;%
        mutate(terminology = map(cleaned, ~filter(., upos %in% c(&amp;quot;NOUN&amp;quot;, &amp;quot;ADJ&amp;quot;)))) %&amp;gt;%
        mutate(terminology = map(terminology, ~select(., textrank_id, &amp;quot;lemma&amp;quot;))) %&amp;gt;%
        select(terminology)

    x_final &amp;lt;- bind_cols(x_text_rank2, x_terminology)

    possibly_textrank_sentences &amp;lt;- possibly(textrank_sentences, otherwise = NULL)

    x_final &amp;lt;- x_final %&amp;gt;%
        mutate(summary = map2(sentences, terminology, possibly_textrank_sentences)) %&amp;gt;%
        select(doc_id, summary)

    name_textrank_data &amp;lt;- str_split(path, &amp;quot;/&amp;quot;, simplify = 1)[1] %&amp;gt;%
        paste0(&amp;quot;_textrank_data.rds&amp;quot;) %&amp;gt;%
        str_sub(start = 9, -1)

    saveRDS(x_final, paste0(&amp;quot;textrank_data/&amp;quot;, name_textrank_data))
}

library(furrr)
plan(multiprocess, workers = 8)

future_map(path_annotatedrds, prepare_textrank)&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;You can download the annotated data sets from the following
&lt;a href=&#34;https://github.com/b-rodrigues/newspapers_shinyapp/tree/master/textrank_data&#34;&gt;link&lt;/a&gt;. This is how
the data looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/textrank_df.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Using the &lt;code&gt;summary()&lt;/code&gt; function on an element of the &lt;em&gt;summary&lt;/em&gt; column returns the 5 most relevant
sentences as extracted by &lt;code&gt;{textrank}&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-2-building-the-shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Part 2: Building the shiny app&lt;/h2&gt;
&lt;p&gt;The most difficult parts are behind us! Building a dashboard is quite easy thanks to the &lt;code&gt;{flexdashboard}&lt;/code&gt;
package. You need to know Markdown and some Shiny, but it’s way easier than building a complete
Shiny app. First of all, install the &lt;code&gt;{fleshdashboard}&lt;/code&gt; package, and start from a template, or
from &lt;a href=&#34;https://rmarkdown.rstudio.com/flexdashboard/layouts.html&#34;&gt;this list of layouts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I think that the only trick worth mentioning is that I put the data in a Github repo, and read it
directly from the Shiny app. Users choose a date, which I save in a reactive variable. I then
build the right url that points towards the right data set, and read it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path_tf_idf &amp;lt;- reactive({
    paste0(&amp;quot;https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_&amp;quot;, as.character(input$date2), &amp;quot;_tf_idf_data.rds&amp;quot;)
})

dfInput &amp;lt;- reactive({
        read_rds(url(path_tf_idf())) %&amp;gt;%
        top_n(as.numeric(input$tf_df_words), tf_idf) %&amp;gt;%
        mutate(word = reorder(lemma, tf_idf)) 
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because I did all the computations beforehand, the app simply reads the data and creates the bar
plots for the tf-idf data, or prints the sentences for the textrank data. To print the sentences
correcly, I had to use some html tags, using the &lt;code&gt;{htmltools}&lt;/code&gt; package. Below you can find the
source code of the app:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the code&lt;/summary&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;Exploring 10 years of daily publications of the Luxembourguish newspaper, *L&amp;#39;Union*&amp;quot;
output: 
  flexdashboard::flex_dashboard:
    theme: yeti
    orientation: columns
    vertical_layout: fill
runtime: shiny
---

`` `{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(tidyverse)
library(textrank)
library(tidytext)
library(udpipe)
library(plotly)
library(ggthemes)
`` `

Sidebar {.sidebar}
=====================================

`` `{r}
dateInput(&amp;#39;date2&amp;#39;,
      label = paste(&amp;#39;Select date&amp;#39;),
      value = as.character(as.Date(&amp;quot;1860-11-14&amp;quot;)),
      min = as.Date(&amp;quot;1860-11-12&amp;quot;), max = as.Date(&amp;quot;1869-12-31&amp;quot;),
      format = &amp;quot;yyyy/mm/dd&amp;quot;,
      startview = &amp;#39;year&amp;#39;, language = &amp;#39;en-GB&amp;#39;, weekstart = 1
    )
selectInput(inputId = &amp;quot;tf_df_words&amp;quot;, 
            label = &amp;quot;Select number of unique words for tf-idf&amp;quot;, 
            choices = seq(1:10),
            selected = 5)
selectInput(inputId = &amp;quot;textrank_n_sentences&amp;quot;, 
            label = &amp;quot;Select the number of sentences for the summary of the article&amp;quot;, 
            choices = seq(1:20), 
            selected = 5)
`` `

*The BnL has digitised over 800.000 pages of Luxembourg newspapers. From those, more than 700.000 
pages have rich metadata using international XML standards such as METS and ALTO. 
Multiple datasets are available for download. Each one is of different size and contains different
newspapers. All the digitised material can also be found on our search platform a-z.lu 
(Make sure to filter by “eluxemburgensia”). All datasets contain XML (METS + ALTO), PDF, original 
TIFF and PNG files for every newspaper issue.* 
Source: https://data.bnl.lu/data/historical-newspapers/

This Shiny app allows you to get summaries of the 10 years of daily issues of the &amp;quot;L&amp;#39;Union&amp;quot; newspaper.
In the first tab, a simple word frequency per article is shown, using the tf-idf method. In the 
second tab, summary sentences have been extracted using the `{textrank}` package.


Word frequency per article
===================================== 
Row
-----------------------------------------------------------------------

### Note: there might be days without any publication. In case of an error, select another date.
    
`` `{r}
path_tf_idf &amp;lt;- reactive({
    paste0(&amp;quot;https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/tf_idf_data/newspaper_lunion_&amp;quot;, as.character(input$date2), &amp;quot;_tf_idf_data.rds&amp;quot;)
})
dfInput &amp;lt;- reactive({
        read_rds(url(path_tf_idf())) %&amp;gt;%
        top_n(as.numeric(input$tf_df_words), tf_idf) %&amp;gt;%
        mutate(word = reorder(lemma, tf_idf)) 
})
renderPlotly({
    df_tf_idf &amp;lt;- dfInput()
    p1 &amp;lt;- ggplot(df_tf_idf,
                 aes(word, tf_idf)) +
                 geom_col(show.legend = FALSE, fill = &amp;quot;#82518c&amp;quot;) +
                 labs(x = NULL, y = &amp;quot;tf-doc_idf&amp;quot;) +
                 facet_wrap(~doc_id, ncol = 2, scales = &amp;quot;free&amp;quot;) +
                 coord_flip() +
                 theme_dark()
    ggplotly(p1)
})
`` `

Summary of articles {data-orientation=rows}
===================================== 
Row 
-----------------------------------------------------------------------

### The sentence in bold is the title of the article. You can show more sentences in the summary by using the input in the sidebar.
    
`` `{r}
print_summary_textrank &amp;lt;- function(doc_id, summary, n_sentences){
    htmltools::HTML(paste0(&amp;quot;&amp;lt;b&amp;gt;&amp;quot;, doc_id, &amp;quot;&amp;lt;/b&amp;gt;&amp;quot;), paste(&amp;quot;&amp;lt;p&amp;gt;&amp;quot;, summary(summary, n_sentences), sep = &amp;quot;&amp;quot;, collapse = &amp;quot;&amp;lt;br/&amp;gt;&amp;quot;), &amp;quot;&amp;lt;/p&amp;gt;&amp;quot;)
}
path_textrank &amp;lt;- reactive({
    paste0(&amp;quot;https://raw.githubusercontent.com/b-rodrigues/newspapers_shinyapp/master/textrank_data/newspaper_lunion_&amp;quot;, as.character(input$date2), &amp;quot;_textrank_data.rds&amp;quot;)
})
dfInput2 &amp;lt;- reactive({
        read_rds(url(path_textrank()))
})
renderUI({
    df_textrank &amp;lt;- dfInput2()
    
df_textrank &amp;lt;- df_textrank %&amp;gt;% 
    mutate(to_print = map2(doc_id, summary, print_summary_textrank, n_sentences = as.numeric(input$textrank_n_sentences)))
df_textrank$to_print
})
`` `
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;I host the app on Shinyapps.io, which is really easy to do from within Rstudio.&lt;/p&gt;
&lt;p&gt;That was quite long, I’m not sure that anyone will read this blog post completely, but oh well.
Better to put the code online, might help someone one day, that leave it to rot on my hard drive.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using Data Science to read 10 years of Luxembourguish newspapers from the 19th century</title>
      <link>/blog/2019-01-31-newspapers_shiny_app/</link>
      <pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-31-newspapers_shiny_app/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://brodriguesco.shinyapps.io/newspapers_app/&#34;&gt;
&lt;img src=&#34;/img/tf_idf.png&#34; title = &#34;Click here to go the app&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I have been playing around with historical newspaper data (see
&lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-04-newspapers/&#34;&gt;here&lt;/a&gt; and
&lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-13-newspapers_mets_alto/&#34;&gt;here&lt;/a&gt;). I have extracted the
data from the largest archive available, as described in the previous blog post, and now created
a shiny dashboard where it is possible to visualize the most common words per article, as well as
read a summary of each article.
The summary was made using a method called &lt;em&gt;textrank&lt;/em&gt;, using the &lt;code&gt;{textrank}&lt;/code&gt; package, which extracts
relevant sentences using the Pagerank (developed by Google) algorithm. You can read the scientific
paper &lt;a href=&#34;https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf&#34;&gt;here&lt;/a&gt; for more info.&lt;/p&gt;
&lt;p&gt;You can play around with the app by clicking &lt;a href=&#34;https://brodriguesco.shinyapps.io/newspapers_app/&#34;&gt;here&lt;/a&gt;.
In the next blog post, I will explain how I created the app, step by step. It’s going to be a long blog post!&lt;/p&gt;
&lt;p&gt;Using the app, I noticed that some war happened around November 1860 in China, which turned out to
be the &lt;a href=&#34;https://en.wikipedia.org/wiki/Second_Opium_War&#34;&gt;Second Opium War&lt;/a&gt;. The war actually ended
in October 1860, but apparently the news took several months to travel to Europe.&lt;/p&gt;
&lt;p&gt;I also learned that already in the 1861, there was public transportation between some Luxembourguish villages,
and French villages that were by the border (see the publication from the 17th of December 1861).&lt;/p&gt;
&lt;p&gt;Let me know if you find about historical events using my app!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making sense of the METS and ALTO XML standards</title>
      <link>/blog/2019-01-13-newspapers_mets_alto/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-13-newspapers_mets_alto/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=V1qpvpH26fo&#34;&gt;
&lt;img src=&#34;/img/union.png&#34; title = &#34;The 19th century was a tough place&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Last week I wrote a &lt;a href=&#34;https://www.brodrigues.co/blog/2019-01-04-newspapers/&#34;&gt;blog post&lt;/a&gt; where I analyzed
one year of newspapers ads from 19th century newspapers. The data is made available by the
&lt;a href=&#34;https://data.bnl.lu/data/historical-newspapers/&#34;&gt;national library of Luxembourg&lt;/a&gt;.
In this blog post, which is part 1 of a 2 part series, I extract data from the 257gb archive, which
contains 10 years of publications of the &lt;em&gt;L’Union&lt;/em&gt;, another 19th century Luxembourguish newspaper
written in French. As I explained in the previous post, to make life easier to data scientists,
the national library also included ALTO and METS files (which are a XML files used to
describe the layout and contents of physical text sources, such as pages of a book or newspaper)
which can be easily parsed by R.&lt;/p&gt;
&lt;p&gt;This is how a ALTO file looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/alto.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Each page of the newspaper of a given day has one ALTO file.
This is how a METS file looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/mets.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;For each daily issue of the newspaper, there is a METS file. So 1 METS file for 4 ALTO files.&lt;/p&gt;
&lt;p&gt;In my last blog post, I only extracted the words from the ALTO file (red rectangles of the first
screenshot) and did not touch the METS file.
The problem of doing this is that I get all the words for each page, without knowing which
come from the same article. If I want to know which words come from the same article, I need to use
the info from the METS file. From the METS file I have the ID of the article, and some other
metadata, such as the title of the article and the type of the article (which can be &lt;em&gt;article&lt;/em&gt;,
&lt;em&gt;advertisement&lt;/em&gt;, etc). The information highlighted with the green rectangles in the METS file
can be linked to the green rectangles from the ALTO files. My goal is to get the following data
frame from the METS file:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/mets_df.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;and this data frame from the ALTO files:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/alto_df.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;As you can see, by combining both data frames I can know which words come from the same article,
which will be helpful for further analysis.
&lt;a href=&#34;https://en.wikipedia.org/wiki/1860s&#34;&gt;A lot of things happened in the 1860s.&lt;/a&gt;
I am really curious to see if and how these events where reported in a Luxembourguish newspaper.
I am particularly curious about how long it took to report certain news from far away, such as the
assassination of Abraham Lincoln. But before that I need to extract the data!&lt;/p&gt;
&lt;p&gt;I will only focus on the METS file. The logic for the ALTO file is the same. All the source code
will be in the appendix of this blog post.&lt;/p&gt;
&lt;p&gt;First, let’s take a look at a METS file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
mets &amp;lt;- read_file(&amp;quot;1533660_newspaper_lunion_1860-11-14/1533660_newspaper_lunion_1860-11-14-mets.xml&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how it looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;&amp;lt;?xml version=&amp;#39;1.0&amp;#39; encoding=&amp;#39;utf-8&amp;#39;?&amp;gt;\r\n&amp;lt;mets xmlns=\&amp;quot;http://www.loc.gov/METS/\&amp;quot; xmlns:mix=\&amp;quot;http://www.loc.gov/mix/v20\&amp;quot; xmlns:mods=\&amp;quot;http://www.loc.gov/mods/v3\&amp;quot; xmlns:xlink=\&amp;quot;http://www.w3.org/1999/xlink\&amp;quot; xmlns:xsi=\&amp;quot;http://www.w3.org/2001/XMLSchema-instance\&amp;quot; LABEL=\&amp;quot;L&amp;#39;UNION. 1860-11-14_01\&amp;quot; OBJID=\&amp;quot;https://persist.lu/ark:/70795/m62fcm\&amp;quot; TYPE=\&amp;quot;Newspaper\&amp;quot; xsi:schemaLocation=\&amp;quot;http://www.loc.gov/METS/ http://www.loc.gov/standards/mets/mets.xsd http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-6.xsd http://www.loc.gov/mix/v20 http://www.loc.gov/standards/mix/mix.xsd\&amp;quot;&amp;gt;\r\n  &amp;lt;metsHdr CREATEDATE=\&amp;quot;2010-12-03T20:35:05\&amp;quot; LASTMODDATE=\&amp;quot;2018-05-09T05:35:51Z\&amp;quot;&amp;gt;\r\n    &amp;lt;agent OTHERTYPE=\&amp;quot;SOFTWARE\&amp;quot; ROLE=\&amp;quot;CREATOR\&amp;quot; TYPE=\&amp;quot;OTHER\&amp;quot;&amp;gt;\r\n      &amp;lt;name&amp;gt;CCS docWORKS/METAe Version 6.4-3&amp;lt;/name&amp;gt;\r\n      &amp;lt;note&amp;gt;docWORKS-ID: 101636&amp;lt;/note&amp;gt;\r\n    &amp;lt;/agent&amp;gt;\r\n  &amp;lt;/metsHdr&amp;gt;\r\n  &amp;lt;dmdSec ID=\&amp;quot;MODSMD_COLLECTION\&amp;quot;&amp;gt;\r\n    &amp;lt;mdWrap LABEL=\&amp;quot;Bibliographic meta-data of the collection\&amp;quot; MDTYPE=\&amp;quot;MODS\&amp;quot; MIMETYPE=\&amp;quot;text/xml\&amp;quot;&amp;gt;\r\n      &amp;lt;xmlData&amp;gt;\r\n        &amp;lt;mods:mods&amp;gt;\r\n          &amp;lt;mods:identifier type=\&amp;quot;local\&amp;quot;&amp;gt;lunion&amp;lt;/mods:identifier&amp;gt;\r\n          &amp;lt;mods:titleInfo ID=\&amp;quot;MODSMD_COLLECTION_TI1\&amp;quot; xml:lang=\&amp;quot;fr\&amp;quot;&amp;gt;\r\n            &amp;lt;mods:title&amp;gt;L&amp;#39;UNION.&amp;lt;/mods:title&amp;gt;\r\n          &amp;lt;/mods:titleInfo&amp;gt;\r\n        &amp;lt;/mods:mods&amp;gt;\r\n      &amp;lt;/xmlData&amp;gt;\r\n    &amp;lt;/mdWrap&amp;gt;\r\n  &amp;lt;/dmdSec&amp;gt;\r\n  &amp;lt;dmdSec ID=\&amp;quot;MODSMD_SECTION1\&amp;quot;&amp;gt;\r\n    &amp;lt;mdWrap MDTYPE=\&amp;quot;MODS\&amp;quot; MIMETYPE=\&amp;quot;text/xml\&amp;quot;&amp;gt;\r\n      &amp;lt;xmlData&amp;gt;\r\n        &amp;lt;mods:mods&amp;gt;\r\n          &amp;lt;mods:titleInfo ID=\&amp;quot;MODSMD_SECTION1_TI1\&amp;quot; xml:lang=\&amp;quot;fr\&amp;quot;&amp;gt;\r\n            &amp;lt;mods:title&amp;gt;Chemins de fer. — Service d&amp;#39;hiver.&amp;lt;/mods:title&amp;gt;\r\n          &amp;lt;/mods:titleInfo&amp;gt;\r\n          &amp;lt;mods:language&amp;gt;\r\n            &amp;lt;mods:languageTerm authority=\&amp;quot;rfc3066\&amp;quot; type=\&amp;quot;code\&amp;quot;&amp;gt;fr&amp;lt;/mods:languageTerm&amp;gt;\r\n ....&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As usual when you import text files like this, it’s always a good idea to split the file. I will
split at the &lt;code&gt;&#34;DMDID&#34;&lt;/code&gt; character. Take a look back at the second screenshot. The very first tag,
first row, first word after &lt;code&gt;div&lt;/code&gt; is &lt;code&gt;&#34;DMDID&#34;&lt;/code&gt;. By splitting at this level, I will get back a list,
where each element is the content of this &lt;code&gt;div DMDID&lt;/code&gt; block. This is exactly what I need, since
this block contains the information from the green rectangles.
So let’s split the &lt;code&gt;mets&lt;/code&gt; variable at this level:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mets_articles &amp;lt;- mets %&amp;gt;%
    str_split(&amp;quot;DMDID&amp;quot;) %&amp;gt;%
    flatten_chr()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;mets_articles&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(mets_articles)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; chr [1:25] &amp;quot;&amp;lt;?xml version=&amp;#39;1.0&amp;#39; encoding=&amp;#39;utf-8&amp;#39;?&amp;gt;\r\n&amp;lt;mets xmlns=\&amp;quot;http://www.loc.gov/METS/\&amp;quot; xmlns:mix=\&amp;quot;http://www.loc.g&amp;quot;| __truncated__ ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Doesn’t seem to be very helpful, but actually it is. We can see that &lt;code&gt;mets_articles&lt;/code&gt; is a now a list
of 25 elements.&lt;/p&gt;
&lt;p&gt;This means that for each element of &lt;code&gt;mets_articles&lt;/code&gt;, I need to get the identifier, the label, the type
(the red rectangles from the screenshot), but also the information from the &lt;code&gt;&#34;BEGIN&#34;&lt;/code&gt; element (the green
rectangle).&lt;/p&gt;
&lt;p&gt;To do this, I’ll be using regular expressions. In general, I start by experimenting in the console,
and then when things start looking good, I write a function. Here is this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extractor &amp;lt;- function(string, regex, all = FALSE){
    if(all) {
        string %&amp;gt;%
            str_extract_all(regex) %&amp;gt;%
            flatten_chr() %&amp;gt;%
            str_extract_all(&amp;quot;[:alnum:]+&amp;quot;, simplify = FALSE) %&amp;gt;%
            map(paste, collapse = &amp;quot;_&amp;quot;) %&amp;gt;%
            flatten_chr()
    } else {
        string %&amp;gt;%
            str_extract(regex) %&amp;gt;%
            str_extract_all(&amp;quot;[:alnum:]+&amp;quot;, simplify = TRUE) %&amp;gt;%
            paste(collapse = &amp;quot; &amp;quot;) %&amp;gt;%
            tolower()
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function may seem complicated, but it simply encapsulates some pretty standard steps to get
the data I need. I had to consider two cases. The first case is when I need to extract all the
elements with &lt;code&gt;str_extract_all()&lt;/code&gt;, or only the first occurrence, with &lt;code&gt;str_extract()&lt;/code&gt;.
Let’s test it on the first article of the &lt;code&gt;mets_articles&lt;/code&gt; list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mets_articles_1 &amp;lt;- mets_articles[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extractor(mets_articles_1, &amp;quot;ID&amp;quot;, all = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;id&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what happens with &lt;code&gt;all = TRUE&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extractor(mets_articles_1, &amp;quot;ID&amp;quot;, all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
##  [15] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
##  [29] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
##  [43] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
##  [57] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
##  [71] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
##  [85] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
##  [99] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
## [113] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;
## [127] &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot; &amp;quot;ID&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This seems to work as intended. Since I need to call this function several times, I’ll be writing
another function that extracts all I need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_mets &amp;lt;- function(article){

    id &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=ID)(.*?)(?=LABEL)&amp;quot;)

    label &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=LABEL)(.*?)(?=TYPE)&amp;quot;)

    type &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=TYPE)(.*?)(?=&amp;gt;)&amp;quot;)

    begins &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=BEGIN)(.*?)(?=BETYPE)&amp;quot;, all = TRUE)

    tibble::tribble(~label, ~type, ~begins, ~id,
                    label, type, begins, id) %&amp;gt;%
        unnest()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function uses complex regular expressions to extract the strings I need, and then puts
the result into a data frame, with the &lt;code&gt;tibble()&lt;/code&gt; function. I then use &lt;code&gt;unnest()&lt;/code&gt;, because &lt;code&gt;label&lt;/code&gt;,
&lt;code&gt;type&lt;/code&gt;, &lt;code&gt;begins&lt;/code&gt; and &lt;code&gt;id&lt;/code&gt; are not the same length. &lt;code&gt;label&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;id&lt;/code&gt; are of length 1, while
&lt;code&gt;begins&lt;/code&gt; is longer. This means that when I put them into a data frame it looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tribble(~a, ~b,
&amp;quot;a&amp;quot;, rep(&amp;quot;b&amp;quot;, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   a     b        
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;gt;   
## 1 a     &amp;lt;chr [4]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;unnest()&lt;/code&gt;, I get a nice data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tribble(~a, ~b,
&amp;quot;a&amp;quot;, rep(&amp;quot;b&amp;quot;, 4)) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   a     b    
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
## 1 a     b    
## 2 a     b    
## 3 a     b    
## 4 a     b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I simply need to map this function to all the files and that’s it! For this, I will write yet
another helper function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mets_csv &amp;lt;- function(page_path){
    
    page &amp;lt;- read_file(page_path)
    
    doc_name &amp;lt;- str_extract(page_path, &amp;quot;(?&amp;lt;=/).*&amp;quot;)
    
    mets_articles &amp;lt;- page %&amp;gt;%
        str_split(&amp;quot;DMDID&amp;quot;) %&amp;gt;%
        flatten_chr()
    
    mets_df &amp;lt;- map_df(mets_articles, extract_mets)
    
    mets_df &amp;lt;- mets_df %&amp;gt;%
        mutate(document = doc_name)
    
    write_csv(mets_df, paste0(page_path, &amp;quot;.csv&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function takes the path to a METS file as input, and processes it using the steps I explained
above. The only difference is that I add a column containing the name of the file that was processed,
and write the resulting data frame directly to disk as a data frame. Finally, I can map this function to all the METS
files:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract content from METS files

pages_mets &amp;lt;- str_match(list.files(path = &amp;quot;./&amp;quot;, all.files = TRUE, recursive = TRUE), &amp;quot;.*mets.xml&amp;quot;) %&amp;gt;%
    discard(is.na)

library(furrr)

plan(multiprocess, workers = 8)

tic &amp;lt;- Sys.time()
future_map(pages_mets, mets_csv)
toc &amp;lt;- Sys.time()

toc - tic&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I use &lt;code&gt;{furrr}&lt;/code&gt; to extract the data from all the files in parallel, by putting 8 cores of my CPU to
work. This took around 3 minutes and 20 seconds to finish.&lt;/p&gt;
&lt;p&gt;That’s it for now, stay tuned for part 2 where I will analyze this fresh data!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_alto &amp;lt;- function(article){
    begins &amp;lt;- article[1] %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=^ID)(.*?)(?=HPOS)&amp;quot;, all = TRUE)

    content &amp;lt;- article %&amp;gt;%
        extractor(&amp;quot;(?&amp;lt;=CONTENT)(.*?)(?=WC)&amp;quot;, all = TRUE)

    tibble::tribble(~begins, ~content,
                    begins, content) %&amp;gt;%
        unnest()
}

alto_csv &amp;lt;- function(page_path){

    page &amp;lt;- read_file(page_path)

    doc_name &amp;lt;- str_extract(page_path, &amp;quot;(?&amp;lt;=/text/).*&amp;quot;)

    alto_articles &amp;lt;- page %&amp;gt;%
        str_split(&amp;quot;TextBlock &amp;quot;) %&amp;gt;%
        flatten_chr()

    alto_df &amp;lt;- map_df(alto_articles, extract_alto)

    alto_df &amp;lt;- alto_df %&amp;gt;%
        mutate(document = doc_name)

    write_csv(alto_df, paste0(page_path, &amp;quot;.csv&amp;quot;))
}


alto &amp;lt;- read_file(&amp;quot;1533660_newspaper_lunion_1860-11-14/text/1860-11-14_01-00001.xml&amp;quot;)


# Extract content from alto files

pages_alto &amp;lt;- str_match(list.files(path = &amp;quot;./&amp;quot;, all.files = TRUE, recursive = TRUE), &amp;quot;.*/text/.*.xml&amp;quot;) %&amp;gt;%
    discard(is.na)


library(furrr)

plan(multiprocess, workers = 8)

tic &amp;lt;- Sys.time()
future_map(pages_alto, alto_csv)
toc &amp;lt;- Sys.time()

toc - tic

#Time difference of 18.64776 mins&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Looking into 19th century ads from a Luxembourguish newspaper with R</title>
      <link>/blog/2019-01-04-newspapers/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-04-newspapers/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0xzN6FM5x_E&#34;&gt;
&lt;img src=&#34;/img/Wales.jpg&#34; title = &#34;Sometimes ads are better than this. Especially if it&#39;s Flex Tape ® ads.&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;a href=&#34;https://data.bnl.lu/data/historical-newspapers/&#34;&gt;national library of Luxembourg&lt;/a&gt; published
some very interesting data sets; scans of historical newspapers! There are several data sets that
you can download, from 250mb up to 257gb. I decided to take a look at the 32gb “ML Starter Pack”.
It contains high quality scans of one year of the &lt;em&gt;L’indépendence Luxembourgeoise&lt;/em&gt; (Luxembourguish
independence) from the year 1877. To make life easier to data scientists, the national library
also included ALTO and METS files (which is a XML schema that is used to describe the layout and
contents of physical text sources, such as pages of a book or newspaper) which can be easily parsed
by R.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;L’indépendence Luxembourgeoise&lt;/em&gt; is quite interesting in that it is a Luxembourguish newspaper written
in French. Luxembourg always had 3 languages that were used in different situations, French, German
and Luxembourguish. Luxembourguish is the language people used (and still use) for day to day life
and to speak to their baker.
Historically however, it was not used for the press or in politics. Instead it was German that
was used for the press (or so I thought) and French in politics (only in
&lt;a href=&#34;http://legilux.public.lu/eli/etat/leg/loi/1984/02/24/n1/jo&#34;&gt;1984&lt;/a&gt; was Luxembourguish made
an official Language of Luxembourg).
It turns out however that &lt;em&gt;L’indépendence Luxembourgeoise&lt;/em&gt;, a daily newspaper that does not exist
anymore, was in French. This piqued my interest, and it also made analysis easier, for 2 reasons:
I first started with the &lt;em&gt;Luxemburger Wort&lt;/em&gt; (Luxembourg’s Word I guess would be a translation), which
still exists today, but which is in German. And at that time, German was written using the Fraktur
font, which makes it barely readable. Look at the alphabet in Fraktur:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;𝕬 𝕭 𝕮 𝕯 𝕰 𝕱 𝕲 𝕳 𝕴 𝕵 𝕶 𝕷 𝕸 𝕹 𝕺 𝕻 𝕼 𝕽 𝕾 𝕿 𝖀 𝖁 𝖂 𝖃 𝖄 𝖅
𝖆 𝖇 𝖈 𝖉 𝖊 𝖋 𝖌 𝖍 𝖎 𝖏 𝖐 𝖑 𝖒 𝖓 𝖔 𝖕 𝖖 𝖗 𝖘 𝖙 𝖚 𝖛 𝖜 𝖝 𝖞 𝖟&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s not like German is already hard enough, they had to invent the least readable font ever to write
German in, to make extra sure it would be hell to decipher.&lt;/p&gt;
&lt;p&gt;So basically I couldn’t be bothered to try to read a German newspaper in Fraktur. That’s when I noticed
the &lt;em&gt;L’indépendence Luxembourgeoise&lt;/em&gt;… A Luxembourguish newspaper? Written in French? Sounds
interesting.&lt;/p&gt;
&lt;p&gt;And oh boy. Interesting it was.&lt;/p&gt;
&lt;p&gt;19th century newspapers articles were something else. There’s this article for instance:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/pray%20for%20senators.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;For those of you that do not read French, this article relates that in France, the ministry of
justice required priests to include prayers on the Sunday that follows the start of the new season
of parliamentary discussions, in order for God to provide senators his help.&lt;/p&gt;
&lt;p&gt;There this gem too:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/tallest_soldier.jpg&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;This article presents the tallest soldier of the German army, called Emhke, and nominated by the
German Emperor himself to accompany him during his visit to Palestine. Emhke was 2.08 meters tall
and weighted 236 pounds (apparently at the time Luxembourg was not fully sold on the metric system).&lt;/p&gt;
&lt;p&gt;Anyway, I decided to take a look at ads. The last paper of this 4 page newspaper always contained
ads and other announcements. For example, there’s this ad for a pharmacy:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/pharmacy.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;that sells tea, and mineral water. Yes, tea and mineral water. In a pharmacy. Or this one:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/upside_down.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;which is literally upside down in the newspaper (the one from the 10th of April 1877). I don’t
know if it’s a mistake or if it’s a marketing ploy, but it did catch my attention, 140 years later,
so &lt;em&gt;bravo&lt;/em&gt;. This is an announcement made by a shop owner that wants to sell all his merchandise
for cheap, perhaps to make space for new stuff coming in?&lt;/p&gt;
&lt;p&gt;So I decided brush up on my natural language processing skills with R and do topic modeling on these ads.
The challenge here is that a single document, the 4th page of the newspaper, contains a lot of ads.
So it will probably be difficult to clearly isolate topics. But let’s try nonetheless.
First of all, let’s load all the &lt;code&gt;.xml&lt;/code&gt; files that contain the data. These files look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;TextLine ID=&amp;quot;LINE6&amp;quot; STYLEREFS=&amp;quot;TS11&amp;quot; HEIGHT=&amp;quot;42&amp;quot; WIDTH=&amp;quot;449&amp;quot; HPOS=&amp;quot;165&amp;quot; VPOS=&amp;quot;493&amp;quot;&amp;gt;
                                    &amp;lt;String ID=&amp;quot;S16&amp;quot; CONTENT=&amp;quot;l’après-midi,&amp;quot; WC=&amp;quot;0.638&amp;quot; CC=&amp;quot;0803367024653&amp;quot; HEIGHT=&amp;quot;42&amp;quot; WIDTH=&amp;quot;208&amp;quot; HPOS=&amp;quot;165&amp;quot; VPOS=&amp;quot;493&amp;quot;/&amp;gt;
                                    &amp;lt;SP ID=&amp;quot;SP11&amp;quot; WIDTH=&amp;quot;24&amp;quot; HPOS=&amp;quot;373&amp;quot; VPOS=&amp;quot;493&amp;quot;/&amp;gt;
                                    &amp;lt;String ID=&amp;quot;S17&amp;quot; CONTENT=&amp;quot;le&amp;quot; WC=&amp;quot;0.8&amp;quot; CC=&amp;quot;40&amp;quot; HEIGHT=&amp;quot;30&amp;quot; WIDTH=&amp;quot;29&amp;quot; HPOS=&amp;quot;397&amp;quot; VPOS=&amp;quot;497&amp;quot;/&amp;gt;
                                    &amp;lt;SP ID=&amp;quot;SP12&amp;quot; WIDTH=&amp;quot;14&amp;quot; HPOS=&amp;quot;426&amp;quot; VPOS=&amp;quot;497&amp;quot;/&amp;gt;
                                    &amp;lt;String ID=&amp;quot;S18&amp;quot; CONTENT=&amp;quot;Gouverne&amp;quot; WC=&amp;quot;0.638&amp;quot; CC=&amp;quot;72370460&amp;quot; HEIGHT=&amp;quot;31&amp;quot; WIDTH=&amp;quot;161&amp;quot; HPOS=&amp;quot;440&amp;quot; VPOS=&amp;quot;496&amp;quot; SUBS_TYPE=&amp;quot;HypPart1&amp;quot; SUBS_CONTENT=&amp;quot;Gouvernement&amp;quot;/&amp;gt;
                                    &amp;lt;HYP CONTENT=&amp;quot;-&amp;quot; WIDTH=&amp;quot;11&amp;quot; HPOS=&amp;quot;603&amp;quot; VPOS=&amp;quot;514&amp;quot;/&amp;gt;
                                  &amp;lt;/TextLine&amp;gt;
                        &amp;lt;TextLine ID=&amp;quot;LINE7&amp;quot; STYLEREFS=&amp;quot;TS11&amp;quot; HEIGHT=&amp;quot;41&amp;quot; WIDTH=&amp;quot;449&amp;quot; HPOS=&amp;quot;166&amp;quot; VPOS=&amp;quot;541&amp;quot;&amp;gt;
                                    &amp;lt;String ID=&amp;quot;S19&amp;quot; CONTENT=&amp;quot;ment&amp;quot; WC=&amp;quot;0.725&amp;quot; CC=&amp;quot;0074&amp;quot; HEIGHT=&amp;quot;26&amp;quot; WIDTH=&amp;quot;81&amp;quot; HPOS=&amp;quot;166&amp;quot; VPOS=&amp;quot;545&amp;quot; SUBS_TYPE=&amp;quot;HypPart2&amp;quot; SUBS_CONTENT=&amp;quot;Gouvernement&amp;quot;/&amp;gt;
                                    &amp;lt;SP ID=&amp;quot;SP13&amp;quot; WIDTH=&amp;quot;24&amp;quot; HPOS=&amp;quot;247&amp;quot; VPOS=&amp;quot;545&amp;quot;/&amp;gt;
                                    &amp;lt;String ID=&amp;quot;S20&amp;quot; CONTENT=&amp;quot;Royal&amp;quot; WC=&amp;quot;0.62&amp;quot; CC=&amp;quot;74503&amp;quot; HEIGHT=&amp;quot;41&amp;quot; WIDTH=&amp;quot;100&amp;quot; HPOS=&amp;quot;271&amp;quot; VPOS=&amp;quot;541&amp;quot;/&amp;gt;
                                    &amp;lt;SP ID=&amp;quot;SP14&amp;quot; WIDTH=&amp;quot;26&amp;quot; HPOS=&amp;quot;371&amp;quot; VPOS=&amp;quot;541&amp;quot;/&amp;gt;
                                    &amp;lt;String ID=&amp;quot;S21&amp;quot; CONTENT=&amp;quot;Grand-Ducal&amp;quot; WC=&amp;quot;0.682&amp;quot; CC=&amp;quot;75260334005&amp;quot; HEIGHT=&amp;quot;32&amp;quot; WIDTH=&amp;quot;218&amp;quot; HPOS=&amp;quot;397&amp;quot; VPOS=&amp;quot;541&amp;quot;/&amp;gt;
                                  &amp;lt;/TextLine&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m interested in the “CONTENT” tag, which contains the words. Let’s first get that into R.&lt;/p&gt;
&lt;p&gt;Load the packages, and the files:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidytext)
library(topicmodels)
library(brotools)

ad_pages &amp;lt;- str_match(list.files(path = &amp;quot;./&amp;quot;, all.files = TRUE, recursive = TRUE), &amp;quot;.*4-alto.xml&amp;quot;) %&amp;gt;%
    discard(is.na)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I save the path of all the pages at once into the &lt;code&gt;ad_pages&lt;/code&gt; variables. To understand how and why
this works, you must take a look at the hierarchy of the folder:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/layout.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Inside each of these folder, there is a &lt;code&gt;text&lt;/code&gt; folder, and inside this folder there are the &lt;code&gt;.xml&lt;/code&gt;
files. Because this structure is bit complex, I use the &lt;code&gt;list.files()&lt;/code&gt; function with the
&lt;code&gt;all.files&lt;/code&gt; and &lt;code&gt;recursive&lt;/code&gt; argument set to &lt;code&gt;TRUE&lt;/code&gt; which allow me to dig deep into the folder
structure and list every single file. I am only interested into the 4th page though, so that’s why
I use &lt;code&gt;str_match()&lt;/code&gt; to only keep the 4th page using the &lt;code&gt;&#34;.*4-alto.xml&#34;&lt;/code&gt; regular expression. This
is the right regular expression, because the files are named like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1877-12-29_01-00004-alto.xml&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So in the end, &lt;code&gt;ad_pages&lt;/code&gt; is a list of all the paths to these files. I then write a function
to extract the contents of the “CONTENT” tag. Here is the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_words &amp;lt;- function(page_path){
    
    page &amp;lt;- read_file(page_path)
    
    page_name &amp;lt;- str_extract(page_path, &amp;quot;1.*(?=-0000)&amp;quot;) 
    
    page %&amp;gt;%  
        str_split(&amp;quot;\n&amp;quot;, simplify = TRUE) %&amp;gt;% 
        keep(str_detect(., &amp;quot;CONTENT&amp;quot;)) %&amp;gt;% 
        str_extract(&amp;quot;(?&amp;lt;=CONTENT)(.*?)(?=WC)&amp;quot;) %&amp;gt;% 
        discard(is.na) %&amp;gt;% 
        str_extract(&amp;quot;[:alpha:]+&amp;quot;) %&amp;gt;% 
        tolower %&amp;gt;% 
        as_tibble %&amp;gt;% 
        rename(tokens = value) %&amp;gt;% 
        mutate(page = page_name)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function takes the path to a page as argument, and returns a tibble with the two columns: one
containing the words, which I called &lt;code&gt;tokens&lt;/code&gt; and the second the name of the document this word
was found. I uploaded on &lt;code&gt;.xml&lt;/code&gt; file
&lt;a href=&#34;https://gist.github.com/b-rodrigues/a22d2aa63dff01d88acc2916c003489d&#34;&gt;here&lt;/a&gt;
so that you can try the function yourself. The difficult part is &lt;code&gt;str_extract(&#34;(?&amp;lt;=CONTENT)(.*?)(?=WC)&#34;)&lt;/code&gt;
which is were the words inside the “CONTENT” tag get extracted.&lt;/p&gt;
&lt;p&gt;I then map this function to all the pages, and get a nice tibble with all the words:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ad_words &amp;lt;- map_dfr(ad_pages, get_words)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ad_words&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,114,662 x 2
##    tokens     page                            
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                           
##  1 afin       1877-01-05_01/text/1877-01-05_01
##  2 de         1877-01-05_01/text/1877-01-05_01
##  3 mettre     1877-01-05_01/text/1877-01-05_01
##  4 mes        1877-01-05_01/text/1877-01-05_01
##  5 honorables 1877-01-05_01/text/1877-01-05_01
##  6 clients    1877-01-05_01/text/1877-01-05_01
##  7 à          1877-01-05_01/text/1877-01-05_01
##  8 même       1877-01-05_01/text/1877-01-05_01
##  9 d          1877-01-05_01/text/1877-01-05_01
## 10 avantages  1877-01-05_01/text/1877-01-05_01
## # … with 1,114,652 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then do some further cleaning, removing stop words (French and German, because there are some
ads in German) and a bunch of garbage characters and words, which are probably when the OCR failed.
I also remove some German words from the few German ads that are in the paper, because they have
a very high tf-idf (I’ll explain below what that is).
I also remove very common words in ads that were just like stopwords. Every ad of a shop mentioned their
clients with &lt;em&gt;honorable clientèle&lt;/em&gt;, or used the word &lt;em&gt;vente&lt;/em&gt;, and so on. This is what you see below
in the very long calls to &lt;code&gt;str_remove_all&lt;/code&gt;. I also compute the &lt;code&gt;tf_idf&lt;/code&gt; and I am grateful to
ThinkR blog post on that, which you can read &lt;a href=&#34;https://thinkr.fr/text-mining-et-topic-modeling-avec-r/&#34;&gt;here&lt;/a&gt;.
It’s in French though, but the idea of the blog post is to present topic modeling with Wikipedia
articles. You can also read the section on tf-idf from the Text Mining with R ebook, &lt;a href=&#34;https://www.tidytextmining.com/tfidf.html&#34;&gt;here&lt;/a&gt;.
tf-idf gives a measure of how common words are. Very common words, like stopwords, have a tf-idf
of 0. So I use this to further remove very common words, by only keeping words with a tf-idf
greater than 0.01. This is why I manually remove garbage words and German words below, because they
are so uncommon that they have a very high tf-idf and mess up the rest of the analysis. To find these words
I had to go back and forth between the tibble of cleaned words and my code, and manually add all
these exceptions. It took some time, but definitely made the results of the next steps better.&lt;br /&gt;
I then use &lt;code&gt;cast_dtm&lt;/code&gt; to cast the tibble into a DocumentTermMatrix object, which
is needed for the &lt;code&gt;LDA()&lt;/code&gt; function that does the topic modeling:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stopwords_fr &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.txt&amp;quot;,
                         col_names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   X1 = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stopwords_de &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt&amp;quot;,
                         col_names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   X1 = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 1 parsing failure.
## row col  expected    actual                                                                                   file
## 157  -- 1 columns 2 columns &amp;#39;https://raw.githubusercontent.com/stopwords-iso/stopwords-de/master/stopwords-de.txt&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ad_words2 &amp;lt;- ad_words %&amp;gt;% 
    filter(!is.na(tokens)) %&amp;gt;% 
    mutate(tokens = str_remove_all(tokens, 
                                   &amp;#39;[|\\|!|&amp;quot;|#|$|%|&amp;amp;|\\*|+|,|-|.|/|:|;|&amp;lt;|=|&amp;gt;|?|@|^|_|`|’|\&amp;#39;|‘|(|)|\\||~|=|]|°|&amp;lt;|&amp;gt;|«|»|\\d{1,100}|©|®|•|—|„|“|-|¦\\\\|”&amp;#39;)) %&amp;gt;%
    mutate(tokens = str_remove_all(tokens,
                                   &amp;quot;j&amp;#39;|j’|m’|m&amp;#39;|n’|n&amp;#39;|c’|c&amp;#39;|qu’|qu&amp;#39;|s’|s&amp;#39;|t’|t&amp;#39;|l’|l&amp;#39;|d’|d&amp;#39;|luxembourg|honneur|rue|prix|maison|frs|ber|adresser|unb|mois|vente|informer|sann|neben|rbudj|artringen|salz|eingetragen|ort|ftofjenb|groifdjen|ort|boch|chem|jahrgang|uoa|genannt|neuwahl|wechsel|sittroe|yerlorenkost|beichsmark|tttr|slpril|ofto|rbudj|felben|acferftücf|etr|eft|sbege|incl|estce|bes|franzosengrund|qne|nne|mme|qni|faire|id|kil&amp;quot;)) %&amp;gt;%
    anti_join(stopwords_de, by = c(&amp;quot;tokens&amp;quot; = &amp;quot;X1&amp;quot;)) %&amp;gt;% 
    filter(!str_detect(tokens, &amp;quot;§&amp;quot;)) %&amp;gt;% 
    mutate(tokens = ifelse(tokens == &amp;quot;inédite&amp;quot;, &amp;quot;inédit&amp;quot;, tokens)) %&amp;gt;% 
    filter(tokens != &amp;quot;&amp;quot;) %&amp;gt;% 
    anti_join(stopwords_fr, by = c(&amp;quot;tokens&amp;quot; = &amp;quot;X1&amp;quot;)) %&amp;gt;% 
    count(page, tokens) %&amp;gt;% 
    bind_tf_idf(tokens, page, n) %&amp;gt;% 
    arrange(desc(tf_idf))

dtm_long &amp;lt;- ad_words2 %&amp;gt;% 
    filter(tf_idf &amp;gt; 0.01) %&amp;gt;% 
    cast_dtm(page, tokens, n)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To read more details on this, I suggest you take a look at the following section of the
Text Mining with R ebook: &lt;a href=&#34;https://www.tidytextmining.com/topicmodeling.html#latent-dirichlet-allocation&#34;&gt;Latent Dirichlet Allocation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I choose to model 10 topics (&lt;code&gt;k = 10&lt;/code&gt;), and set the &lt;code&gt;alpha&lt;/code&gt; parameter to 5. This hyperparamater controls how
many topics are present in one document. Since my ads are all in one page (one document), I
increased it. Let’s fit the model, and plot the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lda_model_long &amp;lt;- LDA(dtm_long, k = 10, control = list(alpha = 5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I plot the per-topic-per-word probabilities, the “beta” from the model and plot the 5 words that
contribute the most to each topic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result &amp;lt;- tidy(lda_model_long, &amp;quot;beta&amp;quot;)

result %&amp;gt;%
    group_by(topic) %&amp;gt;%
    top_n(5, beta) %&amp;gt;%
    ungroup() %&amp;gt;%
    arrange(topic, -beta) %&amp;gt;% 
    mutate(term = reorder(term, beta)) %&amp;gt;%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = &amp;quot;free&amp;quot;) +
    coord_flip() +
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2019-01-04-newspapers_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So some topics seem clear to me, other not at all. For example topic 4 seems to be about shoes made
out of leather. The word &lt;code&gt;semelle&lt;/code&gt;, sole, also appears.
Then there’s a lot of topics that reference either music, bals, or instruments.
I guess these are ads for local music festivals, or similar events. There’s also an ad for what
seems to be bundles of sticks, topic 3: &lt;code&gt;chêne&lt;/code&gt; is oak, &lt;code&gt;copeaux&lt;/code&gt; is shavings and you know
what &lt;code&gt;fagots&lt;/code&gt; is. The first word &lt;code&gt;stère&lt;/code&gt; which I did not know is a unit of volume equal to one
cubic meter (see &lt;a href=&#34;https://en.wikipedia.org/wiki/Stere&#34;&gt;Wikipedia&lt;/a&gt;). So they were likely selling
bundle of oak sticks by the cubic meter. For the other topics, I either
lack context or perhaps I just need to adjust &lt;code&gt;k&lt;/code&gt;, the number of topics to model, and &lt;code&gt;alpha&lt;/code&gt; to get better
results. In the meantime, topic 1 is about shoes (&lt;code&gt;chaussures&lt;/code&gt;), theatre, fuel (&lt;code&gt;combustible&lt;/code&gt;)
and farts (&lt;code&gt;pet&lt;/code&gt;). Really wonder what they were selling in that shop.&lt;/p&gt;
&lt;p&gt;In any case, this was quite an interesting project. I learned a lot about topic modeling
and historical newspapers of my country! I do not know if I will continue exploring it myself,
but I am really curious to see what others will do with it!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R or Python? Why not both? Using Anaconda Python within R with {reticulate}</title>
      <link>/blog/2018-12-30-reticulate/</link>
      <pubDate>Sun, 30 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-12-30-reticulate/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/I8vaCrVIR-Q?t=1h2m26s&#34;&gt;
&lt;img src=&#34;/img/why not both.png&#34; title = &#34;This literally starts playing when you run both R and Python in the same session&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This short blog post illustrates how easy it is to use R and Python in the same R Notebook thanks to the
&lt;code&gt;{reticulate}&lt;/code&gt; package. For this to work, you might need to upgrade RStudio to the &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/preview/&#34;&gt;current preview version&lt;/a&gt;.
Let’s start by importing &lt;code&gt;{reticulate}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;{reticulate}&lt;/code&gt; is an RStudio package that provides “&lt;em&gt;a comprehensive set of tools for interoperability
between Python and R&lt;/em&gt;”. With it, it is possible to call Python and use Python libraries within
an R session, or define Python chunks in R markdown. I think that using R Notebooks is the best way
to work with Python and R; when you want to use Python, you simply use a Python chunk:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```{python}
your python code here
```&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s even autocompletion for Python object methods:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/autocompletion.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Fantastic!&lt;/p&gt;
&lt;p&gt;However, if you wish to use Python interactively within your R session, you must start the Python
REPL with the &lt;code&gt;repl_python()&lt;/code&gt; function, which starts a Python REPL. You can then do whatever you
want, even access objects from your R session, and then when you exit the REPL, any object you
created in Python remains accessible in R. I think that using Python this way is a bit more involved
and would advise using R Notebooks if you need to use both languages.&lt;/p&gt;
&lt;p&gt;I installed the Anaconda Python distribution to have Python on my system. To use it with &lt;code&gt;{reticulate}&lt;/code&gt;
I must first use the &lt;code&gt;use_python()&lt;/code&gt; function that allows me to set which version of Python I want
to use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is an R chunk
use_python(&amp;quot;~/miniconda3/bin/python&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now load a dataset, still using R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is an R chunk
data(mtcars)
head(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and now, to access the &lt;code&gt;mtcars&lt;/code&gt; data frame, I simply use the &lt;code&gt;r&lt;/code&gt; object:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# This is a Python chunk
print(r.mtcars.describe())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              mpg        cyl        disp   ...            am       gear     carb
## count  32.000000  32.000000   32.000000   ...     32.000000  32.000000  32.0000
## mean   20.090625   6.187500  230.721875   ...      0.406250   3.687500   2.8125
## std     6.026948   1.785922  123.938694   ...      0.498991   0.737804   1.6152
## min    10.400000   4.000000   71.100000   ...      0.000000   3.000000   1.0000
## 25%    15.425000   4.000000  120.825000   ...      0.000000   3.000000   2.0000
## 50%    19.200000   6.000000  196.300000   ...      0.000000   4.000000   2.0000
## 75%    22.800000   8.000000  326.000000   ...      1.000000   4.000000   4.0000
## max    33.900000   8.000000  472.000000   ...      1.000000   5.000000   8.0000
## 
## [8 rows x 11 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;.describe()&lt;/code&gt; is a Python Pandas DataFrame method to get summary statistics of our data. This means that
&lt;code&gt;mtcars&lt;/code&gt; was automatically converted from a &lt;code&gt;tibble&lt;/code&gt; object to a Pandas DataFrame! Let’s check its type:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# This is a Python chunk
print(type(r.mtcars))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;class &amp;#39;pandas.core.frame.DataFrame&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s save the summary statistics in a variable:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# This is a Python chunk
summary_mtcars = r.mtcars.describe()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s access this from R, by using the &lt;code&gt;py&lt;/code&gt; object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is an R chunk
class(py$summary_mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try something more complex. Let’s first fit a linear model in Python, and see how R sees it:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# This is a Python chunk
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
model = smf.ols(&amp;#39;mpg ~ hp&amp;#39;, data = r.mtcars).fit()
print(model.summary())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                             OLS Regression Results                            
## ==============================================================================
## Dep. Variable:                    mpg   R-squared:                       0.602
## Model:                            OLS   Adj. R-squared:                  0.589
## Method:                 Least Squares   F-statistic:                     45.46
## Date:                Sun, 10 Feb 2019   Prob (F-statistic):           1.79e-07
## Time:                        00:25:51   Log-Likelihood:                -87.619
## No. Observations:                  32   AIC:                             179.2
## Df Residuals:                      30   BIC:                             182.2
## Df Model:                           1                                         
## Covariance Type:            nonrobust                                         
## ==============================================================================
##                  coef    std err          t      P&amp;gt;|t|      [0.025      0.975]
## ------------------------------------------------------------------------------
## Intercept     30.0989      1.634     18.421      0.000      26.762      33.436
## hp            -0.0682      0.010     -6.742      0.000      -0.089      -0.048
## ==============================================================================
## Omnibus:                        3.692   Durbin-Watson:                   1.134
## Prob(Omnibus):                  0.158   Jarque-Bera (JB):                2.984
## Skew:                           0.747   Prob(JB):                        0.225
## Kurtosis:                       2.935   Cond. No.                         386.
## ==============================================================================
## 
## Warnings:
## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just for fun, I ran the linear regression with the Scikit-learn library too:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# This is a Python chunk
import numpy as np
from sklearn.linear_model import LinearRegression  
regressor = LinearRegression()  
x = r.mtcars[[&amp;quot;hp&amp;quot;]]
y = r.mtcars[[&amp;quot;mpg&amp;quot;]]
model_scikit = regressor.fit(x, y)
print(model_scikit.intercept_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [30.09886054]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(model_scikit.coef_)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[-0.06822828]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s access the &lt;code&gt;model&lt;/code&gt; variable in R and see what type of object it is in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is an R chunk
model_r &amp;lt;- py$model
class(model_r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;statsmodels.regression.linear_model.RegressionResultsWrapper&amp;quot;
## [2] &amp;quot;statsmodels.base.wrapper.ResultsWrapper&amp;quot;                     
## [3] &amp;quot;python.builtin.object&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So because this is a custom Python object, it does not get converted into the equivalent R object.
This is described &lt;a href=&#34;https://rstudio.github.io/reticulate/index.html&#34;&gt;here&lt;/a&gt;. However, you can still
use Python methods from within an R chunk!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is an R chunk
model_r$aic&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 179.2386&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_r$params&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Intercept          hp 
## 30.09886054 -0.06822828&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I must say that I am very impressed with the &lt;code&gt;{reticulate}&lt;/code&gt; package. I think that even if you are
primarily a Python user, this is still very interesting to know in case you need a specific function
from an R package. Just write all your script inside a Python Markdown chunk and then use the R
function you need from an R chunk! Of course there is also a way to use R from Python, a Python library
called &lt;code&gt;rpy2&lt;/code&gt; but I am not very familiar with it. From what I read, it seems to be also quite
simple to use.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some fun with {gganimate}</title>
      <link>/blog/2018-12-27-fun_gganimate/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-12-27-fun_gganimate/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;video width=&#34;864&#34; height=&#34;480&#34; controls&gt;
&lt;source src=&#34;/img/wiid_gganimate.webm&#34; type=&#34;video/webm&#34;&gt;
Your browser does not support the video tag.
&lt;/video&gt;
&lt;/div&gt;
&lt;p&gt;In this short blog post I show you how you can use the &lt;code&gt;{gganimate}&lt;/code&gt; package to create animations
from &lt;code&gt;{ggplot2}&lt;/code&gt; graphs with data from UNU-WIDER.&lt;/p&gt;
&lt;div id=&#34;wiid-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;WIID data&lt;/h2&gt;
&lt;p&gt;Just before Christmas, UNU-WIDER released a new edition of their World Income Inequality Database:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;*NEW &lt;a href=&#34;https://twitter.com/hashtag/DATA?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#DATA&lt;/a&gt;*&lt;br&gt;We’ve just released a new version of the World Income Inequality Database.&lt;br&gt;WIID4 includes &lt;a href=&#34;https://twitter.com/hashtag/data?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#data&lt;/a&gt; from 7 new countries, now totalling 189, and reaches the year 2017. All data is freely available for download on our website: &lt;a href=&#34;https://t.co/XFxuLvyKTC&#34;&gt;https://t.co/XFxuLvyKTC&lt;/a&gt; &lt;a href=&#34;https://t.co/rCf9eXN8D5&#34;&gt;pic.twitter.com/rCf9eXN8D5&lt;/a&gt;&lt;/p&gt;&amp;mdash; UNU-WIDER (@UNUWIDER) &lt;a href=&#34;https://twitter.com/UNUWIDER/status/1076001879556005888?ref_src=twsrc%5Etfw&#34;&gt;December 21, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;The data is available in Excel and STATA formats, and I thought it was a great opportunity to
release it as an R package. You can install it with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/wiid4&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here a short description of the data, taken from UNU-WIDER’s website:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&#34;The World Income Inequality Database (WIID) presents information on income inequality for
developed, developing, and transition countries. It provides the most comprehensive set of income
inequality statistics available and can be downloaded for free.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;WIID4, released in December 2018, covers 189 countries (including historical entities), with over
11,000 data points in total. With the current version, the latest observations now reach the year 2017.&#34;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It was also a good opportunity to play around with the &lt;code&gt;{gganimate}&lt;/code&gt; package. This package
makes it possible to create animations and is an extension to &lt;code&gt;{ggplot2}&lt;/code&gt;. Read more about it
&lt;a href=&#34;https://github.com/thomasp85/gganimate&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparing the data&lt;/h2&gt;
&lt;p&gt;To create a smooth animation, I need to have a cylindrical panel data set; meaning that for each
country in the data set, there are no missing years. I also chose to focus on certain variables
only; net income, all the population of the country (instead of just focusing on the economically
active for instance) as well as all the country itself (and not just the rural areas).
On &lt;a href=&#34;https://www.wider.unu.edu/sites/default/files/WIID/PDF/WIID4%20User%20Guide.pdf&#34;&gt;this link&lt;/a&gt; you
can find a codebook (pdf warning), so you can understand the filters I defined below better.&lt;/p&gt;
&lt;p&gt;Let’s first load the packages, data and perform the necessary transformations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(wiid4)
library(tidyverse)
library(ggrepel)
library(gganimate)
library(brotools)

small_wiid4 &amp;lt;- wiid4 %&amp;gt;%
    mutate(eu = as.character(eu)) %&amp;gt;%
    mutate(eu = case_when(eu == &amp;quot;1&amp;quot; ~ &amp;quot;EU member state&amp;quot;,
                          eu == &amp;quot;0&amp;quot; ~ &amp;quot;Non-EU member state&amp;quot;)) %&amp;gt;%
    filter(resource == 1, popcovr == 1, areacovr == 1, scale == 2) %&amp;gt;%
    group_by(country) %&amp;gt;%
    group_by(country, year) %&amp;gt;%
    filter(quality_score == max(quality_score)) %&amp;gt;%
    filter(source == min(source)) %&amp;gt;%
    filter(!is.na(bottom5)) %&amp;gt;%
    group_by(country) %&amp;gt;%
    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&amp;gt;%
    filter(flag == 1, year &amp;gt; 2003) %&amp;gt;%
    mutate(year = lubridate::ymd(paste0(year, &amp;quot;-01-01&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For some country and some years, there are several sources of data with varying quality. I only
keep the highest quality sources with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    group_by(country, year) %&amp;gt;%
    filter(quality_score == max(quality_score)) %&amp;gt;%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If there are different sources of equal quality, I give priority to the sources that are the most
comparable across country (Luxembourg Income Study, LIS data) to less comparable sources with
(at least that’s my understanding of the &lt;code&gt;source&lt;/code&gt; variable):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    filter(source == min(source)) %&amp;gt;%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then remove missing data with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    filter(!is.na(bottom5)) %&amp;gt;%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;bottom5&lt;/code&gt; and &lt;code&gt;top5&lt;/code&gt; give the share of income that is controlled by the bottom 5% and top 5%
respectively. These are the variables that I want to plot.&lt;/p&gt;
&lt;p&gt;Finally I keep the years 2004 to 2016, without any interruption with the following line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    mutate(flag = ifelse(all(seq(2004, 2016) %in% year), 1, 0)) %&amp;gt;%
    filter(flag == 1, year &amp;gt; 2003) %&amp;gt;%&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ifelse(all(seq(2004, 2016) %in% year), 1, 0))&lt;/code&gt; creates a flag that equals &lt;code&gt;1&lt;/code&gt; only if the years
2004 to 2016 are present in the data without any interruption. Then I only keep the data from 2004
on and only where the flag variable equals 1.&lt;/p&gt;
&lt;p&gt;In the end, I ended up only with European countries. It would have been interesting to have countries
from other continents, but apparently only European countries provide data in an annual basis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-animation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating the animation&lt;/h2&gt;
&lt;p&gt;To create the animation I first started by creating a static ggplot showing what I wanted;
a scatter plot of the income by bottom and top 5%. The size of the bubbles should be proportional
to the GDP of the country (another variable provided in the data). Once the plot looked how I wanted
I added the lines that are specific to &lt;code&gt;{gganimate}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    labs(title = &amp;#39;Year: {frame_time}&amp;#39;, x = &amp;#39;Top 5&amp;#39;, y = &amp;#39;Bottom 5&amp;#39;) +
    transition_time(year) +
    ease_aes(&amp;#39;linear&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I took this from &lt;code&gt;{gganimate}&lt;/code&gt;’s README.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;animation &amp;lt;- ggplot(small_wiid4) +
    geom_point(aes(y = bottom5, x = top5, colour = eu, size = log(gdp_ppp_pc_usd2011))) +
    xlim(c(10, 20)) +
    geom_label_repel(aes(y = bottom5, x = top5, label = country), hjust = 1, nudge_x = 20) +
    theme(legend.position = &amp;quot;bottom&amp;quot;) +
    theme_blog() +
    scale_color_blog() +
    labs(title = &amp;#39;Year: {frame_time}&amp;#39;, x = &amp;#39;Top 5&amp;#39;, y = &amp;#39;Bottom 5&amp;#39;) +
    transition_time(year) +
    ease_aes(&amp;#39;linear&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I use &lt;code&gt;geom_label_repel&lt;/code&gt; to place the countries’ labels on the right of the plot. If I don’t do
this, the labels of the countries would be floating around and the animation would be unreadable.&lt;/p&gt;
&lt;p&gt;I then spent some time trying to render a nice webm instead of a gif. It took some trial and error
and I am still not entirely satisfied with the result, but here is the code to render the animation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;animate(animation, renderer = ffmpeg_renderer(options = list(s = &amp;quot;864x480&amp;quot;, 
                                                             vcodec = &amp;quot;libvpx-vp9&amp;quot;,
                                                             crf = &amp;quot;15&amp;quot;,
                                                             b = &amp;quot;1600k&amp;quot;, 
                                                             vf = &amp;quot;setpts=5*PTS&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The option &lt;code&gt;vf = &#34;setpts=5*PTS&#34;&lt;/code&gt; is important because it slows the video down, so we can actually
see something. &lt;code&gt;crf = &#34;15&#34;&lt;/code&gt; is the quality of the video (lower is better), &lt;code&gt;b = &#34;1600k&#34;&lt;/code&gt; is the
bitrate, and &lt;code&gt;vcodec = &#34;libvpx-vp9&#34;&lt;/code&gt; is the codec I use. The video you saw at the top of this
post is the result. You can also find the video &lt;a href=&#34;https://raw.githubusercontent.com/rbind/b-rodrigues.github.com/master/static/img/wiid_gganimate.webm&#34;&gt;here&lt;/a&gt;,
and here’s a gif if all else fails:&lt;/p&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=3zXx0ReqOOI&#34;&gt;
&lt;img src=&#34;/img/wiid_gganimate_gif.gif&#34; title = &#34;Click to listen to OST of this gif&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I would have preferred if the video was smoother, which should be possible by creating more frames.
I did not find such an option in &lt;code&gt;{gganimate}&lt;/code&gt;, and perhaps there is none, at least for now.&lt;/p&gt;
&lt;p&gt;In any case &lt;code&gt;{gganimate}&lt;/code&gt; is pretty nice to play with, and I’ll definitely use it more!&lt;/p&gt;
&lt;div id=&#34;update&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Update&lt;/h3&gt;
&lt;p&gt;Silly me! It turns out thate the &lt;code&gt;animate()&lt;/code&gt; function has arguments that can control the number of frames
and the duration, without needing to pass options to the renderer. I was looking at options for the
renderer only, without having read the documentation of the &lt;code&gt;animate()&lt;/code&gt; function. It turns out that
you can pass several arguments to the &lt;code&gt;animate()&lt;/code&gt; function; for example, here is how you
can make a GIF that lasts for 20 seconds running and 20 frames per second, pausing for 5
frames at the end and then restarting:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;animate(animation, nframes = 400, duration = 20, fps = 20, end_pause = 5, rewind = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I guess that you should only pass options to the renderer if you really need fine-grained control.&lt;/p&gt;
&lt;p&gt;This took around 2 minutes to finish. You can use the same options with the ffmpeg renderer too.
Here is what the gif looks like:&lt;/p&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=3zXx0ReqOOI&#34;&gt;
&lt;img src=&#34;/img/wiid_gganimate_gif_smooth.gif&#34; title = &#34;Click to listen to OST of this gif&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Much, much smoother!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Objects types and some useful R functions for beginners</title>
      <link>/blog/2018-12-24-modern_objects/</link>
      <pubDate>Mon, 24 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-12-24-modern_objects/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=M-1nTwiHxic&#34;&gt;
&lt;img width = &#34;400&#34; src=&#34;/img/santa_sanders.jpg&#34; title = &#34;The frydiest time of the year&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;This blog post is an excerpt of my ebook &lt;em&gt;Modern R with the tidyverse&lt;/em&gt; that you can read for
free &lt;a href=&#34;https://b-rodrigues.github.io/modern_R/&#34;&gt;here&lt;/a&gt;. This is taken from Chapter 2, which explains
the different R objects you can manipulate as well as some functions to get you started.&lt;/p&gt;
&lt;div id=&#34;objects-types-and-useful-r-functions-to-get-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Objects, types and useful R functions to get started&lt;/h2&gt;
&lt;p&gt;All objects in R have a given &lt;em&gt;type&lt;/em&gt;. You already know most of them, as these types are also used
in mathematics. Integers, floating point numbers, or floats, matrices, etc, are all objects you
are already familiar with. But R has other, maybe lesser known data types (that you can find in a
lot of other programming languages) that you need to become familiar with. But first, we need to
learn how to assign a value to a variable. This can be done in two ways:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a = 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;in very practical terms, there is no difference between the two. I prefer using &lt;code&gt;&amp;lt;-&lt;/code&gt; for assigning
values to variables and reserve &lt;code&gt;=&lt;/code&gt; for passing arguments to functions, for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spam &amp;lt;- mean(x = c(1,2,3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I think this is less confusing than:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spam = mean(x = c(1,2,3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but as I explained above you can use whatever you feel most comfortable with.&lt;/p&gt;
&lt;div id=&#34;the-numeric-class&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;numeric&lt;/code&gt; class&lt;/h3&gt;
&lt;p&gt;To define single numbers, you can do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;class()&lt;/code&gt; function allows you to check the class of an object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Decimals are defined with the character &lt;code&gt;.&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 3.14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R also supports integers. If you find yourself in a situation where you explicitly need an integer
and not a floating point number, you can use the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a  &amp;lt;- as.integer(3)
class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;as.integer()&lt;/code&gt; function is very useful, because it converts its argument into an integer. There
is a whole family of &lt;code&gt;as.*()&lt;/code&gt; functions. To convert &lt;code&gt;a&lt;/code&gt; into a floating point number again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(as.numeric(a))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is also &lt;code&gt;is.numeric()&lt;/code&gt; which tests whether a number is of the &lt;code&gt;numeric&lt;/code&gt; class:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;is.numeric(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These functions are very useful, there is one for any of the supported types in R. Later, we are going
to learn about the &lt;code&gt;{purrr}&lt;/code&gt; package, which is a very powerful package for functional programming. This
package includes further such functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-character-class&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;character&lt;/code&gt; class&lt;/h3&gt;
&lt;p&gt;Use &lt;code&gt;&#34; &#34;&lt;/code&gt; to define characters (called strings in other programming languages):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- &amp;quot;this is a string&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To convert something to a character you can use the &lt;code&gt;as.character()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 4.392

class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(as.character(a))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to convert a character to a numeric:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- &amp;quot;4.392&amp;quot;

class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(as.numeric(a))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this only works if it makes sense:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- &amp;quot;this won&amp;#39;t work, chief&amp;quot;

class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.numeric(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A very nice package to work with characters is &lt;code&gt;{stringr}&lt;/code&gt;, which is also part of the &lt;code&gt;{tidyverse}&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-factor-class&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;factor&lt;/code&gt; class&lt;/h3&gt;
&lt;p&gt;Factors look like characters, but are very different. They are the representation of categorical
variables. A &lt;code&gt;{tidyverse}&lt;/code&gt; package to work with factors is &lt;code&gt;{forcats}&lt;/code&gt;. You would rarely use
factor variables outside of datasets, so for now, it is enough to know that this class exists.
We are going to learn more about factor variables in Chapter 4, by using the &lt;code&gt;{forcats}&lt;/code&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-date-class&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;Date&lt;/code&gt; class&lt;/h3&gt;
&lt;p&gt;Dates also look like characters, but are very different too:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;as.Date(&amp;quot;2019/03/19&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2019-03-19&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(as.Date(&amp;quot;2019/03/19&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Manipulating dates and time can be tricky, but thankfully there’s a &lt;code&gt;{tidyverse}&lt;/code&gt; package for that,
called &lt;code&gt;{lubridate}&lt;/code&gt;. We are going to go over this package in Chapter 4.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logical-class&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;logical&lt;/code&gt; class&lt;/h3&gt;
&lt;p&gt;This class is the result of logical comparisons, for example, if you type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;4 &amp;gt; 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;R returns &lt;code&gt;TRUE&lt;/code&gt;, which is an object of class &lt;code&gt;logical&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- 4 &amp;gt; 3
class(k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;logical&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In other programming languages, &lt;code&gt;logical&lt;/code&gt;s are often called &lt;code&gt;bool&lt;/code&gt;s. A &lt;code&gt;logical&lt;/code&gt; variable can only have
two values, either &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;. You can test the truthiness of a variable with &lt;code&gt;isTRUE()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- 4 &amp;gt; 3
isTRUE(k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How can you test if a variable is false? There is not a &lt;code&gt;isFALSE()&lt;/code&gt; function (at least not without having
to load a package containing this function), but there is way to do it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k &amp;lt;- 4 &amp;gt; 3
!isTRUE(k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;!&lt;/code&gt; operator indicates negation, so the above expression could be translated as &lt;em&gt;is k not TRUE?&lt;/em&gt;.
There are other such operators, namely &lt;code&gt;&amp;amp;, &amp;amp;&amp;amp;, |, ||&lt;/code&gt;. &lt;code&gt;&amp;amp;&lt;/code&gt; means &lt;em&gt;and&lt;/em&gt; and &lt;code&gt;|&lt;/code&gt; stands for &lt;em&gt;or&lt;/em&gt;.
You might be wondering what the difference between &lt;code&gt;&amp;amp;&lt;/code&gt; and &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; is? Or between &lt;code&gt;|&lt;/code&gt; and &lt;code&gt;||&lt;/code&gt;? &lt;code&gt;&amp;amp;&lt;/code&gt; and
&lt;code&gt;|&lt;/code&gt; work on vectors, doing pairwise comparisons:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one &amp;lt;- c(TRUE, FALSE, TRUE, FALSE)
two &amp;lt;- c(FALSE, TRUE, TRUE, TRUE)
one &amp;amp; two&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE FALSE  TRUE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare this to the &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; operator:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one &amp;lt;- c(TRUE, FALSE, TRUE, FALSE)
two &amp;lt;- c(FALSE, TRUE, TRUE, TRUE)
one &amp;amp;&amp;amp; two&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; and &lt;code&gt;||&lt;/code&gt; operators only compare the first element of the vectors and stop as soon as a the return
value can be safely determined. This is called short-circuiting. Consider the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one &amp;lt;- c(TRUE, FALSE, TRUE, FALSE)
two &amp;lt;- c(FALSE, TRUE, TRUE, TRUE)
three &amp;lt;- c(TRUE, TRUE, FALSE, FALSE)
one &amp;amp;&amp;amp; two &amp;amp;&amp;amp; three&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one || two || three&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;||&lt;/code&gt; operator stops as soon it evaluates to &lt;code&gt;TRUE&lt;/code&gt; whereas the &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; stops as soon as it evaluates to &lt;code&gt;FALSE&lt;/code&gt;.
Personally, I rarely use &lt;code&gt;||&lt;/code&gt; or &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; because I get confused. I find using &lt;code&gt;|&lt;/code&gt; or &lt;code&gt;&amp;amp;&lt;/code&gt; in combination with the
&lt;code&gt;all()&lt;/code&gt; or &lt;code&gt;any()&lt;/code&gt; functions much more useful:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;one &amp;lt;- c(TRUE, FALSE, TRUE, FALSE)
two &amp;lt;- c(FALSE, TRUE, TRUE, TRUE)
any(one &amp;amp; two)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all(one &amp;amp; two)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;any()&lt;/code&gt; checks whether any of the vector’s elements are &lt;code&gt;TRUE&lt;/code&gt; and &lt;code&gt;all()&lt;/code&gt; checks if all elements of the vector are
&lt;code&gt;TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As a final note, you should know that is possible to use &lt;code&gt;T&lt;/code&gt; for &lt;code&gt;TRUE&lt;/code&gt; and &lt;code&gt;F&lt;/code&gt; for &lt;code&gt;FALSE&lt;/code&gt; but I would advise against
doing this, because it is not very explicit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors-and-matrices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Vectors and matrices&lt;/h3&gt;
&lt;p&gt;You can create a vector in different ways. But first of all, it is important to understand that a
vector in most programming languages is nothing more than a list of things. These things can be
numbers (either integers or floats), strings, or even other vectors. A vector in R can only contain elements of one
single type. This is not the case for a list, which is much more flexible. We will talk about lists shortly, but
let’s first focus on vectors and matrices.&lt;/p&gt;
&lt;div id=&#34;the-c-function&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The &lt;code&gt;c()&lt;/code&gt; function&lt;/h4&gt;
&lt;p&gt;A very important function that allows you to build a vector is &lt;code&gt;c()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- c(1,2,3,4,5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a vector with elements 1, 2, 3, 4, 5. If you check its class:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be confusing: you where probably expecting a to be of class &lt;em&gt;vector&lt;/em&gt; or
something similar. This is not the case if you use &lt;code&gt;c()&lt;/code&gt; to create the vector, because &lt;code&gt;c()&lt;/code&gt;
doesn’t build a vector in the mathematical sense, but a so-called atomic vector.
Checking its dimension:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;returns &lt;code&gt;NULL&lt;/code&gt; because an atomic vector doesn’t have a dimension.
If you want to create a true vector, you need to use &lt;code&gt;cbind()&lt;/code&gt; or &lt;code&gt;rbind()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But before continuing, be aware that atomic vectors can only contain elements of the same type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(1, 2, &amp;quot;3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;because “3” is a character, all the other values get implicitly converted to characters. You have
to be very careful about this, and if you use atomic vectors in your programming, you have to make
absolutely sure that no characters or logicals or whatever else are going to convert your atomic
vector to something you were not expecting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cbind-and-rbind&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;code&gt;cbind()&lt;/code&gt; and &lt;code&gt;rbind()&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;You can create a &lt;em&gt;true&lt;/em&gt; vector with &lt;code&gt;cbind()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- cbind(1, 2, 3, 4, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Check its class now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;matrix&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is exactly what we expected. Let’s check its dimension:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns the dimension of &lt;code&gt;a&lt;/code&gt; using the LICO notation (number of LInes first, the number of COlumns).&lt;/p&gt;
&lt;p&gt;It is also possible to bind vectors together to create a matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b &amp;lt;- cbind(6,7,8,9,10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s put vector &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; into a matrix called &lt;code&gt;matrix_c&lt;/code&gt; using &lt;code&gt;rbind()&lt;/code&gt;.
&lt;code&gt;rbind()&lt;/code&gt; functions the same way as &lt;code&gt;cbind()&lt;/code&gt; but glues the vectors together by rows and not by columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix_c &amp;lt;- rbind(a,b)
print(matrix_c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    2    3    4    5
## [2,]    6    7    8    9   10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-matrix-class&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The &lt;code&gt;matrix&lt;/code&gt; class&lt;/h4&gt;
&lt;p&gt;R also has support for matrices. For example, you can create a matrix of dimension (5,5) filled
with 0’s with the &lt;code&gt;matrix()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix_a &amp;lt;- matrix(0, nrow = 5, ncol = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to create the following matrix:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
B = \left(
\begin{array}{ccc}
 2 &amp;amp; 4 &amp;amp; 3 \\
 1 &amp;amp; 5 &amp;amp; 7
\end{array} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;you would do it like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;B &amp;lt;- matrix(c(2, 4, 3, 1, 5, 7), nrow = 2, byrow = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The option &lt;code&gt;byrow = TRUE&lt;/code&gt; means that the rows of the matrix will be filled first.&lt;/p&gt;
&lt;p&gt;You can access individual elements of &lt;code&gt;matrix_a&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix_a[2, 3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and R returns its value, 0. We can assign a new value to this element if we want. Try:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix_a[2, 3] &amp;lt;- 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and now take a look at &lt;code&gt;matrix_a&lt;/code&gt; again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(matrix_a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5]
## [1,]    0    0    0    0    0
## [2,]    0    0    7    0    0
## [3,]    0    0    0    0    0
## [4,]    0    0    0    0    0
## [5,]    0    0    0    0    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recall our vector &lt;code&gt;b&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b &amp;lt;- cbind(6,7,8,9,10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To access its third element, you can simply write:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b[3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have heard many people praising R for being a matrix based language. Matrices are indeed useful,
and statisticians are used to working with them. However, I very rarely use matrices in my
day to day work, and prefer an approach based on data frames (which will be discussed below). This
is because working with data frames makes it easier to use R’s advanced functional programming
language capabilities, and this is where R really shines in my opinion. Working with matrices
almost automatically implies using loops and all the iterative programming techniques, &lt;em&gt;à la Fortran&lt;/em&gt;,
which I personally believe are ill-suited for interactive statistical programming (as discussed in
the introduction).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-list-class&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;list&lt;/code&gt; class&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;list&lt;/code&gt; class is a very flexible class, and thus, very useful. You can put anything inside a list,
such as numbers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list1 &amp;lt;- list(3, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or other lists constructed with &lt;code&gt;c()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list2 &amp;lt;- list(c(1, 2), c(3, 4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;you can also put objects of different classes in the same list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list3 &amp;lt;- list(3, c(1, 2), &amp;quot;lists are amazing!&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and of course create list of lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_lists &amp;lt;- list(list1, list2, list3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check the contents of a list, you can use the structure function &lt;code&gt;str()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(my_lists)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 3
##  $ :List of 2
##   ..$ : num 3
##   ..$ : num 2
##  $ :List of 2
##   ..$ : num [1:2] 1 2
##   ..$ : num [1:2] 3 4
##  $ :List of 3
##   ..$ : num 3
##   ..$ : num [1:2] 1 2
##   ..$ : chr &amp;quot;lists are amazing!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or you can use RStudio’s &lt;em&gt;Environment&lt;/em&gt; pane:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/rstudio_environment_list.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;You can also create named lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list4 &amp;lt;- list(&amp;quot;a&amp;quot; = 2, &amp;quot;b&amp;quot; = 8, &amp;quot;c&amp;quot; = &amp;quot;this is a named list&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and you can access the elements in two ways:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list4[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or, for named lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list4$c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;this is a named list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lists are used extensively because they are so flexible. You can build lists of datasets and apply
functions to all the datasets at once, build lists of models, lists of plots, etc… In the later
chapters we are going to learn all about them. Lists are central objects in a functional programming
workflow for interactive statistical analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data.frame-and-tibble-classes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The &lt;code&gt;data.frame&lt;/code&gt; and &lt;code&gt;tibble&lt;/code&gt; classes&lt;/h3&gt;
&lt;p&gt;In the next chapter we are going to learn how to import datasets into R. Once you import data, the
resulting object is either a &lt;code&gt;data.frame&lt;/code&gt; or a &lt;code&gt;tibble&lt;/code&gt; depending on which package you used to
import the data. &lt;code&gt;tibble&lt;/code&gt;s extend &lt;code&gt;data.frame&lt;/code&gt;s so if you know about &lt;code&gt;data.frame&lt;/code&gt; objects already,
working with &lt;code&gt;tibble&lt;/code&gt;s will be very easy. &lt;code&gt;tibble&lt;/code&gt;s have a better &lt;code&gt;print()&lt;/code&gt; method, and some other
niceties.&lt;/p&gt;
&lt;p&gt;However, I want to stress that these objects are central to R and are thus very important; they are
actually special cases of lists, discussed above. There are different ways to print a &lt;code&gt;data.frame&lt;/code&gt; or
a &lt;code&gt;tibble&lt;/code&gt; if you wish to inspect it. You can use &lt;code&gt;View(my_data)&lt;/code&gt; to show the &lt;code&gt;my_data&lt;/code&gt; &lt;code&gt;data.frame&lt;/code&gt;
in the &lt;em&gt;View&lt;/em&gt; pane of RStudio:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/rstudio_view_data.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;You can also use the &lt;code&gt;str()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(my_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if you need to access an individual column, you can use the &lt;code&gt;$&lt;/code&gt; sign, same as for a list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_data$col1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;formulas&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Formulas&lt;/h3&gt;
&lt;p&gt;We will learn more about formulas later, but because it is an important object, it is useful if you
already know about them early on. A formula is defined in the following way:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_formula &amp;lt;- ~x

class(my_formula)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;formula&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Formula objects are defined using the &lt;code&gt;~&lt;/code&gt; symbol. Formulas are useful to define statistical models,
for example for a linear regression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(y ~ x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or also to define anonymous functions, but more on this later.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;A statistical model is an object like any other in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)

my_model &amp;lt;- lm(mpg ~ hp, mtcars)

class(my_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;lm&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;my_model&lt;/code&gt; is an object of class &lt;code&gt;lm&lt;/code&gt;. You can apply different functions to a model object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(my_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ hp, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.7121 -2.1122 -0.8854  1.5819  8.2360 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 30.09886    1.63392  18.421  &amp;lt; 2e-16 ***
## hp          -0.06823    0.01012  -6.742 1.79e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.863 on 30 degrees of freedom
## Multiple R-squared:  0.6024, Adjusted R-squared:  0.5892 
## F-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This class will be explored in later chapters.&lt;/p&gt;
&lt;div id=&#34;null-na-and-nan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;NULL, NA and NaN&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;NULL&lt;/code&gt;, &lt;code&gt;NA&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt; classes are pretty special. &lt;code&gt;NULL&lt;/code&gt; is returned when the result of function is undetermined.
For example, consider &lt;code&gt;list4&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $a
## [1] 2
## 
## $b
## [1] 8
## 
## $c
## [1] &amp;quot;this is a named list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;if you try to access an element that does not exist, such as &lt;code&gt;d&lt;/code&gt;, you will get &lt;code&gt;NULL&lt;/code&gt; back:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list4$d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;NaN&lt;/code&gt; means “Not a Number” and is returned when a function return something that is not a number:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(-1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in sqrt(-1): NaNs produced&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0/0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basically, numbers that cannot be represented as floating point numbers are &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, there’s &lt;code&gt;NA&lt;/code&gt; which is closely related to &lt;code&gt;NaN&lt;/code&gt; but is used for missing values. &lt;code&gt;NA&lt;/code&gt; stands for &lt;code&gt;Not Available&lt;/code&gt;. There are
several types of &lt;code&gt;NA&lt;/code&gt;s:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NA_integer_&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NA_real_&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NA_complex_&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NA_character_&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;but these are in principle only used when you need to program your own functions and need to explicitly test for the missingness of, say,
a character value.&lt;/p&gt;
&lt;p&gt;To test whether a value is &lt;code&gt;NA&lt;/code&gt;, use the &lt;code&gt;is.na()&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;useful-functions-to-get-you-started&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Useful functions to get you started&lt;/h3&gt;
&lt;p&gt;This section will list several basic R functions that are very useful and should be part of your toolbox.&lt;/p&gt;
&lt;div id=&#34;sequences&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Sequences&lt;/h4&gt;
&lt;p&gt;There are several functions that create sequences, &lt;code&gt;seq()&lt;/code&gt;, &lt;code&gt;seq_along()&lt;/code&gt; and &lt;code&gt;rep()&lt;/code&gt;. &lt;code&gt;rep()&lt;/code&gt; is easy enough:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(1, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1 1 1 1 1 1 1 1 1 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simply repeats &lt;code&gt;1&lt;/code&gt; 10 times. You can repeat other objects too:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rep(&amp;quot;HAHA&amp;quot;, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot; &amp;quot;HAHA&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To create a sequence, things are not as straightforward. There is &lt;code&gt;seq()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(70, 80)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 70 71 72 73 74 75 76 77 78 79 80&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is also possible to provide a &lt;code&gt;by&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(1, 10, by = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 3 5 7 9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;seq_along()&lt;/code&gt; behaves similarly, but returns the length of the object passed to it. So if you pass &lt;code&gt;list4&lt;/code&gt; to
&lt;code&gt;seq_along()&lt;/code&gt;, it will return a sequence from 1 to 3:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq_along(list4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which is also true for &lt;code&gt;seq()&lt;/code&gt; actually:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(list4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but these two functions behave differently for arguments of length equal to 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq_along(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So be quite careful about that. I would advise you do not use &lt;code&gt;seq()&lt;/code&gt;, but only &lt;code&gt;seq_along()&lt;/code&gt; and &lt;code&gt;seq_len()&lt;/code&gt;. &lt;code&gt;seq_len()&lt;/code&gt;
only takes arguments of length 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq_len(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq_along(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem with &lt;code&gt;seq()&lt;/code&gt; is that it is unpredictable; depending on its input, the output will either be an integer or a sequence.
When programming, it is better to have function that are stricter and fail when confronted to special cases, instead of returning
some result. This is a bit of a recurrent issue with R, and the functions from the &lt;code&gt;{tidyverse}&lt;/code&gt; mitigate this issue by being
stricter than their base R counterparts. For example, consider the &lt;code&gt;ifelse()&lt;/code&gt; function from base R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ifelse(3 &amp;gt; 5, 1, &amp;quot;this is false&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;this is false&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and compare it to &lt;code&gt;{dplyr}&lt;/code&gt;’s implementation, &lt;code&gt;if_else()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if_else(3 &amp;gt; 5, 1, &amp;quot;this is false&amp;quot;)
Error: `false` must be type double, not character
Call `rlang::last_error()` to see a backtrace&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;if_else()&lt;/code&gt; fails because the return value when &lt;code&gt;FALSE&lt;/code&gt; is not a double (a real number) but a character. This might seem unnecessarily
strict, but at least it is predictable. This makes debugging easier when used inside functions. In Chapter 8 we are going to learn how
to write our own functions, and being strict makes programming easier.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;basic-string-manipulation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Basic string manipulation&lt;/h4&gt;
&lt;p&gt;For now, we have not closely studied &lt;code&gt;character&lt;/code&gt; objects, we only learned how to define them. Later, in Chapter 5 we will learn about the
&lt;code&gt;{stringr}&lt;/code&gt; package which provides useful function to work with strings. However, there are several base R functions that are very
useful that you might want to know nonetheless, such as &lt;code&gt;paste()&lt;/code&gt; and &lt;code&gt;paste0()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste(&amp;quot;Hello&amp;quot;, &amp;quot;amigo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hello amigo&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but you can also change the separator if needed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste(&amp;quot;Hello&amp;quot;, &amp;quot;amigo&amp;quot;, sep = &amp;quot;--&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Hello--amigo&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;paste0()&lt;/code&gt; is the same as &lt;code&gt;paste()&lt;/code&gt; but does not have any &lt;code&gt;sep&lt;/code&gt; argument:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste0(&amp;quot;Hello&amp;quot;, &amp;quot;amigo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Helloamigo&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you provide a vector of characters, you can also use the &lt;code&gt;collapse&lt;/code&gt; argument, which places whatever you provide for &lt;code&gt;collapse&lt;/code&gt; between the
characters of the vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste0(c(&amp;quot;Joseph&amp;quot;, &amp;quot;Mary&amp;quot;, &amp;quot;Jesus&amp;quot;), collapse = &amp;quot;, and &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Joseph, and Mary, and Jesus&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To change the case of characters, you can use &lt;code&gt;toupper()&lt;/code&gt; and &lt;code&gt;tolower()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tolower(&amp;quot;HAHAHAHAH&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;hahahahah&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;toupper(&amp;quot;hueuehuehuheuhe&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;HUEUEHUEHUHEUHE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mathematical-functions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Mathematical functions&lt;/h4&gt;
&lt;p&gt;Finally, there are the classical mathematical functions that you know and love:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sqrt()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;exp()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;log()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;abs()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sin()&lt;/code&gt;, &lt;code&gt;cos()&lt;/code&gt;, &lt;code&gt;tan()&lt;/code&gt;, and others&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sum()&lt;/code&gt;, &lt;code&gt;cumsum()&lt;/code&gt;, &lt;code&gt;prod()&lt;/code&gt;, &lt;code&gt;cumprod()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;max()&lt;/code&gt;, &lt;code&gt;min()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and many others…&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using the tidyverse for more than data manipulation: estimating pi with Monte Carlo methods</title>
      <link>/blog/2018-12-21-tidyverse_pi/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-12-21-tidyverse_pi/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kZJY15dyMig&#34;&gt;
&lt;img width = &#34;400&#34; src=&#34;/img/casino.jpg&#34; title = &#34;Audentes Fortuna Iuvat&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;This blog post is an excerpt of my ebook &lt;em&gt;Modern R with the tidyverse&lt;/em&gt; that you can read for
free &lt;a href=&#34;https://b-rodrigues.github.io/modern_R/&#34;&gt;here&lt;/a&gt;. This is taken from Chapter 5, which presents
the &lt;code&gt;{tidyverse}&lt;/code&gt; packages and how to use them to compute descriptive statistics and manipulate data.
In the text below, I show how you can use the &lt;code&gt;{tidyverse}&lt;/code&gt; functions and principles for the
estimation of &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; using Monte Carlo simulation.&lt;/p&gt;
&lt;div id=&#34;going-beyond-descriptive-statistics-and-data-manipulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Going beyond descriptive statistics and data manipulation&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;{tidyverse}&lt;/code&gt; collection of packages can do much more than simply data manipulation and
descriptive statisics. You can use the principles we have covered and the functions you now know
to do much more. For instance, you can use a few &lt;code&gt;{tidyverse}&lt;/code&gt; functions to do Monte Carlo simulations,
for example to estimate &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Draw the unit circle inside the unit square, the ratio of the area of the circle to the area of the
square will be &lt;span class=&#34;math inline&#34;&gt;\(\pi/4\)&lt;/span&gt;. Then shot K arrows at the square; roughly &lt;span class=&#34;math inline&#34;&gt;\(K*\pi/4\)&lt;/span&gt; should have fallen
inside the circle. So if now you shoot N arrows at the square, and M fall inside the circle, you have
the following relationship &lt;span class=&#34;math inline&#34;&gt;\(M = N*\pi/4\)&lt;/span&gt;. You can thus compute &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; like so: &lt;span class=&#34;math inline&#34;&gt;\(\pi = 4*M/N\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The more arrows N you throw at the square, the better approximation of &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; you’ll have. Let’s
try to do this with a tidy Monte Carlo simulation. First, let’s randomly pick some points inside
the unit square:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(brotools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 5000

set.seed(2019)
points &amp;lt;- tibble(&amp;quot;x&amp;quot; = runif(n), &amp;quot;y&amp;quot; = runif(n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to know if a point is inside the unit circle, we need to check wether &lt;span class=&#34;math inline&#34;&gt;\(x^2 + y^2 &amp;lt; 1\)&lt;/span&gt;. Let’s
add a new column to the &lt;code&gt;points&lt;/code&gt; tibble, called &lt;code&gt;inside&lt;/code&gt; equal to 1 if the point is inside the
unit circle and 0 if not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points &amp;lt;- points %&amp;gt;% 
    mutate(inside = map2_dbl(.x = x, .y = y, ~ifelse(.x**2 + .y**2 &amp;lt; 1, 1, 0))) %&amp;gt;% 
    rowid_to_column(&amp;quot;N&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;points&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5,000 x 4
##        N       x      y inside
##    &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1     1 0.770   0.984       0
##  2     2 0.713   0.0107      1
##  3     3 0.303   0.133       1
##  4     4 0.618   0.0378      1
##  5     5 0.0505  0.677       1
##  6     6 0.0432  0.0846      1
##  7     7 0.820   0.727       0
##  8     8 0.00961 0.0758      1
##  9     9 0.102   0.373       1
## 10    10 0.609   0.676       1
## # … with 4,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;rowid_to_column()&lt;/code&gt; function, from the &lt;code&gt;{tibble}&lt;/code&gt; package, adds a new column to the data frame
with an id, going from 1 to the number of rows in the data frame. Now, I can compute the estimation
of &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; at each row, by computing the cumulative sum of the 1’s in the &lt;code&gt;inside&lt;/code&gt; column and dividing
that by the current value of &lt;code&gt;N&lt;/code&gt; column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points &amp;lt;- points %&amp;gt;% 
    mutate(estimate = 4*cumsum(inside)/N)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;cumsum(inside)&lt;/code&gt; is the &lt;code&gt;M&lt;/code&gt; from the formula. Now, we can finish by plotting the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(points) + 
    geom_line(aes(y = estimate, x = N), colour = &amp;quot;#82518c&amp;quot;) + 
    geom_hline(yintercept = pi) +
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-12-21-tidyverse_pi_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In Chapter 6, we are going to learn all about &lt;code&gt;{ggplot2}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As the number of tries grows, the estimation of &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; gets better.&lt;/p&gt;
&lt;p&gt;Using a data frame as a structure to hold our simulated points and the results makes it very easy
to avoid loops, and thus write code that is more concise and easier to follow.
If you studied a quantitative field in u8niversity, you might have done a similar exercise at the
time, very likely by defining a matrix to hold your points, and an empty vector to hold whether a
particular point was inside the unit circle. Then you wrote a loop to compute whether
a point was inside the unit circle, save this result in the before-defined empty vector and then
compute the estimation of &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;. Again, I take this opportunity here to stress that there is nothing
wrong with this approach per se, but R, with the &lt;code&gt;{tidyverse}&lt;/code&gt; is better suited for a workflow
where lists or data frames are the central objects and where the analyst operates over them
with functional programming techniques.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Manipulate dates easily with {lubridate}</title>
      <link>/blog/2018-12-15-lubridate_africa/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-12-15-lubridate_africa/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=FTQbiNvZqaY&#34;&gt;
&lt;img width = &#34;400&#34; src=&#34;/img/africa.jpg&#34; title = &#34;One of my favourite songs&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;This blog post is an excerpt of my ebook &lt;em&gt;Modern R with the tidyverse&lt;/em&gt; that you can read for
free &lt;a href=&#34;https://b-rodrigues.github.io/modern_R/&#34;&gt;here&lt;/a&gt;. This is taken from Chapter 5, which presents
the &lt;code&gt;{tidyverse}&lt;/code&gt; packages and how to use them to compute descriptive statistics and manipulate data.
In the text below, I scrape a table from Wikipedia, which shows when African countries gained
independence from other countries. Then, using &lt;code&gt;{lubridate}&lt;/code&gt; functions I show you how you can
answers questions such as &lt;em&gt;Which countries gained independence before 1960?&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;set-up-scraping-some-data-from-wikipedia&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set-up: scraping some data from Wikipedia&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;{lubridate}&lt;/code&gt; is yet another tidyverse package, that makes dealing with dates or duration data
(and intervals) as painless as possible. I do not use every function contained in the package
daily, and as such will only focus on some of the functions. However, if you have to deal with
dates often, you might want to explore the package thoroughly.&lt;/p&gt;
&lt;p&gt;Let’s get some data from a Wikipedia table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;page &amp;lt;- read_html(&amp;quot;https://en.wikipedia.org/wiki/Decolonisation_of_Africa&amp;quot;)

independence &amp;lt;- page %&amp;gt;%
    html_node(&amp;quot;.wikitable&amp;quot;) %&amp;gt;%
    html_table(fill = TRUE)

independence &amp;lt;- independence %&amp;gt;%
    select(-Rank) %&amp;gt;%
    map_df(~str_remove_all(., &amp;quot;\\[.*\\]&amp;quot;)) %&amp;gt;%
    rename(country = `Country[a]`,
           colonial_name = `Colonial name`,
           colonial_power = `Colonial power[b]`,
           independence_date = `Independence date[c]`,
           first_head_of_state = `First head of state[d]`,
           independence_won_through = `Independence won through`)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset was scraped from the following Wikipedia &lt;a href=&#34;https://en.wikipedia.org/wiki/Decolonisation_of_Africa#Timeline&#34;&gt;table&lt;/a&gt;.
It shows when African countries gained independence from which colonial powers. In Chapter 11, I
will show you how to scrape Wikipedia pages using R. For now, let’s take a look at the contents
of the dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;independence&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 54 x 6
##    country colonial_name colonial_power independence_da… first_head_of_s…
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;           
##  1 Liberia Liberia       United States  26 July 1847     Joseph Jenkins …
##  2 South … Cape Colony … United Kingdom 31 May 1910      Louis Botha     
##  3 Egypt   Sultanate of… United Kingdom 28 February 1922 Fuad I          
##  4 Eritrea Italian Erit… Italy          10 February 1947 Haile Selassie  
##  5 Libya   British Mili… United Kingdo… 24 December 1951 Idris           
##  6 Sudan   Anglo-Egypti… United Kingdo… 1 January 1956   Ismail al-Azhari
##  7 Tunisia French Prote… France         20 March 1956    Muhammad VIII a…
##  8 Morocco French Prote… France Spain   2 March 19567 A… Mohammed V      
##  9 Ghana   Gold Coast    United Kingdom 6 March 1957     Kwame Nkrumah   
## 10 Guinea  French West … France         2 October 1958   Ahmed Sékou Tou…
## # … with 44 more rows, and 1 more variable: independence_won_through &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as you can see, the date of independence is in a format that might make it difficult to answer questions
such as &lt;em&gt;Which African countries gained independence before 1960 ?&lt;/em&gt; for two reasons. First of all,
the date uses the name of the month instead of the number of the month (well, this is not such a
big deal, but still), and second of all the type of
the independence day column is &lt;em&gt;character&lt;/em&gt; and not “date”. So our first task is to correctly define the column
as being of type date, while making sure that R understands that &lt;em&gt;January&lt;/em&gt; is supposed to be “01”, and so
on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-lubridate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;{lubridate}&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;There are several helpful functions included in &lt;code&gt;{lubridate}&lt;/code&gt; to convert columns to dates. For instance
if the column you want to convert is of the form “2012-11-21”, then you would use the function &lt;code&gt;ymd()&lt;/code&gt;,
for “year-month-day”. If, however the column is “2012-21-11”, then you would use &lt;code&gt;ydm()&lt;/code&gt;. There’s
a few of these helper functions, and they can handle a lot of different formats for dates. In our case,
having the name of the month instead of the number might seem quite problematic, but it turns out
that this is a case that &lt;code&gt;{lubridate}&lt;/code&gt; handles painfully:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;lubridate&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:base&amp;#39;:
## 
##     date&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;independence &amp;lt;- independence %&amp;gt;%
  mutate(independence_date = dmy(independence_date))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 5 failed to parse.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some dates failed to parse, for instance for Morocco. This is because these countries have several
independence dates; this means that the string to convert looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;2 March 1956
7 April 1956
10 April 1958
4 January 1969&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which obviously cannot be converted by &lt;code&gt;{lubridate}&lt;/code&gt; without further manipulation. I ignore these cases for
simplicity’s sake.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the data now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;independence&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 54 x 6
##    country colonial_name colonial_power independence_da… first_head_of_s…
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;          &amp;lt;date&amp;gt;           &amp;lt;chr&amp;gt;           
##  1 Liberia Liberia       United States  1847-07-26       Joseph Jenkins …
##  2 South … Cape Colony … United Kingdom 1910-05-31       Louis Botha     
##  3 Egypt   Sultanate of… United Kingdom 1922-02-28       Fuad I          
##  4 Eritrea Italian Erit… Italy          1947-02-10       Haile Selassie  
##  5 Libya   British Mili… United Kingdo… 1951-12-24       Idris           
##  6 Sudan   Anglo-Egypti… United Kingdo… 1956-01-01       Ismail al-Azhari
##  7 Tunisia French Prote… France         1956-03-20       Muhammad VIII a…
##  8 Morocco French Prote… France Spain   NA               Mohammed V      
##  9 Ghana   Gold Coast    United Kingdom 1957-03-06       Kwame Nkrumah   
## 10 Guinea  French West … France         1958-10-02       Ahmed Sékou Tou…
## # … with 44 more rows, and 1 more variable: independence_won_through &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we now have a date column in the right format. We can now answer questions such as
&lt;em&gt;Which countries gained independence before 1960?&lt;/em&gt; quite easily, by using the functions &lt;code&gt;year()&lt;/code&gt;,
&lt;code&gt;month()&lt;/code&gt; and &lt;code&gt;day()&lt;/code&gt;. Let’s see which countries gained independence before 1960:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;independence %&amp;gt;%
  filter(year(independence_date) &amp;lt;= 1960) %&amp;gt;%
  pull(country)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Liberia&amp;quot;                          &amp;quot;South Africa&amp;quot;                    
##  [3] &amp;quot;Egypt&amp;quot;                            &amp;quot;Eritrea&amp;quot;                         
##  [5] &amp;quot;Libya&amp;quot;                            &amp;quot;Sudan&amp;quot;                           
##  [7] &amp;quot;Tunisia&amp;quot;                          &amp;quot;Ghana&amp;quot;                           
##  [9] &amp;quot;Guinea&amp;quot;                           &amp;quot;Cameroon&amp;quot;                        
## [11] &amp;quot;Togo&amp;quot;                             &amp;quot;Mali&amp;quot;                            
## [13] &amp;quot;Madagascar&amp;quot;                       &amp;quot;Democratic Republic of the Congo&amp;quot;
## [15] &amp;quot;Benin&amp;quot;                            &amp;quot;Niger&amp;quot;                           
## [17] &amp;quot;Burkina Faso&amp;quot;                     &amp;quot;Ivory Coast&amp;quot;                     
## [19] &amp;quot;Chad&amp;quot;                             &amp;quot;Central African Republic&amp;quot;        
## [21] &amp;quot;Republic of the Congo&amp;quot;            &amp;quot;Gabon&amp;quot;                           
## [23] &amp;quot;Mauritania&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You guessed it, &lt;code&gt;year()&lt;/code&gt; extracts the year of the date column and converts it as a &lt;em&gt;numeric&lt;/em&gt; so that we can work
on it. This is the same for &lt;code&gt;month()&lt;/code&gt; or &lt;code&gt;day()&lt;/code&gt;. Let’s try to see if countries gained their independence on
Christmas Eve:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;independence %&amp;gt;%
  filter(month(independence_date) == 12,
         day(independence_date) == 24) %&amp;gt;%
  pull(country)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Libya&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Seems like Libya was the only one! You can also operate on dates. For instance, let’s compute the difference between
two dates, using the &lt;code&gt;interval()&lt;/code&gt; column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;independence %&amp;gt;%
  mutate(today = lubridate::today()) %&amp;gt;%
  mutate(independent_since = interval(independence_date, today)) %&amp;gt;%
  select(country, independent_since)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 54 x 2
##    country      independent_since             
##    &amp;lt;chr&amp;gt;        &amp;lt;S4: Interval&amp;gt;                
##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC
##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC
##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC
##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC
##  5 Libya        1951-12-24 UTC--2019-02-10 UTC
##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC
##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC
##  8 Morocco      NA--NA                        
##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC
## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC
## # … with 44 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;independent_since&lt;/code&gt; column now contains an &lt;em&gt;interval&lt;/em&gt; object that we can convert to years:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;independence %&amp;gt;%
  mutate(today = lubridate::today()) %&amp;gt;%
  mutate(independent_since = interval(independence_date, today)) %&amp;gt;%
  select(country, independent_since) %&amp;gt;%
  mutate(years_independent = as.numeric(independent_since, &amp;quot;years&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 54 x 3
##    country      independent_since              years_independent
##    &amp;lt;chr&amp;gt;        &amp;lt;S4: Interval&amp;gt;                             &amp;lt;dbl&amp;gt;
##  1 Liberia      1847-07-26 UTC--2019-02-10 UTC             172. 
##  2 South Africa 1910-05-31 UTC--2019-02-10 UTC             109. 
##  3 Egypt        1922-02-28 UTC--2019-02-10 UTC              97.0
##  4 Eritrea      1947-02-10 UTC--2019-02-10 UTC              72  
##  5 Libya        1951-12-24 UTC--2019-02-10 UTC              67.1
##  6 Sudan        1956-01-01 UTC--2019-02-10 UTC              63.1
##  7 Tunisia      1956-03-20 UTC--2019-02-10 UTC              62.9
##  8 Morocco      NA--NA                                      NA  
##  9 Ghana        1957-03-06 UTC--2019-02-10 UTC              61.9
## 10 Guinea       1958-10-02 UTC--2019-02-10 UTC              60.4
## # … with 44 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now see for how long the last country to gain independence has been independent.
Because the data is not tidy (in some cases, an African country was colonized by two powers,
see Libya), I will only focus on 4 European colonial powers: Belgium, France, Portugal and the United Kingdom:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;independence %&amp;gt;%
  filter(colonial_power %in% c(&amp;quot;Belgium&amp;quot;, &amp;quot;France&amp;quot;, &amp;quot;Portugal&amp;quot;, &amp;quot;United Kingdom&amp;quot;)) %&amp;gt;%
  mutate(today = lubridate::today()) %&amp;gt;%
  mutate(independent_since = interval(independence_date, today)) %&amp;gt;%
  mutate(years_independent = as.numeric(independent_since, &amp;quot;years&amp;quot;)) %&amp;gt;%
  group_by(colonial_power) %&amp;gt;%
  summarise(last_colony_independent_for = min(years_independent, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   colonial_power last_colony_independent_for
##   &amp;lt;chr&amp;gt;                                &amp;lt;dbl&amp;gt;
## 1 Belgium                               56.6
## 2 France                                41.6
## 3 Portugal                              43.2
## 4 United Kingdom                        42.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;{lubridate}&lt;/code&gt; contains many more functions. If you often work with dates, duration or interval data, &lt;code&gt;{lubridate}&lt;/code&gt;
is a package that you have to master.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What hyper-parameters are, and what to do with them; an illustration with ridge regression</title>
      <link>/blog/2018-12-02-hyper-parameters/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-12-02-hyper-parameters/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=13Gd5kpLzsw&#34;&gt;
&lt;img width = &#34;400&#34; src=&#34;/img/ridge.jpg&#34; title = &#34;Gameboy ridge&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;
&lt;p&gt;This blog post is an excerpt of my ebook &lt;em&gt;Modern R with the tidyverse&lt;/em&gt; that you can read for
free &lt;a href=&#34;https://b-rodrigues.github.io/modern_R/&#34;&gt;here&lt;/a&gt;. This is taken from Chapter 7, which deals
with statistical models. In the text below, I explain what hyper-parameters are, and as an example
I run a ridge regression using the &lt;code&gt;{glmnet}&lt;/code&gt; package. The book is still being written, so
comments are more than welcome!&lt;/p&gt;
&lt;div id=&#34;hyper-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hyper-parameters&lt;/h2&gt;
&lt;p&gt;Hyper-parameters are parameters of the model that cannot be directly learned from the data.
A linear regression does not have any hyper-parameters, but a random forest for instance has several.
You might have heard of ridge regression, lasso and elasticnet. These are
extensions to linear models that avoid over-fitting by penalizing &lt;em&gt;large&lt;/em&gt; models. These
extensions of the linear regression have hyper-parameters that the practitioner has to tune. There
are several ways one can tune these parameters, for example, by doing a grid-search, or a random
search over the grid or using more elaborate methods. To introduce hyper-parameters, let’s get
to know ridge regression, also called Tikhonov regularization.&lt;/p&gt;
&lt;div id=&#34;ridge-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ridge regression&lt;/h3&gt;
&lt;p&gt;Ridge regression is used when the data you are working with has a lot of explanatory variables,
or when there is a risk that a simple linear regression might overfit to the training data, because,
for example, your explanatory variables are collinear.
If you are training a linear model and then you notice that it generalizes very badly to new,
unseen data, it is very likely that the linear model you trained overfits the data.
In this case, ridge regression might prove useful. The way ridge regression works might seem
counter-intuititive; it boils down to fitting a &lt;em&gt;worse&lt;/em&gt; model to the training data, but in return,
this worse model will generalize better to new data.&lt;/p&gt;
&lt;p&gt;The closed form solution of the ordinary least squares estimator is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\widehat{\beta} = (X&amp;#39;X)^{-1}X&amp;#39;Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the design matrix (the matrix made up of the explanatory variables) and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; is the
dependent variable. For ridge regression, this closed form solution changes a little bit:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\widehat{\beta} = (X&amp;#39;X + \lambda I_p)^{-1}X&amp;#39;Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda \in \mathbb{R}\)&lt;/span&gt; is an hyper-parameter and &lt;span class=&#34;math inline&#34;&gt;\(I_p\)&lt;/span&gt; is the identity matrix of dimension &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;
(&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of explanatory variables).
This formula above is the closed form solution to the following optimisation program:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{i=1}^n \left(y_i - \sum_{j=1}^px_{ij}\beta_j\right)^2 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;such that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sum_{j=1}^p(\beta_j)^2 &amp;lt; c
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for any strictly positive &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;glmnet()&lt;/code&gt; function from the &lt;code&gt;{glmnet}&lt;/code&gt; package can be used for ridge regression, by setting
the &lt;code&gt;alpha&lt;/code&gt; argument to 0 (setting it to 1 would do LASSO, and setting it to a number between
0 and 1 would do elasticnet). But in order to compare linear regression and ridge regression,
let me first divide the data into a training set and a testing set. I will be using the &lt;code&gt;Housing&lt;/code&gt;
data from the &lt;code&gt;{Ecdat}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(Ecdat)
library(glmnet)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;index &amp;lt;- 1:nrow(Housing)

set.seed(12345)
train_index &amp;lt;- sample(index, round(0.90*nrow(Housing)), replace = FALSE)

test_index &amp;lt;- setdiff(index, train_index)

train_x &amp;lt;- Housing[train_index, ] %&amp;gt;% 
    select(-price)

train_y &amp;lt;- Housing[train_index, ] %&amp;gt;% 
    pull(price)

test_x &amp;lt;- Housing[test_index, ] %&amp;gt;% 
    select(-price)

test_y &amp;lt;- Housing[test_index, ] %&amp;gt;% 
    pull(price)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I do the train/test split this way, because &lt;code&gt;glmnet()&lt;/code&gt; requires a design matrix as input, and not
a formula. Design matrices can be created using the &lt;code&gt;model.matrix()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_matrix &amp;lt;- model.matrix(train_y ~ ., data = train_x)

test_matrix &amp;lt;- model.matrix(test_y ~ ., data = test_x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run an unpenalized linear regression, we can set the penalty to 0:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_lm_ridge &amp;lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model above provides the same result as a linear regression. Let’s compare the coefficients between the two:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(model_lm_ridge)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 13 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                       s0
## (Intercept) -3247.030393
## (Intercept)     .       
## lotsize         3.520283
## bedrooms     1745.211187
## bathrms     14337.551325
## stories      6736.679470
## drivewayyes  5687.132236
## recroomyes   5701.831289
## fullbaseyes  5708.978557
## gashwyes    12508.524241
## aircoyes    12592.435621
## garagepl     4438.918373
## prefareayes  9085.172469&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and now the coefficients of the linear regression (because I provide a design matrix, I have to use
&lt;code&gt;lm.fit()&lt;/code&gt; instead of &lt;code&gt;lm()&lt;/code&gt; which requires a formula, not a matrix.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(lm.fit(x = train_matrix, y = train_y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  (Intercept)      lotsize     bedrooms      bathrms      stories 
## -3245.146665     3.520357  1744.983863 14336.336858  6737.000410 
##  drivewayyes   recroomyes  fullbaseyes     gashwyes     aircoyes 
##  5686.394123  5700.210775  5709.493884 12509.005265 12592.367268 
##     garagepl  prefareayes 
##  4439.029607  9085.409155&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;as you can see, the coefficients are the same. Let’s compute the RMSE for the unpenalized linear
regression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds_lm &amp;lt;- predict(model_lm_ridge, test_matrix)

rmse_lm &amp;lt;- sqrt(mean((preds_lm - test_y)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The RMSE for the linear unpenalized regression is equal to 14463.08.&lt;/p&gt;
&lt;p&gt;Let’s now run a ridge regression, with &lt;code&gt;lambda&lt;/code&gt; equal to 100, and see if the RMSE is smaller:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_ridge &amp;lt;- glmnet(y = train_y, x = train_matrix, alpha = 0, lambda = 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and let’s compute the RMSE again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;preds &amp;lt;- predict(model_ridge, test_matrix)

rmse &amp;lt;- sqrt(mean((preds - test_y)^2))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The RMSE for the linear penalized regression is equal to 14460.71, which is smaller than before.
But which value of &lt;code&gt;lambda&lt;/code&gt; gives smallest RMSE? To find out, one must run model over a grid of
&lt;code&gt;lambda&lt;/code&gt; values and pick the model with lowest RMSE. This procedure is available in the &lt;code&gt;cv.glmnet()&lt;/code&gt;
function, which picks the best value for &lt;code&gt;lambda&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_model &amp;lt;- cv.glmnet(train_matrix, train_y)
# lambda that minimises the MSE
best_model$lambda.min&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 66.07936&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to &lt;code&gt;cv.glmnet()&lt;/code&gt; the best value for &lt;code&gt;lambda&lt;/code&gt; is 66.0793576.
In the next section, we will implement cross validation ourselves, in order to find the hyper-parameters
of a random forest.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt; or &lt;a href=&#34;https://www.paypal.me/brodriguesco&#34;&gt;paypal.me&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A tutorial on tidy cross-validation with R</title>
      <link>/blog/2018-11-25-tidy_cv/</link>
      <pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-25-tidy_cv/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=7T6pgZdFLP0&#34;&gt;
&lt;image width = &#34;400&#34; src=&#34;/img/cross_validation.gif&#34; title = &#34;Visual representation of cross⁻validation inside your computer *click for virtual weed*&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This blog posts will use several packages from the
&lt;a href=&#34;https://github.com/tidymodels&#34;&gt;&lt;code&gt;{tidymodels}&lt;/code&gt;&lt;/a&gt; collection of packages, namely
&lt;a href=&#34;https://tidymodels.github.io/recipes/&#34;&gt;&lt;code&gt;{recipes}&lt;/code&gt;&lt;/a&gt;,
&lt;a href=&#34;https://tidymodels.github.io/rsample/&#34;&gt;&lt;code&gt;{rsample}&lt;/code&gt;&lt;/a&gt; and
&lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34;&gt;&lt;code&gt;{parsnip}&lt;/code&gt;&lt;/a&gt; to train a random forest the tidy way. I will
also use &lt;a href=&#34;http://mlrmbo.mlr-org.com/&#34;&gt;&lt;code&gt;{mlrMBO}&lt;/code&gt;&lt;/a&gt; to tune the hyper-parameters of the random forest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set up&lt;/h2&gt;
&lt;p&gt;Let’s load the needed packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;tidymodels&amp;quot;)
library(&amp;quot;parsnip&amp;quot;)
library(&amp;quot;brotools&amp;quot;)
library(&amp;quot;mlbench&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the data, included in the &lt;code&gt;{mlrbench}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;BostonHousing2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will train a random forest to predict the housing price, which is the &lt;code&gt;cmedv&lt;/code&gt; column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(BostonHousing2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         town tract      lon     lat medv cmedv    crim zn indus chas   nox
## 1     Nahant  2011 -70.9550 42.2550 24.0  24.0 0.00632 18  2.31    0 0.538
## 2 Swampscott  2021 -70.9500 42.2875 21.6  21.6 0.02731  0  7.07    0 0.469
## 3 Swampscott  2022 -70.9360 42.2830 34.7  34.7 0.02729  0  7.07    0 0.469
## 4 Marblehead  2031 -70.9280 42.2930 33.4  33.4 0.03237  0  2.18    0 0.458
## 5 Marblehead  2032 -70.9220 42.2980 36.2  36.2 0.06905  0  2.18    0 0.458
## 6 Marblehead  2033 -70.9165 42.3040 28.7  28.7 0.02985  0  2.18    0 0.458
##      rm  age    dis rad tax ptratio      b lstat
## 1 6.575 65.2 4.0900   1 296    15.3 396.90  4.98
## 2 6.421 78.9 4.9671   2 242    17.8 396.90  9.14
## 3 7.185 61.1 4.9671   2 242    17.8 392.83  4.03
## 4 6.998 45.8 6.0622   3 222    18.7 394.63  2.94
## 5 7.147 54.2 6.0622   3 222    18.7 396.90  5.33
## 6 6.430 58.7 6.0622   3 222    18.7 394.12  5.21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Only keep relevant columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boston &amp;lt;- BostonHousing2 %&amp;gt;% 
    select(-medv, -town, -lon, -lat) %&amp;gt;% 
    rename(price = cmedv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I remove &lt;code&gt;town&lt;/code&gt;, &lt;code&gt;lat&lt;/code&gt; and &lt;code&gt;lon&lt;/code&gt; because the information contained in the column &lt;code&gt;tract&lt;/code&gt; is enough.&lt;/p&gt;
&lt;p&gt;To train and evaluate the model’s performance, I split the data in two.
One data set, which I call the training set, will be further split into two down below. I won’t
touch the second data set, the test set, until the very end.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_test_split &amp;lt;- initial_split(boston, prop = 0.9)

housing_train &amp;lt;- training(train_test_split)

housing_test &amp;lt;- testing(train_test_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I want to train a random forest to predict price of houses, but random forests have so-called
hyperparameters, which are parameters that cannot be estimated, or learned, from the data. Instead,
these parameters have to be chosen by the analyst. In order to choose them, you can
use values from the literature that seemed to have worked well (like is done in Macro-econometrics)
or you can further split the train set into two, create a grid of hyperparameter, train the model
on one part of the data for all values of the grid, and compare the predictions of the models on the
second part of the data. You then stick with the model that performed the best, for example, the
model with lowest RMSE. The thing is, you can’t estimate the true value of the RMSE with only
one value. It’s like if you wanted to estimate the height of the population by drawing one single
observation from the population. You need a bit more observations. To approach the true value of the
RMSE for a give set of hyperparameters, instead of doing one split, I’ll do 30. I then
compute the average RMSE, which implies training 30 models for each combination of the values of the
hyperparameters I am interested in.&lt;/p&gt;
&lt;p&gt;First, let’s split the training data again, using the &lt;code&gt;mc_cv()&lt;/code&gt; function from &lt;code&gt;{rsample}&lt;/code&gt; package.
This function implements Monte Carlo cross-validation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;validation_data &amp;lt;- mc_cv(housing_train, prop = 0.9, times = 30)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does &lt;code&gt;validation_data&lt;/code&gt; look like?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;validation_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # # Monte Carlo cross-validation (0.9/0.1) with 30 resamples  
## # A tibble: 30 x 2
##    splits           id        
##    &amp;lt;list&amp;gt;           &amp;lt;chr&amp;gt;     
##  1 &amp;lt;split [411/45]&amp;gt; Resample01
##  2 &amp;lt;split [411/45]&amp;gt; Resample02
##  3 &amp;lt;split [411/45]&amp;gt; Resample03
##  4 &amp;lt;split [411/45]&amp;gt; Resample04
##  5 &amp;lt;split [411/45]&amp;gt; Resample05
##  6 &amp;lt;split [411/45]&amp;gt; Resample06
##  7 &amp;lt;split [411/45]&amp;gt; Resample07
##  8 &amp;lt;split [411/45]&amp;gt; Resample08
##  9 &amp;lt;split [411/45]&amp;gt; Resample09
## 10 &amp;lt;split [411/45]&amp;gt; Resample10
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look further down:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;validation_data$splits[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;411/45/456&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first value is the number of rows of the first set, the second value of the second, and the third
was the original amount of values in the training data, before splitting again.&lt;/p&gt;
&lt;p&gt;How should we call these two new data sets? The author of &lt;code&gt;{rsample}&lt;/code&gt;, Max Kuhn, talks about
the &lt;em&gt;analysis&lt;/em&gt; and the &lt;em&gt;assessment&lt;/em&gt; sets:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;rsample convention for now but I intend on using it everywhere. Reusing training and testing is insane.&lt;/p&gt;&amp;mdash; Max Kuhn (@topepos) &lt;a href=&#34;https://twitter.com/topepos/status/1066131042615140353?ref_src=twsrc%5Etfw&#34;&gt;November 24, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now, in order to continue I need pre-process the data. I will do this in three steps.
The first and the second step are used to center and scale the numeric variables and the third step
converts character and factor variables to dummy variables. This is needed because I will train a
random forest, which cannot handle factor variables directly. Let’s define a recipe to do that,
and start by pre-processing the testing set. I write a wrapper function around the recipe,
because I will need to apply this recipe to various data sets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simple_recipe &amp;lt;- function(dataset){
    recipe(price ~ ., data = dataset) %&amp;gt;%
        step_center(all_numeric()) %&amp;gt;%
        step_scale(all_numeric()) %&amp;gt;%
        step_dummy(all_nominal())
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once the recipe is defined, I can use the &lt;code&gt;prep()&lt;/code&gt; function, which estimates the parameters from
the data which are needed to process the data. For example, for centering, &lt;code&gt;prep()&lt;/code&gt; estimates
the mean which will then be subtracted from the variables. With &lt;code&gt;bake()&lt;/code&gt; the estimates are then
applied on the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testing_rec &amp;lt;- prep(simple_recipe(housing_test), testing = housing_test)

test_data &amp;lt;- bake(testing_rec, newdata = housing_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Please use `new_data` instead of `newdata` with `bake`. 
## In recipes versions &amp;gt;= 0.1.4, this will cause an error.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is important to split the data before using &lt;code&gt;prep()&lt;/code&gt; and &lt;code&gt;bake()&lt;/code&gt;, because if not, you will
use observations from the test set in the &lt;code&gt;prep()&lt;/code&gt; step, and thus introduce knowledge from the test
set into the training data. This is called data leakage, and must be avoided. This is why it is
necessary to first split the training data into an analysis and an assessment set, and then also
pre-process these sets separately. However, the &lt;code&gt;validation_data&lt;/code&gt; object cannot now be used with
&lt;code&gt;recipe()&lt;/code&gt;, because it is not a dataframe. No worries, I simply need to write a function that extracts
the analysis and assessment sets from the &lt;code&gt;validation_data&lt;/code&gt; object, applies the pre-processing, trains
the model, and returns the RMSE. This will be a big function, at the center of the analysis.&lt;/p&gt;
&lt;p&gt;But before that, let’s run a simple linear regression, as a benchmark. For the linear regression, I will
not use any CV, so let’s pre-process the training set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainlm_rec &amp;lt;- prep(simple_recipe(housing_train), testing = housing_train)

trainlm_data &amp;lt;- bake(trainlm_rec, newdata = housing_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Please use `new_data` instead of `newdata` with `bake`. 
## In recipes versions &amp;gt;= 0.1.4, this will cause an error.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linreg_model &amp;lt;- lm(price ~ ., data = trainlm_data)

broom::augment(linreg_model, newdata = test_data) %&amp;gt;% 
    rmse(price, .fitted)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 rmse    standard       0.438&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;broom::augment()&lt;/code&gt; adds the predictions to the &lt;code&gt;test_data&lt;/code&gt; in a new column, &lt;code&gt;.fitted&lt;/code&gt;. I won’t
use this trick with the random forest, because there is no &lt;code&gt;augment()&lt;/code&gt; method for random forests
from the &lt;code&gt;{ranger}&lt;/code&gt; which I’ll use. I’ll add the predictions to the data myself.&lt;/p&gt;
&lt;p&gt;Ok, now let’s go back to the random forest and write the big function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_rf &amp;lt;- function(mtry, trees, split, id){
    
    analysis_set &amp;lt;- analysis(split)
    
    analysis_prep &amp;lt;- prep(simple_recipe(analysis_set), training = analysis_set)
    
    analysis_processed &amp;lt;- bake(analysis_prep, newdata = analysis_set)
    
    model &amp;lt;- rand_forest(mtry = mtry, trees = trees) %&amp;gt;%
        set_engine(&amp;quot;ranger&amp;quot;, importance = &amp;#39;impurity&amp;#39;) %&amp;gt;%
        fit(price ~ ., data = analysis_processed)

    assessment_set &amp;lt;- assessment(split)
    
    assessment_prep &amp;lt;- prep(simple_recipe(assessment_set), testing = assessment_set)
    
    assessment_processed &amp;lt;- bake(assessment_prep, newdata = assessment_set)

    tibble::tibble(&amp;quot;id&amp;quot; = id,
        &amp;quot;truth&amp;quot; = assessment_processed$price,
        &amp;quot;prediction&amp;quot; = unlist(predict(model, new_data = assessment_processed)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;rand_forest()&lt;/code&gt; function is available from the &lt;code&gt;{parsnip}&lt;/code&gt; package. This package provides an
unified interface to a lot of other machine learning packages. This means that instead of having to
learn the syntax of &lt;code&gt;range()&lt;/code&gt; and &lt;code&gt;randomForest()&lt;/code&gt; and, and… you can simply use the &lt;code&gt;rand_forest()&lt;/code&gt;
function and change the &lt;code&gt;engine&lt;/code&gt; argument to the one you want (&lt;code&gt;ranger&lt;/code&gt;, &lt;code&gt;randomForest&lt;/code&gt;, etc).&lt;/p&gt;
&lt;p&gt;Let’s try this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results_example &amp;lt;- map2_df(.x = validation_data$splits,
                           .y = validation_data$id,
                           ~my_rf(mtry = 3, trees = 200, split = .x, id = .y))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(results_example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   id           truth prediction
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 Resample01  0.0235    -0.104 
## 2 Resample01 -0.135     -0.0906
## 3 Resample01 -0.378     -0.158 
## 4 Resample01 -0.232      0.0623
## 5 Resample01 -0.0859     0.0173
## 6 Resample01  0.169      0.303&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now compute the RMSE when &lt;code&gt;mtry&lt;/code&gt; = 3 and &lt;code&gt;trees&lt;/code&gt; = 200:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;results_example %&amp;gt;%
    group_by(id) %&amp;gt;%
    rmse(truth, prediction) %&amp;gt;%
    summarise(mean_rmse = mean(.estimate)) %&amp;gt;%
    pull&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4319164&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The random forest has already lower RMSE than the linear regression. The goal now is to lower this
RMSE by tuning the &lt;code&gt;mtry&lt;/code&gt; and &lt;code&gt;trees&lt;/code&gt; hyperparameters. For this, I will use Bayesian Optimization
methods implemented in the &lt;code&gt;{mlrMBO}&lt;/code&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-hyperparameter-optimization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian hyperparameter optimization&lt;/h2&gt;
&lt;p&gt;I will re-use the code from above, and define a function that does everything from pre-processing
to returning the metric I want to minimize by tuning the hyperparameters, the RMSE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tuning &amp;lt;- function(param, validation_data){

    mtry &amp;lt;- param[1]
    trees &amp;lt;- param[2]

    results &amp;lt;- purrr::map2_df(.x = validation_data$splits,
                       .y = validation_data$id,
                       ~my_rf(mtry = mtry, trees = trees, split = .x, id = .y))

    results %&amp;gt;%
        group_by(id) %&amp;gt;%
        rmse(truth, prediction) %&amp;gt;%
        summarise(mean_rmse = mean(.estimate)) %&amp;gt;%
        pull
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is exactly the code from before, but it now returns the RMSE. Let’s try the function
with the values from before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tuning(c(3, 200), validation_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4330951&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s also plot the value of RMSE for &lt;code&gt;mtry = 3&lt;/code&gt; and &lt;code&gt;trees&lt;/code&gt; from 200 to 300. This takes some
time, because I need to evaluate this costly function 100 times. If evaluating the function was
cheap, I could have made a 3D plot by varying values of &lt;code&gt;mtry&lt;/code&gt; too, but then again if evaluating
the function was cheap, I would run an exhaustive grid search to find the hyperparameters instead of
using Bayesian optimization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_points &amp;lt;- crossing(&amp;quot;mtry&amp;quot; = 3, &amp;quot;trees&amp;quot; = seq(200, 300))

plot_data &amp;lt;- plot_points %&amp;gt;% 
    mutate(value = map_dbl(seq(200, 300), ~tuning(c(3, .), validation_data)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_data %&amp;gt;% 
    ggplot(aes(y = value, x = trees)) + 
    geom_line(colour = &amp;quot;#82518c&amp;quot;) + 
    theme_blog() +
    ggtitle(&amp;quot;RMSE for mtry = 3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-25-tidy_cv_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;mtry = 3&lt;/code&gt; the minimum seems to lie around 255. The function to minimize is not smooth at all.&lt;/p&gt;
&lt;p&gt;I now follow the code that can be found in the &lt;a href=&#34;https://arxiv.org/abs/1703.03373&#34;&gt;arxiv&lt;/a&gt; paper to
run the optimization. I think I got the gist of the paper, but I did not understand everything yet.
For now, I am still experimenting with the library at the moment, but from what I understand, a
simpler model, called the surrogate model, is used to look for promising points and to evaluate the
value of the function at these points. This seems somewhat similar (in spirit) to the
&lt;em&gt;Indirect Inference&lt;/em&gt; method as described in &lt;a href=&#34;https://www.jstor.org/stable/2285076&#34;&gt;Gourieroux, Monfort, Renault&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s first load the package and create the function to optimize:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;mlrMBO&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fn &amp;lt;- makeSingleObjectiveFunction(name = &amp;quot;tuning&amp;quot;,
                                 fn = tuning,
                                 par.set = makeParamSet(makeIntegerParam(&amp;quot;x1&amp;quot;, lower = 3, upper = 8),
                                                        makeIntegerParam(&amp;quot;x2&amp;quot;, lower = 50, upper = 500)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function is based on the function I defined before. The parameters to optimize are also
defined as are their bounds. I will look for &lt;code&gt;mtry&lt;/code&gt; between the values of 3 and 8, and &lt;code&gt;trees&lt;/code&gt;
between 50 and 500.&lt;/p&gt;
&lt;p&gt;Now comes the part I didn’t quite get.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create initial random Latin Hypercube Design of 10 points
library(lhs)# for randomLHS
des &amp;lt;- generateDesign(n = 5L * 2L, getParamSet(fn), fun = randomLHS)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I think this means that these 10 points are the points used to start the whole process. I did not
understand why they have to be sampled from a hypercube, but ok. Then I choose the surrogate model,
a random forest too, and predict the standard error. Here also, I did not quite get why the
standard error can be an option.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Specify kriging model with standard error estimation
surrogate &amp;lt;- makeLearner(&amp;quot;regr.ranger&amp;quot;, predict.type = &amp;quot;se&amp;quot;, keep.inbag = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I define some options:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set general controls
ctrl &amp;lt;- makeMBOControl()
ctrl &amp;lt;- setMBOControlTermination(ctrl, iters = 10L)
ctrl &amp;lt;- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And this is the optimization part:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Start optimization
result &amp;lt;- mbo(fn, des, surrogate, ctrl, more.args = list(&amp;quot;validation_data&amp;quot; = validation_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Recommended parameters:
## x1=6; x2=381
## Objective: y = 0.393
## 
## Optimization path
## 10 + 10 entries in total, displaying last 10 (or less):
##    x1  x2         y dob eol error.message exec.time            ei
## 11  6 370 0.3943479   1  NA          &amp;lt;NA&amp;gt;     8.913 -3.134568e-05
## 12  6 362 0.3950402   2  NA          &amp;lt;NA&amp;gt;     8.844 -2.987934e-05
## 13  6 373 0.3939587   3  NA          &amp;lt;NA&amp;gt;     8.939 -2.259674e-05
## 14  6 394 0.3962875   4  NA          &amp;lt;NA&amp;gt;     9.342 -7.427682e-06
## 15  6 368 0.3944954   5  NA          &amp;lt;NA&amp;gt;     8.760 -4.121337e-06
## 16  6 378 0.3938796   6  NA          &amp;lt;NA&amp;gt;     8.949 -4.503591e-07
## 17  6 381 0.3934176   7  NA          &amp;lt;NA&amp;gt;     9.109 -1.141853e-06
## 18  6 380 0.3948077   8  NA          &amp;lt;NA&amp;gt;     9.026 -4.718394e-08
## 19  6 381 0.3932636   9  NA          &amp;lt;NA&amp;gt;     9.022 -9.801395e-08
## 20  6 383 0.3953004  10  NA          &amp;lt;NA&amp;gt;     9.184 -1.579619e-09
##    error.model train.time prop.type propose.time           se      mean
## 11        &amp;lt;NA&amp;gt;      0.014 infill_ei        0.449 0.0010924600 0.3955131
## 12        &amp;lt;NA&amp;gt;      0.012 infill_ei        0.458 0.0007415920 0.3948705
## 13        &amp;lt;NA&amp;gt;      0.012 infill_ei        0.460 0.0006116756 0.3947185
## 14        &amp;lt;NA&amp;gt;      0.012 infill_ei        0.729 0.0003104694 0.3943572
## 15        &amp;lt;NA&amp;gt;      0.023 infill_ei        0.444 0.0003446061 0.3945085
## 16        &amp;lt;NA&amp;gt;      0.013 infill_ei        0.458 0.0002381887 0.3944642
## 17        &amp;lt;NA&amp;gt;      0.013 infill_ei        0.492 0.0002106454 0.3943200
## 18        &amp;lt;NA&amp;gt;      0.013 infill_ei        0.516 0.0002093524 0.3940764
## 19        &amp;lt;NA&amp;gt;      0.014 infill_ei        0.756 0.0002481260 0.3941597
## 20        &amp;lt;NA&amp;gt;      0.013 infill_ei        0.483 0.0001687982 0.3939285&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the recommended parameters are 6 for &lt;code&gt;mtry&lt;/code&gt; and 381 for &lt;code&gt;trees&lt;/code&gt;. The value of the RMSE is lower
than before, and equals 0.393.
Let’s now train the random forest on the training data with this values. First, I pre-process the
training data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;training_rec &amp;lt;- prep(simple_recipe(housing_train), testing = housing_train)

train_data &amp;lt;- bake(training_rec, newdata = housing_train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Please use `new_data` instead of `newdata` with `bake`. 
## In recipes versions &amp;gt;= 0.1.4, this will cause an error.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now train our final model and predict the prices:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_model &amp;lt;- rand_forest(mtry = 6, trees = 381) %&amp;gt;%
        set_engine(&amp;quot;ranger&amp;quot;, importance = &amp;#39;impurity&amp;#39;) %&amp;gt;%
        fit(price ~ ., data = train_data)

price_predict &amp;lt;- predict(final_model, new_data = select(test_data, -price))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s transform the data back and compare the predicted prices to the true ones visually:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cbind(price_predict * sd(housing_train$price) + mean(housing_train$price), 
      housing_test$price)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        .pred housing_test$price
## 1  34.811111               34.7
## 2  20.591304               22.9
## 3  19.463920               18.9
## 4  20.321990               21.7
## 5  19.063132               17.5
## 6  15.969125               14.5
## 7  18.203023               15.6
## 8  17.139943               13.9
## 9  21.393329               24.2
## 10 27.508482               25.0
## 11 24.030162               24.1
## 12 21.222857               21.2
## 13 23.052677               22.2
## 14 20.303233               19.3
## 15 21.134554               21.7
## 16 22.913097               18.5
## 17 20.029506               18.8
## 18 18.045923               16.2
## 19 17.321006               13.3
## 20 18.201785               13.4
## 21 29.928316               32.5
## 22 24.339983               26.4
## 23 45.518316               42.3
## 24 29.551251               26.7
## 25 26.513473               30.1
## 26 42.984738               46.7
## 27 43.513001               48.3
## 28 25.436146               23.3
## 29 21.766247               24.3
## 30 36.328740               36.0
## 31 32.830061               31.0
## 32 38.736098               35.2
## 33 31.573311               32.0
## 34 19.847848               19.4
## 35 23.401032               23.1
## 36 22.000914               19.4
## 37 20.155696               18.7
## 38 21.342003               22.6
## 39 20.846330               19.9
## 40 13.752108               13.8
## 41 12.499064               13.1
## 42 15.019987               16.3
## 43  8.489851                7.2
## 44  7.803981               10.4
## 45 18.629488               20.8
## 46 14.645669               14.3
## 47 15.094423               15.2
## 48 20.470057               17.7
## 49 15.147170               13.3
## 50 15.880035               13.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s now compute the RMSE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble::tibble(&amp;quot;truth&amp;quot; = test_data$price,
        &amp;quot;prediction&amp;quot; = unlist(price_predict)) %&amp;gt;% 
    rmse(truth, prediction)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 rmse    standard       0.327&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Very nice.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The best way to visit Luxembourguish castles is doing data science &#43; combinatorial optimization</title>
      <link>/blog/2018-11-21-lux_castle/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-21-lux_castle/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=XQDm6I3mbMU&#34;&gt;
&lt;img width = &#34;400&#34; src=&#34;/img/harold_kumar.jpg&#34; title = &#34;Only 00&#39;s kids will get the reference&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Inspired by David Schoch’s blog post,
&lt;a href=&#34;http://blog.schochastics.net/post/traveling-beerdrinker-problem/&#34;&gt;Traveling Beerdrinker Problem&lt;/a&gt;.
Check out his blog, he has some amazing posts!&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Luxembourg, as any proper European country, is full of castles. According to Wikipedia,&lt;/p&gt;
&lt;p&gt;“By some optimistic estimates, there are as many as 130 castles in Luxembourg but more realistically
there are probably just over a hundred, although many of these could be considered large residences
or manor houses rather than castles”.&lt;/p&gt;
&lt;p&gt;I see the editors are probably German or French, calling our castles &lt;em&gt;manor houses&lt;/em&gt;! They only say
that because Luxembourg is small, so our castles must be small too, right?&lt;/p&gt;
&lt;p&gt;Banter aside, with that many castles, what is the best way to visit them all? And by best way I mean
shortest way. This is a classical &lt;strong&gt;Travelling salesman problem&lt;/strong&gt;. To solve this, I need the following elements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A list of castles to visit, with their coordinates&lt;/li&gt;
&lt;li&gt;The distances between these castles to each other&lt;/li&gt;
&lt;li&gt;A program to solve the TSP&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s start by loading some packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(magrittr)
library(rvest)
library(curl)
library(brotools)
library(RJSONIO)
library(TSP)
library(ggimage)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First step; scrape the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scraping-the-data-thats-the-data-science-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scraping the data (that’s the data science part)&lt;/h2&gt;
&lt;p&gt;Let’s start by having a list of castles. For this, I go to the French Wikipedia page of
&lt;a href=&#34;https://fr.wikipedia.org/wiki/Liste_de_ch%C3%A2teaux_luxembourgeois&#34;&gt;Luxembourguish castles&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Luxembourguish page is more &lt;a href=&#34;https://lb.wikipedia.org/wiki/L%C3%ABscht_vun_de_L%C3%ABtzebuerger_Buergen_a_Schl%C3%A4sser&#34;&gt;exhaustive&lt;/a&gt;,
but the names are in Luxembourguish, and I doubt that
OpenStreetMap, which I’ll use to get the coordinates, understands Luxembourguish.&lt;/p&gt;
&lt;p&gt;This list has around 50 castles, a reasonable amount of castles. Scraping the table is quite easy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;page &amp;lt;- read_html(&amp;quot;https://fr.wikipedia.org/wiki/Liste_de_ch%C3%A2teaux_luxembourgeois&amp;quot;)

castles &amp;lt;- page %&amp;gt;%
    html_node(&amp;quot;.wikitable&amp;quot;) %&amp;gt;%
    html_table(fill = TRUE) %&amp;gt;%
    select(Nom, Localité) %&amp;gt;%
    mutate(query = paste0(Nom, &amp;quot;, &amp;quot;, Localité))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also add a &lt;code&gt;query&lt;/code&gt; column which concatenates the name of the castle (“Nom”) to where it is found
(“Localité”). The query should be a better choice that simply the castle name to get the coordinates.&lt;/p&gt;
&lt;p&gt;Now, I need to add the coordinates to this data frame. For this, I use a function I found online
that gets the coordinates from OpenStreetMap:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## geocoding function using OSM Nominatim API
## details: http://wiki.openstreetmap.org/wiki/Nominatim
## made by: D.Kisler

#https://datascienceplus.com/osm-nominatim-with-r-getting-locations-geo-coordinates-by-its-address/

nominatim_osm &amp;lt;- function(address = NULL){
    if(suppressWarnings(is.null(address)))
        return(data.frame())
    tryCatch(
        d &amp;lt;- jsonlite::fromJSON(
            gsub(&amp;#39;\\@addr\\@&amp;#39;, gsub(&amp;#39;\\s+&amp;#39;, &amp;#39;\\%20&amp;#39;, address),
                 &amp;#39;http://nominatim.openstreetmap.org/search/@addr@?format=json&amp;amp;addressdetails=0&amp;amp;limit=1&amp;#39;)
        ), error = function(c) return(data.frame())
    )
    if(length(d) == 0) return(data.frame())
    return(data.frame(lon = as.numeric(d$lon), lat = as.numeric(d$lat)))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now easily add the coordinates by mapping the &lt;code&gt;nominatim_osm()&lt;/code&gt; function to the
&lt;code&gt;query&lt;/code&gt; column I built before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;castles_osm &amp;lt;- castles %&amp;gt;%
    mutate(geolocation = map(query, nominatim_osm))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;castles_osm&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(castles_osm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                            Nom    Localité
## 1         Château d&amp;#39;Ansembourg  Ansembourg
## 2 Nouveau Château d&amp;#39;Ansembourg  Ansembourg
## 3             Château d&amp;#39;Aspelt      Aspelt
## 4          Château de Beaufort    Beaufort
## 5            Château de Beggen Dommeldange
## 6       Château de Colmar-Berg Colmar-Berg
##                                      query         geolocation
## 1         Château d&amp;#39;Ansembourg, Ansembourg 6.046748, 49.700693
## 2 Nouveau Château d&amp;#39;Ansembourg, Ansembourg   6.04760, 49.70085
## 3                 Château d&amp;#39;Aspelt, Aspelt 6.222653, 49.524822
## 4            Château de Beaufort, Beaufort 2.757293, 43.297466
## 5           Château de Beggen, Dommeldange 6.137765, 49.643383
## 6      Château de Colmar-Berg, Colmar-Berg 6.087944, 49.814687&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now clean the data. There were several mistakes or castles that were not found, which I added
manually. I did not notice these mistakes immediately, but when I computed the distances matrix I
notices several inconsistencies; 0’s in positions other than the diagonal, as well as NAs. So I went
back to the raw data and corrected what was wrong, this time by looking at Google Maps. Thankfully
there were not that many mistakes. Below the whole workflow:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Little helper function to clean the lon and lat columns
extract_numbers &amp;lt;- function(string){
    str_extract_all(string, &amp;quot;\\d+&amp;quot;, simplify = TRUE) %&amp;gt;%
        paste0(collapse = &amp;quot;.&amp;quot;)
}

castles &amp;lt;- castles_osm %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Château de Wintrange&amp;quot;, &amp;quot;6.3517223, 49.5021975&amp;quot;, geolocation)) %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Septfontaines, Rollingergrund&amp;quot;, &amp;quot;6.1028634, 49.6257147&amp;quot;, geolocation)) %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Château de Septfontaines&amp;quot;, &amp;quot;5.9617443, 49.7006292&amp;quot;, geolocation)) %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Château de Senningen&amp;quot;, &amp;quot;6.2342581, 49.6464632&amp;quot;, geolocation)) %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Château de Schauwenburg&amp;quot;, &amp;quot;6.0478341, 49.6110245&amp;quot;, geolocation)) %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Château de Schuttbourg&amp;quot;, &amp;quot;5.8980951, 49.7878706&amp;quot;, geolocation)) %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Château de Meysembourg&amp;quot;, &amp;quot;6.1864882, 49.7704348&amp;quot;, geolocation)) %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Château de Mamer&amp;quot;, &amp;quot;6.0232432, 49.6262397&amp;quot;, geolocation)) %&amp;gt;%
    mutate(geolocation = 
               ifelse(Nom == &amp;quot;Château de Born&amp;quot;, &amp;quot;6.5125214, 49.7611168&amp;quot;, geolocation)) %&amp;gt;%
    # Found chateau de Betzdorf in Germany, not Luxembourg:
    mutate(geolocation = ifelse(Nom == &amp;quot;Château Betzdorf&amp;quot;, &amp;quot;6.330278, 49.694167&amp;quot;, geolocation)) %&amp;gt;%
    # Found château de Clemency in France, not Luxembourg:
    mutate(geolocation = ifelse(Nom == &amp;quot;Château de Clemency&amp;quot;, &amp;quot;5.874167, 49.598056&amp;quot;, geolocation)) %&amp;gt;%
    separate(geolocation, into = c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;), sep = &amp;quot;,&amp;quot;) %&amp;gt;%
    filter(!is.na(lat)) %&amp;gt;%
    mutate(lon = map(lon, extract_numbers)) %&amp;gt;%
    mutate(lat = map(lat, extract_numbers)) %&amp;gt;%
    # Château de Beaufort found is in southern France, not the one in lux
    # Château de Dudelange is wrong (same as Bettembourg)
    # Château de Pétange is wrong (same as Differdange)
    # Château d&amp;#39;Urspelt is wrong (same as Clervaux)
    # Château d&amp;#39;Hesperange is wrong (same as Palais Grand-Ducal)
    mutate(lon = ifelse(Nom == &amp;quot;Château de Beaufort&amp;quot;, &amp;quot;6.2865176&amp;quot;, lon),
           lat = ifelse(Nom == &amp;quot;Château de Beaufort&amp;quot;, &amp;quot;49.8335306&amp;quot;, lat)) %&amp;gt;%
    mutate(lon = ifelse(Nom == &amp;quot;Château Dudelange&amp;quot;, &amp;quot;6.0578438&amp;quot;, lon),
           lat = ifelse(Nom == &amp;quot;Château Dudelange&amp;quot;, &amp;quot;49.4905049&amp;quot;, lat)) %&amp;gt;%
    mutate(lon = ifelse(Nom == &amp;quot;Château de Pétange&amp;quot;, &amp;quot;6.105703&amp;quot;, lon),
           lat = ifelse(Nom == &amp;quot;Château de Pétange&amp;quot;, &amp;quot;49.7704746&amp;quot;, lat)) %&amp;gt;%
    mutate(lon = ifelse(Nom == &amp;quot;Château d&amp;#39; Urspelt&amp;quot;, &amp;quot;6.043375&amp;quot;, lon),
           lat = ifelse(Nom == &amp;quot;Château d&amp;#39; Urspelt&amp;quot;, &amp;quot;50.075342&amp;quot;, lat)) %&amp;gt;%
    mutate(lon = ifelse(Nom == &amp;quot;Château d&amp;#39;Hesperange&amp;quot;, &amp;quot;6.1524302&amp;quot;, lon),
           lat = ifelse(Nom == &amp;quot;Château d&amp;#39;Hesperange&amp;quot;, &amp;quot;49.573071&amp;quot;, lat)) %&amp;gt;%
    mutate(latlon = paste0(lat, &amp;quot;,&amp;quot;, lon)) %&amp;gt;%
    mutate(lon = as.numeric(lon), lat = as.numeric(lat))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the end, I have 48 castles, 2 of them were not found neither by OpenStreetMap nor
Google Maps.&lt;/p&gt;
&lt;p&gt;Now I can get the distances matrix. For this, I opened an account at &lt;a href=&#34;https://www.graphhopper.com/&#34;&gt;Graphhopper&lt;/a&gt;
and used their &lt;a href=&#34;https://graphhopper.com/api/1/docs/matrix/#matrix-api&#34;&gt;Matrix API&lt;/a&gt;. When you open
a free account, you get a standard account for free for two weeks, which was perfect for this little
exercise.&lt;/p&gt;
&lt;p&gt;To use the Matrix API you can make a call with curl from your terminal, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl &amp;quot;https://graphhopper.com/api/1/matrix?point=49.932707,11.588051&amp;amp;point=50.241935,10.747375&amp;amp;point=50.118817,11.983337&amp;amp;type=json&amp;amp;vehicle=car&amp;amp;debug=true&amp;amp;out_array=weights&amp;amp;out_array=times&amp;amp;out_array=distances&amp;amp;key=[YOUR_KEY]&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use this from R, I use the &lt;code&gt;{curl}&lt;/code&gt; package and the &lt;code&gt;curl_download()&lt;/code&gt; function to download and
write the output to disk.&lt;/p&gt;
&lt;p&gt;I built the url like this. First, the “points” part:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points &amp;lt;- paste(castles$latlon, collapse = &amp;quot;&amp;amp;point=&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the “points” string&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;points&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;49.70069265,6.04674779400653&amp;amp;point=49.7008533,6.04759957386294&amp;amp;point=49.5248216,6.2226525964455&amp;amp;point=49.8335306,6.2865176&amp;amp;point=49.64338295,6.1377647435619&amp;amp;point=49.8146867,6.08794389490417&amp;amp;point=49.5749356,5.9841033&amp;amp;point=49.5173197,6.09641390513718&amp;amp;point=49.8760687,6.22027097982788&amp;amp;point=49.694167,6.330278&amp;amp;point=49.7611168,6.5125214&amp;amp;point=49.70256665,6.21740997690437&amp;amp;point=49.905581,6.07950107769784&amp;amp;point=49.9127745,6.13764166375989&amp;amp;point=49.598056,5.874167&amp;amp;point=50.0544533,6.03028463135369&amp;amp;point=49.75943095,5.82586812555896&amp;amp;point=49.52132545,5.88917535225117&amp;amp;point=49.6345518,6.1386377&amp;amp;point=49.4905049,6.0578438&amp;amp;point=49.8600716,6.11163732377525&amp;amp;point=49.9110418,5.93440053120085&amp;amp;point=49.7475976,6.18681116161273&amp;amp;point=49.61092115,6.13288873913352&amp;amp;point=49.573071,6.1524302&amp;amp;point=49.71207855,6.05156617599082&amp;amp;point=49.6694157,5.9496767&amp;amp;point=49.7704143,6.18888954785334&amp;amp;point=49.6262397,6.0232432&amp;amp;point=49.7478579,6.10315847283333&amp;amp;point=49.7704348,6.1864882&amp;amp;point=49.6328906,6.25941956000154&amp;amp;point=49.7704746,6.105703&amp;amp;point=49.54325715,5.9262570638974&amp;amp;point=49.470114,6.3658507&amp;amp;point=49.719675,6.09334070925783&amp;amp;point=49.7878706,5.8980951&amp;amp;point=49.6110245,6.0478341&amp;amp;point=49.6464632,6.2342581&amp;amp;point=49.7006292,5.9617443&amp;amp;point=49.6257147,6.1028634&amp;amp;point=49.556964,6.380786&amp;amp;point=50.075342,6.043375&amp;amp;point=49.7682266,5.9803414&amp;amp;point=49.9348908,6.20279648757301&amp;amp;point=49.6604088,6.1337864&amp;amp;point=49.9664662,5.93854270968922&amp;amp;point=49.5021975,6.3517223&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;Then, I added my key, and pasted these elements together to form the correct url:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_key &amp;lt;- &amp;quot;my_key_was_here&amp;quot;

url &amp;lt;- paste0(&amp;quot;https://graphhopper.com/api/1/matrix?point=&amp;quot;, points, &amp;quot;&amp;amp;type=json&amp;amp;vehicle=car&amp;amp;debug=true&amp;amp;out_array=weights&amp;amp;out_array=times&amp;amp;out_array=distances&amp;amp;key=&amp;quot;, my_key)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I get the matrix like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;castles_dist &amp;lt;- &amp;quot;distances_graphhopper.json&amp;quot;
curl_download(url, castles_dist)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distances &amp;lt;- castles_dist$distances&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the distance object&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distances&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##  [1]     0    48 46364 38416 16619 20387 19617 31990 31423 46587 60894
## [12] 19171 36961 30701 25734 52929 22843 42618 18138 40015 24860 39395
## [23] 17163 18938 28107  2570 10882 16888 12302  9350 16599 32025 14369
## [34] 40780 56004  6069 17602 16112 31552  8180 14523 49431 53199 13354
## [45] 43769 15868 46237 53617
## 
## [[2]]
##  [1]    48     0 46412 38464 16667 20435 19665 32038 31471 46635 60942
## [12] 19219 37009 30749 25781 52977 22890 42665 18186 40063 24908 39443
## [23] 17211 18986 28155  2618 10930 16936 12350  9398 16647 32073 14417
## [34] 40828 56052  6116 17650 16160 31599  8228 14571 49478 53247 13402
## [45] 43817 15916 46285 53665
## 
## [[3]]
##  [1] 46900 46947     0 48698 30281 45548 30424 17056 56584 31187 52215
## [12] 28799 62122 55862 39130 78090 66375 33585 23961 21009 50021 64556
## [23] 35740 25853 10852 49283 43218 43052 37317 36283 42763 17250 39530
## [34] 31748 14513 33919 60798 31885 22872 44629 37023 14605 78360 44602
## [45] 68930 26320 71398 12126
## 
## [[4]]
##  [1] 38214 38261 48754     0 34949 23582 55661 48848 11274 29579 26880
## [12] 25631 32853 20633 67818 46577 47882 65319 33355 58499 26540 40460
## [23] 24359 38061 43577 37674 61661 21704 55760 29822 25030 36698 27956
## [34] 63482 72862 32945 42596 50328 34302 42495 38740 46782 46847 35141
## [45] 20752 33520 47302 56789
## 
## [[5]]
##  [1] 16494 16541 25311 35192     0 25375 19477 16687 36411 20939 42124
## [12] 13107 41949 35689 27116 57917 44116 30670  1432 26337 29848 44383
## [23] 18673  5767 11224 18877 20958 23922 15058 16110 23633 13255 19357
## [34] 28833 40700 15310 30392 12024 12782 20050  7178 30661 58187 24429
## [45] 48757  2511 51225 33346
## 
## [[6]]
##  [1] 18468 18516 43459 23632 29633     0 33496 43553 15352 45965 60272
## [12] 23583 20890 14630 39612 36858 27623 60024 28268 53203  8789 21614
## [23] 17217 32765 38282 17929 29164 13853 27119  8892 15798 31403  7026
## [34] 58187 67567 13199 22337 27919 30929 22750 26330 48809 37128 14882
## [45] 27698 21128 28456 51494
## 
## [[7]]
##  [1] 19645 19693 30022 55860 21434 35353     0 17740 46389 45070 59377
## [12] 35962 51927 45668 10941 67895 36650 11975 16038 16675 39826 49570
## [23] 42902 13747 19018 22029 13093 32857  7837 24321 32568 30508 29335
## [34]  8659 39662 20998 33544  9053 30034 17958 19337 40306 68165 29296
## [45] 58736 20683 59099 37275
## 
## [[8]]
##  [1] 33194 33242 17113 48244 16576 45094 16196     0 56130 37454 51761
## [12] 28345 61668 55408 26667 77635 52670 21122 15199  8546 49567 64102
## [23] 35286 12148 11167 35578 29512 42597 23612 35829 42308 22891 39076
## [34] 19285 26753 30159 47093 12671 22418 30923 23317 27397 77906 44148
## [45] 68476 25866 70944 24366
## 
## [[9]]
##  [1] 34049 34097 59040 11039 45215 18025 49077 59134     0 35131 34289
## [12] 25588 18148  8956 55193 34944 47717 75605 43849 68785 11835 33390
## [23] 19644 48347 53863 33510 44745 17010 42701 26274 22029 46984 22791
## [34] 73768 83148 28780 42477 43501 46511 38331 41912 64390 35215 29033
## [45] 11504 36710 37265 67075
## 
## [[10]]
##  [1] 40768 40815 31200 29561 26887 47108 44877 38064 35204     0 24550
## [12] 11501 46805 40546 57034 62773 59493 54535 25521 47714 51581 66116
## [23] 18215 27276 32792 40228 50876 23464 44975 37844 23175 16279 41090
## [34] 52698 33741 35479 54253 39544 10472 52287 30906 22876 63043 46162
## [45] 44908 19378 72959 30101
## 
## [[11]]
##  [1] 54812 54860 52014 26837 40931 61152 58921 52108 34149 24114     0
## [12] 30135 56417 42752 71078 69452 73537 68579 39565 61758 65626 80160
## [23] 33264 41320 46836 54272 64920 34961 59020 51888 38287 30323 55134
## [34] 66742 46672 49523 68297 53588 33981 66331 44951 37998 69722 60206
## [45] 39488 41924 87003 44227
## 
## [[12]]
##  [1] 19189 19237 28715 23758 13122 25545 35622 28809 25568 11495 27547
## [12]     0 42119 35859 47779 58087 37930 45280 12217 38459 30018 44553
## [23]  8427 18021 23537 18649 41621 13676 35721 16280 13387 16659 19527
## [34] 43443 52822 13901 32690 30289 10679 28544 17602 34064 58357 24599
## [45] 34791 11692 51395 36749
## 
## [[13]]
##  [1] 36813 36860 61804 32740 47978 20788 51841 61898 18594 46628 54032
## [12] 41927     0 11355 57957 25913 31393 78369 46612 71548 10254 18035
## [23] 33039 51110 56626 36273 47508 29674 45464 29037 31619 49747 26873
## [34] 76531 85911 31544 28499 46264 49274 41094 44675 67153 26183 28043
## [45] 22973 39473 21910 69838
## 
## [[14]]
##  [1] 30553 30601 55544 20685 41718 14528 45581 55638  9008 40368 42945
## [12] 35668 11355     0 51697 26249 44221 72109 40353 65288  8339 26597
## [23] 26779 44850 50367 30014 41249 23415 39204 22778 25360 43488 20613
## [34] 70272 79652 25284 38981 40004 43014 34835 38415 60894 26519 25537
## [45] 13328 33213 30473 63579
## 
## [[15]]
##  [1] 25606 25654 38728 67712 27136 41314 10938 26445 52350 56922 71229
## [12] 37346 57888 51628     0 73856 27918 10658 28655 25381 45787 55480
## [23] 42912 31851 30870 27990 14521 38818 16023 30282 38529 42360 35296
## [34]  9692 48367 26996 35203 18968 41886 19386 25040 49012 74126 25555
## [45] 64696 26385 63610 45980
## 
## [[16]]
##  [1]  52597  52644  77588  46234  63762  36572  67625  77681  34889  62412
## [11]  69484  57711  25729  26142  73741      0  51707  94153  62396  87332
## [21]  30382  29845  48823  66894  72410  52057  63292  45458  61248  44821
## [31]  47403  65531  42657  92315 101695  47328  48813  62048  65058  56878
## [41]  60459  82937   3927  48357  26809  55257  16900  85622
## 
## [[17]]
##  [1] 22742 22790 66383 47823 44393 27506 36726 52009 45012 59978 74285
## [12] 37595 31384 44290 27751 49817     0 38346 45912 60034 38449 23624
## [23] 32592 49107 48126 21474 20735 31486 33280 24718 31198 45415 24781
## [34] 37380 76023 27212  9559 36224 44942 16608 42297 71343 51639 13101
## [45] 57358 43642 33153 73636
## 
## [[18]]
##  [1] 43466 43513 32742 64592 35685 61442 12026 20459 72478 53802 68109
## [12] 44693 78016 71756 10711 93983 38600     0 31781 19395 65915 80450
## [23] 51634 28730 27749 45849 20641 58945 19674 52177 58656 39239 55424
## [34]  6452 42381 40430 45885 20422 38766 25506 33588 43026 94253 53116
## [45] 84824 34934 87292 39994
## 
## [[19]]
##  [1] 17092 17140 23308 33262  1432 30429 15982 15299 41465 25814 40121
## [12] 12039 47003 40743 27715 62970 44714 29283     0 24950 34902 49437
## [23] 17605  4379  9837 19476 21557 22854 15656 21164 22565 11252 24411
## [34] 27446 39313 18800 37574 12622 10778 20648  5791 28657 63241 29483
## [45] 53811  3497 56279 31342
## 
## [[20]]
##  [1] 40369 40417 21028 58519 26851 55368 16853  5415 66404 47729 62035
## [12] 38620 71942 65683 25560 87910 59845 20015 25473     0 59842 74376
## [23] 45561 22423 21442 42753 36687 52872 30786 46104 52583 33166 49351
## [34] 18178 30668 37333 54268 25355 32693 38098 30492 31312 88180 50019
## [45] 78751 31837 81219 28281
## 
## [[21]]
##  [1] 25435 25483 50426 23657 36601  9410 40463 50520  9511 52933 67240
## [12] 30550 10254  8470 46579 30697 31384 66991 35235 60171     0 22368
## [23] 19572 39732 45249 24896 36131 16208 34087 17660 18153 38370 15495
## [34] 65154 74534 20166 26099 34887 37897 29717 33298 55776 30967 18643
## [45] 21538 28096 29210 58461
## 
## [[22]]
##  [1] 39601 39649 64592 40786 50767 21730 50009 64686 34013 67099 81406
## [12] 44716 18356 26775 55533 27246 27163 81157 49401 74337 22735     0
## [23] 35431 53898 59415 39062 38903 32067 42694 31826 34012 52536 26191
## [34] 79320 88700 34332 24269 49053 52063 31939 47464 69942 29069 20615
## [45] 35072 42262 10583 72627
## 
## [[23]]
##  [1] 17968 18016 35386 24320 18687 19877 42293 35480 19624 18209 33230
## [12]  8427 33133 26873 42921 49101 32654 51951 17782 45130 21032 35141
## [23]     0 21756 30208 17139 28664  5714 26043  8833  5426 23330 10461
## [34] 50114 59493 12123 27413 26843 20737 23268 23168 40735 49371 19323
## [45] 29874 17258 41983 43420
## 
## [[24]]
##  [1] 17512 17560 15422 36059  4428 33226 13943 13260 44262 27280 41587
## [12] 14836 49800 43540 24703 65768 41703 27243  3050 22910 37699 52234
## [23] 20402     0  7797 19895 18546 25651 12645 23961 25362 12718 27208
## [34] 25406 37273 16329 31410  8506 12244 19956  6213 26147 66038 32280
## [45] 56608  6317 59076 34886
## 
## [[25]]
##  [1] 27872 27920 10877 44424 10352 41273 18191 11042 52309 33634 47940
## [12] 24525 57847 51588 30348 73815 47348 27849  8975 20692 45747 60281
## [23] 31466  5924     0 30256 24190 38777 18290 32009 38488 19071 35255
## [34] 26012 35056 24837 41771 12858 18598 25601 17995 21917 74085 40327
## [45] 64656 19340 67124 21432
## 
## [[26]]
##  [1]  2570  2618 48748 37876 19003 19847 22001 34374 30884 46048 60355
## [12] 18632 36421 30162 28117 52389 21574 45001 28350 42399 24321 32360
## [23] 17165 21322 30491     0 13266 16059 14686  8521 15770 31485 13830
## [34] 43164 58387  5529 16334 18496 31012 10564 16906 48891 52659 12086
## [45] 43230 18252 45698 56000
## 
## [[27]]
##  [1] 10882 10930 43104 61689 21113 31083 13102 28730 42119 50899 65206
## [12] 31324 47657 41397 14188 63624 20735 20742 22632 36755 35556 38436
## [23] 27859 25828 24847 13266     0 27584 10000 20046 27295 36337 25065
## [34] 19286 52743 16764 22410 12945 35863  6736 19017 48063 63894 18161
## [45] 54465 20362 47965 50356
## 
## [[28]]
##  [1] 16863 16910 42857 21661 23936 13872 32894 42951 16990 23458 34927
## [12] 13676 29769 23509 39010 45736 31548 59422 23031 52601 17668 31777
## [23]  5714 27005 37680 16033 27558     0 26517  7728   640 30801  9331
## [34] 57585 66965 12597 26308 27318 25985 22162 25728 48207 46006 18217
## [45] 26509 22507 38619 50892
## 
## [[29]]
##  [1] 12254 12302 36928 55514 14937 28982  7761 22554 40018 44724 59031
## [12] 25148 45556 39296 16297 61524 33297 19546 16457 30579 33455 42179
## [23] 25758 19652 18671 14638 10140 26486     0 17950 26197 30161 22964
## [34] 16230 46568 14086 26153  3631 29688 11548 12841 41888 61794 21904
## [45] 52364 14186 51708 44180
## 
## [[30]]
##  [1]  9350  9398 36159 28322 22333 12073 24364 36253 23110 38665 52972
## [12] 16282 28647 22388 30480 44615 24850 52724 20968 45903 16547 31081
## [23]  8859 25465 30981  8521 20046  7753 17987     0  7465 24103  5802
## [34] 50887 60266  4067 19610 18787 23629 17344 17198 41508 44885 11519
## [45] 35456 13828 37924 44193
## 
## [[31]]
##  [1] 16574 16622 42568 24986 23647 18483 32605 42662 21985 23169 38253
## [12] 13387 31714 25454 38721 47681 31259 59133 22742 52312 19613 33722
## [23]  5426 26716 37391 15744 27269   640 26229  7439     0 30512  9042
## [34] 57296 66676 12308 26019 27029 25696 21873 25440 47918 47951 17928
## [45] 28454 22218 40564 50603
## 
## [[32]]
##  [1] 26503 26551 16931 35994 12622 32844 30612 23800 43880 15938 30245
## [12] 16095 49418 43158 42770 65386 45229 40271 11257 33450 37317 51852
## [23] 23036 13012 18528 25964 36612 30348 30711 23579 30059     0 26826
## [34] 38433 28491 21215 39989 25280  5038 38023 16642 16486 65656 31898
## [45] 56226 13616 58694 22401
## 
## [[33]]
##  [1] 14182 14230 39173 27991 25347  7318 29210 39267 22746 41679 55986
## [12] 19297 27681 21422 35326 43649 24862 55738 23982 48917 15581 26003
## [23] 10461 28479 33996 13643 24878  9331 22833  5790  9042 27117     0
## [34] 53901 63281  8913 19576 23633 26643 18464 22044 44523 43919 12121
## [45] 29215 16842 35531 47208
## 
## [[34]]
##  [1] 28166 28214 31292 63142 34235 59992  8710 19009 71028 52352 66659
## [12] 43243 76566 70306  9382 92533 36885  6378 30331 17945 64465 64447
## [23] 50184 27280 26299 30550 18965 57495 16357 50727 57206 37789 53974
## [34]     0 40931 38980 44170 17105 37316 23829 32138 41576 92803 34921
## [45] 83374 33484 85842 38544
## 
## [[35]]
##  [1]  56656  56703  14836  71705  40037  68555  40180  26812  79591  33969
## [11]  46710  51806  85129  78869  48886 101097  76131  43341  38660  30765
## [21]  73028  87563  58747  35609  34628  59039  52974  66059  47073  59291
## [31]  65770  28534  62537  41504      0  53620  70554  41641  45879  54384
## [41]  46778  10804 101367  67609  91937  49327  94405   4470
## 
## [[36]]
##  [1]  6069  6116 38793 33128 15430 15099 20926 29765 26135 41299 55606
## [12] 13883 31673 25413 27213 47641 27484 40392 23602 37789 19572 34107
## [23] 11875 28099 33615  5529 16764 12602 14123  4067 12314 26737  9081
## [34] 38555 53778     0 22244 14923 26263 14062 13334 44142 47911 14153
## [45] 38481 11296 40949 46827
## 
## [[37]]
##  [1] 17599 17647 60259 42618 30514 22302 33512 45885 39869 54835 69142
## [12] 32452 28491 39147 35036 46923  9559 56513 37137 53910 33306 20731
## [23] 27449 41635 42002 16331 22406 26343 26197 19575 26054 40272 19576
## [34] 54675 69899 22068     0 30007 39799 15442 28418 57678 48746  7897
## [45] 52215 29998 30260 67512
## 
## [[38]]
##  [1] 16015 16063 31108 49694 12129 29728  8933 12719 40764 38904 53211
## [12] 29795 46302 40042 19294 62270 36294 27361 13648 24759 34201 48736
## [23] 26504  8314 12851 18398 13137 27231  3723 18696 26943 24341 23710
## [34] 17000 40748 14832 29913     0 23868 14545 10032 36068 62540 25665
## [45] 53110 11378 55578 38360
## 
## [[39]]
##  [1] 25665 25713 22868 35156 11784 32006 29774 22961 43042 10466 34023
## [12] 15257 48580 42320 41932 64548 44391 39433 10419 32612 36479 51014
## [23] 22198 12174 17690 25126 35774 29510 29873 22741 29221  5081 25988
## [34] 37595 46975 20377 39151 24442     0 37185 15804 21298 64818 31060
## [45] 55388 12778 57856 30902
## 
## [[40]]
##  [1]  8180  8228 44691 42719 20175 24690 17967 30317 35726 52486 66793
## [12] 28309 41264 35005 19052 57232 16608 25606 21695 38341 29164 31472
## [23] 23306 27415 26433 10564  6736 22201 11607 17344 21912 37924 18672
## [34] 24151 54330 14062 15446 14551 37450     0 18079 49650 57502 11198
## [45] 48073 19424 41001 51943
## 
## [[41]]
##  [1] 14500 14548 37450 38853  8571 28213 17483 23076 39249 31405 45712
## [12] 17630 44787 38527 25122 60755 42122 33703  5844 31101 32686 47221
## [23] 23196  6938 19193 16883 18965 25716 13064 17181 25428 16842 22195
## [34] 31866 47090 13317 28398 10030 16369 18056     0 34248 61025 24150
## [45] 51595  7820 54063 44703
## 
## [[42]]
##  [1] 44607 44655 14655 46763 30726 50948 40494 27126 61984 22848 38359
## [12] 34199 67522 61262 49200 83489 76445 43655 29360 31079 55421 69956
## [23] 41140 24844 22014 44067 53288 48451 47387 41683 48162 16486 44930
## [34] 41818 10865 39319 58092 41955 21254 54699 47093     0 83759 50002
## [45] 74330 31720 76798  7225
## 
## [[43]]
##  [1]  53049  53096  78040  46686  64214  37024  68076  78133  35341  62864
## [11]  69936  58163  26181  26593  74193   3927  51887  94604  62848  87784
## [21]  30834  37251  49275  67346  72862  52509  63744  45910  61700  45273
## [31]  47855  65983  43109  92767 102147  47780  48993  62500  65510  57330
## [41]  60911  83389      0  48032  27261  55709  18722  86074
## 
## [[44]]
##  [1] 13481 13529 44241 35163 30415 14847 29395 44335 31781 46747 61054
## [12] 24364 28035 31060 30271 46467 13182 52395 29050 49792 25219 20275
## [23] 19361 33547 39063 12213 18288 18256 22079 11488 17967 32185 12121
## [34] 50557 68348 13981  7897 25889 31711 11325 24300 49590 53557     0
## [45] 44128 21910 29803 52275
## 
## [[45]]
##  [1] 40446 40494 65437 20517 51611 24421 55474 65531 11556 44883 39488
## [12] 34762 24423 13328 61590 26917 54113 82002 50246 75181 18231 34751
## [23] 29845 54743 60259 39906 51142 26480 49097 32670 28426 53381 29188
## [34] 80164 89544 35177 48873 49897 52907 44727 48308 70786 27187 35430
## [45]     0 43106 33848 73471
## 
## [[46]]
##  [1] 15720 15767 25996 33807  2532 23092 18703 24296 34128 19554 42809
## [12] 11722 39666 33406 26342 55633 43342 34923  4080 32321 27565 42100
## [23] 17288  7632 20818 18103 20185 22537 14284 13827 22248 13940 17074
## [34] 33086 48309 11317 30237 11250 11347 19276  7865 31345 55903 22146
## [45] 46474     0 48942 34030
## 
## [[47]]
##  [1] 48326 48373 73317 53314 59491 30454 61401 73410 39547 75823 76564
## [12] 53440 23890 33221 63609 16972 35356 74204 58125 83061 30123 12418
## [23] 44155 62623 68139 47786 50295 40791 54086 40550 42736 61260 37582
## [34] 73238 97424 43057 32463 57777 60787 43331 56188 78666 18794 32007
## [45] 33889 50986     0 81351
## 
## [[48]]
##  [1] 53987 54035 12168 56733 37369 53583 37512 24144 64619 31546 44287
## [12] 36834 70157 63897 46218 86125 73463 40673 35992 28097 58056 72591
## [23] 43775 32941 31960 56371 50305 51087 44405 44319 50798 22413 47565
## [34] 38836  4459 50952 67886 38973 30907 51716 44110  7225 86395 52637
## [45] 76965 34355 79433     0&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;code&gt;distances&lt;/code&gt; is a list where the first element is the distances from the first castle to all the others.
Let’s make it a matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distances_matrix &amp;lt;- distances %&amp;gt;%
    reduce(rbind)&lt;/code&gt;&lt;/pre&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the distance matrix&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;distances_matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]
## out     0    48 46364 38416 16619 20387 19617 31990 31423 46587 60894
##        48     0 46412 38464 16667 20435 19665 32038 31471 46635 60942
##     46900 46947     0 48698 30281 45548 30424 17056 56584 31187 52215
##     38214 38261 48754     0 34949 23582 55661 48848 11274 29579 26880
##     16494 16541 25311 35192     0 25375 19477 16687 36411 20939 42124
##     18468 18516 43459 23632 29633     0 33496 43553 15352 45965 60272
##     19645 19693 30022 55860 21434 35353     0 17740 46389 45070 59377
##     33194 33242 17113 48244 16576 45094 16196     0 56130 37454 51761
##     34049 34097 59040 11039 45215 18025 49077 59134     0 35131 34289
##     40768 40815 31200 29561 26887 47108 44877 38064 35204     0 24550
##     54812 54860 52014 26837 40931 61152 58921 52108 34149 24114     0
##     19189 19237 28715 23758 13122 25545 35622 28809 25568 11495 27547
##     36813 36860 61804 32740 47978 20788 51841 61898 18594 46628 54032
##     30553 30601 55544 20685 41718 14528 45581 55638  9008 40368 42945
##     25606 25654 38728 67712 27136 41314 10938 26445 52350 56922 71229
##     52597 52644 77588 46234 63762 36572 67625 77681 34889 62412 69484
##     22742 22790 66383 47823 44393 27506 36726 52009 45012 59978 74285
##     43466 43513 32742 64592 35685 61442 12026 20459 72478 53802 68109
##     17092 17140 23308 33262  1432 30429 15982 15299 41465 25814 40121
##     40369 40417 21028 58519 26851 55368 16853  5415 66404 47729 62035
##     25435 25483 50426 23657 36601  9410 40463 50520  9511 52933 67240
##     39601 39649 64592 40786 50767 21730 50009 64686 34013 67099 81406
##     17968 18016 35386 24320 18687 19877 42293 35480 19624 18209 33230
##     17512 17560 15422 36059  4428 33226 13943 13260 44262 27280 41587
##     27872 27920 10877 44424 10352 41273 18191 11042 52309 33634 47940
##      2570  2618 48748 37876 19003 19847 22001 34374 30884 46048 60355
##     10882 10930 43104 61689 21113 31083 13102 28730 42119 50899 65206
##     16863 16910 42857 21661 23936 13872 32894 42951 16990 23458 34927
##     12254 12302 36928 55514 14937 28982  7761 22554 40018 44724 59031
##      9350  9398 36159 28322 22333 12073 24364 36253 23110 38665 52972
##     16574 16622 42568 24986 23647 18483 32605 42662 21985 23169 38253
##     26503 26551 16931 35994 12622 32844 30612 23800 43880 15938 30245
##     14182 14230 39173 27991 25347  7318 29210 39267 22746 41679 55986
##     28166 28214 31292 63142 34235 59992  8710 19009 71028 52352 66659
##     56656 56703 14836 71705 40037 68555 40180 26812 79591 33969 46710
##      6069  6116 38793 33128 15430 15099 20926 29765 26135 41299 55606
##     17599 17647 60259 42618 30514 22302 33512 45885 39869 54835 69142
##     16015 16063 31108 49694 12129 29728  8933 12719 40764 38904 53211
##     25665 25713 22868 35156 11784 32006 29774 22961 43042 10466 34023
##      8180  8228 44691 42719 20175 24690 17967 30317 35726 52486 66793
##     14500 14548 37450 38853  8571 28213 17483 23076 39249 31405 45712
##     44607 44655 14655 46763 30726 50948 40494 27126 61984 22848 38359
##     53049 53096 78040 46686 64214 37024 68076 78133 35341 62864 69936
##     13481 13529 44241 35163 30415 14847 29395 44335 31781 46747 61054
##     40446 40494 65437 20517 51611 24421 55474 65531 11556 44883 39488
##     15720 15767 25996 33807  2532 23092 18703 24296 34128 19554 42809
##     48326 48373 73317 53314 59491 30454 61401 73410 39547 75823 76564
##     53987 54035 12168 56733 37369 53583 37512 24144 64619 31546 44287
##     [,12] [,13] [,14] [,15]  [,16] [,17] [,18] [,19] [,20] [,21] [,22]
## out 19171 36961 30701 25734  52929 22843 42618 18138 40015 24860 39395
##     19219 37009 30749 25781  52977 22890 42665 18186 40063 24908 39443
##     28799 62122 55862 39130  78090 66375 33585 23961 21009 50021 64556
##     25631 32853 20633 67818  46577 47882 65319 33355 58499 26540 40460
##     13107 41949 35689 27116  57917 44116 30670  1432 26337 29848 44383
##     23583 20890 14630 39612  36858 27623 60024 28268 53203  8789 21614
##     35962 51927 45668 10941  67895 36650 11975 16038 16675 39826 49570
##     28345 61668 55408 26667  77635 52670 21122 15199  8546 49567 64102
##     25588 18148  8956 55193  34944 47717 75605 43849 68785 11835 33390
##     11501 46805 40546 57034  62773 59493 54535 25521 47714 51581 66116
##     30135 56417 42752 71078  69452 73537 68579 39565 61758 65626 80160
##         0 42119 35859 47779  58087 37930 45280 12217 38459 30018 44553
##     41927     0 11355 57957  25913 31393 78369 46612 71548 10254 18035
##     35668 11355     0 51697  26249 44221 72109 40353 65288  8339 26597
##     37346 57888 51628     0  73856 27918 10658 28655 25381 45787 55480
##     57711 25729 26142 73741      0 51707 94153 62396 87332 30382 29845
##     37595 31384 44290 27751  49817     0 38346 45912 60034 38449 23624
##     44693 78016 71756 10711  93983 38600     0 31781 19395 65915 80450
##     12039 47003 40743 27715  62970 44714 29283     0 24950 34902 49437
##     38620 71942 65683 25560  87910 59845 20015 25473     0 59842 74376
##     30550 10254  8470 46579  30697 31384 66991 35235 60171     0 22368
##     44716 18356 26775 55533  27246 27163 81157 49401 74337 22735     0
##      8427 33133 26873 42921  49101 32654 51951 17782 45130 21032 35141
##     14836 49800 43540 24703  65768 41703 27243  3050 22910 37699 52234
##     24525 57847 51588 30348  73815 47348 27849  8975 20692 45747 60281
##     18632 36421 30162 28117  52389 21574 45001 28350 42399 24321 32360
##     31324 47657 41397 14188  63624 20735 20742 22632 36755 35556 38436
##     13676 29769 23509 39010  45736 31548 59422 23031 52601 17668 31777
##     25148 45556 39296 16297  61524 33297 19546 16457 30579 33455 42179
##     16282 28647 22388 30480  44615 24850 52724 20968 45903 16547 31081
##     13387 31714 25454 38721  47681 31259 59133 22742 52312 19613 33722
##     16095 49418 43158 42770  65386 45229 40271 11257 33450 37317 51852
##     19297 27681 21422 35326  43649 24862 55738 23982 48917 15581 26003
##     43243 76566 70306  9382  92533 36885  6378 30331 17945 64465 64447
##     51806 85129 78869 48886 101097 76131 43341 38660 30765 73028 87563
##     13883 31673 25413 27213  47641 27484 40392 23602 37789 19572 34107
##     32452 28491 39147 35036  46923  9559 56513 37137 53910 33306 20731
##     29795 46302 40042 19294  62270 36294 27361 13648 24759 34201 48736
##     15257 48580 42320 41932  64548 44391 39433 10419 32612 36479 51014
##     28309 41264 35005 19052  57232 16608 25606 21695 38341 29164 31472
##     17630 44787 38527 25122  60755 42122 33703  5844 31101 32686 47221
##     34199 67522 61262 49200  83489 76445 43655 29360 31079 55421 69956
##     58163 26181 26593 74193   3927 51887 94604 62848 87784 30834 37251
##     24364 28035 31060 30271  46467 13182 52395 29050 49792 25219 20275
##     34762 24423 13328 61590  26917 54113 82002 50246 75181 18231 34751
##     11722 39666 33406 26342  55633 43342 34923  4080 32321 27565 42100
##     53440 23890 33221 63609  16972 35356 74204 58125 83061 30123 12418
##     36834 70157 63897 46218  86125 73463 40673 35992 28097 58056 72591
##     [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33]
## out 17163 18938 28107  2570 10882 16888 12302  9350 16599 32025 14369
##     17211 18986 28155  2618 10930 16936 12350  9398 16647 32073 14417
##     35740 25853 10852 49283 43218 43052 37317 36283 42763 17250 39530
##     24359 38061 43577 37674 61661 21704 55760 29822 25030 36698 27956
##     18673  5767 11224 18877 20958 23922 15058 16110 23633 13255 19357
##     17217 32765 38282 17929 29164 13853 27119  8892 15798 31403  7026
##     42902 13747 19018 22029 13093 32857  7837 24321 32568 30508 29335
##     35286 12148 11167 35578 29512 42597 23612 35829 42308 22891 39076
##     19644 48347 53863 33510 44745 17010 42701 26274 22029 46984 22791
##     18215 27276 32792 40228 50876 23464 44975 37844 23175 16279 41090
##     33264 41320 46836 54272 64920 34961 59020 51888 38287 30323 55134
##      8427 18021 23537 18649 41621 13676 35721 16280 13387 16659 19527
##     33039 51110 56626 36273 47508 29674 45464 29037 31619 49747 26873
##     26779 44850 50367 30014 41249 23415 39204 22778 25360 43488 20613
##     42912 31851 30870 27990 14521 38818 16023 30282 38529 42360 35296
##     48823 66894 72410 52057 63292 45458 61248 44821 47403 65531 42657
##     32592 49107 48126 21474 20735 31486 33280 24718 31198 45415 24781
##     51634 28730 27749 45849 20641 58945 19674 52177 58656 39239 55424
##     17605  4379  9837 19476 21557 22854 15656 21164 22565 11252 24411
##     45561 22423 21442 42753 36687 52872 30786 46104 52583 33166 49351
##     19572 39732 45249 24896 36131 16208 34087 17660 18153 38370 15495
##     35431 53898 59415 39062 38903 32067 42694 31826 34012 52536 26191
##         0 21756 30208 17139 28664  5714 26043  8833  5426 23330 10461
##     20402     0  7797 19895 18546 25651 12645 23961 25362 12718 27208
##     31466  5924     0 30256 24190 38777 18290 32009 38488 19071 35255
##     17165 21322 30491     0 13266 16059 14686  8521 15770 31485 13830
##     27859 25828 24847 13266     0 27584 10000 20046 27295 36337 25065
##      5714 27005 37680 16033 27558     0 26517  7728   640 30801  9331
##     25758 19652 18671 14638 10140 26486     0 17950 26197 30161 22964
##      8859 25465 30981  8521 20046  7753 17987     0  7465 24103  5802
##      5426 26716 37391 15744 27269   640 26229  7439     0 30512  9042
##     23036 13012 18528 25964 36612 30348 30711 23579 30059     0 26826
##     10461 28479 33996 13643 24878  9331 22833  5790  9042 27117     0
##     50184 27280 26299 30550 18965 57495 16357 50727 57206 37789 53974
##     58747 35609 34628 59039 52974 66059 47073 59291 65770 28534 62537
##     11875 28099 33615  5529 16764 12602 14123  4067 12314 26737  9081
##     27449 41635 42002 16331 22406 26343 26197 19575 26054 40272 19576
##     26504  8314 12851 18398 13137 27231  3723 18696 26943 24341 23710
##     22198 12174 17690 25126 35774 29510 29873 22741 29221  5081 25988
##     23306 27415 26433 10564  6736 22201 11607 17344 21912 37924 18672
##     23196  6938 19193 16883 18965 25716 13064 17181 25428 16842 22195
##     41140 24844 22014 44067 53288 48451 47387 41683 48162 16486 44930
##     49275 67346 72862 52509 63744 45910 61700 45273 47855 65983 43109
##     19361 33547 39063 12213 18288 18256 22079 11488 17967 32185 12121
##     29845 54743 60259 39906 51142 26480 49097 32670 28426 53381 29188
##     17288  7632 20818 18103 20185 22537 14284 13827 22248 13940 17074
##     44155 62623 68139 47786 50295 40791 54086 40550 42736 61260 37582
##     43775 32941 31960 56371 50305 51087 44405 44319 50798 22413 47565
##     [,34]  [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42]  [,43] [,44]
## out 40780  56004  6069 17602 16112 31552  8180 14523 49431  53199 13354
##     40828  56052  6116 17650 16160 31599  8228 14571 49478  53247 13402
##     31748  14513 33919 60798 31885 22872 44629 37023 14605  78360 44602
##     63482  72862 32945 42596 50328 34302 42495 38740 46782  46847 35141
##     28833  40700 15310 30392 12024 12782 20050  7178 30661  58187 24429
##     58187  67567 13199 22337 27919 30929 22750 26330 48809  37128 14882
##      8659  39662 20998 33544  9053 30034 17958 19337 40306  68165 29296
##     19285  26753 30159 47093 12671 22418 30923 23317 27397  77906 44148
##     73768  83148 28780 42477 43501 46511 38331 41912 64390  35215 29033
##     52698  33741 35479 54253 39544 10472 52287 30906 22876  63043 46162
##     66742  46672 49523 68297 53588 33981 66331 44951 37998  69722 60206
##     43443  52822 13901 32690 30289 10679 28544 17602 34064  58357 24599
##     76531  85911 31544 28499 46264 49274 41094 44675 67153  26183 28043
##     70272  79652 25284 38981 40004 43014 34835 38415 60894  26519 25537
##      9692  48367 26996 35203 18968 41886 19386 25040 49012  74126 25555
##     92315 101695 47328 48813 62048 65058 56878 60459 82937   3927 48357
##     37380  76023 27212  9559 36224 44942 16608 42297 71343  51639 13101
##      6452  42381 40430 45885 20422 38766 25506 33588 43026  94253 53116
##     27446  39313 18800 37574 12622 10778 20648  5791 28657  63241 29483
##     18178  30668 37333 54268 25355 32693 38098 30492 31312  88180 50019
##     65154  74534 20166 26099 34887 37897 29717 33298 55776  30967 18643
##     79320  88700 34332 24269 49053 52063 31939 47464 69942  29069 20615
##     50114  59493 12123 27413 26843 20737 23268 23168 40735  49371 19323
##     25406  37273 16329 31410  8506 12244 19956  6213 26147  66038 32280
##     26012  35056 24837 41771 12858 18598 25601 17995 21917  74085 40327
##     43164  58387  5529 16334 18496 31012 10564 16906 48891  52659 12086
##     19286  52743 16764 22410 12945 35863  6736 19017 48063  63894 18161
##     57585  66965 12597 26308 27318 25985 22162 25728 48207  46006 18217
##     16230  46568 14086 26153  3631 29688 11548 12841 41888  61794 21904
##     50887  60266  4067 19610 18787 23629 17344 17198 41508  44885 11519
##     57296  66676 12308 26019 27029 25696 21873 25440 47918  47951 17928
##     38433  28491 21215 39989 25280  5038 38023 16642 16486  65656 31898
##     53901  63281  8913 19576 23633 26643 18464 22044 44523  43919 12121
##         0  40931 38980 44170 17105 37316 23829 32138 41576  92803 34921
##     41504      0 53620 70554 41641 45879 54384 46778 10804 101367 67609
##     38555  53778     0 22244 14923 26263 14062 13334 44142  47911 14153
##     54675  69899 22068     0 30007 39799 15442 28418 57678  48746  7897
##     17000  40748 14832 29913     0 23868 14545 10032 36068  62540 25665
##     37595  46975 20377 39151 24442     0 37185 15804 21298  64818 31060
##     24151  54330 14062 15446 14551 37450     0 18079 49650  57502 11198
##     31866  47090 13317 28398 10030 16369 18056     0 34248  61025 24150
##     41818  10865 39319 58092 41955 21254 54699 47093     0  83759 50002
##     92767 102147 47780 48993 62500 65510 57330 60911 83389      0 48032
##     50557  68348 13981  7897 25889 31711 11325 24300 49590  53557     0
##     80164  89544 35177 48873 49897 52907 44727 48308 70786  27187 35430
##     33086  48309 11317 30237 11250 11347 19276  7865 31345  55903 22146
##     73238  97424 43057 32463 57777 60787 43331 56188 78666  18794 32007
##     38836   4459 50952 67886 38973 30907 51716 44110  7225  86395 52637
##     [,45] [,46] [,47] [,48]
## out 43769 15868 46237 53617
##     43817 15916 46285 53665
##     68930 26320 71398 12126
##     20752 33520 47302 56789
##     48757  2511 51225 33346
##     27698 21128 28456 51494
##     58736 20683 59099 37275
##     68476 25866 70944 24366
##     11504 36710 37265 67075
##     44908 19378 72959 30101
##     39488 41924 87003 44227
##     34791 11692 51395 36749
##     22973 39473 21910 69838
##     13328 33213 30473 63579
##     64696 26385 63610 45980
##     26809 55257 16900 85622
##     57358 43642 33153 73636
##     84824 34934 87292 39994
##     53811  3497 56279 31342
##     78751 31837 81219 28281
##     21538 28096 29210 58461
##     35072 42262 10583 72627
##     29874 17258 41983 43420
##     56608  6317 59076 34886
##     64656 19340 67124 21432
##     43230 18252 45698 56000
##     54465 20362 47965 50356
##     26509 22507 38619 50892
##     52364 14186 51708 44180
##     35456 13828 37924 44193
##     28454 22218 40564 50603
##     56226 13616 58694 22401
##     29215 16842 35531 47208
##     83374 33484 85842 38544
##     91937 49327 94405  4470
##     38481 11296 40949 46827
##     52215 29998 30260 67512
##     53110 11378 55578 38360
##     55388 12778 57856 30902
##     48073 19424 41001 51943
##     51595  7820 54063 44703
##     74330 31720 76798  7225
##     27261 55709 18722 86074
##     44128 21910 29803 52275
##         0 43106 33848 73471
##     46474     0 48942 34030
##     33889 50986     0 81351
##     76965 34355 79433     0&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;Let’s baptize the rows and columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(distances_matrix) &amp;lt;- castles$Nom

rownames(distances_matrix) &amp;lt;- castles$Nom&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data, we can solve the TSP.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-the-travelling-salesman-problem-thats-the-combinatorial-optimization-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solving the Travelling salesman problem (that’s the combinatorial optimization part)&lt;/h2&gt;
&lt;p&gt;Let’s first coerce the &lt;code&gt;distances_matrix&lt;/code&gt; to an &lt;code&gt;ATSP&lt;/code&gt; object, which is needed for the solver.
&lt;code&gt;ATSP&lt;/code&gt; stands for asymmetrical TSP. Asymmetrical because the &lt;code&gt;distances_matrix&lt;/code&gt; is not symmetric,
meaning that going from Castle A to Castle B is longer than going from Castle B to Castle A (for
example).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;atsp_castles &amp;lt;- ATSP(distances_matrix)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then define a list of all the available methods:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;methods &amp;lt;- c(&amp;quot;identity&amp;quot;, &amp;quot;random&amp;quot;, &amp;quot;nearest_insertion&amp;quot;,
             &amp;quot;cheapest_insertion&amp;quot;, &amp;quot;farthest_insertion&amp;quot;, &amp;quot;arbitrary_insertion&amp;quot;,
             &amp;quot;nn&amp;quot;, &amp;quot;repetitive_nn&amp;quot;, &amp;quot;two_opt&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And solve the problem with all the methods:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solutions &amp;lt;- map(methods, ~solve_TSP(x = atsp_castles, method = ., two_opt = TRUE, rep = 10,  two_opt_repetitions = 10)) %&amp;gt;%
    set_names(methods)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I do this because the results vary depending on the methods, and I want to be exhaustive (solving
this problem is quite fast, so there’s no reason not to do it):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solutions_df &amp;lt;- solutions %&amp;gt;%
    map_df(as.numeric)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;solutions_df&lt;/code&gt; is a data frame with the order of the castles to visit in rows and the method used
in columns.&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click if you want to see the solutions&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;solutions_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 48 x 9
##    identity random nearest_inserti… cheapest_insert… farthest_insert…
##       &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1        1     10               37               44               15
##  2        2     11               17               37               27
##  3       36      4               22               17               29
##  4       33      9               47               40               38
##  5        6     45               16               27               41
##  6       44     43               43               15               19
##  7       37     16               13               18                5
##  8       17     47               21               34               46
##  9       22     22               14                7               12
## 10       47     13               45               20               23
## # … with 38 more rows, and 4 more variables: arbitrary_insertion &amp;lt;dbl&amp;gt;,
## #   nn &amp;lt;dbl&amp;gt;, repetitive_nn &amp;lt;dbl&amp;gt;, two_opt &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;Now, let’s extract the tour lengths, see which one is the minimum, then plot it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tour_lengths &amp;lt;- solutions %&amp;gt;%
    map_dbl(tour_length)

which.min(tour_lengths)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## arbitrary_insertion 
##                   6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The total length of the tour is 474 kilometers
(that’s 295 miles). Before plotting the data, let’s
re-order it according to the solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;castles_to_visit &amp;lt;- castles[pull(solutions_df, names(which.min(tour_lengths))), ]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-the-solution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot the solution&lt;/h2&gt;
&lt;p&gt;To plot the solution, I first use a data frame I created with the longitude and latitude of
Luxembourguish communes, from the &lt;code&gt;geojson&lt;/code&gt; file available on the
&lt;a href=&#34;https://data.public.lu/en/datasets/limites-administratives-du-grand-duche-de-luxembourg/#resource-39af91a6-9ce4-4c18-8271-313b3ad7c7f5&#34;&gt;OpenData Portal&lt;/a&gt;.
I converted it to a data frame because it is easier to manipulate this way. The code to do that is
in the appendix of this blog post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;communes_df &amp;lt;- read_csv(&amp;quot;communes_df.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   lon = col_double(),
##   lat = col_double(),
##   commune = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can use &lt;code&gt;{ggplot2}&lt;/code&gt; to create the map with the tour. I use &lt;code&gt;geom_polygon()&lt;/code&gt; to build the map,
&lt;code&gt;geom_point()&lt;/code&gt; to add the castles, &lt;code&gt;geom_path()&lt;/code&gt; to connect the points according to the solution I
found and &lt;code&gt;geom_point()&lt;/code&gt; again to highlight the starting castle:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
    geom_polygon(data = communes_df, aes(x = lon, y = lat, group = commune), colour = &amp;quot;grey&amp;quot;, fill = NA) +
    geom_point(data = castles, aes(x = lon, y = lat), colour = &amp;quot;#82518c&amp;quot;, size = 3) +
    geom_path(data = castles_to_visit, aes(x = lon, y = lat), colour = &amp;quot;#647e0e&amp;quot;) +
    geom_point(data = (slice(castles_to_visit, 1)), aes(x = lon, y = lat), colour = &amp;quot;white&amp;quot;, size = 5) +
    theme_void() +
    ggtitle(&amp;quot;The shortest tour to visit 48 Luxembourguish castles&amp;quot;) +
    theme(legend.position = &amp;quot;bottom&amp;quot;,
          legend.title = element_blank(),
          legend.text = element_text(colour = &amp;quot;white&amp;quot;),
          plot.background = element_rect(&amp;quot;#272b30&amp;quot;),
          plot.title = element_text(colour = &amp;quot;white&amp;quot;)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-21-lux_castle_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The white point is the starting point of the tour. As a bonus, let’s do the same plot without
points, but castles emojis instead (using the &lt;code&gt;{ggimage}&lt;/code&gt; package):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
    geom_polygon(data = communes_df, aes(x = lon, y = lat, group = commune), colour = &amp;quot;grey&amp;quot;, fill = NA) +
    geom_emoji(data = castles, aes(x = lon, y = lat, image = &amp;quot;1f3f0&amp;quot;)) + # &amp;lt;- this is the hex code for the &amp;quot;european castle&amp;quot; emoji
    geom_path(data = castles_to_visit, aes(x = lon, y = lat), colour = &amp;quot;#647e0e&amp;quot;) +
    theme_void() +
    ggtitle(&amp;quot;The shortest tour to visit 48 Luxembourguish castles&amp;quot;) +
    theme(legend.position = &amp;quot;bottom&amp;quot;,
          legend.title = element_blank(),
          legend.text = element_text(colour = &amp;quot;white&amp;quot;),
          plot.background = element_rect(&amp;quot;#272b30&amp;quot;),
          plot.title = element_text(colour = &amp;quot;white&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Ignoring unknown parameters: image_colour&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-21-lux_castle_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s horrible.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates and
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;The code below converts the &lt;code&gt;geojson&lt;/code&gt; that can be downloaded from the
&lt;a href=&#34;https://data.public.lu/en/datasets/limites-administratives-du-grand-duche-de-luxembourg/#resource-39af91a6-9ce4-4c18-8271-313b3ad7c7f5&#34;&gt;OpenData Portal&lt;/a&gt;
to &lt;code&gt;csv&lt;/code&gt;. A &lt;code&gt;csv&lt;/code&gt; file is easier to handle. I only focus on the communes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;limadmin &amp;lt;- RJSONIO::fromJSON(&amp;quot;limadmin.geojson&amp;quot;)

communes &amp;lt;- limadmin$communes

extract_communes &amp;lt;- function(features){

    res &amp;lt;- features$geometry$coordinates %&amp;gt;%
        map(lift(rbind)) %&amp;gt;%
        as.data.frame() %&amp;gt;%
        rename(lon = X1,
               lat = X2)

    res %&amp;gt;%
        mutate(commune = features$properties[1])
}

communes_df &amp;lt;- map(limadmin$communes$features, extract_communes)

## Steinfort and Waldbredimus special treatment:

steinfort &amp;lt;- limadmin$communes$features[[5]]$geometry$coordinates[[1]] %&amp;gt;%
    map(lift(rbind)) %&amp;gt;%
    as.data.frame() %&amp;gt;%
    rename(lon = X1,
           lat = X2) %&amp;gt;%
    mutate(commune = &amp;quot;Steinfort&amp;quot;)

waldbredimus &amp;lt;- limadmin$communes$features[[44]]$geometry$coordinates[[1]] %&amp;gt;%
    map(lift(rbind)) %&amp;gt;%
    as.data.frame() %&amp;gt;%
    rename(lon = X1,
           lat = X2) %&amp;gt;%
    mutate(commune = &amp;quot;Waldbredimus&amp;quot;)

communes_df[[5]] &amp;lt;- NULL
communes_df[[43]] &amp;lt;- NULL


communes_df &amp;lt;- bind_rows(communes_df, list(steinfort, waldbredimus))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using a genetic algorithm for the hyperparameter optimization of a SARIMA model</title>
      <link>/blog/2018-11-16-rgenoud_arima/</link>
      <pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-16-rgenoud_arima/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://keiwan.itch.io/evolution&#34;&gt;
&lt;img width = &#34;400&#34; src=&#34;/img/tap-walker.gif&#34; title = &#34;Nietzsche&#39;s Übermensch&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this blog post, I’ll use the data that I cleaned in a previous
&lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-14-luxairport/&#34;&gt;blog post&lt;/a&gt;, which you can download
&lt;a href=&#34;https://github.com/b-rodrigues/avia_par_lu/tree/master&#34;&gt;here&lt;/a&gt;. If you want to follow along,
download the monthly data. In my &lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-15-tidy_gridsearch/&#34;&gt;last blog post&lt;/a&gt;
I showed how to perform a grid search the “tidy” way. As an example, I looked for the right
hyperparameters of a SARIMA model. However, the goal of the post was not hyperparameter optimization
per se, so I did not bother with tuning the hyperparameters on a validation set, and used the test
set for both validation of the hyperparameters and testing the forecast. Of course, this is not great
because doing this might lead to overfitting the hyperparameters to the test set. So in this blog post
I split my data into trainig, validation and testing sets and use a genetic algorithm to look
for the hyperparameters. Again, this is not the most optimal way to go about this problem, since
the &lt;code&gt;{forecast}&lt;/code&gt; package contains the very useful &lt;code&gt;auto.arima()&lt;/code&gt; function. I just wanted to see
what kind of solution a genetic algorithm would return, and also try different cost functions.
If you’re interested, read on!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;Let’s first load some libraries and define some helper functions (the helper functions were explained
in the previous blog posts):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(forecast)
library(rgenoud)
library(parallel)
library(lubridate)
library(furrr)
library(tsibble)
library(brotools)

ihs &amp;lt;- function(x){
    log(x + sqrt(x**2 + 1))
}

to_tibble &amp;lt;- function(forecast_object){
    point_estimate &amp;lt;- forecast_object$mean %&amp;gt;%
        as_tsibble() %&amp;gt;%
        rename(point_estimate = value,
               date = index)

    upper &amp;lt;- forecast_object$upper %&amp;gt;%
        as_tsibble() %&amp;gt;%
        spread(key, value) %&amp;gt;%
        rename(date = index,
               upper80 = `80%`,
               upper95 = `95%`)

    lower &amp;lt;- forecast_object$lower %&amp;gt;%
        as_tsibble() %&amp;gt;%
        spread(key, value) %&amp;gt;%
        rename(date = index,
               lower80 = `80%`,
               lower95 = `95%`)

    reduce(list(point_estimate, upper, lower), full_join)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s load the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/b-rodrigues/avia_par_lu/master/avia_clean_monthy.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   destination = col_character(),
##   date = col_date(format = &amp;quot;&amp;quot;),
##   passengers = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s split the data into a train set, a validation set and a test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_train &amp;lt;- avia_clean_monthly %&amp;gt;%
    select(date, passengers) %&amp;gt;%
    filter(year(date) &amp;lt; 2013) %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total_passengers = sum(passengers)) %&amp;gt;%
    pull(total_passengers) %&amp;gt;%
    ts(., frequency = 12, start = c(2005, 1))

avia_clean_validation &amp;lt;- avia_clean_monthly %&amp;gt;%
    select(date, passengers) %&amp;gt;%
    filter(between(year(date), 2013, 2016)) %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total_passengers = sum(passengers)) %&amp;gt;%
    pull(total_passengers) %&amp;gt;%
    ts(., frequency = 12, start = c(2013, 1))

avia_clean_test &amp;lt;- avia_clean_monthly %&amp;gt;%
    select(date, passengers) %&amp;gt;%
    filter(year(date) &amp;gt;= 2016) %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total_passengers = sum(passengers)) %&amp;gt;%
    pull(total_passengers) %&amp;gt;%
    ts(., frequency = 12, start = c(2016, 1))

logged_test_data &amp;lt;- ihs(avia_clean_test)

logged_validation_data &amp;lt;- ihs(avia_clean_validation)

logged_train_data &amp;lt;- ihs(avia_clean_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will train the models on data from 2005 to 2012, look for the hyperparameters on data from 2013
to 2016 and test the accuracy on data from 2016 to March 2018. For this kind of exercise, the ideal
situation would be to perform cross-validation. Doing this with time-series data is not obvious
because of the autocorrelation between observations, which would be broken by sampling independently
which is required by CV. Also, if for example you do leave-one-out CV,
you would end up trying to predict a point in, say, 2017, with data
from 2018, which does not make sense. So you should be careful about that. &lt;code&gt;{forecast}&lt;/code&gt; is able
to perform &lt;a href=&#34;https://robjhyndman.com/hyndsight/tscv/&#34;&gt;CV for time series&lt;/a&gt; and &lt;code&gt;scikit-learn&lt;/code&gt;, the
Python package, is able to perform
&lt;a href=&#34;https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split&#34;&gt;cross-validation of time series data&lt;/a&gt;
too. I will not do it in this blog post and simply focus on the genetic algorithm part.&lt;/p&gt;
&lt;p&gt;Let’s start by defining the cost function to minimize. I’ll try several, in the first one I will
minimize the RMSE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost_function_rmse &amp;lt;- function(param, train_data, validation_data, forecast_periods){
    order &amp;lt;- param[1:3]
    season &amp;lt;- c(param[4:6], 12)
    model &amp;lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, order = order, 
                                                      seasonal = season,
                                                      method = &amp;quot;ML&amp;quot;)
    if(is.null(model)){
        return(9999999)
    } else {
      forecast_model &amp;lt;- forecast::forecast(model, h = forecast_periods)
      point_forecast &amp;lt;- forecast_model$mean
      sqrt(mean(point_forecast - validation_data) ** 2)
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;arima()&lt;/code&gt; is not able to estimate a model for the given parameters, I force it to return &lt;code&gt;NULL&lt;/code&gt;,
and in that case force the cost function to return a very high cost. If a model was successfully estimated,
then I compute the RMSE.&lt;/p&gt;
&lt;p&gt;Let’s also take a look at what &lt;code&gt;auto.arima()&lt;/code&gt; says:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;starting_model &amp;lt;- auto.arima(logged_train_data)
summary(starting_model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: logged_train_data 
## ARIMA(3,0,0)(0,1,1)[12] with drift 
## 
## Coefficients:
##          ar1     ar2     ar3     sma1   drift
##       0.2318  0.2292  0.3661  -0.8498  0.0029
## s.e.  0.1016  0.1026  0.1031   0.2101  0.0010
## 
## sigma^2 estimated as 0.004009:  log likelihood=107.98
## AIC=-203.97   AICc=-202.88   BIC=-189.38
## 
## Training set error measures:
##                        ME       RMSE        MAE         MPE      MAPE
## Training set 0.0009924108 0.05743719 0.03577996 0.006323241 0.3080978
##                   MASE        ACF1
## Training set 0.4078581 -0.02707016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s compute the cost at this vector of parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost_function_rmse(c(1, 0, 2, 2, 1, 0),
              train_data = logged_train_data,
              validation_data = logged_validation_data,
              forecast_periods = 65)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1731473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, now let’s start with optimizing the hyperparameters. Let’s help the genetic algorithm a little
bit by defining where it should perform the search:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;domains &amp;lt;- matrix(c(0, 3, 0, 2, 0, 3, 0, 3, 0, 2, 0, 3), byrow = TRUE, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This matrix constraints the first parameter to lie between 0 and 3, the second one between 0 and 2,
and so on.&lt;/p&gt;
&lt;p&gt;Let’s call the &lt;code&gt;genoud()&lt;/code&gt; function from the &lt;code&gt;{rgenoud}&lt;/code&gt; package, and use 8 cores:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cl &amp;lt;- makePSOCKcluster(8)
clusterExport(cl, c(&amp;#39;logged_train_data&amp;#39;, &amp;#39;logged_validation_data&amp;#39;))

tic &amp;lt;- Sys.time()

auto_arima_rmse &amp;lt;- genoud(cost_function_rmse,
                     nvars = 6,
                     data.type.int = TRUE,
                     starting.values = c(1, 0, 2, 2, 1, 0), # &amp;lt;- from auto.arima
                     Domains = domains,
                     cluster = cl,
                     train_data = logged_train_data,
                     validation_data = logged_validation_data,
                     forecast_periods = length(logged_validation_data),
                     hard.generation.limit = TRUE)
toc_rmse &amp;lt;- Sys.time() - tic&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;makePSOCKcluster()&lt;/code&gt; is a function from the &lt;code&gt;{parallel}&lt;/code&gt; package. I must also &lt;em&gt;export&lt;/em&gt; the global
variables &lt;code&gt;logged_train_data&lt;/code&gt; or &lt;code&gt;logged_validation_data&lt;/code&gt;. If I don’t do that, the workers called
by &lt;code&gt;genoud()&lt;/code&gt; will not &lt;em&gt;know&lt;/em&gt; about these variables and an error will be returned. The option
&lt;code&gt;data.type.int = TRUE&lt;/code&gt; force the algorithm to look only for integers, and &lt;code&gt;hard.generation.limit = TRUE&lt;/code&gt;
forces the algorithm to stop after 100 generations.&lt;/p&gt;
&lt;p&gt;The process took 7 minutes, which is faster than doing the grid search.
What was the solution found?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auto_arima_rmse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $value
## [1] 0.0001863039
## 
## $par
## [1] 3 2 1 1 2 1
## 
## $gradients
## [1] NA NA NA NA NA NA
## 
## $generations
## [1] 11
## 
## $peakgeneration
## [1] 1
## 
## $popsize
## [1] 1000
## 
## $operators
## [1] 122 125 125 125 125 126 125 126   0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s train the model using the &lt;code&gt;arima()&lt;/code&gt; function at these parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_model_rmse &amp;lt;- arima(logged_train_data, order = auto_arima_rmse$par[1:3], 
                         season = list(order = auto_arima_rmse$par[4:6], period = 12),
                         method = &amp;quot;ML&amp;quot;)

summary(best_model_rmse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = logged_train_data, order = auto_arima_rmse$par[1:3], seasonal = list(order = auto_arima_rmse$par[4:6], 
##     period = 12), method = &amp;quot;ML&amp;quot;)
## 
## Coefficients:
##           ar1      ar2      ar3      ma1     sar1     sma1
##       -0.6999  -0.4541  -0.0476  -0.9454  -0.4996  -0.9846
## s.e.   0.1421   0.1612   0.1405   0.1554   0.1140   0.2193
## 
## sigma^2 estimated as 0.006247:  log likelihood = 57.34,  aic = -100.67
## 
## Training set error measures:
##                         ME       RMSE        MAE          MPE      MAPE
## Training set -0.0006142355 0.06759545 0.04198561 -0.005408262 0.3600483
##                   MASE         ACF1
## Training set 0.4386693 -0.008298546&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s extract the forecasts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_model_rmse_forecast &amp;lt;- forecast::forecast(best_model_rmse, h = 65)

best_model_rmse_forecast &amp;lt;- to_tibble(best_model_rmse_forecast)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;date&amp;quot;
## Joining, by = &amp;quot;date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;starting_model_forecast &amp;lt;- forecast(starting_model, h = 65)

starting_model_forecast &amp;lt;- to_tibble(starting_model_forecast)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;date&amp;quot;
## Joining, by = &amp;quot;date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and plot the forecast to see how it looks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total = sum(passengers)) %&amp;gt;%
    mutate(total_ihs = ihs(total)) %&amp;gt;%
    ggplot() +
    ggtitle(&amp;quot;Minimization of RMSE&amp;quot;) +
    geom_line(aes(y = total_ihs, x = date), colour = &amp;quot;#82518c&amp;quot;) +
    scale_x_date(date_breaks = &amp;quot;1 year&amp;quot;, date_labels = &amp;quot;%m-%Y&amp;quot;) +
    geom_ribbon(data = best_model_rmse_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = &amp;quot;#666018&amp;quot;, alpha = 0.2) +
    geom_line(data = best_model_rmse_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = &amp;quot;#8e9d98&amp;quot;) +
    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = &amp;quot;#98431e&amp;quot;, alpha = 0.2) +
    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = &amp;quot;#a53031&amp;quot;) +
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-16-rgenoud_arima_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The yellowish line and confidence intervals come from minimizing the genetic algorithm, and the
redish from &lt;code&gt;auto.arima()&lt;/code&gt;. Interesting; the point estimate is very precise, but the confidence
intervals are very wide. Low bias, high variance.&lt;/p&gt;
&lt;p&gt;Now, let’s try with another cost function, where I minimize the BIC, similar to the &lt;code&gt;auto.arima()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost_function_bic &amp;lt;- function(param, train_data, validation_data, forecast_periods){
    order &amp;lt;- param[1:3]
    season &amp;lt;- c(param[4:6], 12)
    model &amp;lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, order = order, 
                                                      seasonal = season,
                                                      method = &amp;quot;ML&amp;quot;)
    if(is.null(model)){
        return(9999999)
    } else {
        BIC(model)
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the cost at the parameter values returned by &lt;code&gt;auto.arima()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost_function_bic(c(1, 0, 2, 2, 1, 0),
              train_data = logged_train_data,
              validation_data = logged_validation_data,
              forecast_periods = 65)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -184.6397&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let the genetic algorithm run again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cl &amp;lt;- makePSOCKcluster(8)
clusterExport(cl, c(&amp;#39;logged_train_data&amp;#39;, &amp;#39;logged_validation_data&amp;#39;))

tic &amp;lt;- Sys.time()

auto_arima_bic &amp;lt;- genoud(cost_function_bic,
                     nvars = 6,
                     data.type.int = TRUE,
                     starting.values = c(1, 0, 2, 2, 1, 0), # &amp;lt;- from auto.arima
                     Domains = domains,
                     cluster = cl,
                     train_data = logged_train_data,
                     validation_data = logged_validation_data,
                     forecast_periods = length(logged_validation_data),
                     hard.generation.limit = TRUE)
toc_bic &amp;lt;- Sys.time() - tic&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This time, it took 6 minutes, a bit slower than before. Let’s take a look at the solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auto_arima_bic&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $value
## [1] -201.0656
## 
## $par
## [1] 0 1 1 1 0 1
## 
## $gradients
## [1] NA NA NA NA NA NA
## 
## $generations
## [1] 12
## 
## $peakgeneration
## [1] 1
## 
## $popsize
## [1] 1000
## 
## $operators
## [1] 122 125 125 125 125 126 125 126   0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s train the model at these parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_model_bic &amp;lt;- arima(logged_train_data, order = auto_arima_bic$par[1:3], 
                        season = list(order = auto_arima_bic$par[4:6], period = 12),
                        method = &amp;quot;ML&amp;quot;)

summary(best_model_bic)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = logged_train_data, order = auto_arima_bic$par[1:3], seasonal = list(order = auto_arima_bic$par[4:6], 
##     period = 12), method = &amp;quot;ML&amp;quot;)
## 
## Coefficients:
##           ma1    sar1    sma1
##       -0.6225  0.9968  -0.832
## s.e.   0.0835  0.0075   0.187
## 
## sigma^2 estimated as 0.004145:  log likelihood = 109.64,  aic = -211.28
## 
## Training set error measures:
##                       ME       RMSE        MAE        MPE      MAPE
## Training set 0.003710982 0.06405303 0.04358164 0.02873561 0.3753513
##                   MASE        ACF1
## Training set 0.4553447 -0.03450603&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And let’s plot the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_model_bic_forecast &amp;lt;- forecast::forecast(best_model_bic, h = 65)

best_model_bic_forecast &amp;lt;- to_tibble(best_model_bic_forecast)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;date&amp;quot;
## Joining, by = &amp;quot;date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total = sum(passengers)) %&amp;gt;%
    mutate(total_ihs = ihs(total)) %&amp;gt;%
    ggplot() +
    ggtitle(&amp;quot;Minimization of BIC&amp;quot;) +
    geom_line(aes(y = total_ihs, x = date), colour = &amp;quot;#82518c&amp;quot;) +
    scale_x_date(date_breaks = &amp;quot;1 year&amp;quot;, date_labels = &amp;quot;%m-%Y&amp;quot;) +
    geom_ribbon(data = best_model_bic_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = &amp;quot;#5160a0&amp;quot;, alpha = 0.2) +
    geom_line(data = best_model_bic_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = &amp;quot;#208480&amp;quot;) +
    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = &amp;quot;#98431e&amp;quot;, alpha = 0.2) +
    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = &amp;quot;#a53031&amp;quot;) +
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-16-rgenoud_arima_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The solutions are very close, both in terms of point estimates and confidence intervals. Bias
increased, but variance lowered… This gives me an idea! What if I minimize the RMSE, while
keeping the number of parameters low, as a kind of regularization? This is somewhat what minimising
BIC does, but let’s try to do it a more “naive” approach:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost_function_rmse_low_k &amp;lt;- function(param, train_data, validation_data, forecast_periods, max.order){
    order &amp;lt;- param[1:3]
    season &amp;lt;- c(param[4:6], 12)
    if(param[1] + param[3] + param[4] + param[6] &amp;gt; max.order){
        return(9999999)
    } else {
        model &amp;lt;- purrr::possibly(arima, otherwise = NULL)(x = train_data, 
                                                          order = order, 
                                                          seasonal = season,
                                                          method = &amp;quot;ML&amp;quot;)
    }
    if(is.null(model)){
        return(9999999)
    } else {
        forecast_model &amp;lt;- forecast::forecast(model, h = forecast_periods)
        point_forecast &amp;lt;- forecast_model$mean
        sqrt(mean(point_forecast - validation_data) ** 2)
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is also similar to what &lt;code&gt;auto.arima()&lt;/code&gt; does; by default, the &lt;code&gt;max.order&lt;/code&gt; argument in &lt;code&gt;auto.arima()&lt;/code&gt;
is set to 5, and is the sum of &lt;code&gt;p + q + P + Q&lt;/code&gt;. So I’ll try something similar.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the cost at the parameter values returned by &lt;code&gt;auto.arima()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cost_function_rmse_low_k(c(1, 0, 2, 2, 1, 0),
              train_data = logged_train_data,
              validation_data = logged_validation_data,
              forecast_periods = 65,
              max.order = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1731473&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what will happen:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cl &amp;lt;- makePSOCKcluster(8)
clusterExport(cl, c(&amp;#39;logged_train_data&amp;#39;, &amp;#39;logged_validation_data&amp;#39;))

tic &amp;lt;- Sys.time()

auto_arima_rmse_low_k &amp;lt;- genoud(cost_function_rmse_low_k,
                         nvars = 6,
                         data.type.int = TRUE,
                         starting.values = c(1, 0, 2, 2, 1, 0), # &amp;lt;- from auto.arima
                         max.order = 5,
                         Domains = domains,
                         cluster = cl,
                         train_data = logged_train_data,
                         validation_data = logged_validation_data,
                         forecast_periods = length(logged_validation_data),
                         hard.generation.limit = TRUE)
toc_rmse_low_k &amp;lt;- Sys.time() - tic&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It took 1 minute to train this one, quite fast! Let’s take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auto_arima_rmse_low_k&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $value
## [1] 0.002503478
## 
## $par
## [1] 1 2 0 3 1 0
## 
## $gradients
## [1] NA NA NA NA NA NA
## 
## $generations
## [1] 11
## 
## $peakgeneration
## [1] 1
## 
## $popsize
## [1] 1000
## 
## $operators
## [1] 122 125 125 125 125 126 125 126   0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And let’s plot it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_model_rmse_low_k &amp;lt;- arima(logged_train_data, order = auto_arima_rmse_low_k$par[1:3], 
                               season = list(order = auto_arima_rmse_low_k$par[4:6], period = 12),
                               method = &amp;quot;ML&amp;quot;)

summary(best_model_rmse_low_k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## arima(x = logged_train_data, order = auto_arima_rmse_low_k$par[1:3], seasonal = list(order = auto_arima_rmse_low_k$par[4:6], 
##     period = 12), method = &amp;quot;ML&amp;quot;)
## 
## Coefficients:
##           ar1     sar1     sar2     sar3
##       -0.6468  -0.7478  -0.5263  -0.1143
## s.e.   0.0846   0.1171   0.1473   0.1446
## 
## sigma^2 estimated as 0.01186:  log likelihood = 57.88,  aic = -105.76
## 
## Training set error measures:
##                        ME      RMSE        MAE         MPE      MAPE
## Training set 0.0005953302 0.1006917 0.06165919 0.003720452 0.5291736
##                   MASE       ACF1
## Training set 0.6442205 -0.3706693&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_model_rmse_low_k_forecast &amp;lt;- forecast::forecast(best_model_rmse_low_k, h = 65)

best_model_rmse_low_k_forecast &amp;lt;- to_tibble(best_model_rmse_low_k_forecast)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;date&amp;quot;
## Joining, by = &amp;quot;date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total = sum(passengers)) %&amp;gt;%
    mutate(total_ihs = ihs(total)) %&amp;gt;%
    ggplot() +
    ggtitle(&amp;quot;Minimization of RMSE + low k&amp;quot;) +
    geom_line(aes(y = total_ihs, x = date), colour = &amp;quot;#82518c&amp;quot;) +
    scale_x_date(date_breaks = &amp;quot;1 year&amp;quot;, date_labels = &amp;quot;%m-%Y&amp;quot;) +
    geom_ribbon(data = best_model_rmse_low_k_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = &amp;quot;#5160a0&amp;quot;, alpha = 0.2) +
    geom_line(data = best_model_rmse_low_k_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = &amp;quot;#208480&amp;quot;) +
    geom_ribbon(data = starting_model_forecast, aes(x = date, ymin = lower95, ymax = upper95),
                fill = &amp;quot;#98431e&amp;quot;, alpha = 0.2) +
    geom_line(data = starting_model_forecast, aes(x = date, y = point_estimate), 
              linetype = 2, colour = &amp;quot;#a53031&amp;quot;) +
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-16-rgenoud_arima_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like this was not the right strategy. There might be a better cost function than what I have
tried, but looks like minimizing the BIC is the way to go.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates or
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Searching for the optimal hyper-parameters of an ARIMA model in parallel: the tidy gridsearch approach</title>
      <link>/blog/2018-11-15-tidy_gridsearch/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-15-tidy_gridsearch/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://youtu.be/3NxM-AL18lU?t=33s&#34;&gt;
&lt;img width = &#34;400&#34; src=&#34;/img/dank_memes.jpg&#34; title = &#34;What a time to be alive&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this blog post, I’ll use the data that I cleaned in a previous
&lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-14-luxairport/&#34;&gt;blog post&lt;/a&gt;, which you can download
&lt;a href=&#34;https://github.com/b-rodrigues/avia_par_lu/tree/master&#34;&gt;here&lt;/a&gt;. If you want to follow along,
download the monthly data.&lt;/p&gt;
&lt;p&gt;In the previous blog post, I used the &lt;code&gt;auto.arima()&lt;/code&gt; function to very quickly get a “good-enough”
model to predict future monthly total passengers flying from LuxAirport. “Good-enough” models can
be all you need in a lot of situations, but perhaps you’d like to have a better model. I will show
here how you can get a better model by searching through a grid of hyper-parameters.&lt;/p&gt;
&lt;p&gt;This blog post was partially inspired by: &lt;a href=&#34;https://drsimonj.svbtle.com/grid-search-in-the-tidyverse&#34; class=&#34;uri&#34;&gt;https://drsimonj.svbtle.com/grid-search-in-the-tidyverse&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;SARIMA models have a lot of hyper-parameters, 7 in total! Three trend hyper-parameters, &lt;em&gt;p, d, q&lt;/em&gt;,
same as for an ARIMA model, and four seasonal hyper-parameters, &lt;em&gt;P, D, Q, S&lt;/em&gt;. The traditional way t
o search for these hyper-parameters is the so-called Box-Jenkins method. You can read about it
&lt;a href=&#34;https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc44a.htm&#34;&gt;here&lt;/a&gt;. This method was described
in a 1970 book, &lt;em&gt;Time series analysis: Forecasting and control&lt;/em&gt; by Box and Jenkins. The method
requires that you first prepare the data by logging it and differencing it, in order to make the
time series stationary. You then need to analyze ACF and PACF plots, in order to determine the
right amount of lags… It take some time, but this method made sense in a time were computing
power was very expensive. Today, we can simply let our computer search through thousands of models,
check memes on the internet, and come back to the best fit. This blog post is for you, the busy
data scientist meme connoisseurs who cannot waste time with theory and other such useless time drains,
when there are literally thousands of new memes being created and shared every day. Every second counts.
To determine what model is best, I will do pseudo out-of-sample forecasting and compute the RMSE
for each model. I will then choose the model that has the lowest RMSE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;Let’s first load some libraries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(forecast)
library(lubridate)
library(furrr)
library(tsibble)
library(brotools)

ihs &amp;lt;- function(x){
    log(x + sqrt(x**2 + 1))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s load the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/b-rodrigues/avia_par_lu/master/avia_clean_monthy.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   destination = col_character(),
##   date = col_date(format = &amp;quot;&amp;quot;),
##   passengers = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s split the data into a training set and into a testing set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_train &amp;lt;- avia_clean_monthly %&amp;gt;%
    select(date, passengers) %&amp;gt;%
    filter(year(date) &amp;lt; 2015) %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total_passengers = sum(passengers)) %&amp;gt;%
    pull(total_passengers) %&amp;gt;%
    ts(., frequency = 12, start = c(2005, 1))

avia_clean_test &amp;lt;- avia_clean_monthly %&amp;gt;%
    select(date, passengers) %&amp;gt;%
    filter(year(date) &amp;gt;= 2015) %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total_passengers = sum(passengers)) %&amp;gt;%
    pull(total_passengers) %&amp;gt;%
    ts(., frequency = 12, start = c(2015, 1))

logged_train_data &amp;lt;- ihs(avia_clean_train)

logged_test_data &amp;lt;- ihs(avia_clean_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also define a helper function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;to_tibble &amp;lt;- function(forecast_object){
    point_estimate &amp;lt;- forecast_object$mean %&amp;gt;%
        as_tsibble() %&amp;gt;%
        rename(point_estimate = value,
               date = index)

    upper &amp;lt;- forecast_object$upper %&amp;gt;%
        as_tsibble() %&amp;gt;%
        spread(key, value) %&amp;gt;%
        rename(date = index,
               upper80 = `80%`,
               upper95 = `95%`)

    lower &amp;lt;- forecast_object$lower %&amp;gt;%
        as_tsibble() %&amp;gt;%
        spread(key, value) %&amp;gt;%
        rename(date = index,
               lower80 = `80%`,
               lower95 = `95%`)

    reduce(list(point_estimate, upper, lower), full_join)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function takes a &lt;code&gt;forecast&lt;/code&gt; object as argument, and returns a nice tibble. This will be useful
later, and is based on the code I already used in my previous
&lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-14-luxairport/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, let’s take a closer look at the &lt;code&gt;arima()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ARIMA Modelling of Time Series

Description

Fit an ARIMA model to a univariate time series.

Usage

arima(x, order = c(0L, 0L, 0L),
      seasonal = list(order = c(0L, 0L, 0L), period = NA),
      xreg = NULL, include.mean = TRUE,
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c(&amp;quot;CSS-ML&amp;quot;, &amp;quot;ML&amp;quot;, &amp;quot;CSS&amp;quot;), n.cond,
      SSinit = c(&amp;quot;Gardner1980&amp;quot;, &amp;quot;Rossignol2011&amp;quot;),
      optim.method = &amp;quot;BFGS&amp;quot;,
      optim.control = list(), kappa = 1e6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The user is supposed to enter the hyper-parameters as two lists, one called &lt;code&gt;order&lt;/code&gt; for &lt;em&gt;p, d, q&lt;/em&gt;
and one called &lt;code&gt;seasonal&lt;/code&gt; for &lt;em&gt;P, D, Q, S&lt;/em&gt;. So what we need is to define these lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;order_list &amp;lt;- list(&amp;quot;p&amp;quot; = seq(0, 3),
                   &amp;quot;d&amp;quot; = seq(0, 2),
                   &amp;quot;q&amp;quot; = seq(0, 3)) %&amp;gt;%
    cross() %&amp;gt;%
    map(lift(c))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I first start with &lt;code&gt;order_list&lt;/code&gt;. This list has 3 elements, “p”, “d” and “q”. Each element is a
sequence from 0 to 3 (2 in the case of “d”). When I pass this list to &lt;code&gt;purrr::cross()&lt;/code&gt; I get the
product set of the starting list, so in this case a list of 4*3*4 = 48 elements. However, this
list looks pretty bad:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list(&amp;quot;p&amp;quot; = seq(0, 3),
     &amp;quot;d&amp;quot; = seq(0, 2),
     &amp;quot;q&amp;quot; = seq(0, 3)) %&amp;gt;%
    cross() %&amp;gt;%
    head(3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [[1]]$p
## [1] 0
## 
## [[1]]$d
## [1] 0
## 
## [[1]]$q
## [1] 0
## 
## 
## [[2]]
## [[2]]$p
## [1] 1
## 
## [[2]]$d
## [1] 0
## 
## [[2]]$q
## [1] 0
## 
## 
## [[3]]
## [[3]]$p
## [1] 2
## 
## [[3]]$d
## [1] 0
## 
## [[3]]$q
## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I would like to have something like this instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[1]]
p d q 
0 0 0 

[[2]]
p d q 
1 0 0 

[[3]]
p d q 
2 0 0 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is possible with the last line, &lt;code&gt;map(lift(c))&lt;/code&gt;. There’s a lot going on in this very small
line of code. First of all, there’s &lt;code&gt;map()&lt;/code&gt;. &lt;code&gt;map()&lt;/code&gt; iterates over lists, and applies a function,
in this case &lt;code&gt;lift(c)&lt;/code&gt;. &lt;code&gt;purrr::lift()&lt;/code&gt; is a very interesting function that lifts the domain of
definition of a function from one type of input to another. The function whose input I am lifting
is &lt;code&gt;c()&lt;/code&gt;. So now, &lt;code&gt;c()&lt;/code&gt; can take a list instead of a vector. Compare the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The usual

c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Nothing happens
c(list(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;a&amp;quot;
## 
## [[2]]
## [1] &amp;quot;b&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Magic happens
lift(c)(list(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So &lt;code&gt;order_list&lt;/code&gt; is exactly what I wanted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(order_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## p d q 
## 0 0 0 
## 
## [[2]]
## p d q 
## 1 0 0 
## 
## [[3]]
## p d q 
## 2 0 0 
## 
## [[4]]
## p d q 
## 3 0 0 
## 
## [[5]]
## p d q 
## 0 1 0 
## 
## [[6]]
## p d q 
## 1 1 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I do the same for &lt;code&gt;season_list&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;season_list &amp;lt;- list(&amp;quot;P&amp;quot; = seq(0, 3),
                    &amp;quot;D&amp;quot; = seq(0, 2),
                    &amp;quot;Q&amp;quot; = seq(0, 3),
                    &amp;quot;period&amp;quot; = 12)  %&amp;gt;%
    cross() %&amp;gt;%
    map(lift(c))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now coerce these two lists of vectors to tibbles:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;orderdf &amp;lt;- tibble(&amp;quot;order&amp;quot; = order_list)

seasondf &amp;lt;- tibble(&amp;quot;season&amp;quot; = season_list)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And I can now finally create the grid of hyper-parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hyper_parameters_df &amp;lt;- crossing(orderdf, seasondf)

nrows &amp;lt;- nrow(hyper_parameters_df)

head(hyper_parameters_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   order     season   
##   &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;   
## 1 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt;
## 2 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt;
## 3 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt;
## 4 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt;
## 5 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt;
## 6 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;hyper_parameters_df&lt;/code&gt; data frame has 2304 rows, meaning, I will now estimate 2304
models, and will do so in parallel. Let’s just take a quick look at the internals of &lt;code&gt;hyper_parameters_df&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(hyper_parameters_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 2,304
## Variables: 2
## $ order  &amp;lt;list&amp;gt; [&amp;lt;0, 0, 0&amp;gt;, &amp;lt;0, 0, 0&amp;gt;, &amp;lt;0, 0, 0&amp;gt;, &amp;lt;0, 0, 0&amp;gt;, &amp;lt;0, 0, 0&amp;gt;, …
## $ season &amp;lt;list&amp;gt; [&amp;lt;0, 0, 0, 12&amp;gt;, &amp;lt;1, 0, 0, 12&amp;gt;, &amp;lt;2, 0, 0, 12&amp;gt;, &amp;lt;3, 0, 0, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So in the &lt;code&gt;order&lt;/code&gt; column, the vector &lt;code&gt;0, 0, 0&lt;/code&gt; is repeated as many times as there are combinations
of &lt;em&gt;P, D, Q, S&lt;/em&gt; for &lt;code&gt;season&lt;/code&gt;. Same for all the other vectors of the &lt;code&gt;order&lt;/code&gt; column.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-the-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training the models&lt;/h2&gt;
&lt;p&gt;Because training these models might take some time, I will use the fantastic &lt;code&gt;{furrr}&lt;/code&gt; package
by &lt;a href=&#34;https://twitter.com/dvaughan32&#34;&gt;Davis Vaughan&lt;/a&gt; to train the &lt;code&gt;arima()&lt;/code&gt; function in parallel.
For this, I first define 8 workers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plan(multiprocess, workers = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then I run the code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic &amp;lt;- Sys.time()
models_df &amp;lt;- hyper_parameters_df %&amp;gt;%
    mutate(models = future_map2(.x = order,
                         .y = season,
                         ~possibly(arima, otherwise = NULL)(x = logged_train_data,
                                                                           order = .x, seasonal = .y)))
running_time &amp;lt;- Sys.time() - tic&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I use &lt;code&gt;future_map2()&lt;/code&gt;, which is just like &lt;code&gt;map2()&lt;/code&gt; but running in parallel.
I add a new column to the data called &lt;code&gt;models&lt;/code&gt;, which will contain the models trained over all the
different combinations of &lt;code&gt;order&lt;/code&gt; and &lt;code&gt;season&lt;/code&gt;. The models are trained on the &lt;code&gt;logged_train_data&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Training the 2304 models took 18 minutes, which is
plenty of time to browse the latest memes, but still quick enough that it justifies the whole approach.
Let’s take a look at the &lt;code&gt;models_df&lt;/code&gt; object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(models_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   order     season    models 
##   &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt; 
## 1 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt;
## 2 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt;
## 3 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt;
## 4 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt;
## 5 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt;
## 6 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the &lt;code&gt;models&lt;/code&gt; column contains all the trained models. The model on the first row,
was trained with the hyperparameters of row 1, and so on. But, our work is not over! We now need
to find the best model. First, I add a new column to the tibble, which contains the forecast. From
the forecast, I extract the point estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models_df %&amp;gt;%
    mutate(forecast = map(models, ~possibly(forecast, otherwise = NULL)(., h = 39))) %&amp;gt;%
    mutate(point_forecast = map(forecast, ~.$`mean`)) %&amp;gt;%
    ....&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You have to be familiar with a &lt;code&gt;forecast&lt;/code&gt; object to understand the last line: a &lt;code&gt;forecast&lt;/code&gt; object
is a list with certain elements, the point estimates, the confidence intervals, and so on. To get
the point estimates, I have to extract the “mean” element from the list. Hence the weird &lt;code&gt;~.$mean&lt;/code&gt;.
Then I need to add a new list-column, where each element is the vector of true values, meaning the data
from 2015 to 2018. Because I have to add it as a list of size 2304, I do that with &lt;code&gt;purrr::rerun()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rerun(5, c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;
## 
## [[2]]
## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;
## 
## [[3]]
## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;
## 
## [[4]]
## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;
## 
## [[5]]
## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is then easy to compute the RMSE, which I add as a column to the original data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;... %&amp;gt;%
    mutate(true_value = rerun(nrows, logged_test_data)) %&amp;gt;%
    mutate(rmse = map2_dbl(point_forecast, true_value,
                           ~sqrt(mean((.x - .y) ** 2))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The whole workflow is here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models_df &amp;lt;- models_df %&amp;gt;%
    mutate(forecast = map(models, ~possibly(forecast, otherwise = NULL)(., h = 39))) %&amp;gt;%
    mutate(point_forecast = map(forecast, ~.$`mean`)) %&amp;gt;%
    mutate(true_value = rerun(nrows, logged_test_data)) %&amp;gt;%
    mutate(rmse = map2_dbl(point_forecast, true_value,
                           ~sqrt(mean((.x - .y) ** 2))))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how &lt;code&gt;models_df&lt;/code&gt; looks now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(models_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##   order     season    models  forecast   point_forecast true_value  rmse
##   &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;  &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;         &amp;lt;list&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt; &amp;lt;forecast&amp;gt; &amp;lt;ts&amp;gt;           &amp;lt;ts&amp;gt;       0.525
## 2 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt; &amp;lt;forecast&amp;gt; &amp;lt;ts&amp;gt;           &amp;lt;ts&amp;gt;       0.236
## 3 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt; &amp;lt;forecast&amp;gt; &amp;lt;ts&amp;gt;           &amp;lt;ts&amp;gt;       0.235
## 4 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt; &amp;lt;forecast&amp;gt; &amp;lt;ts&amp;gt;           &amp;lt;ts&amp;gt;       0.217
## 5 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt; &amp;lt;forecast&amp;gt; &amp;lt;ts&amp;gt;           &amp;lt;ts&amp;gt;       0.190
## 6 &amp;lt;int [3]&amp;gt; &amp;lt;dbl [4]&amp;gt; &amp;lt;Arima&amp;gt; &amp;lt;forecast&amp;gt; &amp;lt;ts&amp;gt;           &amp;lt;ts&amp;gt;       0.174&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I can finally select the best performing model. I select the model with minimum RMSE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;best_model &amp;lt;- models_df %&amp;gt;%
    filter(rmse == min(rmse, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And save the forecast into a new variable, as a &lt;code&gt;tibble&lt;/code&gt;, using my &lt;code&gt;to_tibble()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(best_model_forecast &amp;lt;- to_tibble(best_model$forecast[[1]]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;date&amp;quot;
## Joining, by = &amp;quot;date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tsibble: 39 x 6 [1M]
##        date point_estimate upper80 upper95 lower80 lower95
##       &amp;lt;mth&amp;gt;          &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1 2015 Jan           11.9    12.1    12.1    11.8    11.7
##  2 2015 Feb           11.9    12.0    12.1    11.7    11.6
##  3 2015 Mar           12.1    12.3    12.3    11.9    11.9
##  4 2015 Apr           12.2    12.3    12.4    12.0    11.9
##  5 2015 May           12.2    12.4    12.5    12.1    12.0
##  6 2015 Jun           12.3    12.4    12.5    12.1    12.0
##  7 2015 Jul           12.2    12.3    12.4    12.0    11.9
##  8 2015 Aug           12.3    12.5    12.6    12.2    12.1
##  9 2015 Sep           12.3    12.5    12.6    12.2    12.1
## 10 2015 Oct           12.2    12.4    12.5    12.1    12.0
## # … with 29 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, I can plot it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total = sum(passengers)) %&amp;gt;%
    mutate(total_ihs = ihs(total)) %&amp;gt;%
    ggplot() +
    ggtitle(&amp;quot;Logged data&amp;quot;) +
    geom_line(aes(y = total_ihs, x = date), colour = &amp;quot;#82518c&amp;quot;) +
    scale_x_date(date_breaks = &amp;quot;1 year&amp;quot;, date_labels = &amp;quot;%m-%Y&amp;quot;) +
    geom_ribbon(data = best_model_forecast, aes(x = date, ymin = lower95, ymax = upper95), 
                fill = &amp;quot;#666018&amp;quot;, alpha = 0.2) +
    geom_line(data = best_model_forecast, aes(x = date, y = point_estimate), linetype = 2, colour = &amp;quot;#8e9d98&amp;quot;) +
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-15-tidy_gridsearch_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Compared to the previous &lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-14-luxairport/&#34;&gt;blog post&lt;/a&gt;, the
dotted line now seems to follow the true line even better! However, this is not suprising, as I
am using the test set as a validation set, which might lead to overfitting the hyperparameters
to the test set. Also, I am not saying that you should always do a gridsearch whenever you have a
problem like this one. In the case of univariate time series, I am still doubtful that a gridsearch like this is really necessary. The goal of this blog post was not to teach you how to look for
hyperparameters per se, but more to show you how to do a grid search the tidy way. I’ll be writing
about &lt;em&gt;proper&lt;/em&gt; hyperparameter optimization in a future blog post.
Also, the other thing I wanted to show was the power of &lt;code&gt;{furrr}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates or
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Easy time-series prediction with R: a tutorial with air traffic data from Lux Airport</title>
      <link>/blog/2018-11-14-luxairport/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-14-luxairport/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=GIQn8pab8Vc&#34;&gt;
&lt;img src=&#34;/img/lx_aie.jpg&#34; title = &#34;Luxembourg&#39;s largest aircraft landing&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this blog post, I will show you how you can quickly and easily forecast a univariate time series.
I am going to use data from the EU Open Data Portal on air passenger transport. You can find the
data &lt;a href=&#34;https://data.europa.eu/euodp/en/data/dataset/2EwfWXj5d94BUOzfoABKSQ&#34;&gt;here&lt;/a&gt;. I downloaded
the data in the TSV format for Luxembourg Airport, but you could repeat the analysis for any airport.&lt;/p&gt;
&lt;p&gt;Once you have the data, load some of the package we are going to need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(forecast)
library(tsibble)
library(brotools)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and define the following function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ihs &amp;lt;- function(x){
    log(x + sqrt(x**2 + 1))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function, the inverse hyperbolic sine, is useful to transform data in a manner that is very
close to logging it, but that allows for 0’s. The data from Eurostat is not complete for some reason,
so there are some 0 sometimes. To avoid having to log 0, which in R yields &lt;code&gt;-Inf&lt;/code&gt;, I use this
transformation.&lt;/p&gt;
&lt;p&gt;Now, let’s load the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia &amp;lt;- read_tsv(&amp;quot;avia_par_lu.tsv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   .default = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See spec(...) for full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(avia)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 238
##   `unit,tra_meas,… `2018Q1` `2018M03` `2018M02` `2018M01` `2017Q4` `2017Q3`
##   &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;   
## 1 FLIGHT,CAF_PAS,… 511      172       161       178       502      475     
## 2 FLIGHT,CAF_PAS,… :        :         :         :         :        :       
## 3 FLIGHT,CAF_PAS,… :        :         :         :         399      306     
## 4 FLIGHT,CAF_PAS,… 485      167       151       167       493      497     
## 5 FLIGHT,CAF_PAS,… 834      293       267       274       790      728     
## 6 FLIGHT,CAF_PAS,… :        :         :         :         :        :       
## # … with 231 more variables: `2017Q2` &amp;lt;chr&amp;gt;, `2017Q1` &amp;lt;chr&amp;gt;,
## #   `2017M12` &amp;lt;chr&amp;gt;, `2017M11` &amp;lt;chr&amp;gt;, `2017M10` &amp;lt;chr&amp;gt;, `2017M09` &amp;lt;chr&amp;gt;,
## #   `2017M08` &amp;lt;chr&amp;gt;, `2017M07` &amp;lt;chr&amp;gt;, `2017M06` &amp;lt;chr&amp;gt;, `2017M05` &amp;lt;chr&amp;gt;,
## #   `2017M04` &amp;lt;chr&amp;gt;, `2017M03` &amp;lt;chr&amp;gt;, `2017M02` &amp;lt;chr&amp;gt;, `2017M01` &amp;lt;chr&amp;gt;,
## #   `2017` &amp;lt;chr&amp;gt;, `2016Q4` &amp;lt;chr&amp;gt;, `2016Q3` &amp;lt;chr&amp;gt;, `2016Q2` &amp;lt;chr&amp;gt;,
## #   `2016Q1` &amp;lt;chr&amp;gt;, `2016M12` &amp;lt;chr&amp;gt;, `2016M11` &amp;lt;chr&amp;gt;, `2016M10` &amp;lt;chr&amp;gt;,
## #   `2016M09` &amp;lt;chr&amp;gt;, `2016M08` &amp;lt;chr&amp;gt;, `2016M07` &amp;lt;chr&amp;gt;, `2016M06` &amp;lt;chr&amp;gt;,
## #   `2016M05` &amp;lt;chr&amp;gt;, `2016M04` &amp;lt;chr&amp;gt;, `2016M03` &amp;lt;chr&amp;gt;, `2016M02` &amp;lt;chr&amp;gt;,
## #   `2016M01` &amp;lt;chr&amp;gt;, `2016` &amp;lt;chr&amp;gt;, `2015Q4` &amp;lt;chr&amp;gt;, `2015Q3` &amp;lt;chr&amp;gt;,
## #   `2015Q2` &amp;lt;chr&amp;gt;, `2015Q1` &amp;lt;chr&amp;gt;, `2015M12` &amp;lt;chr&amp;gt;, `2015M11` &amp;lt;chr&amp;gt;,
## #   `2015M10` &amp;lt;chr&amp;gt;, `2015M09` &amp;lt;chr&amp;gt;, `2015M08` &amp;lt;chr&amp;gt;, `2015M07` &amp;lt;chr&amp;gt;,
## #   `2015M06` &amp;lt;chr&amp;gt;, `2015M05` &amp;lt;chr&amp;gt;, `2015M04` &amp;lt;chr&amp;gt;, `2015M03` &amp;lt;chr&amp;gt;,
## #   `2015M02` &amp;lt;chr&amp;gt;, `2015M01` &amp;lt;chr&amp;gt;, `2015` &amp;lt;chr&amp;gt;, `2014Q4` &amp;lt;chr&amp;gt;,
## #   `2014Q3` &amp;lt;chr&amp;gt;, `2014Q2` &amp;lt;chr&amp;gt;, `2014Q1` &amp;lt;chr&amp;gt;, `2014M12` &amp;lt;chr&amp;gt;,
## #   `2014M11` &amp;lt;chr&amp;gt;, `2014M10` &amp;lt;chr&amp;gt;, `2014M09` &amp;lt;chr&amp;gt;, `2014M08` &amp;lt;chr&amp;gt;,
## #   `2014M07` &amp;lt;chr&amp;gt;, `2014M06` &amp;lt;chr&amp;gt;, `2014M05` &amp;lt;chr&amp;gt;, `2014M04` &amp;lt;chr&amp;gt;,
## #   `2014M03` &amp;lt;chr&amp;gt;, `2014M02` &amp;lt;chr&amp;gt;, `2014M01` &amp;lt;chr&amp;gt;, `2014` &amp;lt;chr&amp;gt;,
## #   `2013Q4` &amp;lt;chr&amp;gt;, `2013Q3` &amp;lt;chr&amp;gt;, `2013Q2` &amp;lt;chr&amp;gt;, `2013Q1` &amp;lt;chr&amp;gt;,
## #   `2013M12` &amp;lt;chr&amp;gt;, `2013M11` &amp;lt;chr&amp;gt;, `2013M10` &amp;lt;chr&amp;gt;, `2013M09` &amp;lt;chr&amp;gt;,
## #   `2013M08` &amp;lt;chr&amp;gt;, `2013M07` &amp;lt;chr&amp;gt;, `2013M06` &amp;lt;chr&amp;gt;, `2013M05` &amp;lt;chr&amp;gt;,
## #   `2013M04` &amp;lt;chr&amp;gt;, `2013M03` &amp;lt;chr&amp;gt;, `2013M02` &amp;lt;chr&amp;gt;, `2013M01` &amp;lt;chr&amp;gt;,
## #   `2013` &amp;lt;chr&amp;gt;, `2012Q4` &amp;lt;chr&amp;gt;, `2012Q3` &amp;lt;chr&amp;gt;, `2012Q2` &amp;lt;chr&amp;gt;,
## #   `2012Q1` &amp;lt;chr&amp;gt;, `2012M12` &amp;lt;chr&amp;gt;, `2012M11` &amp;lt;chr&amp;gt;, `2012M10` &amp;lt;chr&amp;gt;,
## #   `2012M09` &amp;lt;chr&amp;gt;, `2012M08` &amp;lt;chr&amp;gt;, `2012M07` &amp;lt;chr&amp;gt;, `2012M06` &amp;lt;chr&amp;gt;,
## #   `2012M05` &amp;lt;chr&amp;gt;, `2012M04` &amp;lt;chr&amp;gt;, `2012M03` &amp;lt;chr&amp;gt;, `2012M02` &amp;lt;chr&amp;gt;,
## #   `2012M01` &amp;lt;chr&amp;gt;, `2012` &amp;lt;chr&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So yeah, useless in that state. The first column actually is composed of 3 columns, merged together,
and instead of having one column with the date, and another with the value, we have one column
per date. Some cleaning is necessary before using this data.&lt;/p&gt;
&lt;p&gt;Let’s start with going from a wide to a long data set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia %&amp;gt;%
    select(&amp;quot;unit,tra_meas,airp_pr\\time&amp;quot;, contains(&amp;quot;20&amp;quot;)) %&amp;gt;%
    gather(date, passengers, -`unit,tra_meas,airp_pr\\time`)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line makes it possible to only select the columns that contain the string “20”, so
selecting columns from 2000 onward. Then, using gather, I go from long to wide. The data looks
like this now:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 117,070 x 3
##    `unit,tra_meas,airp_pr\\time`  date   passengers
##    &amp;lt;chr&amp;gt;                          &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     
##  1 FLIGHT,CAF_PAS,LU_ELLX_AT_LOWW 2018Q1 511       
##  2 FLIGHT,CAF_PAS,LU_ELLX_BE_EBBR 2018Q1 :         
##  3 FLIGHT,CAF_PAS,LU_ELLX_CH_LSGG 2018Q1 :         
##  4 FLIGHT,CAF_PAS,LU_ELLX_CH_LSZH 2018Q1 485       
##  5 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDF 2018Q1 834       
##  6 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDI 2018Q1 :         
##  7 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDM 2018Q1 1095      
##  8 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDR 2018Q1 :         
##  9 FLIGHT,CAF_PAS,LU_ELLX_DE_EDDT 2018Q1 :         
## 10 FLIGHT,CAF_PAS,LU_ELLX_DK_EKCH 2018Q1 :         
## # … with 117,060 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s separate the first column into 3 columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia %&amp;gt;%
    select(&amp;quot;unit,tra_meas,airp_pr\\time&amp;quot;, contains(&amp;quot;20&amp;quot;)) %&amp;gt;%
    gather(date, passengers, -`unit,tra_meas,airp_pr\\time`) %&amp;gt;%
     separate(col = `unit,tra_meas,airp_pr\\time`, into = c(&amp;quot;unit&amp;quot;, &amp;quot;tra_meas&amp;quot;, &amp;quot;air_pr\\time&amp;quot;), sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This separates the first column into 3 new columns, “unit”, “tra_meas” and “air_pr\time”. This step
is not necessary for the rest of the analysis, but might as well do it. The data looks like this now:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 117,070 x 5
##    unit   tra_meas `air_pr\\time`  date   passengers
##    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;           &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     
##  1 FLIGHT CAF_PAS  LU_ELLX_AT_LOWW 2018Q1 511       
##  2 FLIGHT CAF_PAS  LU_ELLX_BE_EBBR 2018Q1 :         
##  3 FLIGHT CAF_PAS  LU_ELLX_CH_LSGG 2018Q1 :         
##  4 FLIGHT CAF_PAS  LU_ELLX_CH_LSZH 2018Q1 485       
##  5 FLIGHT CAF_PAS  LU_ELLX_DE_EDDF 2018Q1 834       
##  6 FLIGHT CAF_PAS  LU_ELLX_DE_EDDI 2018Q1 :         
##  7 FLIGHT CAF_PAS  LU_ELLX_DE_EDDM 2018Q1 1095      
##  8 FLIGHT CAF_PAS  LU_ELLX_DE_EDDR 2018Q1 :         
##  9 FLIGHT CAF_PAS  LU_ELLX_DE_EDDT 2018Q1 :         
## 10 FLIGHT CAF_PAS  LU_ELLX_DK_EKCH 2018Q1 :         
## # … with 117,060 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next steps are simple renamings. I have copy-pasted the information from the Eurostat page
where you can &lt;a href=&#34;http://appsso.eurostat.ec.europa.eu/nui/show.do?dataset=avia_par_lu&amp;amp;lang=en&#34;&gt;view the data online&lt;/a&gt;.
If you click here:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/eurostat_click_here.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;you will be able to select the variables you want displayed in the table, as well as the dictionary
of the variables. I simply copy pasted it and recoded the variables. You can take a look at the
whole cleaning workflow by clicking “Click to expand” below:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click here to take a look at the whole cleaning workflow&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean &amp;lt;- avia %&amp;gt;%
    select(&amp;quot;unit,tra_meas,airp_pr\\time&amp;quot;, contains(&amp;quot;20&amp;quot;)) %&amp;gt;%
    gather(date, passengers, -`unit,tra_meas,airp_pr\\time`) %&amp;gt;%
    separate(col = `unit,tra_meas,airp_pr\\time`, into = c(&amp;quot;unit&amp;quot;, &amp;quot;tra_meas&amp;quot;, &amp;quot;air_pr\\time&amp;quot;), sep = &amp;quot;,&amp;quot;) %&amp;gt;%
    mutate(tra_meas = fct_recode(tra_meas,
         `Passengers on board` = &amp;quot;PAS_BRD&amp;quot;,
         `Passengers on board (arrivals)` = &amp;quot;PAS_BRD_ARR&amp;quot;,
         `Passengers on board (departures)` = &amp;quot;PAS_BRD_DEP&amp;quot;,
         `Passengers carried` = &amp;quot;PAS_CRD&amp;quot;,
         `Passengers carried (arrival)` = &amp;quot;PAS_CRD_ARR&amp;quot;,
         `Passengers carried (departures)` = &amp;quot;PAS_CRD_DEP&amp;quot;,
         `Passengers seats available` = &amp;quot;ST_PAS&amp;quot;,
         `Passengers seats available (arrivals)` = &amp;quot;ST_PAS_ARR&amp;quot;,
         `Passengers seats available (departures)` = &amp;quot;ST_PAS_DEP&amp;quot;,
         `Commercial passenger air flights` = &amp;quot;CAF_PAS&amp;quot;,
         `Commercial passenger air flights (arrivals)` = &amp;quot;CAF_PAS_ARR&amp;quot;,
         `Commercial passenger air flights (departures)` = &amp;quot;CAF_PAS_DEP&amp;quot;)) %&amp;gt;%
    mutate(unit = fct_recode(unit,
                             Passenger = &amp;quot;PAS&amp;quot;,
                             Flight = &amp;quot;FLIGHT&amp;quot;,
                             `Seats and berths` = &amp;quot;SEAT&amp;quot;)) %&amp;gt;%
    mutate(destination = fct_recode(`air_pr\\time`,
                                     `WIEN-SCHWECHAT` = &amp;quot;LU_ELLX_AT_LOWW&amp;quot;,
                                     `BRUSSELS` = &amp;quot;LU_ELLX_BE_EBBR&amp;quot;,
                                     `GENEVA` = &amp;quot;LU_ELLX_CH_LSGG&amp;quot;,
                                     `ZURICH` = &amp;quot;LU_ELLX_CH_LSZH&amp;quot;,
                                     `FRANKFURT/MAIN` = &amp;quot;LU_ELLX_DE_EDDF&amp;quot;,
                                     `HAMBURG` = &amp;quot;LU_ELLX_DE_EDDH&amp;quot;,
                                     `BERLIN-TEMPELHOF` = &amp;quot;LU_ELLX_DE_EDDI&amp;quot;,
                                     `MUENCHEN` = &amp;quot;LU_ELLX_DE_EDDM&amp;quot;,
                                     `SAARBRUECKEN` = &amp;quot;LU_ELLX_DE_EDDR&amp;quot;,
                                     `BERLIN-TEGEL` = &amp;quot;LU_ELLX_DE_EDDT&amp;quot;,
                                     `KOBENHAVN/KASTRUP` = &amp;quot;LU_ELLX_DK_EKCH&amp;quot;,
                                     `HURGHADA / INTL` = &amp;quot;LU_ELLX_EG_HEGN&amp;quot;,
                                     `IRAKLION/NIKOS KAZANTZAKIS` = &amp;quot;LU_ELLX_EL_LGIR&amp;quot;,
                                     `FUERTEVENTURA` = &amp;quot;LU_ELLX_ES_GCFV&amp;quot;,
                                     `GRAN CANARIA` = &amp;quot;LU_ELLX_ES_GCLP&amp;quot;,
                                     `LANZAROTE` = &amp;quot;LU_ELLX_ES_GCRR&amp;quot;,
                                     `TENERIFE SUR/REINA SOFIA` = &amp;quot;LU_ELLX_ES_GCTS&amp;quot;,
                                     `BARCELONA/EL PRAT` = &amp;quot;LU_ELLX_ES_LEBL&amp;quot;,
                                     `ADOLFO SUAREZ MADRID-BARAJAS` = &amp;quot;LU_ELLX_ES_LEMD&amp;quot;,
                                     `MALAGA/COSTA DEL SOL` = &amp;quot;LU_ELLX_ES_LEMG&amp;quot;,
                                     `PALMA DE MALLORCA` = &amp;quot;LU_ELLX_ES_LEPA&amp;quot;,
                                     `SYSTEM - PARIS` = &amp;quot;LU_ELLX_FR_LF90&amp;quot;,
                                     `NICE-COTE D&amp;#39;AZUR` = &amp;quot;LU_ELLX_FR_LFMN&amp;quot;,
                                     `PARIS-CHARLES DE GAULLE` = &amp;quot;LU_ELLX_FR_LFPG&amp;quot;,
                                     `STRASBOURG-ENTZHEIM` = &amp;quot;LU_ELLX_FR_LFST&amp;quot;,
                                     `KEFLAVIK` = &amp;quot;LU_ELLX_IS_BIKF&amp;quot;,
                                     `MILANO/MALPENSA` = &amp;quot;LU_ELLX_IT_LIMC&amp;quot;,
                                     `BERGAMO/ORIO AL SERIO` = &amp;quot;LU_ELLX_IT_LIME&amp;quot;,
                                     `ROMA/FIUMICINO` = &amp;quot;LU_ELLX_IT_LIRF&amp;quot;,
                                     `AGADIR/AL MASSIRA` = &amp;quot;LU_ELLX_MA_GMAD&amp;quot;,
                                     `AMSTERDAM/SCHIPHOL` = &amp;quot;LU_ELLX_NL_EHAM&amp;quot;,
                                     `WARSZAWA/CHOPINA` = &amp;quot;LU_ELLX_PL_EPWA&amp;quot;,
                                     `PORTO` = &amp;quot;LU_ELLX_PT_LPPR&amp;quot;,
                                     `LISBOA` = &amp;quot;LU_ELLX_PT_LPPT&amp;quot;,
                                     `STOCKHOLM/ARLANDA` = &amp;quot;LU_ELLX_SE_ESSA&amp;quot;,
                                     `MONASTIR/HABIB BOURGUIBA` = &amp;quot;LU_ELLX_TN_DTMB&amp;quot;,
                                     `ENFIDHA-HAMMAMET INTERNATIONAL` = &amp;quot;LU_ELLX_TN_DTNH&amp;quot;,
                                     `ENFIDHA ZINE EL ABIDINE BEN ALI` = &amp;quot;LU_ELLX_TN_DTNZ&amp;quot;,
                                     `DJERBA/ZARZIS` = &amp;quot;LU_ELLX_TN_DTTJ&amp;quot;,
                                     `ANTALYA (MIL-CIV)` = &amp;quot;LU_ELLX_TR_LTAI&amp;quot;,
                                     `ISTANBUL/ATATURK` = &amp;quot;LU_ELLX_TR_LTBA&amp;quot;,
                                     `SYSTEM - LONDON` = &amp;quot;LU_ELLX_UK_EG90&amp;quot;,
                                     `MANCHESTER` = &amp;quot;LU_ELLX_UK_EGCC&amp;quot;,
                                     `LONDON GATWICK` = &amp;quot;LU_ELLX_UK_EGKK&amp;quot;,
                                     `LONDON/CITY` = &amp;quot;LU_ELLX_UK_EGLC&amp;quot;,
                                     `LONDON HEATHROW` = &amp;quot;LU_ELLX_UK_EGLL&amp;quot;,
                                     `LONDON STANSTED` = &amp;quot;LU_ELLX_UK_EGSS&amp;quot;,
                                     `NEWARK LIBERTY INTERNATIONAL, NJ.` = &amp;quot;LU_ELLX_US_KEWR&amp;quot;,
                                     `O.R TAMBO INTERNATIONAL` = &amp;quot;LU_ELLX_ZA_FAJS&amp;quot;)) %&amp;gt;%
    mutate(passengers = as.numeric(passengers)) %&amp;gt;%
    select(unit, tra_meas, destination, date, passengers)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: NAs introduced by coercion&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;There is quarterly data and monthly data. Let’s separate the two:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_quarterly &amp;lt;- avia_clean %&amp;gt;%
    filter(tra_meas == &amp;quot;Passengers on board (arrivals)&amp;quot;,
           !is.na(passengers)) %&amp;gt;%
    filter(str_detect(date, &amp;quot;Q&amp;quot;)) %&amp;gt;%
    mutate(date = yq(date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the “date” column, I detect the observations with “Q” in their name, indicating that it is quarterly data.
I do the same for monthly data, but I have to add the string “01” to the dates. This transforms
a date that looks like this “2018M1” to this “2018M101”. “2018M101” can then be converted into a
date by using the &lt;code&gt;ymd()&lt;/code&gt; function from lubridate. &lt;code&gt;yq()&lt;/code&gt; was used for the quarterly data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly &amp;lt;- avia_clean %&amp;gt;%
    filter(tra_meas == &amp;quot;Passengers on board (arrivals)&amp;quot;,
           !is.na(passengers)) %&amp;gt;%
    filter(str_detect(date, &amp;quot;M&amp;quot;)) %&amp;gt;%
    mutate(date = paste0(date, &amp;quot;01&amp;quot;)) %&amp;gt;%
    mutate(date = ymd(date)) %&amp;gt;%
    select(destination, date, passengers)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time for some plots. Let’s start with the raw data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total = sum(passengers)) %&amp;gt;%
    ggplot() +
    ggtitle(&amp;quot;Raw data&amp;quot;) +
    geom_line(aes(y = total, x = date), colour = &amp;quot;#82518c&amp;quot;) +
    scale_x_date(date_breaks = &amp;quot;1 year&amp;quot;, date_labels = &amp;quot;%m-%Y&amp;quot;) + 
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-14-luxairport_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And now with the logged data (or rather, the data transformed using the inverted hyperbolic sine
transformation):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total = sum(passengers)) %&amp;gt;%
    mutate(total_ihs = ihs(total)) %&amp;gt;%
    ggplot() +
    ggtitle(&amp;quot;Logged data&amp;quot;) +
    geom_line(aes(y = total_ihs, x = date), colour = &amp;quot;#82518c&amp;quot;) +
    scale_x_date(date_breaks = &amp;quot;1 year&amp;quot;, date_labels = &amp;quot;%m-%Y&amp;quot;) + 
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-14-luxairport_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We clearly see a seasonal pattern in the data. There is also an upward trend. We will have to deal
with these two problems if we want to do some forecasting. For this, let’s limit ourselves to data
from before 2015, and convert the “passengers” column from the data to a time series object, using
the &lt;code&gt;ts()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_train &amp;lt;- avia_clean_monthly %&amp;gt;%
    select(date, passengers) %&amp;gt;%
    filter(year(date) &amp;lt; 2015) %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total_passengers = sum(passengers)) %&amp;gt;%
    pull(total_passengers) %&amp;gt;%
    ts(., frequency = 12, start = c(2005, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will try to &lt;em&gt;pseudo&lt;/em&gt;-forecast the data from 2015 to the last point available, March 2018.
First, let’s tranform the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logged_data &amp;lt;- ihs(avia_clean_train)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking the log, or ihs of the data deals with stabilizing the variance of the time series.&lt;/p&gt;
&lt;p&gt;There might also be a need to difference the data. Computing the differences between consecutive
observations makes the time-series stationary. This will be taken care of by the &lt;code&gt;auto.arima()&lt;/code&gt;
function, if needed. The &lt;code&gt;auto.arima()&lt;/code&gt; function returns the best ARIMA model according to different
statistical criterions, such as the AIC, AICc or BIC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(model_fit &amp;lt;- auto.arima(logged_data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: logged_data 
## ARIMA(2,1,1)(2,1,0)[12] 
## 
## Coefficients:
##           ar1      ar2      ma1     sar1     sar2
##       -0.4061  -0.2431  -0.3562  -0.5590  -0.3282
## s.e.   0.2003   0.1432   0.1994   0.0911   0.0871
## 
## sigma^2 estimated as 0.004503:  log likelihood=137.11
## AIC=-262.21   AICc=-261.37   BIC=-246.17&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;auto.arima()&lt;/code&gt; found that the best model would be an &lt;span class=&#34;math inline&#34;&gt;\(ARIMA(2, 1, 1)(2, 1, 0)_{12}\)&lt;/span&gt;. This is an
seasonal autoregressive model, with p = 2, d = 1, q = 1, P = 2 and D = 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_forecast &amp;lt;- forecast(model_fit, h = 39)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now forecast the model for the next 39 months (which correspond to the data available).&lt;/p&gt;
&lt;p&gt;To plot the forecast, one could do a simple call to the plot function. But the resulting plot
is not very aesthetic. To plot my own, I have to grab the data that was forecast, and do some
munging again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;point_estimate &amp;lt;- model_forecast$mean %&amp;gt;%
    as_tsibble() %&amp;gt;%
    rename(point_estimate = value,
           date = index)

upper &amp;lt;- model_forecast$upper %&amp;gt;%
    as_tsibble() %&amp;gt;%
    spread(key, value) %&amp;gt;%
    rename(date = index,
           upper80 = `80%`,
           upper95 = `95%`)

lower &amp;lt;- model_forecast$lower %&amp;gt;%
    as_tsibble() %&amp;gt;%
    spread(key, value) %&amp;gt;%
    rename(date = index,
           lower80 = `80%`,
           lower95 = `95%`)

estimated_data &amp;lt;- reduce(list(point_estimate, upper, lower), full_join, by = &amp;quot;date&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;as_tsibble()&lt;/code&gt; is a function from the &lt;code&gt;{tsibble}&lt;/code&gt; package that converts objects that are &lt;em&gt;time-series aware&lt;/em&gt;
to &lt;em&gt;time-aware&lt;/em&gt; tibbles. If you are not familiar with &lt;code&gt;ts_tibble()&lt;/code&gt;, I urge you to run the above lines
one by one, and especially to compare &lt;code&gt;as_tsibble()&lt;/code&gt; with the standard &lt;code&gt;as_tibble()&lt;/code&gt; from the &lt;code&gt;{tibble}&lt;/code&gt;
package.&lt;/p&gt;
&lt;p&gt;This is how &lt;code&gt;estimated_data&lt;/code&gt; looks:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(estimated_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tsibble: 6 x 6 [1M]
##       date point_estimate upper80 upper95 lower80 lower95
##      &amp;lt;mth&amp;gt;          &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 2015 Jan           11.9    12.0    12.1    11.8    11.8
## 2 2015 Feb           11.9    12.0    12.0    11.8    11.7
## 3 2015 Mar           12.1    12.2    12.3    12.0    12.0
## 4 2015 Apr           12.2    12.3    12.4    12.1    12.1
## 5 2015 May           12.3    12.4    12.4    12.2    12.1
## 6 2015 Jun           12.3    12.4    12.5    12.2    12.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now plot the data, with the forecast, and with the 95% confidence interval:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;avia_clean_monthly %&amp;gt;%
    group_by(date) %&amp;gt;%
    summarise(total = sum(passengers)) %&amp;gt;%
    mutate(total_ihs = ihs(total)) %&amp;gt;%
    ggplot() +
    ggtitle(&amp;quot;Logged data&amp;quot;) +
    geom_line(aes(y = total_ihs, x = date), colour = &amp;quot;#82518c&amp;quot;) +
    scale_x_date(date_breaks = &amp;quot;1 year&amp;quot;, date_labels = &amp;quot;%m-%Y&amp;quot;) +
    geom_ribbon(data = estimated_data, aes(x = date, ymin = lower95, ymax = upper95), fill = &amp;quot;#666018&amp;quot;, alpha = 0.2) +
    geom_line(data = estimated_data, aes(x = date, y = point_estimate), linetype = 2, colour = &amp;quot;#8e9d98&amp;quot;) +
    theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-14-luxairport_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The pseudo-forecast (the dashed line) is not very far from the truth, only overestimating the
seasonal peaks, but the true line is within the 95% confidence interval, which is good!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates or
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing NetHack data, part 2: What players kill the most</title>
      <link>/blog/2018-11-10-nethack_analysis_part2/</link>
      <pubDate>Sat, 10 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-10-nethack_analysis_part2/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=VnW2g6qbbrA&#34;&gt;
&lt;img src=&#34;/img/monsters.png&#34; title = &#34;Wizard of Yendor battle music&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Link to &lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-01-nethack/&#34;&gt;webscraping the data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Link to &lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-03-nethack_analysis/&#34;&gt;Analysis, part 1&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is the third blog post that deals with data from the game NetHack, and oh boy, did a lot of
things happen since the last blog post! Here’s a short timeline of the events:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I scraped data from &lt;a href=&#34;https://alt.org/nethack/&#34;&gt;alt.org/nethack&lt;/a&gt; and made a package with the data available on Github
(that package was too big for CRAN)&lt;/li&gt;
&lt;li&gt;Then, I analyzed the data, focusing on what monsters kill the players the most, and also where
players die the most&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;citation&#34;&gt;@GridSageGames&lt;/span&gt;, developer of the roguelike Cogmind and moderator of the roguelike subreddit,
posted the blog post on reddit&lt;/li&gt;
&lt;li&gt;I noticed that actually, by scraping the data like I did, I only got a sample of 100 daily games&lt;/li&gt;
&lt;li&gt;This point was also discussed on Reddit, and bhhak, an UnNetHack developer (UnNetHack is a fork
of NetHack) suggested I used the xlogfiles instead&lt;/li&gt;
&lt;li&gt;xlogfiles are log files generated by NetHack, and are also available on alt.org/nethack&lt;/li&gt;
&lt;li&gt;I started scraping them, and getting a lot more data&lt;/li&gt;
&lt;li&gt;I got contacted on twitter by &lt;span class=&#34;citation&#34;&gt;@paxed&lt;/span&gt;, an admin of alt.org/nethack:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;There was no need for scraping, we&amp;#39;ll happily give out the whole database if you want.&lt;/p&gt;&amp;mdash; Pasi Kallinen (@paxed) &lt;a href=&#34;https://twitter.com/paxed/status/1059333642592366593?ref_src=twsrc%5Etfw&#34;&gt;November 5, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;ul&gt;
&lt;li&gt;He gave me access to ALL THE DATA on alt.org/nethack!&lt;/li&gt;
&lt;li&gt;The admins of &lt;a href=&#34;https://alt.org/nethack/&#34;&gt;alt.org/nethack&lt;/a&gt; will release all the data to the public!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, I will now continue with the blog post I wanted to do in the first place; focusing now on what
roles players choose to play the most, and also which monsters they kill the most. BUT! Since all the data
will be released to the public, my &lt;code&gt;{nethack}&lt;/code&gt; package that contains data that I scraped is not
that useful anymore. So I changed the nature of the package.
Now the package contains some functions: a function to parse and prepare the xlogfiles from NetHack that
you can download from &lt;a href=&#34;https://alt.org/nethack/&#34;&gt;alt.org/nethack&lt;/a&gt; (or from any other public server), a function
to download dumplogs such as this &lt;a href=&#34;http://archive.is/7awsb&#34;&gt;one&lt;/a&gt;. These dumplogs contain a lot of
info that I will extract in this blog post, using another function included in the &lt;code&gt;{nethack}&lt;/code&gt; package.
The package also contains a sample of 6000 runs from NetHack version 3.6.1.&lt;/p&gt;
&lt;p&gt;You can install the package with the following command line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/nethack&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-nethack-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;code&gt;{nethack}&lt;/code&gt; package&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-03-nethack_analysis/&#34;&gt;part 1&lt;/a&gt; I showed what killed
players the most. Here, I will focus on what monsters players kill the most.
Let’s start by loading some packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(magrittr)
library(ggridges)
library(brotools)
library(rvest)
library(nethack)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s first describe the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brotools::describe(nethack) %&amp;gt;% 
  print(n = Inf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 17
##    variable type    nobs     mean       sd mode       min     max      q05
##    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 deathdn… Nume… 6.00e3  8.45e-1  1.30e+0 2       0.      7.00e0   0.    
##  2 deathlev Nume… 6.00e3  4.32e+0  3.69e+0 10     -5.00e0  4.50e1   1.00e0
##  3 deaths   Nume… 6.00e3  8.88e-1  3.54e-1 1       0.      5.00e0   0.    
##  4 endtime  Nume… 6.00e3  1.53e+9  4.72e+6 1534…   1.52e9  1.54e9   1.53e9
##  5 hp       Nume… 6.00e3  6.64e+0  4.96e+1 -1     -9.40e1  1.79e3  -8.00e0
##  6 maxhp    Nume… 6.00e3  3.82e+1  5.29e+1 57      2.00e0  1.80e3   1.10e1
##  7 maxlvl   Nume… 6.00e3  5.52e+0  6.36e+0 10      1.00e0  5.30e1   1.00e0
##  8 points   Nume… 6.00e3  4.69e+4  4.18e+5 10523   0.      9.92e6   1.40e1
##  9 realtime Nume… 6.00e3  4.42e+3  1.60e+4 4575    0.      3.23e5   6.90e1
## 10 startti… Nume… 6.00e3  1.53e+9  4.72e+6 1534…   1.52e9  1.54e9   1.53e9
## 11 turns    Nume… 6.00e3  3.60e+3  9.12e+3 6797    3.10e1  1.97e5   9.49e1
## 12 align    Char… 6.00e3 NA       NA       Cha    NA      NA       NA     
## 13 align0   Char… 6.00e3 NA       NA       Cha    NA      NA       NA     
## 14 death    Char… 6.00e3 NA       NA       kill…  NA      NA       NA     
## 15 gender   Char… 6.00e3 NA       NA       Fem    NA      NA       NA     
## 16 gender0  Char… 6.00e3 NA       NA       Fem    NA      NA       NA     
## 17 killed_… Char… 6.00e3 NA       NA       fain…  NA      NA       NA     
## 18 name     Char… 6.00e3 NA       NA       drud…  NA      NA       NA     
## 19 race     Char… 6.00e3 NA       NA       Elf    NA      NA       NA     
## 20 role     Char… 6.00e3 NA       NA       Wiz    NA      NA       NA     
## 21 dumplog  List  1.33e6 NA       NA       &amp;lt;NA&amp;gt;   NA      NA       NA     
## 22 birthda… Date  6.00e3 NA       NA       &amp;lt;NA&amp;gt;   NA      NA       NA     
## 23 deathda… Date  6.00e3 NA       NA       &amp;lt;NA&amp;gt;   NA      NA       NA     
## # ... with 8 more variables: q25 &amp;lt;dbl&amp;gt;, median &amp;lt;dbl&amp;gt;, q75 &amp;lt;dbl&amp;gt;,
## #   q95 &amp;lt;dbl&amp;gt;, n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;, starting_date &amp;lt;date&amp;gt;,
## #   ending_date &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All these columns are included in xlogfiles. The data was prepared using two functions, included
in &lt;code&gt;{nethack}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xlog &amp;lt;- read_delim(&amp;quot;~/path/to/nethack361_xlog.csv&amp;quot;, &amp;quot;\t&amp;quot;, escape_double = FALSE, 
                   col_names = FALSE, trim_ws = TRUE)

xlog_df &amp;lt;- clean_xlog(xlog)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;nethack361_xlog.csv&lt;/code&gt; is the raw xlogfiles that you can get from NetHack public servers.
&lt;code&gt;clean_xlog()&lt;/code&gt; is a function that parses an xlogfile and returns a clean data frame.
&lt;code&gt;xlog_df&lt;/code&gt; will be a data frame that will look just as the one included in &lt;code&gt;{nethack}&lt;/code&gt;. It is
then possible to get the dumplog from each run included in &lt;code&gt;xlog_df&lt;/code&gt; using &lt;code&gt;get_dumplog()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xlog_df &amp;lt;- get_dumplog(xlog_df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function adds a column called &lt;code&gt;dumplog&lt;/code&gt; with the dumplog of that run. I will now analyze
the dumplog file, by focusing on monsters vanquished, genocided or extinct. In a future blogpost
I will focus on other achievements.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;roles-played-and-other-starting-stats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Roles played (and other starting stats)&lt;/h2&gt;
&lt;p&gt;I will take a look at the races, roles, gender and alignment players start with the most. I will do
pie charts to visualize these variable, so first, let’s start by writing a general function that
allows me to do just that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_pie &amp;lt;- function(dataset, variable, repel = FALSE){

  if(repel){
    geom_label &amp;lt;- function(...){
      ggrepel::geom_label_repel(...)
    }
  }

  variable &amp;lt;- enquo(variable)

  dataset %&amp;gt;%
    count((!!variable)) %&amp;gt;%
    mutate(total = sum(n),
           freq = n/total,
           labels = scales::percent(freq)) %&amp;gt;% 
    arrange(desc(freq)) %&amp;gt;%
    ggplot(aes(x = &amp;quot;&amp;quot;, y = freq, fill = (!!variable))) + 
    geom_col() + 
    geom_label(aes(label = labels), position = position_stack(vjust = 0.25), show.legend = FALSE) + 
    coord_polar(&amp;quot;y&amp;quot;) + 
    theme_blog() + 
    scale_fill_blog() + 
    theme(legend.title = element_blank(),
          panel.grid = element_blank(),
          axis.text = element_blank(),
          axis.title = element_blank())
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can easily plot the share of races chosen:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_pie(nethack, race)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;or the share of alignment:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_pie(nethack, align0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Same for the share of gender:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_pie(nethack, gender0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;and finally for the share of roles:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_pie(nethack, role, repel = TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;create_pie()&lt;/code&gt; is possible thanks to &lt;em&gt;tidy evaluation&lt;/em&gt; in &lt;a href=&#34;https://www.tidyverse.org/articles/2018/07/ggplot2-3-0-0/&#34;&gt;&lt;code&gt;{ggplot2}&lt;/code&gt;&lt;/a&gt;,
which makes it possible to write a function that passes data frame columns down to &lt;code&gt;aes()&lt;/code&gt;. Before
version 3.0 of &lt;code&gt;{ggplot2}&lt;/code&gt; this was not possible, and writing such a function would have been a bit
more complicated. Now, it’s as easy as pie, if I dare say.&lt;/p&gt;
&lt;p&gt;Something else I want to look at, is the distribution of turns by role:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  filter(turns &amp;lt; quantile(turns, 0.98)) %&amp;gt;%
  ggplot(aes(x = turns, y = role, group = role, fill = role)) +
    geom_density_ridges(scale = 6, size = 0.25, rel_min_height = 0.01) + 
    theme_blog() + 
    scale_fill_blog() + 
    theme(axis.text.y = element_blank(),
          axis.title.y = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 486&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I use the very cool &lt;code&gt;{ggridges}&lt;/code&gt; package for that. The distribution seems to mostly be the same
(of course, one should do a statistical test to be sure), but the one for the role “Valkyrie”
seems to be quite different from the others. It is known that it is easier to win the game playing
as a Valkyrie, but a question remains: is it really easier as a Valkyrie, or do good players tend
to play as Valkyries more often?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creatures-vanquished-genocided-or-extinct&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creatures vanquished, genocided or extinct&lt;/h2&gt;
&lt;p&gt;The dumplog lists which, and how many of which, creatures were vanquished during the run, as well
as creatures that were genocided and extinct. The player can genocide an entire species by reading
a &lt;em&gt;scroll of genocide&lt;/em&gt; (or by sitting on a throne). A species gets extinct if the player manages to
kill every monster from that species (there’s other ways too, but for the sake of simplicity, let’s
just say that when the players kills every monster from a species, the species is extinct).
The following lines are an extract of a dumplog:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;Vanquished creatures:&amp;quot;
&amp;quot;    Baalzebub&amp;quot;
&amp;quot;    Orcus&amp;quot;
&amp;quot;    Juiblex&amp;quot;
&amp;quot;the Wizard of Yendor (4 times)&amp;quot;
&amp;quot;    Pestilence (thrice)&amp;quot;
&amp;quot;    Famine&amp;quot;
&amp;quot;    Vlad the Impaler&amp;quot;
&amp;quot;  4 arch-liches&amp;quot;
&amp;quot;  an arch-lich&amp;quot;
&amp;quot;  a high priest&amp;quot;
&amp;quot;...&amp;quot;
&amp;quot;...&amp;quot;
&amp;quot;...&amp;quot;
&amp;quot;2873 creatures vanquished.&amp;quot; &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I want to analyze this, I have to first solve some problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Replace “a” and “an” by “1”&lt;/li&gt;
&lt;li&gt;Put the digit in the string “(4 times)” in front of the name of the monster (going from “the Wizard of Yendor (4 times)” to “4 the Wizard of Yendor”)&lt;/li&gt;
&lt;li&gt;Do something similar for “twice” and “thrice”&lt;/li&gt;
&lt;li&gt;Put everything into singular (for example, arch-liches into arch-lich)&lt;/li&gt;
&lt;li&gt;Trim whitespace&lt;/li&gt;
&lt;li&gt;Extract the genocided or extinct status from the dumplog too&lt;/li&gt;
&lt;li&gt;Finally, return a data frame with all the needed info&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I wrote a function called &lt;code&gt;extracted_defeated_monsters()&lt;/code&gt; and included it in the &lt;code&gt;{nethack}&lt;/code&gt; package.
I discuss this function in appendix, but what it does is extracting information from dumplog files
about vanquished, genocided or extinct monsters and returns a tidy dataframe with that info. This
function has a lot of things going on inside it, so if you’re interested in learning more about
regular expression and other &lt;code&gt;{tidyverse}&lt;/code&gt; tricks, I really encourage you to read its source code.&lt;/p&gt;
&lt;p&gt;I can now easily add this info to my data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;lt;&amp;gt;%
  mutate(monsters_destroyed = map(dumplog, ~possibly(extract_defeated_monsters, otherwise = NA)(.)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at one of them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack$monsters_destroyed[[117]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 285 x 3
##    value monster              status
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt; 
##  1     1 baalzebub            &amp;lt;NA&amp;gt;  
##  2     1 orcu                 &amp;lt;NA&amp;gt;  
##  3     1 juiblex              &amp;lt;NA&amp;gt;  
##  4     4 the wizard of yendor &amp;lt;NA&amp;gt;  
##  5     3 pestilence           &amp;lt;NA&amp;gt;  
##  6     1 famine               &amp;lt;NA&amp;gt;  
##  7     1 vlad the impaler     &amp;lt;NA&amp;gt;  
##  8     4 arch-lich            &amp;lt;NA&amp;gt;  
##  9     1 high priest          &amp;lt;NA&amp;gt;  
## 10     1 medusa               &amp;lt;NA&amp;gt;  
## # ... with 275 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack$monsters_destroyed[[117]] %&amp;gt;% 
  count(status)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   status        n
##   &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt;
## 1 extinct       2
## 2 genocided     7
## 3 &amp;lt;NA&amp;gt;        276&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The status variable tells us if that monster was genocided or extinct during that run. &lt;code&gt;status&lt;/code&gt;
equal to “NA” means vanquished.&lt;/p&gt;
&lt;p&gt;It is now possible to look at, say, the top 15 vanquished monsters (normalized):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  filter(!is.na(monsters_destroyed)) %&amp;gt;%
  pull(monsters_destroyed) %&amp;gt;%
  bind_rows %&amp;gt;%
  group_by(monster) %&amp;gt;%
  summarise(total = sum(value)) %&amp;gt;%
  top_n(15) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(norm_total = (total - min(total))/(max(total) - min(total))) %&amp;gt;%
  mutate(monster = fct_reorder(monster, norm_total, .desc = FALSE)) %&amp;gt;%
  ggplot() + 
  geom_col(aes(y = norm_total, x = monster)) + 
  coord_flip() + 
  theme_blog() + 
  scale_fill_blog() + 
  ylab(&amp;quot;Ranking&amp;quot;) +
  xlab(&amp;quot;Monster&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Selecting by total&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this type of graph, the most vanquished monster, “gnome” has a value of 1, and the least
vanquished one, 0. This normalization step is also used in the pre-processing step of machine learning
algorithms. This helps convergence of the gradient descent algorithm for instance.&lt;/p&gt;
&lt;p&gt;Monsters can also get genocided or extinct. Let’s make a pie chart of the proportion of genocided
and extinct monsters (I lump monsters that are genocided or extinct less than 5% of the times
into a category called other). Because I want two pie charts, I nest the data after having grouped
it by the status variable. This is a trick I discussed in this blog &lt;a href=&#34;https://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/&#34;&gt;post&lt;/a&gt;
and that I use very often:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  filter(!is.na(monsters_destroyed)) %&amp;gt;%
  pull(monsters_destroyed) %&amp;gt;%
  bind_rows %&amp;gt;%
  filter(!is.na(status)) %&amp;gt;%
  group_by(status) %&amp;gt;% 
  count(monster) %&amp;gt;% 
  mutate(monster = fct_lump(monster, prop = 0.05, w = n)) %&amp;gt;% 
  group_by(status, monster) %&amp;gt;% 
  summarise(total_count = sum(n)) %&amp;gt;%
  mutate(freq = total_count/sum(total_count),
         labels = scales::percent(freq)) %&amp;gt;%
  arrange(desc(freq)) %&amp;gt;%
  group_by(status) %&amp;gt;%
  nest() %&amp;gt;%
  mutate(pie_chart = map2(.x = status,
                          .y = data,
                          ~ggplot(data = .y,
                                  aes(x = &amp;quot;&amp;quot;, y = freq, fill = (monster))) + 
    geom_col() + 
    ggrepel::geom_label_repel(aes(label = labels), position = position_stack(vjust = 0.25), show.legend = FALSE) + 
    coord_polar(&amp;quot;y&amp;quot;) + 
    theme_blog() + 
    scale_fill_blog() + 
      ggtitle(.x) +
    theme(legend.title = element_blank(),
          panel.grid = element_blank(),
          axis.text = element_blank(),
          axis.title = element_blank())
  )) %&amp;gt;%
  pull(pie_chart)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in mutate_impl(.data, dots): Unequal factor levels: coercing to
## character&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in mutate_impl(.data, dots): binding character and factor vector,
## coercing into character vector

## Warning in mutate_impl(.data, dots): binding character and factor vector,
## coercing into character vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-17-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That was it for this one, the graphs are not that super sexy, but the amount of work that went
into making them was quite consequent. The main reason was that parsing xlogfiles was a bit tricky, but
the main challenge was extracting information from dumplog files. This proved to be a bit more
complicated than expected (just take a look at the source code of &lt;code&gt;extract_defeated_monsters()&lt;/code&gt;
to get an idea…).&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates or
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bonus-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bonus plot&lt;/h2&gt;
&lt;div id=&#34;correct-number-of-daily-games&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Correct number of daily games&lt;/h3&gt;
&lt;p&gt;The daily number of games are available &lt;a href=&#34;https://alt.org/nethack/dailygames_ct.html&#34;&gt;here&lt;/a&gt;. Let’s
extract this info and remake the plot that shows the number of runs per day:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;games &amp;lt;- read_html(&amp;quot;https://alt.org/nethack/dailygames_ct.html&amp;quot;) %&amp;gt;%
        html_nodes(xpath = &amp;#39;//table&amp;#39;) %&amp;gt;%
        html_table(fill = TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This extracts all the tables and puts them into a list. Let’s take a look at one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(games[[1]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   2018  2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018
## 1         NA    1    2    3    4    5    6    7    8    9   10   11   12
## 2  Jan 11639  275  370  394  363  392  276  288  324  297  411  413  430
## 3  Feb 10819  375  384  359  376  440  345  498  457  416  376  421  416
## 4  Mar 12148  411  403  421  392  447  391  451  298  350  309  309  369
## 5  Apr 13957  456  513  482  516  475  490  397  431  436  438  541  493
## 6  May 13361  595  509  576  620  420  443  407  539  440  446  404  282
##   2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018 2018
## 1   13   14   15   16   17   18   19   20   21   22   23   24   25   26
## 2  331  341  318  483  408  424  464  412  371  430  348  315  359  375
## 3  385  367  443  324  283  341  385  398  361  379  399  276  455  460
## 4  390  358  362  345  388  360  411  382  371  400  410  417  328  431
## 5  593  537  396  578  403  435  526  448  339  377  476  492  528  393
## 6  265  358  419  564  483  429  423  299  424  404  450  408  355  409
##   2018 2018 2018 2018 2018
## 1   27   28   29   30   31
## 2  432  371  385  440  399
## 3  353  347   NA   NA   NA
## 4  386  484  493  486  395
## 5  407  421  463  477   NA
## 6  417  433  360  391  389&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s clean this up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_table &amp;lt;- function(df){
  # Promotes first row to header
  colnames(df) &amp;lt;- df[1, ]
  df &amp;lt;- df[-1, ]
  
  # Remove column with total from the month
  df &amp;lt;- df[, -2]
  
  # Name the first column &amp;quot;month&amp;quot;
  
  colnames(df)[1] &amp;lt;- &amp;quot;month&amp;quot;
  
  # Now put it in a tidy format
  df %&amp;gt;%
    gather(day, games_played, -month)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can clean up all the tables. I apply this function to each element of the list &lt;code&gt;games&lt;/code&gt;. I
also add a year column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;games &amp;lt;- map(games, clean_table) %&amp;gt;%
  map2_dfr(.x = ., 
       .y = seq(2018, 2001),
       ~mutate(.x, year = .y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can easily create the plot I wanted&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;games %&amp;lt;&amp;gt;%
  mutate(date = lubridate::ymd(paste(year, month, day, sep = &amp;quot;-&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 122 failed to parse.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(games, aes(y = games_played, x = date)) + 
  geom_point(colour = &amp;quot;#0f4150&amp;quot;) + 
  geom_smooth(colour = &amp;quot;#82518c&amp;quot;) + 
  theme_blog() + 
  ylab(&amp;quot;Total games played&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 452 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 452 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-10-nethack_analysis_part2_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s actually a lot more games than 50 per day being played!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;div id=&#34;fuzzy-matching&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fuzzy matching&lt;/h3&gt;
&lt;p&gt;If you take a look at the &lt;code&gt;extract_defeated_monsters()&lt;/code&gt; source code, you’ll see that at some point
I “singularize” monster names. I decided to deal with this singular/plural issue, “by hand”,
but also explored other possibilities, such as matching the plural nouns with the singular
nouns fuzzily. In the end it didn’t work out so well, but here’s the code for future reference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monster_list &amp;lt;- read_html(&amp;quot;https://nethackwiki.com/wiki/Monsters_(by_difficulty)&amp;quot;) %&amp;gt;%
    html_nodes(&amp;quot;.prettytable&amp;quot;) %&amp;gt;% 
    .[[1]] %&amp;gt;%
    html_table(fill = TRUE)

monster_list %&amp;lt;&amp;gt;%
    select(monster = Name)

head(monster_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      monster
## 1 Demogorgon
## 2   Asmodeus
## 3  Baalzebub
## 4   Dispater
## 5     Geryon
## 6      Orcus&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fuzzyjoin)

test_vanquished &amp;lt;- extract_defeated_monsters(nethack$dumplog[[117]])

head(test_vanquished)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   value monster              status
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt; 
## 1     1 baalzebub            &amp;lt;NA&amp;gt;  
## 2     1 orcu                 &amp;lt;NA&amp;gt;  
## 3     1 juiblex              &amp;lt;NA&amp;gt;  
## 4     4 the wizard of yendor &amp;lt;NA&amp;gt;  
## 5     3 pestilence           &amp;lt;NA&amp;gt;  
## 6     1 famine               &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can take a look at the result by expanding:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to expand&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stringdist_left_join(test_vanquished, monster_list) %&amp;gt;% 
  count(monster.y) %&amp;gt;%
  print(n = Inf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining by: &amp;quot;monster&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 297 x 2
##     monster.y                   n
##     &amp;lt;chr&amp;gt;                   &amp;lt;int&amp;gt;
##   1 acid blob                   1
##   2 air elemental               2
##   3 Aleax                       1
##   4 aligned priest              1
##   5 Angel                       1
##   6 ape                         2
##   7 arch-lich                   1
##   8 Baalzebub                   1
##   9 baby black dragon           1
##  10 baby crocodile              1
##  11 baby gray dragon            1
##  12 baby green dragon           1
##  13 baby long worm              1
##  14 baby orange dragon          1
##  15 baby white dragon           1
##  16 baby yellow dragon          1
##  17 balrog                      1
##  18 baluchitherium              1
##  19 barbed devil                1
##  20 barrow wight                1
##  21 bat                         1
##  22 black dragon                1
##  23 black light                 1
##  24 black naga                  1
##  25 black pudding               1
##  26 black unicorn               1
##  27 blue dragon                 1
##  28 blue jelly                  1
##  29 bone devil                  1
##  30 brown mold                  1
##  31 brown pudding               1
##  32 bugbear                     1
##  33 captain                     1
##  34 carnivorous ape             1
##  35 cave spider                 1
##  36 centipede                   1
##  37 chameleon                   1
##  38 chickatrice                 2
##  39 clay golem                  1
##  40 cobra                       1
##  41 cockatrice                  2
##  42 couatl                      1
##  43 coyote                      1
##  44 crocodile                   1
##  45 demilich                    1
##  46 dingo                       1
##  47 disenchanter                1
##  48 dog                         1
##  49 doppelganger                1
##  50 dust vortex                 1
##  51 dwarf                       2
##  52 dwarf king                  1
##  53 dwarf lord                  1
##  54 dwarf mummy                 1
##  55 dwarf zombie                1
##  56 earth elemental             1
##  57 electric eel                1
##  58 elf                         1
##  59 elf mummy                   1
##  60 elf zombie                  1
##  61 elf-lord                    1
##  62 Elvenking                   1
##  63 energy vortex               1
##  64 erinys                      1
##  65 ettin                       1
##  66 ettin mummy                 1
##  67 ettin zombie                1
##  68 Famine                      1
##  69 fire ant                    2
##  70 fire elemental              2
##  71 fire giant                  2
##  72 fire vortex                 2
##  73 flaming sphere              1
##  74 flesh golem                 1
##  75 floating eye                1
##  76 fog cloud                   1
##  77 forest centaur              1
##  78 fox                         1
##  79 freezing sphere             1
##  80 frost giant                 1
##  81 gargoyle                    1
##  82 garter snake                1
##  83 gas spore                   1
##  84 gecko                       1
##  85 gelatinous cube             1
##  86 ghost                       2
##  87 ghoul                       2
##  88 giant ant                   3
##  89 giant bat                   3
##  90 giant beetle                1
##  91 giant eel                   1
##  92 giant mimic                 1
##  93 giant mummy                 1
##  94 giant rat                   3
##  95 giant spider                1
##  96 giant zombie                1
##  97 glass piercer               1
##  98 gnome                       1
##  99 gnome king                  1
## 100 gnome lord                  1
## 101 gnome mummy                 1
## 102 gnome zombie                1
## 103 gnomish wizard              1
## 104 goblin                      1
## 105 gold golem                  2
## 106 golden naga                 1
## 107 golden naga hatchling       1
## 108 gray ooze                   1
## 109 gray unicorn                1
## 110 Green-elf                   1
## 111 gremlin                     1
## 112 Grey-elf                    1
## 113 grid bug                    1
## 114 guardian naga               1
## 115 guardian naga hatchling     1
## 116 hell hound                  1
## 117 hell hound pup              1
## 118 hezrou                      1
## 119 high priest                 1
## 120 hill giant                  1
## 121 hill orc                    1
## 122 hobbit                      1
## 123 hobgoblin                   1
## 124 homunculus                  1
## 125 horned devil                1
## 126 horse                       2
## 127 housecat                    1
## 128 human                       1
## 129 human mummy                 1
## 130 human zombie                1
## 131 ice devil                   1
## 132 ice troll                   1
## 133 ice vortex                  2
## 134 iguana                      1
## 135 imp                         1
## 136 incubus                     1
## 137 iron golem                  1
## 138 iron piercer                1
## 139 jabberwock                  1
## 140 jackal                      1
## 141 jaguar                      1
## 142 jellyfish                   1
## 143 Juiblex                     1
## 144 Keystone Kop                1
## 145 ki-rin                      1
## 146 killer bee                  1
## 147 kitten                      1
## 148 kobold                      1
## 149 kobold lord                 1
## 150 kobold mummy                1
## 151 kobold shaman               1
## 152 kobold zombie               1
## 153 Kop Lieutenant              1
## 154 Kop Sergeant                1
## 155 kraken                      2
## 156 large cat                   1
## 157 large dog                   1
## 158 large kobold                1
## 159 large mimic                 1
## 160 leather golem               1
## 161 leocrotta                   1
## 162 leprechaun                  1
## 163 lich                        2
## 164 lichen                      2
## 165 lieutenant                  1
## 166 little dog                  1
## 167 lizard                      1
## 168 long worm                   1
## 169 Lord Surtur                 1
## 170 lurker above                1
## 171 lynx                        1
## 172 manes                       1
## 173 marilith                    1
## 174 master lich                 1
## 175 master mind flayer          1
## 176 Medusa                      1
## 177 mind flayer                 1
## 178 minotaur                    1
## 179 monk                        2
## 180 monkey                      1
## 181 Mordor orc                  1
## 182 mountain centaur            1
## 183 mountain nymph              1
## 184 mumak                       1
## 185 nalfeshnee                  1
## 186 Nazgul                      1
## 187 newt                        1
## 188 Norn                        1
## 189 nurse                       2
## 190 ochre jelly                 1
## 191 ogre                        1
## 192 ogre king                   1
## 193 ogre lord                   1
## 194 Olog-hai                    1
## 195 orange dragon               1
## 196 orc                         3
## 197 orc mummy                   1
## 198 orc shaman                  1
## 199 orc zombie                  1
## 200 orc-captain                 1
## 201 Orcus                       1
## 202 owlbear                     1
## 203 page                        2
## 204 panther                     1
## 205 paper golem                 1
## 206 Pestilence                  1
## 207 piranha                     1
## 208 pit fiend                   1
## 209 pit viper                   1
## 210 plains centaur              1
## 211 pony                        1
## 212 purple worm                 1
## 213 pyrolisk                    1
## 214 python                      1
## 215 quantum mechanic            1
## 216 quasit                      1
## 217 queen bee                   1
## 218 quivering blob              1
## 219 rabid rat                   1
## 220 ranger                      1
## 221 raven                       2
## 222 red dragon                  1
## 223 red mold                    1
## 224 red naga                    1
## 225 rock mole                   1
## 226 rock piercer                1
## 227 rock troll                  1
## 228 rogue                       2
## 229 rope golem                  1
## 230 roshi                       1
## 231 rothe                       1
## 232 rust monster                1
## 233 salamander                  1
## 234 sandestin                   1
## 235 sasquatch                   1
## 236 scorpion                    1
## 237 sergeant                    1
## 238 sewer rat                   1
## 239 shade                       3
## 240 shark                       2
## 241 shocking sphere             1
## 242 shrieker                    1
## 243 silver dragon               1
## 244 skeleton                    1
## 245 small mimic                 1
## 246 snake                       2
## 247 soldier                     1
## 248 soldier ant                 1
## 249 spotted jelly               1
## 250 stalker                     1
## 251 steam vortex                1
## 252 stone giant                 2
## 253 stone golem                 1
## 254 storm giant                 2
## 255 straw golem                 1
## 256 succubus                    1
## 257 tengu                       1
## 258 tiger                       1
## 259 titanothere                 1
## 260 trapper                     1
## 261 troll                       1
## 262 umber hulk                  1
## 263 Uruk-hai                    1
## 264 vampire                     1
## 265 vampire bat                 1
## 266 vampire lord                1
## 267 violet fungus               1
## 268 Vlad the Impaler            1
## 269 vrock                       1
## 270 warg                        2
## 271 warhorse                    1
## 272 water elemental             1
## 273 water moccasin              1
## 274 water nymph                 1
## 275 werejackal                  2
## 276 wererat                     2
## 277 werewolf                    2
## 278 white dragon                1
## 279 white unicorn               1
## 280 winged gargoyle             1
## 281 winter wolf                 1
## 282 winter wolf cub             1
## 283 wizard                      1
## 284 wolf                        1
## 285 wood golem                  2
## 286 wood nymph                  1
## 287 Woodland-elf                1
## 288 wraith                      1
## 289 wumpus                      1
## 290 xan                         3
## 291 xorn                        2
## 292 yellow dragon               1
## 293 yellow light                1
## 294 yellow mold                 1
## 295 yeti                        1
## 296 zruty                       1
## 297 &amp;lt;NA&amp;gt;                        1&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;As you can see, some matches fail, especially for words that end in “y” in the singular, so “ies”
in plural, or “fire vortices” that does not get matched to “fire vortex”. I tried all the methods
but it’s either worse, or marginally better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extracting-info-from-dumplogfiles&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extracting info from dumplogfiles&lt;/h3&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click here to take a look at the source code from extract_defeated_monsters&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#&amp;#39; Extract information about defeated monsters from an xlogfile
#&amp;#39; @param xlog A raw xlogfile
#&amp;#39; @return A data frame with information on vanquished, genocided and extincted monsters
#&amp;#39; @importFrom dplyr mutate select filter bind_rows full_join
#&amp;#39; @importFrom tidyr separate
#&amp;#39; @importFrom tibble as_tibble tibble
#&amp;#39; @importFrom magrittr &amp;quot;%&amp;gt;%&amp;quot;
#&amp;#39; @importFrom purrr map2 possibly is_empty modify_if simplify discard
#&amp;#39; @importFrom readr read_lines
#&amp;#39; @importFrom stringr str_which str_replace_all str_replace str_trim str_detect str_to_lower str_extract_all str_extract
#&amp;#39; @export
#&amp;#39; @examples
#&amp;#39; \dontrun{
#&amp;#39; get_dumplog(xlog)
#&amp;#39; }
extract_defeated_monsters &amp;lt;- function(dumplog){

    if(any(str_detect(dumplog, &amp;quot;No creatures were vanquished.&amp;quot;))){
        return(NA)
    } else {

        start &amp;lt;- dumplog %&amp;gt;% # &amp;lt;- dectect the start of the list
            str_which(&amp;quot;Vanquished creatures&amp;quot;)

        end &amp;lt;- dumplog %&amp;gt;% # &amp;lt;- detect the end of the list
            str_which(&amp;quot;\\d+ creatures vanquished.&amp;quot;)

        if(is_empty(end)){ # This deals with the situation of only one vanquished creature
            end &amp;lt;- start + 2
        }

        list_creatures &amp;lt;- dumplog[(start + 1):(end - 1)] %&amp;gt;% # &amp;lt;- extract the list
            str_replace_all(&amp;quot;\\s+an? &amp;quot;, &amp;quot;1 &amp;quot;) %&amp;gt;% # &amp;lt;- replace a or an by 1
            str_trim() # &amp;lt;- trim white space

        # The following function first extracts the digit in the string (123 times)
        # and replaces the 1 with this digit
        # This means that: &amp;quot;1 the Wizard of Yendor (4 times)&amp;quot; becomes &amp;quot;4 the Wizard of Yendor (4 times)&amp;quot;
        str_extract_replace &amp;lt;- function(string){
            times &amp;lt;- str_extract(string, &amp;quot;\\d+(?=\\stimes)&amp;quot;)
            str_replace(string, &amp;quot;1&amp;quot;, times)
        }

        result &amp;lt;- list_creatures %&amp;gt;%
            # If a string starts with a letter, add a 1
            # This means that: &amp;quot;Baalzebub&amp;quot; becomes &amp;quot;1 Baalzebub&amp;quot;
            modify_if(str_detect(., &amp;quot;^[:alpha:]&amp;quot;), ~paste(&amp;quot;1&amp;quot;, .)) %&amp;gt;%
            # If the string &amp;quot;(twice)&amp;quot; is detected, replace &amp;quot;1&amp;quot; (that was added the line before) with &amp;quot;2&amp;quot;
            modify_if(str_detect(., &amp;quot;(twice)&amp;quot;), ~str_replace(., &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;)) %&amp;gt;%
            # Same for &amp;quot;(thrice)&amp;quot;
            modify_if(str_detect(., &amp;quot;(thrice)&amp;quot;), ~str_replace(., &amp;quot;1&amp;quot;, &amp;quot;3&amp;quot;)) %&amp;gt;%
            # Exctract the digit in &amp;quot;digit times&amp;quot; and replace the &amp;quot;1&amp;quot; with digit
            modify_if(str_detect(., &amp;quot;(\\d+ times)&amp;quot;), str_extract_replace) %&amp;gt;%
            # Replace &amp;quot;(times)&amp;quot; or &amp;quot;(twice)&amp;quot; etc with &amp;quot;&amp;quot;
            str_replace_all(&amp;quot;\\(.*\\)&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
            str_trim() %&amp;gt;%
            simplify() %&amp;gt;%
            # Convert the resulting list to a tibble. This tibble has one column:
            # value
            # 1 Baalzebub
            # 2 dogs
            #...
            as_tibble() %&amp;gt;%
            # Use tidyr::separate to separate the &amp;quot;value&amp;quot; column into two columns. The extra pieces get merged
            # So for example &amp;quot;1 Vlad the Impaler&amp;quot; becomes &amp;quot;1&amp;quot; &amp;quot;Vlad the Impaler&amp;quot; instead of &amp;quot;1&amp;quot; &amp;quot;Vlad&amp;quot; which
            # would be the case without &amp;quot;extra = &amp;quot;merge&amp;quot;&amp;quot;
            separate(value, into = c(&amp;quot;value&amp;quot;, &amp;quot;monster&amp;quot;), extra = &amp;quot;merge&amp;quot;) %&amp;gt;%
            mutate(value = as.numeric(value)) %&amp;gt;%
            mutate(monster = str_to_lower(monster))

        # This function singularizes names:
        singularize_monsters &amp;lt;- function(nethack_data){
            nethack_data %&amp;gt;%
                mutate(monster = str_replace_all(monster, &amp;quot;mummies&amp;quot;, &amp;quot;mummy&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;jellies&amp;quot;, &amp;quot;jelly&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;vortices&amp;quot;, &amp;quot;vortex&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;elves&amp;quot;, &amp;quot;elf&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;wolves&amp;quot;, &amp;quot;wolf&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;dwarves&amp;quot;, &amp;quot;dwarf&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;liches&amp;quot;, &amp;quot;lich&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;baluchiteria&amp;quot;, &amp;quot;baluchiterium&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;homunculi&amp;quot;, &amp;quot;homonculus&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;mumakil&amp;quot;, &amp;quot;mumak&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;sasquatches&amp;quot;, &amp;quot;sasquatch&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;watchmen&amp;quot;, &amp;quot;watchman&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;zruties&amp;quot;, &amp;quot;zruty&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;xes$&amp;quot;, &amp;quot;x&amp;quot;),
                       monster = str_replace_all(monster, &amp;quot;s$&amp;quot;, &amp;quot;&amp;quot;))
        }

        result &amp;lt;- singularize_monsters(result)
    }
    # If a player did not genocide or extinct any species, return the result:
    if(any(str_detect(dumplog, &amp;quot;No species were genocided or became extinct.&amp;quot;))){
        result &amp;lt;- result %&amp;gt;%
            mutate(status = NA_character_)
        return(result)
    } else {

        # If the player genocided or extincted species, add this info:
        start &amp;lt;- dumplog %&amp;gt;% # &amp;lt;- dectect the start of the list
            str_which(&amp;quot;Genocided or extinct species:&amp;quot;) # &amp;lt;- sometimes this does not appear in the xlogfile

        end &amp;lt;- dumplog %&amp;gt;% # &amp;lt;- detect the end of the list
            str_which(&amp;quot;Voluntary challenges&amp;quot;)

       if(is_empty(start)){# This deals with the situation start does not exist
           start &amp;lt;- end - 2
       }

        list_creatures &amp;lt;- dumplog[(start + 1):(end - 1)] %&amp;gt;% # &amp;lt;- extract the list
            str_trim() # &amp;lt;- trim white space

        extinct_species &amp;lt;- list_creatures %&amp;gt;%
            str_extract_all(&amp;quot;[:alpha:]+\\s(?=\\(extinct\\))&amp;quot;, simplify = T) %&amp;gt;%
            str_trim %&amp;gt;%
            discard(`==`(., &amp;quot;&amp;quot;))

        extinct_species_df &amp;lt;- tibble(monster = extinct_species, status = &amp;quot;extinct&amp;quot;)

        genocided_species_index &amp;lt;- list_creatures %&amp;gt;%
            str_detect(pattern = &amp;quot;extinct|species&amp;quot;) %&amp;gt;%
            `!`

        genocided_species &amp;lt;- list_creatures[genocided_species_index]

        genocided_species_df &amp;lt;- tibble(monster = genocided_species, status = &amp;quot;genocided&amp;quot;)

        genocided_or_extinct_df &amp;lt;- singularize_monsters(bind_rows(extinct_species_df, genocided_species_df))

        result &amp;lt;- full_join(result, genocided_or_extinct_df, by = &amp;quot;monster&amp;quot;) %&amp;gt;%
            filter(monster != &amp;quot;&amp;quot;) # &amp;lt;- this is to remove lines that were added by mistake, for example if start was empty

        return(result)
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing NetHack data, part 1: What kills the players</title>
      <link>/blog/2018-11-03-nethack_analysis/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-03-nethack_analysis/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=dpM2o4dRLto&#34;&gt;
&lt;img src=&#34;/img/deepfried_loss.png&#34; title = &#34;Click here to listen to epic music while reading&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this post, I will analyse the data I scraped and put into an R package, which I called &lt;code&gt;{nethack}&lt;/code&gt;.
NetHack is a roguelike game; for more context, read my previous blog
&lt;a href=&#34;https://www.brodrigues.co/blog/2018-11-01-nethack/&#34;&gt;post&lt;/a&gt;.
You can install the &lt;code&gt;{nethack}&lt;/code&gt; package and play around with the data yourself by installing it from github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/nethack&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to use it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nethack)
data(&amp;quot;nethack&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I
will analyze the data in a future blog post. This post focuses on getting and then sharing the
data. By the way, all the content from the public server I scrape is under the CC BY 4.0 license.&lt;/p&gt;
&lt;p&gt;I built the package by using the very useful &lt;code&gt;{devtools}&lt;/code&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;What I want from this first analysis are several, simple things: how many players manage to ascend
(meaning, winning), what monster kills most players, and finally extract data from the &lt;code&gt;dumplog&lt;/code&gt;
column. The &lt;code&gt;dumplog&lt;/code&gt; column is a bit special; each element of the dumplog column is a log file
that contains a lot of information from the last turns of a player. I will leave this for a future
blog post, though.&lt;/p&gt;
&lt;p&gt;Let’s load some packages first:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nethack)
library(tidyverse)
library(lubridate)
library(magrittr)
library(brotools)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;{brotools}&lt;/code&gt; is my own package that contains some functions that I use daily. If you want to
install it, run the following line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/brotools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The documentation is not up-to-date, I think I’ll do that and release it on CRAN. Some day.&lt;/p&gt;
&lt;p&gt;Now, let’s load the “nethack” data, included in the &lt;code&gt;{nethack}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##   rank score     name time turns lev_max hp_max role race gender alignment
## 1    1   360      jkm &amp;lt;NA&amp;gt;    NA     2/2  -2/25  Sam  Hum    Mal       Law
## 2    2   172 yosemite &amp;lt;NA&amp;gt;    NA     1/1  -1/10  Tou  Hum    Fem       Neu
## 3    3  2092    dtype &amp;lt;NA&amp;gt;    NA     6/7  -2/47  Val  Hum    Fem       Neu
## 4    4    32   joorko &amp;lt;NA&amp;gt;    NA     1/1   0/15  Sam  Hum    Mal       Law
## 5    5   118    jorko &amp;lt;NA&amp;gt;    NA     1/1   0/11  Rog  Orc    Fem       Cha
## 6    6  1757   aaronl &amp;lt;NA&amp;gt;    NA     5/5   0/60  Bar  Hum    Mal       Neu
##                                                      death       date
## 1                                   killed by a brown mold 2001-10-24
## 2                                       killed by a jackal 2001-10-24
## 3                                     killed by a fire ant 2001-10-24
## 4                                       killed by a jackal 2001-10-24
## 5                                       killed by a jackal 2001-10-24
## 6 killed by a hallucinogen-distorted ghoul, while helpless 2001-10-24
##   dumplog
## 1      NA
## 2      NA
## 3      NA
## 4      NA
## 5      NA
## 6      NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;nethack&amp;quot;)

head(nethack)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s create some variables that might be helpful (or perhaps not, we’ll see):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;lt;&amp;gt;% 
  mutate(date = ymd(date),
         year = year(date),
         month = month(date),
         day = day(date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This makes it easy to look at the data from, say, June 2017:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  filter(year == 2017, month == 6) %&amp;gt;%
  brotools::describe()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17 x 17
##    variable type   nobs    mean      sd mode    min     max   q05   q25
##    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 day      Nume…  1451    17.4  9.00e0 1         1      30     2   10 
##  2 month    Nume…  1451     6    0.     6         6       6     6    6 
##  3 rank     Nume…  1451    47.1  2.95e1 1         1     100     4   20 
##  4 score    Nume…  1451 38156.   3.39e5 488       0 5966425    94  402.
##  5 turns    Nume…  1451  4179.   1.23e4 812       1  291829   204  860.
##  6 year     Nume…  1451  2017    0.     2017   2017    2017  2017 2017 
##  7 alignme… Char…  1451    NA   NA      Law      NA      NA    NA   NA 
##  8 death    Char…  1451    NA   NA      kill…    NA      NA    NA   NA 
##  9 gender   Char…  1451    NA   NA      Mal      NA      NA    NA   NA 
## 10 hp_max   Char…  1451    NA   NA      -1/16    NA      NA    NA   NA 
## 11 lev_max  Char…  1451    NA   NA      4/4      NA      NA    NA   NA 
## 12 name     Char…  1451    NA   NA      ohno…    NA      NA    NA   NA 
## 13 race     Char…  1451    NA   NA      Hum      NA      NA    NA   NA 
## 14 role     Char…  1451    NA   NA      Kni      NA      NA    NA   NA 
## 15 time     Char…  1451    NA   NA      01:1…    NA      NA    NA   NA 
## 16 dumplog  List   1451    NA   NA      &amp;lt;NA&amp;gt;     NA      NA    NA   NA 
## 17 date     Date   1451    NA   NA      &amp;lt;NA&amp;gt;     NA      NA    NA   NA 
## # … with 7 more variables: median &amp;lt;dbl&amp;gt;, q75 &amp;lt;dbl&amp;gt;, q95 &amp;lt;dbl&amp;gt;,
## #   n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;, starting_date &amp;lt;date&amp;gt;,
## #   ending_date &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s also take a look at a dumplog:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to expand; the dumplog is quite long&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
    filter(year == 2018, month == 10) %&amp;gt;%
    slice(1) %&amp;gt;%
    pull(dumplog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##   [1] &amp;quot;Unix NetHack Version 3.6.1 - last build Fri Apr 27 19:25:48 2018. (d4ebae12f1a709d1833cf466dd0c553fb97518d2)&amp;quot;
##   [2] &amp;quot;&amp;quot;                                                                                                            
##   [3] &amp;quot;Game began 2018-09-30 22:27:18, ended 2018-10-01 00:01:12.&amp;quot;                                                  
##   [4] &amp;quot;&amp;quot;                                                                                                            
##   [5] &amp;quot;brothertrebius, neutral female gnomish Ranger&amp;quot;                                                               
##   [6] &amp;quot;&amp;quot;                                                                                                            
##   [7] &amp;quot;                  -----&amp;quot;                                                                                     
##   [8] &amp;quot;   --------       |....#     -----      --------&amp;quot;                                                            
##   [9] &amp;quot;   |/..%.=|      #...^|######|...|    ##.......|&amp;quot;                                                            
##  [10] &amp;quot;   |/[%..%|      #|...|     #|...|    # |......|&amp;quot;                                                            
##  [11] &amp;quot;   |......|      #-----     #-...-######....&amp;lt;..|&amp;quot;                                                            
##  [12] &amp;quot;   -----|--      ###         -|-.-  #   |......|&amp;quot;                                                            
##  [13] &amp;quot;        ##         ##           #   #   ----f---&amp;quot;                                                            
##  [14] &amp;quot;         ####       #           #  ##       f@Y&amp;quot;                                                             
##  [15] &amp;quot;            #       #           #  #&amp;quot;                                                                        
##  [16] &amp;quot;       -----.-------#           #  #&amp;quot;                                                                        
##  [17] &amp;quot;       |........%..|#           #  #&amp;quot;                                                                        
##  [18] &amp;quot;       |............#           #  #&amp;quot;                                                                        
##  [19] &amp;quot;       |...........|          0##  #&amp;quot;                                                                        
##  [20] &amp;quot;       |...........|         -.--- #&amp;quot;                                                                        
##  [21] &amp;quot;       -------------         |^..|##&amp;quot;                                                                        
##  [22] &amp;quot;                             |...|#&amp;quot;                                                                         
##  [23] &amp;quot;                             |0&amp;gt;..#&amp;quot;                                                                         
##  [24] &amp;quot;                             -----&amp;quot;                                                                          
##  [25] &amp;quot;&amp;quot;                                                                                                            
##  [26] &amp;quot;Brothertre the Trailblazer   St:15 Dx:12 Co:16 In:13 Wi:15 Ch:6  Neutral&amp;quot;                                    
##  [27] &amp;quot;Dlvl:6  $:59 HP:0(54) Pw:40(40) AC:0  Exp:8 T:7398  Satiated Burdened&amp;quot;                                       
##  [28] &amp;quot;&amp;quot;                                                                                                            
##  [29] &amp;quot;Latest messages:&amp;quot;                                                                                            
##  [30] &amp;quot; In what direction? l&amp;quot;                                                                                       
##  [31] &amp;quot; You shoot 2 arrows.&amp;quot;                                                                                        
##  [32] &amp;quot; The 1st arrow hits the ape.&amp;quot;                                                                                
##  [33] &amp;quot; The 2nd arrow hits the ape!&amp;quot;                                                                                
##  [34] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [35] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [36] &amp;quot; The ape bites!&amp;quot;                                                                                             
##  [37] &amp;quot; You ready: q - 9 uncursed arrows.&amp;quot;                                                                          
##  [38] &amp;quot; In what direction? l&amp;quot;                                                                                       
##  [39] &amp;quot; The arrow hits the ape.&amp;quot;                                                                                    
##  [40] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [41] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [42] &amp;quot; The ape bites!&amp;quot;                                                                                             
##  [43] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [44] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [45] &amp;quot; The ape misses!&amp;quot;                                                                                            
##  [46] &amp;quot; In what direction? l&amp;quot;                                                                                       
##  [47] &amp;quot; You shoot 2 arrows.&amp;quot;                                                                                        
##  [48] &amp;quot; The 1st arrow hits the ape!&amp;quot;                                                                                
##  [49] &amp;quot; The 2nd arrow hits the ape.&amp;quot;                                                                                
##  [50] &amp;quot; The ape misses!&amp;quot;                                                                                            
##  [51] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [52] &amp;quot; The ape misses!&amp;quot;                                                                                            
##  [53] &amp;quot; In what direction? l&amp;quot;                                                                                       
##  [54] &amp;quot; You shoot 2 arrows.&amp;quot;                                                                                        
##  [55] &amp;quot; The 1st arrow misses the ape.&amp;quot;                                                                              
##  [56] &amp;quot; The 2nd arrow hits the ape.&amp;quot;                                                                                
##  [57] &amp;quot; The ape misses!&amp;quot;                                                                                            
##  [58] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [59] &amp;quot; The ape bites!&amp;quot;                                                                                             
##  [60] &amp;quot; In what direction? l&amp;quot;                                                                                       
##  [61] &amp;quot; The arrow hits the ape!&amp;quot;                                                                                    
##  [62] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [63] &amp;quot; The ape misses!&amp;quot;                                                                                            
##  [64] &amp;quot; The ape bites!&amp;quot;                                                                                             
##  [65] &amp;quot; You hear someone cursing shoplifters.&amp;quot;                                                                      
##  [66] &amp;quot; The ape misses!&amp;quot;                                                                                            
##  [67] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [68] &amp;quot; The ape bites!&amp;quot;                                                                                             
##  [69] &amp;quot; What do you want to write with? [- amnqsvBJM-OWZ or ?*] -&amp;quot;                                                  
##  [70] &amp;quot; You write in the dust with your fingertip.&amp;quot;                                                                 
##  [71] &amp;quot; What do you want to write in the dust here? Elbereth&amp;quot;                                                       
##  [72] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [73] &amp;quot; The ape hits!&amp;quot;                                                                                              
##  [74] &amp;quot; You die...&amp;quot;                                                                                                 
##  [75] &amp;quot; Do you want your possessions identified? [ynq] (y) y&amp;quot;                                                       
##  [76] &amp;quot; Do you want to see your attributes? [ynq] (y) n&amp;quot;                                                            
##  [77] &amp;quot; Do you want an account of creatures vanquished? [ynaq] (y) n&amp;quot;                                               
##  [78] &amp;quot; Do you want to see your conduct? [ynq] (y) n&amp;quot;                                                               
##  [79] &amp;quot; Do you want to see the dungeon overview? [ynq] (y) q&amp;quot;                                                       
##  [80] &amp;quot;&amp;quot;                                                                                                            
##  [81] &amp;quot;Inventory:&amp;quot;                                                                                                  
##  [82] &amp;quot; Coins&amp;quot;                                                                                                      
##  [83] &amp;quot;  $ - 59 gold pieces&amp;quot;                                                                                        
##  [84] &amp;quot; Weapons&amp;quot;                                                                                                    
##  [85] &amp;quot;  m - 17 blessed +1 arrows&amp;quot;                                                                                  
##  [86] &amp;quot;  n - a blessed +0 arrow&amp;quot;                                                                                    
##  [87] &amp;quot;  q - 3 +0 arrows (in quiver)&amp;quot;                                                                               
##  [88] &amp;quot;  s - a +0 bow (weapon in hand)&amp;quot;                                                                             
##  [89] &amp;quot;  B - 11 +1 darts&amp;quot;                                                                                           
##  [90] &amp;quot;  N - 11 +0 darts&amp;quot;                                                                                           
##  [91] &amp;quot;  a - a +1 dagger (alternate weapon; not wielded)&amp;quot;                                                           
##  [92] &amp;quot; Armor&amp;quot;                                                                                                      
##  [93] &amp;quot;  T - an uncursed +0 dwarvish iron helm (being worn)&amp;quot;                                                        
##  [94] &amp;quot;  z - an uncursed +0 pair of leather gloves (being worn)&amp;quot;                                                    
##  [95] &amp;quot;  U - a cursed -4 pair of iron shoes (being worn)&amp;quot;                                                           
##  [96] &amp;quot;  e - an uncursed +2 cloak of displacement (being worn)&amp;quot;                                                     
##  [97] &amp;quot;  h - a blessed +0 dwarvish mithril-coat (being worn)&amp;quot;                                                       
##  [98] &amp;quot; Comestibles&amp;quot;                                                                                                
##  [99] &amp;quot;  f - 3 uncursed cram rations&amp;quot;                                                                               
## [100] &amp;quot;  j - 2 uncursed food rations&amp;quot;                                                                               
## [101] &amp;quot;  L - an uncursed food ration&amp;quot;                                                                               
## [102] &amp;quot;  P - an uncursed lembas wafer&amp;quot;                                                                              
## [103] &amp;quot;  I - an uncursed lizard corpse&amp;quot;                                                                             
## [104] &amp;quot;  o - an uncursed tin of spinach&amp;quot;                                                                            
## [105] &amp;quot; Scrolls&amp;quot;                                                                                                    
## [106] &amp;quot;  G - 2 uncursed scrolls of blank paper&amp;quot;                                                                     
## [107] &amp;quot;  t - an uncursed scroll of confuse monster&amp;quot;                                                                 
## [108] &amp;quot;  V - an uncursed scroll of identify&amp;quot;                                                                        
## [109] &amp;quot; Potions&amp;quot;                                                                                                    
## [110] &amp;quot;  x - an uncursed potion of gain ability&amp;quot;                                                                    
## [111] &amp;quot;  H - a blessed potion of sleeping&amp;quot;                                                                          
## [112] &amp;quot;  g - 3 uncursed potions of water&amp;quot;                                                                           
## [113] &amp;quot; Rings&amp;quot;                                                                                                      
## [114] &amp;quot;  O - an uncursed ring of slow digestion (on left hand)&amp;quot;                                                     
## [115] &amp;quot;  v - an uncursed ring of stealth (on right hand)&amp;quot;                                                           
## [116] &amp;quot; Tools&amp;quot;                                                                                                      
## [117] &amp;quot;  p - an uncursed magic lamp&amp;quot;                                                                                
## [118] &amp;quot;  k - an uncursed magic whistle&amp;quot;                                                                             
## [119] &amp;quot;  Q - an uncursed mirror&amp;quot;                                                                                    
## [120] &amp;quot;  C - an uncursed saddle&amp;quot;                                                                                    
## [121] &amp;quot;  D - an uncursed stethoscope&amp;quot;                                                                               
## [122] &amp;quot;  y - a +0 unicorn horn&amp;quot;                                                                                     
## [123] &amp;quot;  i - 7 uncursed wax candles&amp;quot;                                                                                
## [124] &amp;quot; Gems/Stones&amp;quot;                                                                                                
## [125] &amp;quot;  W - an uncursed flint stone&amp;quot;                                                                               
## [126] &amp;quot;  M - an uncursed worthless piece of red glass&amp;quot;                                                              
## [127] &amp;quot;  Z - an uncursed worthless piece of violet glass&amp;quot;                                                           
## [128] &amp;quot;  J - an uncursed worthless piece of white glass&amp;quot;                                                            
## [129] &amp;quot;&amp;quot;                                                                                                            
## [130] &amp;quot;Brothertrebius the Ranger&amp;#39;s attributes:&amp;quot;                                                                     
## [131] &amp;quot;&amp;quot;                                                                                                            
## [132] &amp;quot;Background:&amp;quot;                                                                                                 
## [133] &amp;quot; You were a Trailblazer, a level 8 female gnomish Ranger.&amp;quot;                                                   
## [134] &amp;quot; You were neutral, on a mission for Venus&amp;quot;                                                                   
## [135] &amp;quot; who was opposed by Mercury (lawful) and Mars (chaotic).&amp;quot;                                                    
## [136] &amp;quot;&amp;quot;                                                                                                            
## [137] &amp;quot;Final Characteristics:&amp;quot;                                                                                      
## [138] &amp;quot; You had 0 hit points (max:54).&amp;quot;                                                                             
## [139] &amp;quot; You had 40 magic power (max:40).&amp;quot;                                                                           
## [140] &amp;quot; Your armor class was 0.&amp;quot;                                                                                    
## [141] &amp;quot; You had 1552 experience points.&amp;quot;                                                                            
## [142] &amp;quot; You entered the dungeon 7398 turns ago.&amp;quot;                                                                    
## [143] &amp;quot; Your strength was 15 (limit:18/50).&amp;quot;                                                                        
## [144] &amp;quot; Your dexterity was 12 (limit:18).&amp;quot;                                                                          
## [145] &amp;quot; Your constitution was 16 (limit:18).&amp;quot;                                                                       
## [146] &amp;quot; Your intelligence was 13 (limit:19).&amp;quot;                                                                       
## [147] &amp;quot; Your wisdom was 15 (limit:18).&amp;quot;                                                                             
## [148] &amp;quot; Your charisma was 6 (limit:18).&amp;quot;                                                                            
## [149] &amp;quot;&amp;quot;                                                                                                            
## [150] &amp;quot;Final Status:&amp;quot;                                                                                               
## [151] &amp;quot; You were satiated.&amp;quot;                                                                                         
## [152] &amp;quot; You were burdened; movement was slightly slowed.&amp;quot;                                                           
## [153] &amp;quot; You were wielding a bow.&amp;quot;                                                                                   
## [154] &amp;quot;&amp;quot;                                                                                                            
## [155] &amp;quot;Final Attributes:&amp;quot;                                                                                           
## [156] &amp;quot; You were piously aligned.&amp;quot;                                                                                  
## [157] &amp;quot; You were telepathic.&amp;quot;                                                                                       
## [158] &amp;quot; You had automatic searching.&amp;quot;                                                                               
## [159] &amp;quot; You had infravision.&amp;quot;                                                                                       
## [160] &amp;quot; You were displaced.&amp;quot;                                                                                        
## [161] &amp;quot; You were stealthy.&amp;quot;                                                                                         
## [162] &amp;quot; You had slower digestion.&amp;quot;                                                                                  
## [163] &amp;quot; You were guarded.&amp;quot;                                                                                          
## [164] &amp;quot; You are dead.&amp;quot;                                                                                              
## [165] &amp;quot;&amp;quot;                                                                                                            
## [166] &amp;quot;Vanquished creatures:&amp;quot;                                                                                       
## [167] &amp;quot;  a warhorse&amp;quot;                                                                                                
## [168] &amp;quot;  a tengu&amp;quot;                                                                                                   
## [169] &amp;quot;  a quivering blob&amp;quot;                                                                                          
## [170] &amp;quot; an iron piercer&amp;quot;                                                                                            
## [171] &amp;quot;  2 black lights&amp;quot;                                                                                            
## [172] &amp;quot;  a gold golem&amp;quot;                                                                                              
## [173] &amp;quot;  a werewolf&amp;quot;                                                                                                
## [174] &amp;quot;  3 lizards&amp;quot;                                                                                                 
## [175] &amp;quot;  2 dingoes&amp;quot;                                                                                                 
## [176] &amp;quot;  a housecat&amp;quot;                                                                                                
## [177] &amp;quot;  a white unicorn&amp;quot;                                                                                           
## [178] &amp;quot;  2 dust vortices&amp;quot;                                                                                           
## [179] &amp;quot;  a plains centaur&amp;quot;                                                                                          
## [180] &amp;quot; an ape&amp;quot;                                                                                                     
## [181] &amp;quot;  a Woodland-elf&amp;quot;                                                                                            
## [182] &amp;quot;  2 soldier ants&amp;quot;                                                                                            
## [183] &amp;quot;  a bugbear&amp;quot;                                                                                                 
## [184] &amp;quot; an imp&amp;quot;                                                                                                     
## [185] &amp;quot;  a wood nymph&amp;quot;                                                                                              
## [186] &amp;quot;  a water nymph&amp;quot;                                                                                             
## [187] &amp;quot;  a rock piercer&amp;quot;                                                                                            
## [188] &amp;quot;  a pony&amp;quot;                                                                                                    
## [189] &amp;quot;  3 fog clouds&amp;quot;                                                                                              
## [190] &amp;quot;  a yellow light&amp;quot;                                                                                            
## [191] &amp;quot;  a violet fungus&amp;quot;                                                                                           
## [192] &amp;quot;  2 gnome lords&amp;quot;                                                                                             
## [193] &amp;quot;  2 gnomish wizards&amp;quot;                                                                                         
## [194] &amp;quot;  2 gray oozes&amp;quot;                                                                                              
## [195] &amp;quot;  2 elf zombies&amp;quot;                                                                                             
## [196] &amp;quot;  a straw golem&amp;quot;                                                                                             
## [197] &amp;quot;  a paper golem&amp;quot;                                                                                             
## [198] &amp;quot;  2 giant ants&amp;quot;                                                                                              
## [199] &amp;quot;  2 little dogs&amp;quot;                                                                                             
## [200] &amp;quot;  3 floating eyes&amp;quot;                                                                                           
## [201] &amp;quot;  8 dwarves&amp;quot;                                                                                                 
## [202] &amp;quot;  a homunculus&amp;quot;                                                                                              
## [203] &amp;quot;  3 kobold lords&amp;quot;                                                                                            
## [204] &amp;quot;  3 kobold shamans&amp;quot;                                                                                          
## [205] &amp;quot; 13 hill orcs&amp;quot;                                                                                               
## [206] &amp;quot;  4 rothes&amp;quot;                                                                                                  
## [207] &amp;quot;  2 centipedes&amp;quot;                                                                                              
## [208] &amp;quot;  3 giant bats&amp;quot;                                                                                              
## [209] &amp;quot;  6 dwarf zombies&amp;quot;                                                                                           
## [210] &amp;quot;  a werejackal&amp;quot;                                                                                              
## [211] &amp;quot;  3 iguanas&amp;quot;                                                                                                 
## [212] &amp;quot; 23 killer bees&amp;quot;                                                                                             
## [213] &amp;quot; an acid blob&amp;quot;                                                                                               
## [214] &amp;quot;  a coyote&amp;quot;                                                                                                  
## [215] &amp;quot;  3 gas spores&amp;quot;                                                                                              
## [216] &amp;quot;  5 hobbits&amp;quot;                                                                                                 
## [217] &amp;quot;  7 manes&amp;quot;                                                                                                   
## [218] &amp;quot;  2 large kobolds&amp;quot;                                                                                           
## [219] &amp;quot;  a hobgoblin&amp;quot;                                                                                               
## [220] &amp;quot;  2 giant rats&amp;quot;                                                                                              
## [221] &amp;quot;  2 cave spiders&amp;quot;                                                                                            
## [222] &amp;quot;  a yellow mold&amp;quot;                                                                                             
## [223] &amp;quot;  6 gnomes&amp;quot;                                                                                                  
## [224] &amp;quot;  8 garter snakes&amp;quot;                                                                                           
## [225] &amp;quot;  2 gnome zombies&amp;quot;                                                                                           
## [226] &amp;quot;  8 geckos&amp;quot;                                                                                                  
## [227] &amp;quot; 11 jackals&amp;quot;                                                                                                 
## [228] &amp;quot;  5 foxes&amp;quot;                                                                                                   
## [229] &amp;quot;  2 kobolds&amp;quot;                                                                                                 
## [230] &amp;quot;  2 goblins&amp;quot;                                                                                                 
## [231] &amp;quot;  a sewer rat&amp;quot;                                                                                               
## [232] &amp;quot;  6 grid bugs&amp;quot;                                                                                               
## [233] &amp;quot;  3 lichens&amp;quot;                                                                                                 
## [234] &amp;quot;  2 kobold zombies&amp;quot;                                                                                          
## [235] &amp;quot;  5 newts&amp;quot;                                                                                                   
## [236] &amp;quot;206 creatures vanquished.&amp;quot;                                                                                   
## [237] &amp;quot;&amp;quot;                                                                                                            
## [238] &amp;quot;No species were genocided or became extinct.&amp;quot;                                                                
## [239] &amp;quot;&amp;quot;                                                                                                            
## [240] &amp;quot;Voluntary challenges:&amp;quot;                                                                                       
## [241] &amp;quot; You never genocided any monsters.&amp;quot;                                                                          
## [242] &amp;quot; You never polymorphed an object.&amp;quot;                                                                           
## [243] &amp;quot; You never changed form.&amp;quot;                                                                                    
## [244] &amp;quot; You used no wishes.&amp;quot;                                                                                        
## [245] &amp;quot;&amp;quot;                                                                                                            
## [246] &amp;quot;The Dungeons of Doom: levels 1 to 6&amp;quot;                                                                         
## [247] &amp;quot;   Level 1:&amp;quot;                                                                                                 
## [248] &amp;quot;      A fountain.&amp;quot;                                                                                           
## [249] &amp;quot;   Level 2:&amp;quot;                                                                                                 
## [250] &amp;quot;      A sink.&amp;quot;                                                                                               
## [251] &amp;quot;   Level 3:&amp;quot;                                                                                                 
## [252] &amp;quot;      A general store, a fountain.&amp;quot;                                                                          
## [253] &amp;quot;   Level 4:&amp;quot;                                                                                                 
## [254] &amp;quot;      A general store, a fountain.&amp;quot;                                                                          
## [255] &amp;quot;      Stairs down to The Gnomish Mines.&amp;quot;                                                                     
## [256] &amp;quot;   Level 5:&amp;quot;                                                                                                 
## [257] &amp;quot;      A fountain.&amp;quot;                                                                                           
## [258] &amp;quot;   Level 6: &amp;lt;- You were here.&amp;quot;                                                                               
## [259] &amp;quot;      A general store.&amp;quot;                                                                                      
## [260] &amp;quot;      Final resting place for&amp;quot;                                                                               
## [261] &amp;quot;         you, killed by an ape.&amp;quot;                                                                             
## [262] &amp;quot;The Gnomish Mines: levels 5 to 8&amp;quot;                                                                            
## [263] &amp;quot;   Level 5:&amp;quot;                                                                                                 
## [264] &amp;quot;   Level 6:&amp;quot;                                                                                                 
## [265] &amp;quot;   Level 7:&amp;quot;                                                                                                 
## [266] &amp;quot;      Many shops, a temple, some fountains.&amp;quot;                                                                 
## [267] &amp;quot;   Level 8:&amp;quot;                                                                                                 
## [268] &amp;quot;&amp;quot;                                                                                                            
## [269] &amp;quot;Game over:&amp;quot;                                                                                                  
## [270] &amp;quot;                       ----------&amp;quot;                                                                           
## [271] &amp;quot;                      /          \\&amp;quot;                                                                         
## [272] &amp;quot;                     /    REST    \\&amp;quot;                                                                        
## [273] &amp;quot;                    /      IN      \\&amp;quot;                                                                       
## [274] &amp;quot;                   /     PEACE      \\&amp;quot;                                                                      
## [275] &amp;quot;                  /                  \\&amp;quot;                                                                     
## [276] &amp;quot;                  |  brothertrebius  |&amp;quot;                                                                      
## [277] &amp;quot;                  |      59 Au       |&amp;quot;                                                                      
## [278] &amp;quot;                  | killed by an ape |&amp;quot;                                                                      
## [279] &amp;quot;                  |                  |&amp;quot;                                                                      
## [280] &amp;quot;                  |                  |&amp;quot;                                                                      
## [281] &amp;quot;                  |                  |&amp;quot;                                                                      
## [282] &amp;quot;                  |       2018       |&amp;quot;                                                                      
## [283] &amp;quot;                 *|     *  *  *      | *&amp;quot;                                                                    
## [284] &amp;quot;        _________)/\\\\_//(\\/(/\\)/\\//\\/|_)_______&amp;quot;                                                       
## [285] &amp;quot;&amp;quot;                                                                                                            
## [286] &amp;quot;Goodbye brothertrebius the Ranger...&amp;quot;                                                                        
## [287] &amp;quot;&amp;quot;                                                                                                            
## [288] &amp;quot;You died in The Dungeons of Doom on dungeon level 6 with 6652 points,&amp;quot;                                       
## [289] &amp;quot;and 59 pieces of gold, after 7398 moves.&amp;quot;                                                                    
## [290] &amp;quot;You were level 8 with a maximum of 54 hit points when you died.&amp;quot;                                             
## [291] &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;Now, I am curious to see how many games are played per day:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;runs_per_day &amp;lt;- nethack %&amp;gt;%
  group_by(date) %&amp;gt;%
  count() %&amp;gt;%
  ungroup() 


ggplot(runs_per_day, aes(y = n, x = date)) + 
  geom_point(colour = &amp;quot;#0f4150&amp;quot;) + 
  geom_smooth(colour = &amp;quot;#82518c&amp;quot;) + 
  theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39; and formula &amp;#39;y ~ s(x, bs = &amp;quot;cs&amp;quot;)&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The number of games seems to be stable since 2015, around 50. But what is also interesting is not
only the number of games played, but also how many of these games resulted in a win.&lt;/p&gt;
&lt;p&gt;For this, let’s also add a new column that tells us whether the played &lt;em&gt;ascended&lt;/em&gt; (won the game)
or not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;lt;&amp;gt;%
  mutate(Ascended = ifelse(death == &amp;quot;ascended&amp;quot;, &amp;quot;Ascended&amp;quot;, &amp;quot;Died an horrible death&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m curious to see how many players managed to ascend… NetHack being as hard as diamonds, probably
not a lot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ascensions_per_day &amp;lt;- nethack %&amp;gt;%
  group_by(date, Ascended) %&amp;gt;%
  count() %&amp;gt;%
  rename(Total = n)

ggplot(ascensions_per_day) + 
  geom_area(aes(y = Total, x = as.Date(date), fill = Ascended)) +
  theme_blog() +
  labs(y = &amp;quot;Number of runs&amp;quot;, x = &amp;quot;Date&amp;quot;) +
  scale_fill_blog() +
  theme(legend.title = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yeah, just as expected. Because there is so much data, it’s difficult to see clearly, though. Depending on
the size of the screen you’re reading this, it might seem that in some days there are a lot of ascensions.
This is only an impression due to the resolution of the picture. Let’s see the share of ascensions per
year (and how many times the quests fail miserably), and this will become more apparent:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ascensions_per_day %&amp;gt;%
  mutate(Year = year(as.Date(date))) %&amp;gt;%
  group_by(Year, Ascended) %&amp;gt;%
  summarise(Total = sum(Total, na.rm = TRUE)) %&amp;gt;%
  group_by(Year) %&amp;gt;%
  mutate(denom = sum(Total, na.rm = TRUE)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(Share = Total/denom) %&amp;gt;%
  ggplot() + 
  geom_col(aes(y = Share, x = Year, fill = Ascended)) + 
  theme_blog() + 
  scale_fill_blog() + 
  theme(legend.title = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I will now convert the “time” column to seconds. I am not yet sure that this column is really useful,
because NetHack is a turn based game. This means that when the player does not move, neither do the
monsters. So the seconds spent playing might not be a good proxy for actual time spent playing.
But it makes for a good exercise:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;convert_to_seconds &amp;lt;- function(time_string){
    time_numeric &amp;lt;- time_string %&amp;gt;%
        str_split(&amp;quot;:&amp;quot;, simplify = TRUE) %&amp;gt;%
        as.numeric

    time_in_seconds &amp;lt;- sum(time_numeric * c(3600, 60, 1))

    time_in_seconds 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The strings I want to convert are of the form “01:34:43”, so I split at the “:” and then convert
the result to numeric. I end up with an atomic vector (&lt;code&gt;c(1, 34, 43)&lt;/code&gt;). Then I multiple each element
by the right number of seconds, and sum that to get the total. Let’s apply it to my data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;lt;&amp;gt;%
  mutate(time_in_seconds = map_dbl(time, convert_to_seconds))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is the distribution of “time_in_seconds”?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  describe(time_in_seconds)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 15
##   variable type    nobs   mean     sd mode    min    max   q05   q25 median
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 time_in… Nume… 322485 23529. 2.73e5 &amp;lt;NA&amp;gt;     61 2.72e7   141   622   1689
## # … with 4 more variables: q75 &amp;lt;dbl&amp;gt;, q95 &amp;lt;dbl&amp;gt;, n_missing &amp;lt;int&amp;gt;,
## #   n_unique &amp;lt;lgl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that the minimum of &lt;code&gt;time_in_seconds&lt;/code&gt; is 61 whereas the maximum is of the order of 27200000…
This must be a mistake, because that is almost one year!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  filter(time_in_seconds == max(time_in_seconds, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   rank      score   name       time   turns lev_max  hp_max role race
## 1   28 3173960108 fisted 7553:41:49 6860357    4/47 362/362  Wiz  Elf
##   gender alignment                      death       date dumplog year
## 1    Mal       Neu drowned in a pool of water 2017-02-02      NA 2017
##   month day               Ascended time_in_seconds
## 1     2   2 Died an horrible death        27193309&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well… maybe “fisted” wanted to break the record of the longest NetHack game ever. Congratulations!&lt;/p&gt;
&lt;p&gt;Let’s take a look at the density but cut it at 90th percentile:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  filter(!is.na(time_in_seconds),
         time_in_seconds &amp;lt; quantile(time_in_seconds, 0.9, na.rm = TRUE)) %&amp;gt;%
  ggplot() + 
  geom_density(aes(x = time_in_seconds), colour = &amp;quot;#82518c&amp;quot;) + 
  theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, the distribution is right skewed. However, as explained above NetHack is a turn based
game, meaning that if the player does not move, the monsters won’t move either. Perhaps it makes more
sense to look at the &lt;code&gt;turns&lt;/code&gt; column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  describe(turns)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 15
##   variable type    nobs  mean     sd mode    min    max   q05   q25 median
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 turns    Nume… 322485 4495. 19853. &amp;lt;NA&amp;gt;      1 6.86e6   202   871   1818
## # … with 4 more variables: q75 &amp;lt;dbl&amp;gt;, q95 &amp;lt;dbl&amp;gt;, n_missing &amp;lt;int&amp;gt;,
## #   n_unique &amp;lt;lgl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The maximum is quite large too. Just like before, let’s focus by cutting the variable at the 90th percentile:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
  filter(!is.na(turns),
         turns &amp;lt; quantile(turns, 0.9, na.rm = TRUE)) %&amp;gt;% 
  ggplot() + 
  geom_density(aes(x = turns), colour = &amp;quot;#82518c&amp;quot;) + 
  theme_blog()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think that using &lt;code&gt;turns&lt;/code&gt; makes more sense. In the a future blog post, I will estimate a survival
model and see how long players survive, and will use &lt;code&gt;turns&lt;/code&gt; instead of &lt;code&gt;time_in_seconds&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;div id=&#34;what-kills-the-players&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What kills the players&lt;/h3&gt;
&lt;p&gt;To know what kills players so much, some cleaning of the &lt;code&gt;death&lt;/code&gt; column is in order. Death can
occur from poisoning, starvation, accidents, drowning… of course monsters can kill the player too.
Here are some values of the &lt;code&gt;death&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;burned by a tower of flame
choked on a lichen corpse
died of starvation
fell into a pit of iron spikes
killed by a gnome
killed by a gnome called Blabla
killed by a gnome called Blabla while sleeping
slipped while mounting a saddled pony
slipped while mounting a saddled pony called Jolly Jumper
zapped her/himself with a spell&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To know what is the most frequent cause of death, I have to do some cleaning, because if not,
“killed by a gnome” and “killed by a gnome called Blabla” would be two different causes of death.
In the end, what interests me is to know how many times the player got killed by a gnome.&lt;/p&gt;
&lt;p&gt;The following lines do a cleanup of the &lt;code&gt;death&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;lt;&amp;gt;% 
  mutate(death2 = case_when(str_detect(death, &amp;quot;poisoned&amp;quot;) ~ &amp;quot;poisoned&amp;quot;,
                            str_detect(death, &amp;quot;slipped&amp;quot;) ~ &amp;quot;accident&amp;quot;,
                            str_detect(death, &amp;quot;petrified&amp;quot;) ~ &amp;quot;petrified&amp;quot;,
                            str_detect(death, &amp;quot;choked&amp;quot;) ~ &amp;quot;accident&amp;quot;,
                            str_detect(death, &amp;quot;caught.*self&amp;quot;) ~ &amp;quot;accident&amp;quot;,
                            str_detect(death, &amp;quot;starvation&amp;quot;) ~ &amp;quot;starvation&amp;quot;,
                            str_detect(death, &amp;quot;drowned&amp;quot;) ~ &amp;quot;drowned&amp;quot;,
                            str_detect(death, &amp;quot;fell&amp;quot;) ~ &amp;quot;fell&amp;quot;,
                            str_detect(death, &amp;quot;zapped&amp;quot;) ~ &amp;quot;zapped&amp;quot;,
                            str_detect(death, &amp;quot;killed&amp;quot;) ~ &amp;quot;killed&amp;quot;,
                            TRUE ~ death)) %&amp;gt;%
  mutate(death3 = str_extract(death, &amp;quot;(?&amp;lt;=by|while).*&amp;quot;)) %&amp;gt;%
  mutate(death3 = case_when(str_detect(death3, &amp;quot;,|\\bcalled\\b&amp;quot;) ~ str_extract(death3, &amp;quot;(.*?),|(.*?)\\bcalled\\b&amp;quot;), 
                            TRUE ~ death3)) %&amp;gt;%
  mutate(death3 = str_remove(death3, &amp;quot;,|called|\\ban?&amp;quot;),
         death3 = str_trim(death3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;death2&lt;/code&gt; is a new variable, in which I broadly categorize causes of death. Using regular expressions
I detect causes of death and aggregate some categories, for instance “slipped” and “chocked” into
“accident”. Then, I want to extract everything that comes after the strings “by” or while, and put
the result into a new variable called &lt;code&gt;death3&lt;/code&gt;. Then I detect the string “,” or “called”; if one
of these strings is present, I extract everything that comes before “,” or that comes before
“called”. Finally, I remove “,”, “called” or “a” or “an” from the string and trim the whitespaces.&lt;/p&gt;
&lt;p&gt;Let’s take a look at these new variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
nethack %&amp;gt;%
    select(name, death, death2, death3) %&amp;gt;%
    sample_n(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              name                     death   death2        death3
## 92740   DianaFury     killed by a death ray   killed     death ray
## 254216    Oddabit         killed by a tiger   killed         tiger
## 131889    shachaf      killed by a fire ant   killed      fire ant
## 284758        a43  poisoned by a killer bee poisoned    killer bee
## 303283      goast         killed by a gecko   killed         gecko
## 14692     liberty    killed by a gnome king   killed    gnome king
## 170303     arch18                  ascended ascended          &amp;lt;NA&amp;gt;
## 287786 foolishwtf           killed by a bat   killed           bat
## 177826    Renleve     killed by a giant bat   killed     giant bat
## 147248      TheOV killed by a black unicorn   killed black unicorn&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, it is quite easy to know what monsters are the meanest buttholes; let’s focus on the top 15.
Most likely, these are going to be early game monsters. Let’ see:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
    filter(!is.na(death3)) %&amp;gt;%
    count(death3) %&amp;gt;%
    top_n(15) %&amp;gt;%
    mutate(death3 = fct_reorder(death3, n, .desc = FALSE)) %&amp;gt;%
    ggplot() + 
    geom_col(aes(y = n, x = death3)) + 
    coord_flip() + 
    theme_blog() + 
    scale_fill_blog() + 
    ylab(&amp;quot;Number of deaths caused&amp;quot;) +
    xlab(&amp;quot;Monster&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Selecting by n&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Seems like soldier ants are the baddest, followed by jackals and dwarfs. As expected, these are
mostly early game monsters. Thus, it would be interesting to look at this distribution, but at
different stages in the game. Let’s create a categorical variable that discretizes &lt;code&gt;turns&lt;/code&gt;,
and then create one plot per category:&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Click to expand&lt;/summary&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack %&amp;gt;%
    filter(!is.na(death3)) %&amp;gt;%
    filter(!is.na(turns)) %&amp;gt;%
    mutate(turn_flag = case_when(between(turns, 1, 5000) ~ &amp;quot;Less than 5000&amp;quot;,
                                 between(turns, 5001, 10000) ~ &amp;quot;Between 5001 and 10000&amp;quot;,
                                 between(turns, 10001, 20000) ~ &amp;quot;Between 10001 and 20000&amp;quot;,
                                 between(turns, 20001, 40000) ~ &amp;quot;Between 20001 and 40000&amp;quot;,
                                 between(turns, 40001, 60000) ~ &amp;quot;Between 40001 and 60000&amp;quot;,
                                 turns &amp;gt; 60000 ~ &amp;quot;More than 60000&amp;quot;)) %&amp;gt;%
    mutate(turn_flag = factor(turn_flag, levels = c(&amp;quot;Less than 5000&amp;quot;, 
                                                    &amp;quot;Between 5001 and 10000&amp;quot;,
                                                    &amp;quot;Between 10001 and 20000&amp;quot;,
                                                    &amp;quot;Between 20001 and 40000&amp;quot;,
                                                    &amp;quot;Between 40001 and 60000&amp;quot;,
                                                    &amp;quot;More than 60000&amp;quot;), ordered = TRUE)) %&amp;gt;%
    group_by(turn_flag) %&amp;gt;%
    count(death3) %&amp;gt;%
    top_n(15) %&amp;gt;%
    nest() %&amp;gt;%
    mutate(data = map(data, ~mutate(., death3 = fct_reorder(death3, n, .desc = TRUE))))  %&amp;gt;%
    mutate(plots = map2(.x = turn_flag,
                         .y = data,
                         ~ggplot(data = .y) + 
                             geom_col(aes(y = n, x = death3)) + 
                             coord_flip() + 
                             theme_blog() + 
                             scale_fill_blog() + 
                             ylab(&amp;quot;Number of deaths caused&amp;quot;) +
                             xlab(&amp;quot;Monster&amp;quot;) + 
                             ggtitle(.x))) %&amp;gt;%
    pull(plots)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Selecting by n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-26-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-26-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[4]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-26-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[5]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-26-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## [[6]]&lt;/code&gt;&lt;/pre&gt;
&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-26-6.png&#34; width=&#34;672&#34; /&gt;
&lt;/details&gt;
&lt;p&gt;Finally, for this section, I want to know if there are levels, or floors, where players die more
often than others. For this, we can take a look at the &lt;code&gt;lev_max&lt;/code&gt; column. Observations in this
column are of the form “8/10”. This means that the player died on level 8, but the lowest level
that was explored was the 10th. Let’s do this for the year 2017 first. Before anything, I have
to explain the layout of the levels of the game. You can see a diagram
&lt;a href=&#34;https://nethackwiki.com/wiki/Mazes_of_Menace#Map&#34;&gt;here&lt;/a&gt;. The player starts on floor 1,
and goes down to level 53. Then, the player can ascend, by going on levels -1 to -5. But there
are more levels than these ones. -6 and -9 are the sky, and the player can teleport there (but will
fall to his death). If the player teleports to level -10, he’ll enter heaven (and die too). Because
these levels are special, I do not consider them here. I do not consider level 0 either, which is
“Nowhere”. Let’s get the number of players who died on each floor, but also compute the cumulative
death count:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;died_on_level &amp;lt;- nethack %&amp;gt;%
    filter(Ascended == &amp;quot;Died an horrible death&amp;quot;) %&amp;gt;%
    mutate(died_on = str_extract(lev_max, &amp;quot;-?\\d{1,}&amp;quot;)) %&amp;gt;%
    mutate(died_on = as.numeric(died_on)) %&amp;gt;%
    group_by(year) %&amp;gt;%
    count(died_on) %&amp;gt;% 
    filter(died_on &amp;gt;= -5, died_on != 0) %&amp;gt;%
    mutate(died_on = case_when(died_on == -1 ~ 54,
                               died_on == -2 ~ 55,
                               died_on == -3 ~ 56,
                               died_on == -4 ~ 57,
                               died_on == -5 ~ 58,
                               TRUE ~ died_on)) %&amp;gt;%
    arrange(desc(died_on)) %&amp;gt;%
    mutate(cumul_deaths = cumsum(n))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(died_on_level)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
## # Groups:   year [6]
##    year died_on     n cumul_deaths
##   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt;
## 1  2002      58     5            5
## 2  2003      58    11           11
## 3  2004      58    19           19
## 4  2005      58    28           28
## 5  2006      58    25           25
## 6  2007      58    22           22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s compute the number of players who ascended and add this to the cumulative count:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ascended_yearly &amp;lt;- nethack %&amp;gt;%
    filter(Ascended == &amp;quot;Ascended&amp;quot;) %&amp;gt;%
    group_by(year) %&amp;gt;%
    count(Ascended)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ascended_yearly)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
## # Groups:   year [6]
##    year Ascended     n
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
## 1  2001 Ascended     4
## 2  2002 Ascended    38
## 3  2003 Ascended   132
## 4  2004 Ascended   343
## 5  2005 Ascended   329
## 6  2006 Ascended   459&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will modify the dataset a little bit and merge it with the previous one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ascended_yearly %&amp;lt;&amp;gt;%
  rename(ascended_players = `n`) %&amp;gt;%
  select(-Ascended)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s add this to the data frame from before by merging both, and then we can compute the
surviving players:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;died_on_level %&amp;lt;&amp;gt;%
  full_join(ascended_yearly, by = &amp;quot;year&amp;quot;) %&amp;gt;%
  mutate(surviving_players = cumul_deaths + ascended_players)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can compute the share of players who died on each level:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;died_on_level %&amp;gt;%
    mutate(death_rate = n/surviving_players) %&amp;gt;% 
    ggplot(aes(y = death_rate, x = as.factor(died_on))) + 
    geom_line(aes(group = year, alpha = year), colour = &amp;quot;#82518c&amp;quot;) +
    theme_blog() + 
    ylab(&amp;quot;Death rate&amp;quot;) +
    xlab(&amp;quot;Level&amp;quot;) + 
    theme(axis.text.x = element_text(angle = 90),
          legend.position = &amp;quot;none&amp;quot;) + 
    scale_y_continuous(labels = scales::percent)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-11-03-nethack_analysis_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like level 7 is consistently the most dangerous! The death rate there is more than 35%!&lt;/p&gt;
&lt;p&gt;That’s it for this blog post, in the next one, I will focus on what players kill!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates or
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>From webscraping data to releasing it as an R package to share with the world: a full tutorial with data from NetHack</title>
      <link>/blog/2018-11-01-nethack/</link>
      <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-11-01-nethack/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;If someone told me a decade ago (back before I&amp;#39;d ever heard the term &amp;quot;roguelike&amp;quot;) what I&amp;#39;d be doing today, I would have trouble believing this...&lt;br&gt;&lt;br&gt;Yet here we are. &lt;a href=&#34;https://t.co/N6Hh6A4tWl&#34;&gt;pic.twitter.com/N6Hh6A4tWl&lt;/a&gt;&lt;/p&gt;&amp;mdash; Josh Ge (@GridSageGames) &lt;a href=&#34;https://twitter.com/GridSageGames/status/1009664438683648001?ref_src=twsrc%5Etfw&#34;&gt;June 21, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;update-07-11-2018&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Update 07-11-2018&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;{nethack}&lt;/code&gt; package currently on Github contains a sample of 6000 NetHack games played
on the alt.org/nethack public server between April and November 2018. This data was kindly provided by &lt;a href=&#34;https://twitter.com/paxed&#34;&gt;&lt;code&gt;@paxed&lt;/code&gt;&lt;/a&gt;.
The tutorial in this blog post is still useful if you want to learn more about scraping with R
and building a data package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this post, I am going to show you how you can scrape tables from a website, and then create a package
with the tidied data to share with the world. The data I am going to scrape comes from a NetHack
public server (&lt;a href=&#34;https://alt.org/nethack/&#34;&gt;link&lt;/a&gt;). The data I discuss in this blog post is
available in the &lt;code&gt;{nethack}&lt;/code&gt; package I created and I will walk you through the process of releasing
your package on CRAN. However, &lt;code&gt;{nethack}&lt;/code&gt; is too large to be on CRAN (75 mb, while the maximum
allowed is 5mb), so you can install it to play around with the data from github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/nethack&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to use it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nethack)
data(&amp;quot;nethack&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data contains information on games played from 2001 to 2018; 322485 rows and 14 columns. I
will analyze the data in a future blog post. This post focuses on getting and then sharing the
data. By the way, all the content from the public server I scrape is under the CC BY 4.0 license.&lt;/p&gt;
&lt;p&gt;I built the package by using the very useful &lt;code&gt;{devtools}&lt;/code&gt; package.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;NetHack is a game released in 1987 that is still being played and developed today.
NetHack is a roguelike game, meaning that is has procedurally generated dungeons and permadeath.
If you die, you have to start over, and because the dungeons are procedurally generated, this means
that you cannot learn the layout of the dungeons you explore or know when ennemies are going
to attack or even what ennemies are going to attack. Ennemies are not the only thing that you have
to be careful about; you can die from a lot of different events, as you will see in this post.
Objects that you find, such as a silver ring, might be helpful in a run, but be cursed in the next run.&lt;/p&gt;
&lt;p&gt;The latest version of the game, 3.6.1, was released on April 27th 2018, and this is how it looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/f/ff/Nethack.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The graphics are… bare-bones to say the least. The game runs inside a terminal emulator and is
available for any platform. The goal of NetHack is to explore a dungeon and go down every
level until you find the Amulet of Yendor. Once you find this Amulet, you have to go all the way
back upstairs, enter and fight your way through the Elemental Planes, enter the final Astral Plane,
and then finally offer the Amulet of Yendor to your god to finish the game. Needless to say,
NetHack is very difficult and players can go years without ever finishing the game.&lt;/p&gt;
&lt;p&gt;When you start an new game, you have to create a character, which can have several attributes.
You have to choose a race (human, elf, orc, etc), a role (tourist, samurai, mage, etc) and an
alignment (neutral, law, chaos) and these choices impact your base stats.&lt;/p&gt;
&lt;p&gt;If you can’t get past the ASCII graphics, you can play NetHack with tileset:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://vignette.wikia.nocookie.net/nethack/images/8/80/Vultures_eye.png/revision/latest?cb=20070313215112&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;You can install NetHack on your computer or you can play online on a public server, such as
this &lt;a href=&#34;https://alt.org/nethack/&#34;&gt;one&lt;/a&gt;. There are several advantages when playing on a pubic server;
the player does not have to install anyhing, and we data enthusiasts have access to a mine of
information! For example, you can view the following &lt;a href=&#34;https://alt.org/nethack/gamesday.php?date=20181025&#34;&gt;table&lt;/a&gt;
which contains data on all the games played on October 25th 2018. These tables start in the year 2001,
and I am going to scrape the info from these tables, which will allow me to answer several questions.
For instance, what is the floor most players die on? What kills most players?
What role do players choose more often? I will explore this questions in a future blog post, but for
now I will focus on scraping the data and realeasing it as a package to CRAN.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scraping-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scraping the data&lt;/h2&gt;
&lt;p&gt;To scrape the data I wrote a big function that does several things:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;rvest&amp;quot;)


scrape_one_day &amp;lt;- function(link){

    convert_to_seconds &amp;lt;- function(time_string){
        time_numeric &amp;lt;- time_string %&amp;gt;%
            str_split(&amp;quot;:&amp;quot;, simplify = TRUE) %&amp;gt;%
            as.numeric
     
     time_in_seconds &amp;lt;- time_numeric * c(3600, 60, 1)
     
     if(is.na(time_in_seconds)){
         time_in_seconds &amp;lt;- 61
     } else {
         time_in_seconds &amp;lt;- sum(time_in_seconds)
     }
     return(time_in_seconds)
    }

    Sys.sleep(1)

    date &amp;lt;- str_extract(link, &amp;quot;\\d{8}&amp;quot;)

    read_lines_slow &amp;lt;- function(...){
        Sys.sleep(1)
        read_lines(...)
    }
    
    page &amp;lt;- read_html(link)

        # Get links
    dumplogs &amp;lt;- page %&amp;gt;% 
        html_nodes(xpath = &amp;#39;//*[(@id = &amp;quot;perday&amp;quot;)]//td&amp;#39;) %&amp;gt;%
        html_children() %&amp;gt;%
        html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
        keep(str_detect(., &amp;quot;dumplog&amp;quot;))

    # Get table
    table &amp;lt;- page %&amp;gt;%
        html_node(xpath = &amp;#39;//*[(@id = &amp;quot;perday&amp;quot;)]&amp;#39;) %&amp;gt;%
        html_table(fill = TRUE)

    if(is_empty(dumplogs)){
        print(&amp;quot;dumplogs empty&amp;quot;)
        dumplogs &amp;lt;- rep(NA, nrow(table))
    } else {
        dumplogs &amp;lt;- dumplogs
    }
    
    final &amp;lt;- table %&amp;gt;%
        janitor::clean_names() %&amp;gt;%
        mutate(dumplog_links = dumplogs)

    print(paste0(&amp;quot;cleaning data of date &amp;quot;, date))
    
    clean_final &amp;lt;- final %&amp;gt;%
        select(-x) %&amp;gt;%
        rename(role = x_2,
               race = x_3,
               gender = x_4,
               alignment = x_5) %&amp;gt;%
        mutate(time_in_seconds = map(time, convert_to_seconds)) %&amp;gt;%
        filter(!(death %in% c(&amp;quot;quit&amp;quot;, &amp;quot;escaped&amp;quot;)), time_in_seconds &amp;gt; 60) %&amp;gt;%
        mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&amp;gt;%
        mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))

    saveRDS(clean_final, paste0(&amp;quot;datasets/data_&amp;quot;, date, &amp;quot;.rds&amp;quot;))

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s go through each part. The first part is a function that converts strings like “02:21:76” to
seconds:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;convert_to_seconds &amp;lt;- function(time_string){
    time_numeric &amp;lt;- time_string %&amp;gt;%
        str_split(&amp;quot;:&amp;quot;, simplify = TRUE) %&amp;gt;%
        as.numeric
 
time_in_seconds &amp;lt;- time_numeric * c(3600, 60, 1)
 
if(is.na(time_in_seconds)){
  time_in_seconds &amp;lt;- 61
  } else {
    time_in_seconds &amp;lt;- sum(time_in_seconds)
    }
return(time_in_seconds)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will use this function on the column that gives the length of the run. However,
before March 2008 this column is always empty, this is why I have the &lt;code&gt;if()...else()&lt;/code&gt; statement at
the end; if the time in seconds is &lt;code&gt;NA&lt;/code&gt;, then I make it 61 seconds. I do this because I want to keep
runs longer than 60 seconds, something I use &lt;code&gt;filter()&lt;/code&gt; for later. But when filtering, if the condition
returns &lt;code&gt;NA&lt;/code&gt; (which happens when you do &lt;code&gt;NA &amp;gt; 60&lt;/code&gt;) then you get an error, and the function fails.&lt;/p&gt;
&lt;p&gt;The website links I am going to scrape all have the date of the day the runs took place. I am going to
keep this date because I will need to name the datasets I am going to write to disk:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;date &amp;lt;- str_extract(link, &amp;quot;\\d{8}&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, I define this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_lines_slow &amp;lt;- function(...){
    Sys.sleep(1)
    read_lines(...)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is a wrapper around the &lt;code&gt;readr::read_lines()&lt;/code&gt; with a call to &lt;code&gt;Sys.sleep(1)&lt;/code&gt;. I will be scraping
a lot of pages, so letting one second pass between each page will not overload the servers so much.&lt;/p&gt;
&lt;p&gt;I then read the link with &lt;code&gt;read_html()&lt;/code&gt; and start by getting the links of the dumplogs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;page &amp;lt;- read_html(link)

# Get links
dumplogs &amp;lt;- page %&amp;gt;% 
    html_nodes(xpath = &amp;#39;//*[(@id = &amp;quot;perday&amp;quot;)]//td&amp;#39;) %&amp;gt;%
    html_children() %&amp;gt;%
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
    keep(str_detect(., &amp;quot;dumplog&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might be wondering what are dumplogs. Take a look at this screenshot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::include_graphics(&amp;quot;/img/dumplogs.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/dumplogs.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;When you click on those &lt;code&gt;d&lt;/code&gt;‘s, you land on a page like this &lt;a href=&#34;http://archive.is/wljb3&#34;&gt;one&lt;/a&gt; (I
archived it to be sure that this link will not die). These logs contain a lot of info that I want
to keep. To find the right ’xpath’ to scrape the links, &#34;’//&lt;em&gt;&lt;span class=&#34;citation&#34;&gt;[(@id = &#34;perday&#34;)]&lt;/span&gt;//td’&#34;, I used
the &lt;/em&gt;SelectorGadget* extension for Chrome. First I chose the table:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::include_graphics(&amp;quot;/img/selectorgadget1.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/selectorgadget1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;and then the links I am interested in:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::include_graphics(&amp;quot;/img/selectorgadget2.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/selectorgadget2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Putting them together, I get the right “xpath”. But just as with the time of the run, dumplogs are
only available after a certain date. So in case the &lt;code&gt;dumplogs&lt;/code&gt; column is empty, I relpace it with &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(is_empty(dumplogs)){
    print(&amp;quot;dumplogs empty&amp;quot;)
    dumplogs &amp;lt;- rep(NA, nrow(table))
} else {
    dumplogs &amp;lt;- dumplogs
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The rest is quite simple:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get table
table &amp;lt;- page %&amp;gt;%
    html_node(xpath = &amp;#39;//*[(@id = &amp;quot;perday&amp;quot;)]&amp;#39;) %&amp;gt;%
    html_table(fill = TRUE)
               
final &amp;lt;- table %&amp;gt;%
    janitor::clean_names() %&amp;gt;%
    mutate(dumplog_links = dumplogs)
           
print(paste0(&amp;quot;cleaning data of date &amp;quot;, date))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I scrape the table, and then join the dumplog links to the table inside a new column called “dumplog_links”.&lt;/p&gt;
&lt;p&gt;Because what follows is a long process, I print a message to let me know the progress of the scraping.&lt;/p&gt;
&lt;p&gt;Now the last part:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_final &amp;lt;- final %&amp;gt;%
    select(-x) %&amp;gt;%
    rename(role = x_2,
           race = x_3,
           gender = x_4,
           alignment = x_5) %&amp;gt;%
    mutate(time_in_seconds = map(time, convert_to_seconds)) %&amp;gt;%
    filter(!(death %in% c(&amp;quot;quit&amp;quot;, &amp;quot;escaped&amp;quot;)), time_in_seconds &amp;gt; 60) %&amp;gt;%
    mutate(dumplog = map(dumplog_links, ~possibly(read_lines_slow, otherwise = NA)(.))) %&amp;gt;%
    mutate(time_in_seconds = ifelse(time_in_seconds == 61, NA, time_in_seconds))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I first remove and remane columns. Then I convert the “time” column into seconds and also remove
runs that lasted less than 60 seconds or that ended either in “quit” (the player left the game)
or “escaped” (the player left the dungeon and the game ended immediately). There are a lot of runs
like that and they’re not interesting. Finally, and this is what takes long, I create a new
list-column where each element is the contents of the dumplog for that run. I wrap &lt;code&gt;read_lines_slow()&lt;/code&gt;
around &lt;code&gt;purrr::possibly()&lt;/code&gt; because dumplogs are missing for certains runs and when I try to read
them I get an 404 error back. Getting such an error stops the whole process, so with &lt;code&gt;purrr::possibly()&lt;/code&gt;
I can specify that in that case I want &lt;code&gt;NA&lt;/code&gt; back. Basically, a function wrapped inside &lt;code&gt;purrr::possibly()&lt;/code&gt;
never fails! Finally, if a game lasts for 61 seconds, I convert it back to &lt;code&gt;NA&lt;/code&gt; (remember this was
used to avoid having problems with the &lt;code&gt;filter()&lt;/code&gt; function).&lt;/p&gt;
&lt;p&gt;Finally, I export what I scraped to disk:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;saveRDS(clean_final, paste0(&amp;quot;datasets/data_&amp;quot;, date, &amp;quot;.rds&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where I use the date; to name the data. This is really important because scraping takes
a very long time, so if I don’t write the progress to disk as it goes, I might lose hours of work
if my internet goes down, or if computer freezes or whatever.&lt;/p&gt;
&lt;p&gt;In the lines below I build the links that I am going to scrape. They’re all of the form:
&lt;code&gt;https://alt.org/nethack/gamesday.php?date=YYYYMMDD&lt;/code&gt; so it’s quite easy to create a list of
dates to scrape, for example, for the year 2017:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;link &amp;lt;- &amp;quot;https://alt.org/nethack/gamesday.php?date=&amp;quot;

dates &amp;lt;- seq(as.Date(&amp;quot;2017/01/01&amp;quot;), as.Date(&amp;quot;2017/12/31&amp;quot;), by = &amp;quot;day&amp;quot;) %&amp;gt;%
    str_remove_all(&amp;quot;-&amp;quot;)

links &amp;lt;- paste0(link, dates)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can easily scrape the data. To make extra sure that I will not have problems during the
scraping process, for example if on a given day no games were played (and thus there is no table
to scrape, which would result in an error) , I use the same trick as above by using &lt;code&gt;purrr::possibly()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(links, ~possibly(scrape_one_day, otherwise = NULL)(.))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The scraping process took a very long time. I scraped all the data by letting my computer run for
three days!&lt;/p&gt;
&lt;p&gt;After this long process, I import all the &lt;code&gt;.rds&lt;/code&gt; files into R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path_to_data &amp;lt;- Sys.glob(&amp;quot;datasets/*.rds&amp;quot;)
nethack_data &amp;lt;- map(path_to_data, readRDS)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and take a look at one of them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack_data[[5812]] %&amp;gt;% 
  View()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s convert the “score” column to integer. For this, I will need to convert strings that look
like “12,232” to integers. I’ll write a short function to do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;to_numeric &amp;lt;- function(string){
  str_remove_all(string, &amp;quot;,&amp;quot;) %&amp;gt;%
    as.numeric
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack_data &amp;lt;- nethack_data %&amp;gt;%
  map(~mutate(., score = to_numeric(score)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s merge the data into a single data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack_data &amp;lt;- bind_rows(nethack_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I have a nice data frame, I will remove some columns and start the process of making a
packages. I remove the columns that I created and that are now useless (such as the &lt;code&gt;dumplog_links&lt;/code&gt;
column).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack_data &amp;lt;- nethack_data %&amp;gt;%
  select(rank, score, name, time, turns, lev_max, hp_max, role, race, gender, alignment, death,
         date, dumplog)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Export this to &lt;code&gt;.rds&lt;/code&gt; format, as it will be needed later:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;saveRDS(nethack_data, &amp;quot;nethack_data.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;making-a-package-to-share-your-data-with-the-world&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Making a package to share your data with the world&lt;/h2&gt;
&lt;p&gt;As stated in the beginning of this post, I will walk you through the process of creating and
releasing your package on CRAN. However, the data I scraped was too large to be made available
as a CRAN package. But you can still get the data from Github (link is in the abstract at the
beginning of the post).&lt;/p&gt;
&lt;p&gt;Making a data package is a great way to learn how to make packages, because it is relatively easy
to do (for example, you do not need to write unit tests). First, let’s start a new project
in RStudio:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/r_package1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Then select “R package”:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/r_package2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Then name your package, create a git repository and then click on “Create Project”:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/r_package3.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;RStudio wil open the &lt;code&gt;hello.R&lt;/code&gt; script which you can now modify. You got to learn from the best, so
I suggest that you modify &lt;code&gt;hello.R&lt;/code&gt; by taking inspiration from the &lt;code&gt;babynames&lt;/code&gt; package made by Hadley
Wickham which you can find &lt;a href=&#34;https://github.com/hadley/babynames/blob/master/R/data.R&#34;&gt;here&lt;/a&gt;.
You do not need the first two lines, and can focus on lines 4 to 13. Then, rename the script to
&lt;code&gt;data.R&lt;/code&gt;. This is how &lt;code&gt;{nethack}&#39;s&lt;/code&gt; looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#&amp;#39; NetHack runs data.
#&amp;#39;
#&amp;#39; Data on NetHack runs scraped from https://alt.org/nethack/gamesday.php
#&amp;#39;
#&amp;#39; @format A data frame with 14 variables: \code{rank}, \code{score},
#&amp;#39;   \code{name}, \code{time}, \code{turns}, \code{lev_max}, \code{hp_max}, \code{role}, \code{race},
#&amp;#39;   \code{gender}, \code{alignment}, \code{death}, \code{date} and \code{dumplog}
#&amp;#39; \describe{
#&amp;#39; \item{rank}{The rank of the player on that day}
#&amp;#39; \item{score}{The score the player achieved on that run}
#&amp;#39; \item{name}{The name of the player}
#&amp;#39; \item{time}{The time the player took to finish the game}
#&amp;#39; \item{turns}{The number of turns the player played before finishing the game}
#&amp;#39; \item{lev_max}{First digit: the level the player died on; second digit: the deepest explored level}
#&amp;#39; \item{hp_max}{The maximum character health points the player achieved}
#&amp;#39; \item{role}{The role the player chose to play as}
#&amp;#39; \item{race}{The race the player chose to play as}
#&amp;#39; \item{gender}{The gender the playr chose to play as}
#&amp;#39; \item{alignement}{The alignement the playr chose to play as}
#&amp;#39; \item{death}{The reason of death of the character}
#&amp;#39; \item{date}{The date the game took place}
#&amp;#39; \item{dumplog}{The log of the end game; this is a list column}
#&amp;#39; }
&amp;quot;nethack&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The comments are special, the “#” is followed by a &lt;code&gt;&#39;&lt;/code&gt;; these are special comments that will be
parsed by &lt;code&gt;roxygen2::roxygenise()&lt;/code&gt; and converted to documentation files.&lt;/p&gt;
&lt;p&gt;Next is the &lt;code&gt;DESCRIPTION&lt;/code&gt; file. Here is how &lt;code&gt;{nethack}&lt;/code&gt;’s looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Package: nethack
Type: Package
Title: Data from the Video Game NetHack
Version: 0.1.0
Authors@R: person(&amp;quot;Bruno André&amp;quot;, &amp;quot;Rodrigues Coelho&amp;quot;, email = &amp;quot;bruno@brodrigues.co&amp;quot;,
                  role = c(&amp;quot;aut&amp;quot;, &amp;quot;cre&amp;quot;))
Description: Data from NetHack runs played between 2001 to 2018 on 
    &amp;lt;https://alt.org/nethack/&amp;gt;, a NetHack public server.
Depends: R (&amp;gt;= 2.10)
License: CC BY 4.0
Encoding: UTF-8
LazyData: true
RoxygenNote: 6.1.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adapt yours accordingly. I chose the license &lt;code&gt;CC BY 4.0&lt;/code&gt; because this was the licence under which
the original data was published. It is also a good idea to add a &lt;em&gt;Vignette&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::use_vignette(&amp;quot;the_nethack_package&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Vignettes are very useful documentation with more details and examples.&lt;/p&gt;
&lt;p&gt;It is also good practice to add the script that was used to scrape the data. Such scripts go into
&lt;code&gt;data-raw/&lt;/code&gt;. Create this folder with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::use_data_raw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates the &lt;code&gt;data-raw/&lt;/code&gt; folder where I save the script that scrapes the data. Now is time to
put the data in the package. Start by importing the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nethack &amp;lt;- readRDS(&amp;quot;nethack_data.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To add the data to your package, you can use the following command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::use_data(nethack, compress = &amp;quot;xz&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will create the &lt;code&gt;data/&lt;/code&gt; folder and put the data in there in the &lt;code&gt;.rda&lt;/code&gt; format. I use the “compress”
option to make the data smaller. You can now create the documentation by running:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roxygen2::roxygenise()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pay attention to the log messages: you might need to remove files (for example the documentation
&lt;code&gt;hello.R&lt;/code&gt;, under the folder &lt;code&gt;man/&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now you can finaly run &lt;code&gt;R CMD Check&lt;/code&gt; by clicking the &lt;code&gt;Check&lt;/code&gt; button on the “Build” pane:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/r_package_check.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;This will extensively check the package for &lt;code&gt;ERRORS&lt;/code&gt;, &lt;code&gt;WARNINGS&lt;/code&gt; and &lt;code&gt;NOTES&lt;/code&gt;. You need to make sure
that the check passes without any &lt;code&gt;ERRORS&lt;/code&gt; or &lt;code&gt;WARNINGS&lt;/code&gt; and try as much as possible to remove all
&lt;code&gt;NOTES&lt;/code&gt; too. If you cannot remove a &lt;code&gt;NOTE&lt;/code&gt;, for example in my case the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;checking installed package size ... NOTE
  installed size is 169.7Mb
  sub-directories of 1Mb or more:
    data  169.6Mb
R CMD check results
0 errors | 0 warnings  | 1 note &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should document it in a new file called &lt;code&gt;cran-comments.md&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Test environments
* local openSUSE Tumbleweed install, R 3.5.1
* win-builder (devel and release)

## R CMD check results
There were no ERRORs or WARNINGs.

There was 1 NOTE:

    *   installed size is 169.7Mb
sub-directories of 1Mb or more:
    data  169.6Mb

The dataset contains 17 years of NetHack games played, hence the size. This package will not be updated often (max once a year).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you have eliminated all errors and warnings, you are almost ready to go.&lt;/p&gt;
&lt;p&gt;You need now to test the package on different platforms. This depends a bit on the system you run,
for me, because I run openSUSE (a GNU+Linux distribution) I have to test on Windows. This can be done
with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; devtools::build_win(version = &amp;quot;R-release&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; devtools::build_win(version = &amp;quot;R-devel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Explain that you have tested your package on several platforms in the &lt;code&gt;cran-comments.md&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;Finally you can add a &lt;code&gt;README.md&lt;/code&gt; and a &lt;code&gt;NEWS.md&lt;/code&gt; file and start the process of publishing the
package on CRAN:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools:release()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want many more details than what you can find in this blog post, I urge you to read
“R Packages” by Hadley Wickham, which you can read for free &lt;a href=&#34;http://r-pkgs.had.co.nz/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates or
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Maps with pie charts on top of each administrative division: an example with Luxembourg&#39;s elections data</title>
      <link>/blog/2018-10-27-lux_elections_analysis/</link>
      <pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10-27-lux_elections_analysis/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Bw8g_1VEEL8&#34;&gt;
&lt;img src=&#34;/img/europe_map_lux.png&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;You can find the data used in this blog post here: &lt;a href=&#34;https://github.com/b-rodrigues/elections_lux&#34; class=&#34;uri&#34;&gt;https://github.com/b-rodrigues/elections_lux&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a follow up to a &lt;a href=&#34;https://www.brodrigues.co/blog/2018-10-21-lux_elections/&#34;&gt;previous blog post&lt;/a&gt;
where I extracted data of the 2018 Luxembourguish elections from Excel Workbooks.
Now that I have the data, I will create a map of Luxembourg by commune, with pie charts of the
results on top of each commune! To do this, I use good ol’ &lt;code&gt;{ggplot2}&lt;/code&gt; and another packages
called &lt;code&gt;{scatterpie}&lt;/code&gt;. As a bonus, I have added the code to extract the data from the 2013
elections from Excel. You’ll find this code in the appendix at the end of the blog post.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Before importing the data for the elections of 2018, let’s install some packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;#39;rgeos&amp;#39;, type=&amp;#39;source&amp;#39;) # Dependency of rgdal
install.packages(&amp;#39;rgdal&amp;#39;, type=&amp;#39;source&amp;#39;) # To read in the shapefile&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These packages might be very tricky to install on OSX and Linux, but they’re needed to import the
shapefile of the country, which is needed to draw a map. So to make things easier, I have
created an &lt;code&gt;rds&lt;/code&gt; object, from the shapefile of Luxembourg, that you can import natively in R without
needing these two packages. But if you want to use them, here is how:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;communes &amp;lt;- readOGR(&amp;quot;Limadmin_SHP/LIMADM_COMMUNES.shp&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the way, you can download the shapefile for Luxembourg &lt;a href=&#34;https://data.public.lu/en/datasets/limites-administratives-du-grand-duche-de-luxembourg/#_&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ll use my shapefile though (that you can download from the same github repo as the data):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;communes_df &amp;lt;- readRDS(&amp;quot;commune_shapefile.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how it looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(communes_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       long      lat order  hole piece      group       id
## 1 91057.65 101536.6     1 FALSE     1 Beaufort.1 Beaufort
## 2 91051.79 101487.3     2 FALSE     1 Beaufort.1 Beaufort
## 3 91043.43 101461.7     3 FALSE     1 Beaufort.1 Beaufort
## 4 91043.37 101449.8     4 FALSE     1 Beaufort.1 Beaufort
## 5 91040.42 101432.1     5 FALSE     1 Beaufort.1 Beaufort
## 6 91035.44 101405.6     6 FALSE     1 Beaufort.1 Beaufort&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s load some packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;tidyxl&amp;quot;)
library(&amp;quot;ggplot2&amp;quot;)
library(&amp;quot;scatterpie&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, now, let’s import the elections results data, which is the output of
&lt;a href=&#34;https://www.brodrigues.co/blog/2018-10-21-lux_elections/&#34;&gt;last week’s blog post&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections &amp;lt;- read_csv(&amp;quot;elections_2018.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   Party = col_character(),
##   Year = col_double(),
##   Variables = col_character(),
##   Values = col_double(),
##   locality = col_character(),
##   division = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will only focus on the data at the commune level, and only use the share of votes for each party:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_map &amp;lt;- elections %&amp;gt;%
    filter(division == &amp;quot;Commune&amp;quot;,
           Variables == &amp;quot;Pourcentage&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I need to make sure that the names of the communes are the same between the elections data
and the shapefile. Usual suspects are the “Haute-Sûre” and the “Redange-sur-Attert” communes,
but let’s take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;locality_elections &amp;lt;- unique(elections_map$locality)
locality_shapefile &amp;lt;- unique(communes_df$id)

setdiff(locality_elections, locality_shapefile)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Lac de la Haute-Sûre&amp;quot; &amp;quot;Redange Attert&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yep, exactly as expected. I’ve had problems with the names of these two communes in the past already.
Let’s rename these two communes in the elections data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_map &amp;lt;- elections_map %&amp;gt;%
    mutate(commune = case_when(locality == &amp;quot;Lac de la Haute-Sûre&amp;quot; ~ &amp;quot;Lac de la Haute Sûre&amp;quot;,
                          locality == &amp;quot;Redange Attert&amp;quot; ~ &amp;quot;Redange&amp;quot;,
                          TRUE ~ locality))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I can select the relevant columns from the shapefile:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;communes_df &amp;lt;- communes_df %&amp;gt;%
    select(long, lat, commune = id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and from the elections data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_map &amp;lt;- elections_map %&amp;gt;%
    select(commune, Party, Variables, Values)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-data-on-a-map&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting the data on a map&lt;/h2&gt;
&lt;p&gt;Now, for the type of plot I want to make, using the &lt;code&gt;{scatterpie}&lt;/code&gt; package, I need the data to be
in the wide format, not long. For this I will use &lt;code&gt;tidyr::spread()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_map &amp;lt;- elections_map %&amp;gt;% 
    spread(Party, Values)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how the data looks now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(elections_map)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 102
## Variables: 10
## $ commune     &amp;lt;chr&amp;gt; &amp;quot;Beaufort&amp;quot;, &amp;quot;Bech&amp;quot;, &amp;quot;Beckerich&amp;quot;, &amp;quot;Berdorf&amp;quot;, &amp;quot;Bertran…
## $ Variables   &amp;lt;chr&amp;gt; &amp;quot;Pourcentage&amp;quot;, &amp;quot;Pourcentage&amp;quot;, &amp;quot;Pourcentage&amp;quot;, &amp;quot;Pource…
## $ ADR         &amp;lt;dbl&amp;gt; 0.12835106, 0.09848661, 0.08596748, 0.16339234, 0.04…
## $ CSV         &amp;lt;dbl&amp;gt; 0.2426239, 0.2945285, 0.3004751, 0.2604552, 0.290278…
## $ `déi gréng` &amp;lt;dbl&amp;gt; 0.15695672, 0.21699651, 0.24072721, 0.15619529, 0.15…
## $ `déi Lénk`  &amp;lt;dbl&amp;gt; 0.04043732, 0.03934808, 0.05435776, 0.02295273, 0.04…
## $ DP          &amp;lt;dbl&amp;gt; 0.15875393, 0.19394645, 0.12899689, 0.15444466, 0.30…
## $ KPL         &amp;lt;dbl&amp;gt; 0.015875393, 0.006519208, 0.004385164, 0.011476366, …
## $ LSAP        &amp;lt;dbl&amp;gt; 0.11771754, 0.11455180, 0.08852549, 0.16592103, 0.09…
## $ PIRATEN     &amp;lt;dbl&amp;gt; 0.13928411, 0.03562282, 0.09656496, 0.06516242, 0.04…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this to work, I need two datasets; one to draw the map (&lt;code&gt;commune_df&lt;/code&gt;) and one to draw the
pie charts over each commune, with the data to draw the charts, but also the position of where I
want the pie charts. For this, I will compute the average of the longitude and latitude, which
should be good enough:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scatterpie_data &amp;lt;- communes_df %&amp;gt;%
    group_by(commune) %&amp;gt;%
    summarise(long = mean(long),
              lat = mean(lat))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s join the two datasets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_data &amp;lt;- left_join(scatterpie_data, elections_map, by = &amp;quot;commune&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have all the ingredients to finally plot the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
    geom_polygon(data = communes_df, aes(x = long, y = lat, group = commune), colour = &amp;quot;grey&amp;quot;, fill = NA) +
    geom_scatterpie(data = final_data, aes(x=long, y=lat, group=commune), 
                    cols = c(&amp;quot;ADR&amp;quot;, &amp;quot;CSV&amp;quot;, &amp;quot;déi gréng&amp;quot;, &amp;quot;déi Lénk&amp;quot;, &amp;quot;DP&amp;quot;, &amp;quot;KPL&amp;quot;, &amp;quot;LSAP&amp;quot;, &amp;quot;PIRATEN&amp;quot;)) +
    labs(title = &amp;quot;Share of total vote in each commune, 2018 elections&amp;quot;) +
    theme_void() +
    theme(legend.position = &amp;quot;bottom&amp;quot;,
          legend.title = element_blank(),
          legend.text = element_text(colour = &amp;quot;white&amp;quot;),
          plot.background = element_rect(&amp;quot;#272b30&amp;quot;),
          plot.title = element_text(colour = &amp;quot;white&amp;quot;)) +
    scale_fill_manual(values = c(&amp;quot;ADR&amp;quot; = &amp;quot;#009dd1&amp;quot;,
                                 &amp;quot;CSV&amp;quot; = &amp;quot;#ee7d00&amp;quot;,
                                 &amp;quot;déi gréng&amp;quot; = &amp;quot;#45902c&amp;quot;,
                                 &amp;quot;déi Lénk&amp;quot; = &amp;quot;#e94067&amp;quot;,
                                 &amp;quot;DP&amp;quot; = &amp;quot;#002a54&amp;quot;,
                                 &amp;quot;KPL&amp;quot; = &amp;quot;#ff0000&amp;quot;,
                                 &amp;quot;LSAP&amp;quot; = &amp;quot;#ad3648&amp;quot;,
                                 &amp;quot;PIRATEN&amp;quot; = &amp;quot;#ad5ea9&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not too bad, but we can’t really read anything from the pie charts. I will now make their size
proportional to the number of voters in each commune. For this, I need to go back to the Excel
sheets, and look for the right cell:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/electeurs_inscrits.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;It will be easy to extract this info. It located in cell “E5”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_raw_2018 &amp;lt;- xlsx_cells(&amp;quot;leg-2018-10-14-22-58-09-737.xlsx&amp;quot;)

electors_commune &amp;lt;- elections_raw_2018 %&amp;gt;%
    filter(!(sheet %in% c(&amp;quot;Le Grand-Duché de Luxembourg&amp;quot;, &amp;quot;Centre&amp;quot;, &amp;quot;Est&amp;quot;, &amp;quot;Nord&amp;quot;, &amp;quot;Sud&amp;quot;, &amp;quot;Sommaire&amp;quot;))) %&amp;gt;%
    filter(address == &amp;quot;E5&amp;quot;) %&amp;gt;%
    select(sheet, numeric) %&amp;gt;%
    rename(commune = sheet,
           electors = numeric)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now add this to the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_data &amp;lt;- final_data %&amp;gt;% 
    full_join(electors_commune) %&amp;gt;%
    mutate(log_electors = log(electors) * 200)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;commune&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the last line, I create a new column called &lt;code&gt;log_electors&lt;/code&gt; that I then multiply by 200. This
will be useful later.&lt;/p&gt;
&lt;p&gt;Now I can add the &lt;code&gt;r&lt;/code&gt; argument inside the &lt;code&gt;aes()&lt;/code&gt; function on the third line, to make the pie chart
size proportional to the number of electors in that commune:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  geom_polygon(data = communes_df, aes(x = long, y = lat, group = commune), colour = &amp;quot;grey&amp;quot;, fill = NA) +
    geom_scatterpie(data = final_data, aes(x=long, y=lat, group = commune, r = electors), 
                    cols = c(&amp;quot;ADR&amp;quot;, &amp;quot;CSV&amp;quot;, &amp;quot;déi gréng&amp;quot;, &amp;quot;déi Lénk&amp;quot;, &amp;quot;DP&amp;quot;, &amp;quot;KPL&amp;quot;, &amp;quot;LSAP&amp;quot;, &amp;quot;PIRATEN&amp;quot;)) +
    labs(title = &amp;quot;Share of total vote in each commune, 2018 elections&amp;quot;) +
    theme_void() +
    theme(legend.position = &amp;quot;bottom&amp;quot;,
          legend.title = element_blank(),
          legend.text = element_text(colour = &amp;quot;white&amp;quot;),
          plot.background = element_rect(&amp;quot;#272b30&amp;quot;),
          plot.title = element_text(colour = &amp;quot;white&amp;quot;)) +
    scale_fill_manual(values = c(&amp;quot;ADR&amp;quot; = &amp;quot;#009dd1&amp;quot;,
                                 &amp;quot;CSV&amp;quot; = &amp;quot;#ee7d00&amp;quot;,
                                 &amp;quot;déi gréng&amp;quot; = &amp;quot;#45902c&amp;quot;,
                                 &amp;quot;déi Lénk&amp;quot; = &amp;quot;#182024&amp;quot;,
                                 &amp;quot;DP&amp;quot; = &amp;quot;#002a54&amp;quot;,
                                 &amp;quot;KPL&amp;quot; = &amp;quot;#ff0000&amp;quot;,
                                 &amp;quot;LSAP&amp;quot; = &amp;quot;#ad3648&amp;quot;,
                                 &amp;quot;PIRATEN&amp;quot; = &amp;quot;#ad5ea9&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 32 rows containing non-finite values (stat_pie).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, that was not a good idea! Perhaps the best option would be to have one map per circonscription.
For this, I need the list of communes by circonscription. This is available on Wikipedia. Here are
the lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;centre &amp;lt;- c(&amp;quot;Bissen&amp;quot;, &amp;quot;Colmar-Berg&amp;quot;, &amp;quot;Fischbach&amp;quot;, &amp;quot;Heffingen&amp;quot;, &amp;quot;Larochette&amp;quot;,
            &amp;quot;Lintgen&amp;quot;, &amp;quot;Lorentzweiler&amp;quot;, &amp;quot;Mersch&amp;quot;, &amp;quot;Nommern&amp;quot;, &amp;quot;Helperknapp&amp;quot;, &amp;quot;Bertrange&amp;quot;, &amp;quot;Contern&amp;quot;, 
            &amp;quot;Hesperange&amp;quot;, &amp;quot;Luxembourg&amp;quot;, &amp;quot;Niederanven&amp;quot;, &amp;quot;Sandweiler&amp;quot;, &amp;quot;Schuttrange&amp;quot;, &amp;quot;Steinsel&amp;quot;, 
            &amp;quot;Strassen&amp;quot;, &amp;quot;Walferdange&amp;quot;, &amp;quot;Weiler-la-Tour&amp;quot;)

est &amp;lt;- c(&amp;quot;Beaufort&amp;quot;, &amp;quot;Bech&amp;quot;, &amp;quot;Berdorf&amp;quot;, &amp;quot;Consdorf&amp;quot;, &amp;quot;Echternach&amp;quot;, &amp;quot;Rosport-Mompach&amp;quot;, &amp;quot;Waldbillig&amp;quot;,
         &amp;quot;Betzdorf&amp;quot;, &amp;quot;Biwer&amp;quot;, &amp;quot;Flaxweiler&amp;quot;, &amp;quot;Grevenmacher&amp;quot;, &amp;quot;Junglinster&amp;quot;, &amp;quot;Manternach&amp;quot;, &amp;quot;Mertert&amp;quot;,
         &amp;quot;Wormeldange&amp;quot;,&amp;quot;Bous&amp;quot;, &amp;quot;Dalheim&amp;quot;, &amp;quot;Lenningen&amp;quot;, &amp;quot;Mondorf-les-Bains&amp;quot;, &amp;quot;Remich&amp;quot;, &amp;quot;Schengen&amp;quot;,
         &amp;quot;Stadtbredimus&amp;quot;, &amp;quot;Waldbredimus&amp;quot;)

nord &amp;lt;- c(&amp;quot;Clervaux&amp;quot;, &amp;quot;Parc Hosingen&amp;quot;, &amp;quot;Troisvierges&amp;quot;, &amp;quot;Weiswampach&amp;quot;, &amp;quot;Wincrange&amp;quot;, &amp;quot;Bettendorf&amp;quot;, 
          &amp;quot;Bourscheid&amp;quot;, &amp;quot;Diekirch&amp;quot;, &amp;quot;Erpeldange-sur-Sûre&amp;quot;, &amp;quot;Ettelbruck&amp;quot;, &amp;quot;Feulen&amp;quot;, &amp;quot;Mertzig&amp;quot;, &amp;quot;Reisdorf&amp;quot;, 
          &amp;quot;Schieren&amp;quot;, &amp;quot;Vallée de l&amp;#39;Ernz&amp;quot;, &amp;quot;Beckerich&amp;quot;, &amp;quot;Ell&amp;quot;, &amp;quot;Grosbous&amp;quot;, &amp;quot;Préizerdaul&amp;quot;, 
          &amp;quot;Rambrouch&amp;quot;, &amp;quot;Redange&amp;quot;, &amp;quot;Saeul&amp;quot;, &amp;quot;Useldange&amp;quot;, &amp;quot;Vichten&amp;quot;, &amp;quot;Wahl&amp;quot;, &amp;quot;Putscheid&amp;quot;, &amp;quot;Tandel&amp;quot;,
          &amp;quot;Vianden&amp;quot;, &amp;quot;Boulaide&amp;quot;, &amp;quot;Esch-sur-Sûre&amp;quot;, &amp;quot;Goesdorf&amp;quot;, &amp;quot;Kiischpelt&amp;quot;, &amp;quot;Lac de la Haute Sûre&amp;quot;,
          &amp;quot;Wiltz&amp;quot;, &amp;quot;Winseler&amp;quot;)

sud &amp;lt;- c(&amp;quot;Dippach&amp;quot;, &amp;quot;Garnich&amp;quot;, &amp;quot;Käerjeng&amp;quot;, &amp;quot;Kehlen&amp;quot;, &amp;quot;Koerich&amp;quot;, &amp;quot;Kopstal&amp;quot;, &amp;quot;Mamer&amp;quot;, 
         &amp;quot;Habscht&amp;quot;, &amp;quot;Steinfort&amp;quot;, &amp;quot;Bettembourg&amp;quot;, &amp;quot;Differdange&amp;quot;, &amp;quot;Dudelange&amp;quot;, &amp;quot;Esch-sur-Alzette&amp;quot;, 
         &amp;quot;Frisange&amp;quot;, &amp;quot;Kayl&amp;quot;, &amp;quot;Leudelange&amp;quot;, &amp;quot;Mondercange&amp;quot;, &amp;quot;Pétange&amp;quot;, &amp;quot;Reckange-sur-Mess&amp;quot;, &amp;quot;Roeser&amp;quot;,
         &amp;quot;Rumelange&amp;quot;, &amp;quot;Sanem&amp;quot;, &amp;quot;Schifflange&amp;quot;)

circonscriptions &amp;lt;- list(&amp;quot;centre&amp;quot; = centre, &amp;quot;est&amp;quot; = est,
                         &amp;quot;nord&amp;quot; = nord, &amp;quot;sud&amp;quot; = sud)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I can make one map per circonscription. First, let’s split the data sets by circonscription:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;communes_df_by_circonscription &amp;lt;- circonscriptions %&amp;gt;%
    map(~filter(communes_df, commune %in% .))

final_data_by_circonscription &amp;lt;- circonscriptions %&amp;gt;%
    map(~filter(final_data, commune %in% .))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using &lt;code&gt;pmap()&lt;/code&gt;, I can reuse the code to generate the plot to each element of the two lists.
This is nice because I do not need to copy and paste the code 4 times:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pmap(list(x = communes_df_by_circonscription,
          y = final_data_by_circonscription,
          z = names(communes_df_by_circonscription)),
     function(x, y, z){
         ggplot() +
        geom_polygon(data = x, aes(x = long, y = lat, group = commune), 
                     colour = &amp;quot;grey&amp;quot;, fill = NA) +
        geom_scatterpie(data = y, aes(x=long, y=lat, group = commune), 
                        cols = c(&amp;quot;ADR&amp;quot;, &amp;quot;CSV&amp;quot;, &amp;quot;déi gréng&amp;quot;, &amp;quot;déi Lénk&amp;quot;, &amp;quot;DP&amp;quot;, &amp;quot;KPL&amp;quot;, &amp;quot;LSAP&amp;quot;, &amp;quot;PIRATEN&amp;quot;)) +
        labs(title = paste0(&amp;quot;Share of total vote in each commune, 2018 elections for circonscription &amp;quot;, z)) +
        theme_void() +
        theme(legend.position = &amp;quot;bottom&amp;quot;,
              legend.title = element_blank(),
              legend.text = element_text(colour = &amp;quot;white&amp;quot;),
              plot.background = element_rect(&amp;quot;#272b30&amp;quot;),
              plot.title = element_text(colour = &amp;quot;white&amp;quot;)) + 
        scale_fill_manual(values = c(&amp;quot;ADR&amp;quot; = &amp;quot;#009dd1&amp;quot;,
                                     &amp;quot;CSV&amp;quot; = &amp;quot;#ee7d00&amp;quot;,
                                     &amp;quot;déi gréng&amp;quot; = &amp;quot;#45902c&amp;quot;,
                                     &amp;quot;déi Lénk&amp;quot; = &amp;quot;#182024&amp;quot;,
                                     &amp;quot;DP&amp;quot; = &amp;quot;#002a54&amp;quot;,
                                     &amp;quot;KPL&amp;quot; = &amp;quot;#ff0000&amp;quot;,
                                     &amp;quot;LSAP&amp;quot; = &amp;quot;#ad3648&amp;quot;,
                                     &amp;quot;PIRATEN&amp;quot; = &amp;quot;#ad5ea9&amp;quot;))
     }
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $centre&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## $est&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-24-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## $nord&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-24-3.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## $sud&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-24-4.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I created an anonymous function of three argument, &lt;code&gt;x&lt;/code&gt;, &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;z&lt;/code&gt;. If you are unfamiliar with
&lt;code&gt;pmap()&lt;/code&gt;, study the above code closely. If you have questions, do not hesitate to reach out!&lt;/p&gt;
&lt;p&gt;The pie charts are still quite small, but if I try to change the size of the pie charts,
I’ll have the same problem as before: inside the same circonscription, some communes have really a
lot of electors, and some a very small number. Perhaps I can try with the log of the electors?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pmap(list(x = communes_df_by_circonscription,
          y = final_data_by_circonscription,
          z = names(communes_df_by_circonscription)),
     function(x, y, z){
         ggplot() +
        geom_polygon(data = x, aes(x = long, y = lat, group = commune), 
                     colour = &amp;quot;grey&amp;quot;, fill = NA) +
        geom_scatterpie(data = y, aes(x=long, y=lat, group = commune, r = log_electors), 
                        cols = c(&amp;quot;ADR&amp;quot;, &amp;quot;CSV&amp;quot;, &amp;quot;déi gréng&amp;quot;, &amp;quot;déi Lénk&amp;quot;, &amp;quot;DP&amp;quot;, &amp;quot;KPL&amp;quot;, &amp;quot;LSAP&amp;quot;, &amp;quot;PIRATEN&amp;quot;)) +
        labs(title = paste0(&amp;quot;Share of total vote in each commune, 2018 elections for circonscription &amp;quot;, z)) +
        theme_void() +
        theme(legend.position = &amp;quot;bottom&amp;quot;,
              legend.title = element_blank(),
              legend.text = element_text(colour = &amp;quot;white&amp;quot;),
              plot.background = element_rect(&amp;quot;#272b30&amp;quot;),
              plot.title = element_text(colour = &amp;quot;white&amp;quot;)) + 
        scale_fill_manual(values = c(&amp;quot;ADR&amp;quot; = &amp;quot;#009dd1&amp;quot;,
                                     &amp;quot;CSV&amp;quot; = &amp;quot;#ee7d00&amp;quot;,
                                     &amp;quot;déi gréng&amp;quot; = &amp;quot;#45902c&amp;quot;,
                                     &amp;quot;déi Lénk&amp;quot; = &amp;quot;#182024&amp;quot;,
                                     &amp;quot;DP&amp;quot; = &amp;quot;#002a54&amp;quot;,
                                     &amp;quot;KPL&amp;quot; = &amp;quot;#ff0000&amp;quot;,
                                     &amp;quot;LSAP&amp;quot; = &amp;quot;#ad3648&amp;quot;,
                                     &amp;quot;PIRATEN&amp;quot; = &amp;quot;#ad5ea9&amp;quot;))
     }
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $centre&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## $est&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-25-2.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## $nord&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 16 rows containing non-finite values (stat_pie).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-25-3.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## $sud&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-27-lux_elections_analysis_files/figure-html/unnamed-chunk-25-4.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This looks better now!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Having data in a machine readable format is really important. The amount of code I had to write
to go from the Excel Workbooks that contained the data to this plots is quite large, but if the
data was in a machine readable format to start with, I could have focused on the plots immediately.&lt;/p&gt;
&lt;p&gt;The good thing is that I got to practice my skills and discovered &lt;code&gt;{scatterpie}&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;Hope you enjoyed! If you found this blog post useful, you might want to follow
me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates or
&lt;a href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;buy me an espresso&lt;/a&gt;.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;The following lines of code extract the data (from the 2013 elections) from the Excel Workbooks
that can be found in Luxembourguish &lt;a href=&#34;https://data.public.lu/fr/datasets/elections-legislatives-2013-donnees-officieuses/#_&#34;&gt;Open Data Portal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will not comment them, as they work in a similar way than in the previous blog post where I
extracted the data from the 2018 elections. The only difference, is that the sheet with the
national level data was totally different, so I did not extract it. The first reason is because
I don’t need it for this blog post, the second is because I was lazy. For me, that’s two pretty
good reasons not to do something. If you have a question concerning the code below, don’t
hesitate to reach out though!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;tidyxl&amp;quot;)
library(&amp;quot;brotools&amp;quot;)

path &amp;lt;- Sys.glob(&amp;quot;content/blog/2013*xlsx&amp;quot;)[-5]

elections_raw_2013 &amp;lt;- map(path, xlsx_cells) %&amp;gt;%
    map(~filter(., sheet != &amp;quot;Sommaire&amp;quot;))

elections_sheets_2013 &amp;lt;- map(map(path, xlsx_sheet_names), ~`%-l%`(., &amp;quot;Sommaire&amp;quot;))

list_targets &amp;lt;- list(&amp;quot;Centre&amp;quot; = seq(9, 32),
                    &amp;quot;Est&amp;quot; = seq(9, 18),
                    &amp;quot;Nord&amp;quot; = seq(9, 20),
                    &amp;quot;Sud&amp;quot; = seq(9, 34))

position_parties_national &amp;lt;- seq(1, 24, by = 3)

extract_party &amp;lt;- function(dataset, starting_col, target_rows){
    
    almost_clean &amp;lt;- dataset %&amp;gt;%
        filter(row %in% target_rows) %&amp;gt;%
        filter(col %in% c(starting_col, starting_col + 1)) %&amp;gt;%
        select(character, numeric) %&amp;gt;%
        fill(numeric, .direction = &amp;quot;up&amp;quot;) %&amp;gt;%
        filter(!is.na(character))
    
    party_name &amp;lt;- almost_clean$character[1]
    
    almost_clean$character[1] &amp;lt;- &amp;quot;Pourcentage&amp;quot;
    
    almost_clean$party &amp;lt;- party_name
    
    colnames(almost_clean) &amp;lt;- c(&amp;quot;Variables&amp;quot;, &amp;quot;Values&amp;quot;, &amp;quot;Party&amp;quot;)
    
    almost_clean %&amp;gt;%
        mutate(Year = 2013) %&amp;gt;%
        select(Party, Year, Variables, Values)
    
}


# Treat one district

extract_district &amp;lt;- function(dataset, sheets, target_rows, position_parties_national){

    list_data_districts &amp;lt;- map(sheets, ~filter(.data = dataset, sheet == .)) 

    elections_districts_2013 &amp;lt;- map(.x = list_data_districts,
                                    ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = target_rows))

    map2(.y = elections_districts_2013, .x = sheets,
         ~mutate(.y, locality = .x, division = &amp;quot;Commune&amp;quot;, Year = &amp;quot;2013&amp;quot;)) %&amp;gt;%
        bind_rows()
}

elections_2013 &amp;lt;- pmap_dfr(list(x = elections_raw_2013, 
          y = elections_sheets_2013,
          z = list_targets), 
     function(x, y, z){
         map_dfr(position_parties_national, 
             ~extract_district(dataset = x, sheets = y, target_rows = z, position_parties_national = .))
     })

# Correct districts
elections_2013 &amp;lt;- elections_2013 %&amp;gt;%
    mutate(division = case_when(locality == &amp;quot;CENTRE&amp;quot; ~ &amp;quot;Electoral district&amp;quot;,
                                locality == &amp;quot;EST&amp;quot; ~ &amp;quot;Electoral district&amp;quot;,
                                locality == &amp;quot;NORD&amp;quot; ~ &amp;quot;Electoral district&amp;quot;,
                                locality == &amp;quot;SUD&amp;quot; ~ &amp;quot;Electoral district&amp;quot;,
                                TRUE ~ division))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Getting the data from the Luxembourguish elections out of Excel</title>
      <link>/blog/2018-10-21-lux_elections/</link>
      <pubDate>Sun, 21 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10-21-lux_elections/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yjzUxDhuXig&#34;&gt;
&lt;img src=&#34;/img/gambia.png&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this blog post, similar to a &lt;a href=&#34;https://www.brodrigues.co/blog/2018-09-11-human_to_machine/&#34;&gt;previous blog post&lt;/a&gt;
I am going to show you how we can go from an Excel workbook that contains data to flat file. I will
taking advantage of the structure of the tables inside the Excel sheets by writing a function
that extracts the tables and then mapping it to each sheet!&lt;/p&gt;
&lt;p&gt;Last week, October 14th, Luxembourguish nationals went to the polls to elect the Grand Duke! No,
actually, the Grand Duke does not get elected. But Luxembourguish citizen did go to the polls
to elect the new members of the Chamber of Deputies (a sort of parliament if you will).
The way the elections work in Luxembourg is quite interesting; you can vote for a party, or vote
for individual candidates from different parties. The candidates that get the most votes will
then seat in the parliament. If you vote for a whole party,
each of the candidates get a vote. You get as many votes as there are candidates to vote for. So,
for example, if you live in the capital city, also called Luxembourg, you get 21 votes to distribute.
You could decide to give 10 votes to 10 candidates of party A and 11 to 11 candidates of party B.
Why 21 votes? The chamber of Deputies is made up 60 deputies, and the country is divided into four
legislative circonscriptions. So each voter in a circonscription gets an amount of votes that is
proportional to the population size of that circonscription.&lt;/p&gt;
&lt;p&gt;Now you certainly wonder why I put the flag of Gambia on top of this post? This is because the
government that was formed after the 2013 elections was made up of a coalition of 3 parties;
the Luxembourg Socialist Worker’s Party, the Democratic Party and The Greens.
The LSAP managed to get 13 seats in the Chamber, while the DP got 13 and The Greens 6,
meaning 32 seats out of 60. So because they made this coalition, they could form the government,
and this coalition was named the Gambia coalition because of the colors of these 3 parties:
red, blue and green. If you want to take a look at the ballot from 2013 for the southern circonscription,
click &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Specimen_Elections_legislatives_Luxembourg_2013.png/1280px-Specimen_Elections_legislatives_Luxembourg_2013.png&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that you have the context, we can go back to some data science. The results of the elections
of last week can be found on Luxembourg’s Open Data portal, right &lt;a href=&#34;https://data.public.lu/fr/datasets/elections-legislatives-du-14-octobre-2018-donnees-officieuses/&#34;&gt;here&lt;/a&gt;.
The data is trapped inside Excel sheets; just like I explained in a &lt;a href=&#34;https://www.brodrigues.co/blog/2018-09-11-human_to_machine/&#34;&gt;previous blog post&lt;/a&gt;
the data is easily read by human, but not easily digested by any type of data analysis software.
So I am going to show you how we are going from this big Excel workbook to a flat file.&lt;/p&gt;
&lt;p&gt;First of all, if you open the Excel workbook, you will notice that there are a lot of sheets; there
is one for the whole country, named “Le Grand-Duché de Luxembourg”, one for the four circonscriptions,
“Centre”, “Nord”, “Sud”, “Est” and 102 more for each &lt;strong&gt;commune&lt;/strong&gt; of the country (a commune is an
administrative division). However, the tables are all very similarly shaped, and roughly at the
same position.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/elections_data.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;This is good, because we can write a function to extracts the data and then map it over
all the sheets. First, let’s load some packages and the data for the country:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;tidyxl&amp;quot;)
library(&amp;quot;brotools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# National Level 2018
elections_raw_2018 &amp;lt;- xlsx_cells(&amp;quot;leg-2018-10-14-22-58-09-737.xlsx&amp;quot;,
                        sheets = &amp;quot;Le Grand-Duché de Luxembourg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;{brotools}&lt;/code&gt; is my own package. You can install it with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/brotools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;it contains a function that I will use down below. The function I wrote to extract the tables
is not very complex, but requires that you are familiar with how &lt;code&gt;{tidyxl}&lt;/code&gt; imports Excel workbooks.
So if you are not familiar with it, study the imported data frame for a few minutes. It will make
understanding the next function easier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_party &amp;lt;- function(dataset, starting_col, target_rows){

    almost_clean &amp;lt;- dataset %&amp;gt;%
        filter(row %in% target_rows) %&amp;gt;%
        filter(col %in% c(starting_col, starting_col + 1)) %&amp;gt;%
        select(character, numeric) %&amp;gt;%
        fill(numeric, .direction = &amp;quot;up&amp;quot;) %&amp;gt;%
        filter(!is.na(character))

    party_name &amp;lt;- almost_clean$character[1] %&amp;gt;%
        str_split(&amp;quot;-&amp;quot;, simplify = TRUE) %&amp;gt;%
        .[2] %&amp;gt;%
        str_trim()

    almost_clean$character[1] &amp;lt;- &amp;quot;Pourcentage&amp;quot;

    almost_clean$party &amp;lt;- party_name

    colnames(almost_clean) &amp;lt;- c(&amp;quot;Variables&amp;quot;, &amp;quot;Values&amp;quot;, &amp;quot;Party&amp;quot;)

    almost_clean %&amp;gt;%
        mutate(Year = 2018) %&amp;gt;%
        select(Party, Year, Variables, Values)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function has three arguments, &lt;code&gt;dataset&lt;/code&gt;, &lt;code&gt;starting_col&lt;/code&gt; and &lt;code&gt;target_rows&lt;/code&gt;. &lt;code&gt;dataset&lt;/code&gt; is the
data I loaded with &lt;code&gt;xlsx_cells&lt;/code&gt; from the &lt;code&gt;{tidyxl}&lt;/code&gt; package. I think the following picture illustrates
easily what the function does:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/elections_logic.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;So the function first filters only the rows we are interested in, then the cols. I then select
the columns I want which are called &lt;code&gt;character&lt;/code&gt; and &lt;code&gt;numeric&lt;/code&gt; (if the Excel cell contains characters then
you will find them in the character column, if it contains numbers you will them in the numeric
column), then I fill the empty cells with the values from the &lt;code&gt;numeric&lt;/code&gt; column and the I remove
the NA’s. These two last steps might not be so clear; this is how the data looks like up until the
&lt;code&gt;select()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; elections_raw_2018 %&amp;gt;%
+     filter(row %in% seq(11,19)) %&amp;gt;%
+     filter(col %in% c(1, 2)) %&amp;gt;%
+     select(character, numeric)
# A tibble: 18 x 2
   character                       numeric
   &amp;lt;chr&amp;gt;                             &amp;lt;dbl&amp;gt;
 1 1 - PIRATEN - PIRATEN           NA     
 2 NA                               0.0645
 3 Suffrage total                  NA     
 4 NA                          227549     
 5 Suffrages de liste              NA     
 6 NA                          181560     
 7 Suffrage nominatifs             NA     
 8 NA                           45989     
 9 Pourcentage pondéré             NA     
10 NA                               0.0661
11 Suffrage total pondéré          NA     
12 NA                           13394.    
13 Suffrages de liste pondéré      NA     
14 NA                           10308     
15 Suffrage nominatifs pondéré     NA     
16 NA                            3086.    
17 Mandats attribués               NA     
18 NA                               2  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So by filling the NA’s in the numeric the data now looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; elections_raw_2018 %&amp;gt;%
+     filter(row %in% seq(11,19)) %&amp;gt;%
+     filter(col %in% c(1, 2)) %&amp;gt;%
+     select(character, numeric) %&amp;gt;%
+     fill(numeric, .direction = &amp;quot;up&amp;quot;)
# A tibble: 18 x 2
   character                       numeric
   &amp;lt;chr&amp;gt;                             &amp;lt;dbl&amp;gt;
 1 1 - PIRATEN - PIRATEN            0.0645
 2 NA                               0.0645
 3 Suffrage total              227549     
 4 NA                          227549     
 5 Suffrages de liste          181560     
 6 NA                          181560     
 7 Suffrage nominatifs          45989     
 8 NA                           45989     
 9 Pourcentage pondéré              0.0661
10 NA                               0.0661
11 Suffrage total pondéré       13394.    
12 NA                           13394.    
13 Suffrages de liste pondéré   10308     
14 NA                           10308     
15 Suffrage nominatifs pondéré   3086.    
16 NA                            3086.    
17 Mandats attribués                2     
18 NA                               2 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then I filter out the NA’s from the character column, and that’s almost it! I simply need
to add a new column with the party’s name and rename the other columns. I also add a “Year” colmun.&lt;/p&gt;
&lt;p&gt;Now, each party will have a different starting column. The table with the data for the first party
starts on column 1, for the second party it starts on column 4, column 7 for the third party…
So the following vector contains all the starting columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;position_parties_national &amp;lt;- seq(1, 24, by = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(If you study the Excel workbook closely, you will notice that I do not extract the last two parties.
This is because these parties were not present in all of the 4 circonscriptions and are very, very,
very small.)&lt;/p&gt;
&lt;p&gt;The target rows are always the same, from 11 to 19. Now, I simply need to map this function to
this list of positions and I get the data for all the parties:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_national_2018 &amp;lt;- map_df(position_parties_national, extract_party, 
                         dataset = elections_raw_2018, target_rows = seq(11, 19)) %&amp;gt;%
    mutate(locality = &amp;quot;Grand-Duchy of Luxembourg&amp;quot;, division = &amp;quot;National&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also added the &lt;code&gt;locality&lt;/code&gt; and &lt;code&gt;division&lt;/code&gt; columns to the data.&lt;/p&gt;
&lt;p&gt;Let’s take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(elections_national_2018)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 72
## Variables: 6
## $ Party     &amp;lt;chr&amp;gt; &amp;quot;PIRATEN&amp;quot;, &amp;quot;PIRATEN&amp;quot;, &amp;quot;PIRATEN&amp;quot;, &amp;quot;PIRATEN&amp;quot;, &amp;quot;PIRATEN&amp;quot;,…
## $ Year      &amp;lt;dbl&amp;gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …
## $ Variables &amp;lt;chr&amp;gt; &amp;quot;Pourcentage&amp;quot;, &amp;quot;Suffrage total&amp;quot;, &amp;quot;Suffrages de liste&amp;quot;,…
## $ Values    &amp;lt;dbl&amp;gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…
## $ locality  &amp;lt;chr&amp;gt; &amp;quot;Grand-Duchy of Luxembourg&amp;quot;, &amp;quot;Grand-Duchy of Luxembour…
## $ division  &amp;lt;chr&amp;gt; &amp;quot;National&amp;quot;, &amp;quot;National&amp;quot;, &amp;quot;National&amp;quot;, &amp;quot;National&amp;quot;, &amp;quot;Natio…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Very nice.&lt;/p&gt;
&lt;p&gt;Now we need to do the same for the 4 electoral circonscriptions. First, let’s load the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Electoral districts 2018
districts &amp;lt;- c(&amp;quot;Centre&amp;quot;, &amp;quot;Nord&amp;quot;, &amp;quot;Sud&amp;quot;, &amp;quot;Est&amp;quot;)

elections_district_raw_2018 &amp;lt;- xlsx_cells(&amp;quot;leg-2018-10-14-22-58-09-737.xlsx&amp;quot;,
                                      sheets = districts)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now things get trickier. Remember I said that the number of seats is proportional to the population
of each circonscription? We simply can’t use the same target rows as before. For example, for the
“Centre” circonscription, the target rows go from 12 to 37, but for the “Est” circonscription
only from 12 to 23. Ideally, we would need a function that would return the target rows.&lt;/p&gt;
&lt;p&gt;This is that function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The target rows I need to extract are different from district to district
get_target_rows &amp;lt;- function(dataset, sheet_to_extract, reference_address){

    last_row &amp;lt;- dataset %&amp;gt;%
        filter(sheet == sheet_to_extract) %&amp;gt;%
        filter(address == reference_address) %&amp;gt;%
        pull(numeric)

    seq(12, (11 + 5 + last_row))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function needs a &lt;code&gt;dataset&lt;/code&gt;, a &lt;code&gt;sheet_to_extract&lt;/code&gt; and a &lt;code&gt;reference_address&lt;/code&gt;. The reference
address is a cell that actually contains the number of seats in that circonscription, in our
case “B5”. We can easily get the list of target rows now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the target rows
list_targets &amp;lt;- map(districts, get_target_rows, dataset = elections_district_raw_2018, 
                    reference_address = &amp;quot;B5&amp;quot;)

list_targets&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [24] 35 36 37
## 
## [[2]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25
## 
## [[3]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [24] 35 36 37 38 39
## 
## [[4]]
##  [1] 12 13 14 15 16 17 18 19 20 21 22 23&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s split the data we imported into a list, where each element of the list is a dataframe
with the data from one circonscription:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_data_districts &amp;lt;- map(districts, ~filter(.data = elections_district_raw_2018, sheet == .)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can easily map the function I defined above, &lt;code&gt;extract_party&lt;/code&gt; to this list of datasets. Well,
I say easily, but it’s a bit more complicated than before because I have now a list of datasets
and a list of target rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_district_2018 &amp;lt;- map2(.x = list_data_districts, .y = list_targets,
     ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The way to understand this is that for each element of &lt;code&gt;list_data_districts&lt;/code&gt; and &lt;code&gt;list_targets&lt;/code&gt;,
I have to map &lt;code&gt;extract_party&lt;/code&gt; to each element of &lt;code&gt;position_parties_national&lt;/code&gt;. This gives the intented
result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_district_2018&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 208 x 4
##    Party    Year Variables               Values
##    &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                    &amp;lt;dbl&amp;gt;
##  1 PIRATEN  2018 Pourcentage             0.0514
##  2 PIRATEN  2018 CLEMENT Sven (1)     8007     
##  3 PIRATEN  2018 WEYER Jerry (2)      3446     
##  4 PIRATEN  2018 CLEMENT Pascal (3)   3418     
##  5 PIRATEN  2018 KUNAKOVA Lucie (4)   2860     
##  6 PIRATEN  2018 WAMPACH Jo (14)      2693     
##  7 PIRATEN  2018 LAUX Cynthia (6)     2622     
##  8 PIRATEN  2018 ISEKIN Christian (5) 2610     
##  9 PIRATEN  2018 SCHWEICH Georges (9) 2602     
## 10 PIRATEN  2018 LIESCH Mireille (8)  2551     
## # … with 198 more rows
## 
## [[2]]
## # A tibble: 112 x 4
##    Party    Year Variables                             Values
##    &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                                  &amp;lt;dbl&amp;gt;
##  1 PIRATEN  2018 Pourcentage                           0.0767
##  2 PIRATEN  2018 COLOMBERA Jean (2)                 5074     
##  3 PIRATEN  2018 ALLARD Ben (1)                     4225     
##  4 PIRATEN  2018 MAAR Andy (3)                      2764     
##  5 PIRATEN  2018 GINTER Joshua (8)                  2536     
##  6 PIRATEN  2018 DASBACH Angelika (4)               2473     
##  7 PIRATEN  2018 GRÜNEISEN Sam (6)                  2408     
##  8 PIRATEN  2018 BAUMANN Roy (5)                    2387     
##  9 PIRATEN  2018 CONRAD Pierre (7)                  2280     
## 10 PIRATEN  2018 TRAUT ép. MOLITOR Angela Maria (9) 2274     
## # … with 102 more rows
## 
## [[3]]
## # A tibble: 224 x 4
##    Party    Year Variables                    Values
##    &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                         &amp;lt;dbl&amp;gt;
##  1 PIRATEN  2018 Pourcentage                  0.0699
##  2 PIRATEN  2018 GOERGEN Marc (1)          9818     
##  3 PIRATEN  2018 FLOR Starsky (2)          6737     
##  4 PIRATEN  2018 KOHL Martine (3)          6071     
##  5 PIRATEN  2018 LIESCH Camille (4)        6025     
##  6 PIRATEN  2018 KOHL Sylvie (6)           5628     
##  7 PIRATEN  2018 WELTER Christian (5)      5619     
##  8 PIRATEN  2018 DA GRAÇA DIAS Yanick (10) 5307     
##  9 PIRATEN  2018 WEBER Jules (7)           5301     
## 10 PIRATEN  2018 CHMELIK Libor (8)         5247     
## # … with 214 more rows
## 
## [[4]]
## # A tibble: 96 x 4
##    Party    Year Variables                           Values
##    &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                                &amp;lt;dbl&amp;gt;
##  1 PIRATEN  2018 Pourcentage                         0.0698
##  2 PIRATEN  2018 FRÈRES Daniel (1)                4152     
##  3 PIRATEN  2018 CLEMENT Jill (7)                 1943     
##  4 PIRATEN  2018 HOUDREMONT Claire (2)            1844     
##  5 PIRATEN  2018 BÖRGER Nancy (3)                 1739     
##  6 PIRATEN  2018 MARTINS DOS SANTOS Catarina (6)  1710     
##  7 PIRATEN  2018 BELLEVILLE Tatjana (4)           1687     
##  8 PIRATEN  2018 CONTRERAS Gerald (5)             1687     
##  9 PIRATEN  2018 Suffrages total                 14762     
## 10 PIRATEN  2018 Suffrages de liste              10248     
## # … with 86 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now need to add the &lt;code&gt;locality&lt;/code&gt; and &lt;code&gt;division&lt;/code&gt; columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_district_2018 &amp;lt;- map2(.y = elections_district_2018, .x = districts, 
     ~mutate(.y, locality = .x, division = &amp;quot;Electoral district&amp;quot;)) %&amp;gt;%
    bind_rows()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re almost done! Now we need to do the same for the 102 remaining sheets, one for each &lt;strong&gt;commune&lt;/strong&gt;
of Luxembourg. This will now go very fast, because we got all the building blocks from before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;communes &amp;lt;- xlsx_sheet_names(&amp;quot;leg-2018-10-14-22-58-09-737.xlsx&amp;quot;)

communes &amp;lt;- communes %-l% 
    c(&amp;quot;Le Grand-Duché de Luxembourg&amp;quot;, &amp;quot;Centre&amp;quot;, &amp;quot;Est&amp;quot;, &amp;quot;Nord&amp;quot;, &amp;quot;Sud&amp;quot;, &amp;quot;Sommaire&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let me introduce the following function: &lt;code&gt;%-l%&lt;/code&gt;. This function removes elements from lists:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;) %-l% c(&amp;quot;a&amp;quot;, &amp;quot;d&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can think of it as “minus for lists”. This is called an infix operator.&lt;/p&gt;
&lt;p&gt;So this function is very useful to get the list of communes, and is part of my package, &lt;code&gt;{brotools}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As before, I load the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_communes_raw_2018 &amp;lt;- xlsx_cells(&amp;quot;leg-2018-10-14-22-58-09-737.xlsx&amp;quot;,
                                 sheets = communes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then get my list of targets, but I need to change the reference address. It’s “B8” now, not “B7”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the target rows
list_targets &amp;lt;- map(communes, get_target_rows, 
                    dataset = elections_communes_raw_2018, reference_address = &amp;quot;B8&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now create a list of communes by mapping a filter function to the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_data_communes &amp;lt;- map(communes, ~filter(.data = elections_communes_raw_2018, sheet == .)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And just as before, I get the data I need by using &lt;code&gt;extract_party&lt;/code&gt;, and adding the “locality” and
“division” columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elections_communes_2018 &amp;lt;- map2(.x = list_data_communes, .y = list_targets,
                                ~map_df(position_parties_national, extract_party, dataset = .x, target_rows = .y))

elections_communes_2018 &amp;lt;- map2(.y = elections_communes_2018, .x = communes,
                                ~mutate(.y, locality = .x, division = &amp;quot;Commune&amp;quot;)) %&amp;gt;%
    bind_rows()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The steps are so similar for the four circonscriptions and for the 102 &lt;strong&gt;communes&lt;/strong&gt; that I could
have write a big wrapper function and the use it for the circonscription and &lt;strong&gt;communes&lt;/strong&gt; at once.
But I was lazy.&lt;/p&gt;
&lt;p&gt;Finally, I bind everything together and have a nice, tidy, flat file:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Final results

elections_2018 &amp;lt;- bind_rows(list(elections_national_2018, elections_district_2018, elections_communes_2018))

glimpse(elections_2018)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 15,544
## Variables: 6
## $ Party     &amp;lt;chr&amp;gt; &amp;quot;PIRATEN&amp;quot;, &amp;quot;PIRATEN&amp;quot;, &amp;quot;PIRATEN&amp;quot;, &amp;quot;PIRATEN&amp;quot;, &amp;quot;PIRATEN&amp;quot;,…
## $ Year      &amp;lt;dbl&amp;gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, …
## $ Variables &amp;lt;chr&amp;gt; &amp;quot;Pourcentage&amp;quot;, &amp;quot;Suffrage total&amp;quot;, &amp;quot;Suffrages de liste&amp;quot;,…
## $ Values    &amp;lt;dbl&amp;gt; 6.446204e-02, 2.275490e+05, 1.815600e+05, 4.598900e+04…
## $ locality  &amp;lt;chr&amp;gt; &amp;quot;Grand-Duchy of Luxembourg&amp;quot;, &amp;quot;Grand-Duchy of Luxembour…
## $ division  &amp;lt;chr&amp;gt; &amp;quot;National&amp;quot;, &amp;quot;National&amp;quot;, &amp;quot;National&amp;quot;, &amp;quot;National&amp;quot;, &amp;quot;Natio…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This blog post is already quite long, so I will analyze the data now that R can easily ingest it
in a future blog post.&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exporting editable plots from R to Powerpoint: making ggplot2 purrr with officer</title>
      <link>/blog/2018-10-05-ggplot2_purrr_officer/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-10-05-ggplot2_purrr_officer/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oc9XOxUcvLY&#34;&gt;
&lt;img src=&#34;/img/officer_meme.jpg&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;A kind reader let me know that the function &lt;code&gt;create_pptx()&lt;/code&gt; is now outdated, and
proposed an update which you can find here:
&lt;a href=&#34;https://gist.github.com/b-rodrigues/ef4e97ed75028ca1ddd5987bb4085c1c&#34;&gt;here&lt;/a&gt;.
Thank you
&lt;a href=&#34;https://twitter.com/jerry_stones/status/1239625489578254336&#34;&gt;@Jeremy&lt;/a&gt;!
&lt;/p&gt;


&lt;p&gt;I was recently confronted to the following problem: creating hundreds of plots that could still be
edited by our client. What this meant was that I needed to export the graphs in Excel or Powerpoint
or some other such tool that was familiar to the client, and not export the plots directly to pdf or
png as I would normally do. I still wanted to use R to do it though, because I could do what I always
do to when I need to perform repetitive tasks such as producing hundreds of plots; map over a list
of, say, countries, and make one plot per country. This is something I discussed in a previous
blog post, &lt;a href=&#34;http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/&#34;&gt;Make ggplot2 purrr&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, after some online seaching, I found the &lt;code&gt;{officer}&lt;/code&gt; package. This package allows you to put
objects into Microsoft documents. For example, editable plots in a Powerpoint document. This is what
I will show in this blog post.&lt;/p&gt;
&lt;p&gt;Let’s start by loading the required packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;officer&amp;quot;)
library(&amp;quot;rvg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I will use the data from the time use survey, which I discussed in a previous blog post
&lt;a href=&#34;http://www.brodrigues.co/blog/2018-09-11-human_to_machine/&#34;&gt;Going from a human readable Excel file to a machine-readable csv with {tidyxl}&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can download the data &lt;a href=&#34;https://github.com/rbind/b-rodrigues.github.com/blob/master/content/blog/clean_data.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s import and prepare it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time_use &amp;lt;- rio::import(&amp;quot;clean_data.csv&amp;quot;)


time_use &amp;lt;- time_use %&amp;gt;%
    filter(population %in% c(&amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;)) %&amp;gt;%
    filter(activities %in% c(&amp;quot;Personal care&amp;quot;, &amp;quot;Sleep&amp;quot;, &amp;quot;Eating&amp;quot;, 
                             &amp;quot;Employment&amp;quot;, &amp;quot;Household and family care&amp;quot;)) %&amp;gt;%
    group_by(day) %&amp;gt;%
    nest()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I only kept two categories, “Male” and “Female” and 5 activities. Then I grouped by day and nested
the data. This is how it looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time_use&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   day                         data             
##   &amp;lt;chr&amp;gt;                       &amp;lt;list&amp;gt;           
## 1 Year 2014_Monday til Friday &amp;lt;tibble [10 × 4]&amp;gt;
## 2 Year 2014_Saturday          &amp;lt;tibble [10 × 4]&amp;gt;
## 3 Year 2014_Sunday            &amp;lt;tibble [10 × 4]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As shown, &lt;code&gt;time_use&lt;/code&gt; is a tibble with 2 columns, the first &lt;code&gt;day&lt;/code&gt; contains the days, and the second
&lt;code&gt;data&lt;/code&gt;, is of type list, and each element of these lists are tibbles themselves. Let’s take a look
inside one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;time_use$data[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 10 x 4
##    population activities                time  time_in_minutes
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;                     &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;
##  1 Male       Personal care             11:00             660
##  2 Male       Sleep                     08:24             504
##  3 Male       Eating                    01:46             106
##  4 Male       Employment                08:11             491
##  5 Male       Household and family care 01:59             119
##  6 Female     Personal care             11:15             675
##  7 Female     Sleep                     08:27             507
##  8 Female     Eating                    01:48             108
##  9 Female     Employment                06:54             414
## 10 Female     Household and family care 03:49             229&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now create plots for each of the days with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plots &amp;lt;- time_use %&amp;gt;%
    mutate(plots = map2(.y = day, .x = data, ~ggplot(data = .x) + theme_minimal() +
                       geom_col(aes(y = time_in_minutes, x = activities, fill = population), 
                                position = &amp;quot;dodge&amp;quot;) +
                       ggtitle(.y) +
                       ylab(&amp;quot;Time in minutes&amp;quot;) +
                       xlab(&amp;quot;Activities&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These steps are all detailled in my blog post
&lt;a href=&#34;http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/&#34;&gt;Make ggplot2 purrr&lt;/a&gt;.
Let’s take a look at &lt;code&gt;my_plots&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   day                         data              plots 
##   &amp;lt;chr&amp;gt;                       &amp;lt;list&amp;gt;            &amp;lt;list&amp;gt;
## 1 Year 2014_Monday til Friday &amp;lt;tibble [10 × 4]&amp;gt; &amp;lt;gg&amp;gt;  
## 2 Year 2014_Saturday          &amp;lt;tibble [10 × 4]&amp;gt; &amp;lt;gg&amp;gt;  
## 3 Year 2014_Sunday            &amp;lt;tibble [10 × 4]&amp;gt; &amp;lt;gg&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last column, called &lt;code&gt;plots&lt;/code&gt; is a list where each element is a plot! We can take a look at one:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_plots$plots[1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-10-05-ggplot2_purrr_officer_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, this is where I could export these plots as pdfs or pngs. But this is not what I need. I need
to export these plots as editable charts for Powerpoint. To do this for one image, I would do the
following (as per &lt;code&gt;{officer}&lt;/code&gt;’s documentation):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_pptx() %&amp;gt;%
    add_slide(layout = &amp;quot;Title and Content&amp;quot;, master = &amp;quot;Office Theme&amp;quot;) %&amp;gt;%
    ph_with_vg(code = print(one_plot), type = &amp;quot;body&amp;quot;) %&amp;gt;% 
    print(target = path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To map this over a list of arguments, I wrote a wrapper:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_pptx &amp;lt;- function(plot, path){
    if(!file.exists(path)) {
        out &amp;lt;- read_pptx()
    } else {
        out &amp;lt;- read_pptx(path)
    }
    
    out %&amp;gt;%
        add_slide(layout = &amp;quot;Title and Content&amp;quot;, master = &amp;quot;Office Theme&amp;quot;) %&amp;gt;%
        ph_with_vg(code = print(plot), type = &amp;quot;body&amp;quot;) %&amp;gt;% 
        print(target = path)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function takes two arguments, &lt;code&gt;plot&lt;/code&gt; and &lt;code&gt;path&lt;/code&gt;. &lt;code&gt;plot&lt;/code&gt; must be an plot object such as the ones
contained inside the &lt;code&gt;plots&lt;/code&gt; column of &lt;code&gt;my_plots&lt;/code&gt; tibble. &lt;code&gt;path&lt;/code&gt; is the path of where I want to save
the pptx.&lt;/p&gt;
&lt;p&gt;The first lines check if the file exists, if yes, the slides get added to the existing file, if not
a new pptx gets created. The rest of the code is very similar to the one from the documentation. Now,
to create my pptx I simple need to map over the &lt;code&gt;plots&lt;/code&gt; column and provide a &lt;code&gt;path&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(my_plots$plots, create_pptx, path = &amp;quot;test.pptx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]

## Warning in doc_parse_file(con, encoding = encoding, as_html = as_html,
## options = options): Failed to parse QName &amp;#39;xsi:xmlns:&amp;#39; [202]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx&amp;quot;
## 
## [[2]]
## [1] &amp;quot;/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx&amp;quot;
## 
## [[3]]
## [1] &amp;quot;/home/cbrunos/Documents/b-rodrigues.github.com/content/blog/test.pptx&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the end result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/editable_plots.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Inside Powerpoint (or in this case Libreoffice), the plots are geometric shapes that can now
be edited!&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How Luxembourguish residents spend their time: a small {flexdashboard} demo using the Time use survey data</title>
      <link>/blog/2018-09-15-time_use/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-09-15-time_use/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://brodriguesco.shinyapps.io/time_use_luxembourg/&#34;&gt;
&lt;img src=&#34;/img/time_use_dashboard.png&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In a &lt;a href=&#34;http://www.brodrigues.co/blog/2018-09-11-human_to_machine/&#34;&gt;previous blog post&lt;/a&gt; I have showed
how you could use the &lt;code&gt;{tidyxl}&lt;/code&gt; package to go from a human readable Excel Workbook to a tidy
data set (or flat file, as they are also called). Some people then contributed their solutions,
which is always something I really enjoy when it happens. This way, I also get to learn things!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/expersso&#34;&gt;&lt;code&gt;@expersso&lt;/code&gt;&lt;/a&gt; proposed a solution without &lt;code&gt;{tidyxl}&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Interesting data wrangling exercise in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt;. &lt;br&gt;My solution (without using {tidyxl}): &lt;a href=&#34;https://t.co/VjuOoM82yX&#34;&gt;https://t.co/VjuOoM82yX&lt;/a&gt; &lt;a href=&#34;https://t.co/VsXFyowigu&#34;&gt;https://t.co/VsXFyowigu&lt;/a&gt;
&lt;/p&gt;
— Eric (&lt;span class=&#34;citation&#34;&gt;@expersso&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/expersso/status/1039894727808757761?ref_src=twsrc%5Etfw&#34;&gt;September 12, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;&lt;a href=&#34;https://www.benstenhaug.com/&#34;&gt;Ben Stenhaug&lt;/a&gt; also proposed a solution on his &lt;a href=&#34;https://github.com/stenhaug/shared/blob/master/tidyxl_bruno_blog.md&#34;&gt;github&lt;/a&gt;
which is simpler than my code in a lot of ways!&lt;/p&gt;
&lt;p&gt;Update: &lt;a href=&#34;https://twitter.com/nacnudus&#34;&gt;&lt;code&gt;@nacnudus&lt;/code&gt;&lt;/a&gt; also contributed his own version using &lt;code&gt;{unpivotr}&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;en&#34;&gt;
&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;
Here&#39;s a version using unpivotr &lt;a href=&#34;https://t.co/l2hy6zCuKj&#34;&gt;https://t.co/l2hy6zCuKj&lt;/a&gt;
&lt;/p&gt;
— Duncan Garmonsway (&lt;span class=&#34;citation&#34;&gt;@nacnudus&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/nacnudus/status/1040905626317217792?ref_src=twsrc%5Etfw&#34;&gt;September 15, 2018&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;Now, it would be too bad not to further analyze this data. I’ve been wanting to play around with
the &lt;code&gt;{flexdashboard}&lt;/code&gt; package for some time now, but never really got the opportunity to do so.
The opportunity has now arrived. Using the cleaned data from the last post, I will further tweak
it a little bit, and then produce a very simple dashboard using &lt;code&gt;{flexdashboard}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you want to skip the rest of the blog post and go directly to the dashboard, just click &lt;a href=&#34;https://brodriguesco.shinyapps.io/time_use_luxembourg/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To make the data useful, I need to convert the strings that represent the amount of time spent
doing a task (for example “1:23”) to minutes. For this I use the &lt;code&gt;{chron}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_data &amp;lt;- clean_data %&amp;gt;%
    mutate(time_in_minutes = paste0(time, &amp;quot;:00&amp;quot;)) %&amp;gt;% # I need to add &amp;quot;:00&amp;quot; for the seconds else it won&amp;#39;t work
    mutate(time_in_minutes = 
               chron::hours(chron::times(time_in_minutes)) * 60 + 
               chron::minutes(chron::times(time_in_minutes)))

rio::export(clean_data, &amp;quot;clean_data.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to go! Below is the code to build the dashboard; if you want to try, you should
copy and paste the code inside a Rmd document:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;---
title: &amp;quot;Time Use Survey of Luxembourguish residents&amp;quot;
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
runtime: shiny
---

`` `{r setup, include=FALSE}
library(flexdashboard)
library(shiny)
library(tidyverse)
library(plotly)
library(ggthemes)

main_categories &amp;lt;- c(&amp;quot;Personal care&amp;quot;,
                     &amp;quot;Employment&amp;quot;,
                     &amp;quot;Study&amp;quot;,
                     &amp;quot;Household and family care&amp;quot;,
                     &amp;quot;Voluntary work and meetings&amp;quot;,
                     &amp;quot;Social life and entertainment&amp;quot;,
                     &amp;quot;Sports and outdoor activities&amp;quot;,
                     &amp;quot;Hobbies and games&amp;quot;,
                     &amp;quot;Media&amp;quot;,
                     &amp;quot;Travel&amp;quot;)

df &amp;lt;- read.csv(&amp;quot;clean_data.csv&amp;quot;) %&amp;gt;%
    rename(Population = population) %&amp;gt;%
    rename(Activities = activities)
`` `

Inputs {.sidebar}
-----------------------------------------------------------------------

`` `{r}

selectInput(inputId = &amp;quot;activitiesName&amp;quot;, 
            label = &amp;quot;Choose an activity&amp;quot;, 
            choices = unique(df$Activities))

selectInput(inputId = &amp;quot;dayName&amp;quot;, 
            label = &amp;quot;Choose a day&amp;quot;, 
            choices = unique(df$day), 
            selected = &amp;quot;Year 2014_Monday til Friday&amp;quot;)

selectInput(inputId = &amp;quot;populationName&amp;quot;, 
            label = &amp;quot;Choose a population&amp;quot;, 
            choices = unique(df$Population), 
            multiple = TRUE, selected = c(&amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;))

`` `

The Time Use Survey (TUS) aims to measure accurately how people allocate their time across different day-to-day activities. To this end, people are asked to keep records of all their activities in a time diary. For each activity, additional information is collected about whether or not the person was alone doing it or together with other persons, where did the activity take place, etc. The main studies on time use have been conducted to calculate indicators making possible comparative analysis of quality of life within the same population or between countries. International studies care more about specific activities such as work (unpaid or not), free time, leisure, personal care (including sleep), etc.
Source: http://statistiques.public.lu/en/surveys/espace-households/time-use/index.html

Layout based on https://jjallaire.shinyapps.io/shiny-biclust/

Row
-----------------------------------------------------------------------

### Minutes spent per day on certain activities
    
`` `{r}
dfInput &amp;lt;- reactive({
        df %&amp;gt;% filter(Activities == input$activitiesName,
                      Population %in% input$populationName,
                      day %in% input$dayName)
    })

    dfInput2 &amp;lt;- reactive({
        df %&amp;gt;% filter(Activities %in% main_categories,
                      Population %in% input$populationName,
                      day %in% input$dayName)
    })
    
  renderPlotly({

        df1 &amp;lt;- dfInput()

        p1 &amp;lt;- ggplot(df1, 
                     aes(x = Activities, y = time_in_minutes, fill = Population)) +
            geom_col(position = &amp;quot;dodge&amp;quot;) + 
            theme_minimal() + 
            xlab(&amp;quot;Activities&amp;quot;) + 
            ylab(&amp;quot;Time in minutes&amp;quot;) +
            scale_fill_gdocs()

        ggplotly(p1)})
`` `

Row 
-----------------------------------------------------------------------

### Proportion of the day spent on main activities
    
`` `{r}
renderPlotly({
    
       df2 &amp;lt;- dfInput2()
       
       p2 &amp;lt;- ggplot(df2, 
                   aes(x = Population, y = time_in_minutes, fill = Activities)) +
           geom_bar(stat=&amp;quot;identity&amp;quot;, position=&amp;quot;fill&amp;quot;) + 
            xlab(&amp;quot;Proportion&amp;quot;) + 
            ylab(&amp;quot;Proportion&amp;quot;) +
           theme_minimal() +
           scale_fill_gdocs()
       
       ggplotly(p2)
   })
`` `&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will see that I have defined the following atomic vector:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;main_categories &amp;lt;- c(&amp;quot;Personal care&amp;quot;,
                     &amp;quot;Employment&amp;quot;,
                     &amp;quot;Study&amp;quot;,
                     &amp;quot;Household and family care&amp;quot;,
                     &amp;quot;Voluntary work and meetings&amp;quot;,
                     &amp;quot;Social life and entertainment&amp;quot;,
                     &amp;quot;Sports and outdoor activities&amp;quot;,
                     &amp;quot;Hobbies and games&amp;quot;,
                     &amp;quot;Media&amp;quot;,
                     &amp;quot;Travel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you go back to the raw Excel file, you will see that these main categories are then split into
secondary activities. The first bar plot of the dashboard does not distinguish between the main and
secondary activities, whereas the second barplot only considers the main activities. I could
have added another column to the data that helped distinguish whether an activity was a main or secondary one,
but I was lazy. The source code of the dashboard is very simple as it uses R Markdown. To have
interactivity, I’ve used Shiny to dynamically filter the data, and built the plots with &lt;code&gt;{ggplot2}&lt;/code&gt;.
Finally, I’ve passed the plots to the &lt;code&gt;ggplotly()&lt;/code&gt; function from the &lt;code&gt;{plotly}&lt;/code&gt; package for some
quick and easy javascript goodness!&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going from a human readable Excel file to a machine-readable csv with {tidyxl}</title>
      <link>/blog/2018-09-11-human_to_machine/</link>
      <pubDate>Tue, 11 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-09-11-human_to_machine/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=m-PE3OkJ8XE&#34;&gt;
&lt;img src=&#34;/img/1a9.gif&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I won’t write a very long introduction; we all know that Excel is ubiquitous in business, and that
it has a lot of very nice features, especially for business practitioners that do not know any
programming. However, when people use Excel for purposes it was not designed for, it can be a
hassle. Often, people use Excel as a reporting tool, which it is not; they create very elaborated
and complicated spreadsheets that are human readable, but impossible to import within any other tool.&lt;/p&gt;
&lt;p&gt;In this blog post (which will probably be part of a series), I show you how you can go from this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/time_use.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;to this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/time_use2.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;You can find the data I will use &lt;a href=&#34;http://statistiques.public.lu/stat/ReportFolders/ReportFolder.aspx?IF_Language=eng&amp;amp;MainTheme=3&amp;amp;FldrName=1&amp;amp;RFPath=14306&#34;&gt;here&lt;/a&gt;. Click on the “Time use” folder and you can download the workbook.&lt;/p&gt;
&lt;p&gt;The Excel workbook contains several sheets (in French and English) of the amount of time Luxembourguish
citizens spend from Monday to Sunday. For example, on average, people that are in employment spend
almost 8 hours sleeping during the week days, and 8:45 hours on Saturday.&lt;/p&gt;
&lt;p&gt;As you can see from the screenshot, each sheet contains several tables that have lots of headers
and these tables are next to one another. Trying to import these sheets with good ol’ &lt;code&gt;readxl::read_excel()&lt;/code&gt;
produces a monster.&lt;/p&gt;
&lt;p&gt;This is where &lt;code&gt;{tidyxl}&lt;/code&gt; comes into play. Let’s import the workbook with &lt;code&gt;{tidyxl}&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(tidyxl)

time_use_xl &amp;lt;- xlsx_cells(&amp;quot;time-use.xlsx&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what happened:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(time_use_xl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 21
##   sheet address   row   col is_blank data_type error logical numeric
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;lgl&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;lgl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 Index A1          1     1 FALSE    character &amp;lt;NA&amp;gt;  NA           NA
## 2 Index B1          1     2 TRUE     blank     &amp;lt;NA&amp;gt;  NA           NA
## 3 Index C1          1     3 TRUE     blank     &amp;lt;NA&amp;gt;  NA           NA
## 4 Index D1          1     4 TRUE     blank     &amp;lt;NA&amp;gt;  NA           NA
## 5 Index E1          1     5 TRUE     blank     &amp;lt;NA&amp;gt;  NA           NA
## 6 Index F1          1     6 TRUE     blank     &amp;lt;NA&amp;gt;  NA           NA
## # … with 12 more variables: date &amp;lt;dttm&amp;gt;, character &amp;lt;chr&amp;gt;,
## #   character_formatted &amp;lt;list&amp;gt;, formula &amp;lt;chr&amp;gt;, is_array &amp;lt;lgl&amp;gt;,
## #   formula_ref &amp;lt;chr&amp;gt;, formula_group &amp;lt;int&amp;gt;, comment &amp;lt;chr&amp;gt;, height &amp;lt;dbl&amp;gt;,
## #   width &amp;lt;dbl&amp;gt;, style_format &amp;lt;chr&amp;gt;, local_format_id &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the sheet was imported, but the result might be unexpected. Actually, &lt;code&gt;time_use_xl&lt;/code&gt;
is a &lt;code&gt;tibble&lt;/code&gt; object, where each row is one cell of the Excel sheet. This might seem very complicated
to handle, but you will see that it actually makes things way easier.&lt;/p&gt;
&lt;p&gt;I only want to work on the English sheets so I use the following code to ignore the French ones:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sheets &amp;lt;- xlsx_sheet_names(&amp;quot;time-use.xlsx&amp;quot;) %&amp;gt;%
    keep(grepl(pattern = &amp;quot;.*day$&amp;quot;, .))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, there’s a sheet that aggregates the results for week days and weekends, which I also ignore.&lt;/p&gt;
&lt;p&gt;Now, to extract the tables from each sheet I wrote the following function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_data &amp;lt;- function(sheet){
    activities &amp;lt;- sheet %&amp;gt;%
        filter(col == 2) %&amp;gt;%
        select(row, character) %&amp;gt;%
        filter(row %in% seq(6,58)) %&amp;gt;%
        rename(activities = character) %&amp;gt;%
        select(-row)
    
    cols_to_extract &amp;lt;- sheet %&amp;gt;% 
        filter(grepl(&amp;quot;Population who completed.*&amp;quot;, character)) %&amp;gt;% 
        pull(col)
    
    headers_pos &amp;lt;- cols_to_extract - 1
    
    headers &amp;lt;- sheet %&amp;gt;%
        filter(col %in% headers_pos, row == 3) %&amp;gt;%
        pull(character)
    
    cols_to_extract %&amp;gt;% 
        map(~filter(sheet, col %in% .)) %&amp;gt;%
        map(~select(., sheet, address, row, col, character)) %&amp;gt;%
        map(~filter(., row %in% seq(6,58))) %&amp;gt;%
        map(~select(., character)) %&amp;gt;%
        map2(.x = ., .y = headers, ~mutate(.x, &amp;quot;population&amp;quot; = .y)) %&amp;gt;%
        map(., ~bind_cols(activities, .)) %&amp;gt;%
        bind_rows()
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s study it step by step and see how it works. First, there’s the argument, &lt;code&gt;sheet&lt;/code&gt;. This function
will be mapped to each sheet of the workbook. Then, the first block I wrote, extracts the
activities:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    activities &amp;lt;- sheet %&amp;gt;%
        filter(col == 2) %&amp;gt;%
        select(row, character) %&amp;gt;%
        filter(row %in% seq(6,58)) %&amp;gt;%
        rename(activities = character) %&amp;gt;%
        select(-row)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I only keep the second column (&lt;code&gt;filter(col == 2)&lt;/code&gt;); &lt;code&gt;col&lt;/code&gt; is a column of the &lt;code&gt;tibble&lt;/code&gt; and if you
look inside the workbook, you will notice that the activities are on the second column, or the B
column. Then, I select two columns, the &lt;code&gt;row&lt;/code&gt; and the &lt;code&gt;character&lt;/code&gt; column. &lt;code&gt;row&lt;/code&gt; is self-explanatory
and &lt;code&gt;character&lt;/code&gt; actually contains whatever is written inside the cells. Then, I only keep rows
6 to 58, because that is what interests me; the rest is either empty cells, or unneeded. Finally,
I rename the &lt;code&gt;character&lt;/code&gt; column to activities and remove the &lt;code&gt;row&lt;/code&gt; column.&lt;/p&gt;
&lt;p&gt;The second block:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    cols_to_extract &amp;lt;- sheet %&amp;gt;% 
        filter(grepl(&amp;quot;Population who completed.*&amp;quot;, character)) %&amp;gt;% 
        pull(col)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;returns the index of the columns I want to extract. I am only interested in the people that have
completed the activities, so using &lt;code&gt;grepl()&lt;/code&gt; inside &lt;code&gt;filter()&lt;/code&gt;, I located these columns, and use
&lt;code&gt;pull()&lt;/code&gt;… to pull them out of the data frame! &lt;code&gt;cols_to_extract&lt;/code&gt; is thus a nice atomic vector of
columns that I want to keep.&lt;/p&gt;
&lt;p&gt;In the third block, I extract the headers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    headers_pos &amp;lt;- cols_to_extract - 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why &lt;code&gt;- 1&lt;/code&gt;? This is because if you look in the Excel, you will see that the headers are one column before
the column labeled “People who completed the activity”. For example on column G, I have “People who completed the activity”
and on column F I have the header, in this case “Male”.&lt;/p&gt;
&lt;p&gt;Now I actually extract the headers:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    headers &amp;lt;- sheet %&amp;gt;%
        filter(col %in% headers_pos, row == 3) %&amp;gt;%
        pull(character)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Headers are always on the third row, but on different columns, hence the &lt;code&gt;col %in% headers_pos&lt;/code&gt;. I
then pull out the values inside the cells with &lt;code&gt;pull(character)&lt;/code&gt;. So my &lt;code&gt;headers&lt;/code&gt; object will be
an atomic vector with “All”, “Male”, “Female”, “10 - 19 years”, etc… everything on row 3.&lt;/p&gt;
&lt;p&gt;Finally, the last block, actually extracts the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    cols_to_extract %&amp;gt;% 
        map(~filter(sheet, col %in% .)) %&amp;gt;%
        map(~select(., sheet, address, row, col, character)) %&amp;gt;%
        map(~filter(., row %in% seq(6,58))) %&amp;gt;%
        map(~select(., character)) %&amp;gt;%
        map2(.x = ., .y = headers, ~mutate(.x, &amp;quot;population&amp;quot; = .y)) %&amp;gt;%
        map(., ~bind_cols(activities, .)) %&amp;gt;%
        bind_rows()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;cols_to_extract&lt;/code&gt; is a vector with the positions of the columns that interest me. So for example
“4”, “7”, “10” and so on. I map this vector to the sheet, which returns me a list of extracted
data frames. I pass this down to a &lt;code&gt;select()&lt;/code&gt; (which is inside &lt;code&gt;map()&lt;/code&gt;… why? Because the input
parameter is a list of data frames). So for each data frame inside the list, I select the columns
&lt;code&gt;sheet&lt;/code&gt;, &lt;code&gt;address&lt;/code&gt;, &lt;code&gt;row&lt;/code&gt;, &lt;code&gt;col&lt;/code&gt; and &lt;code&gt;character&lt;/code&gt;. Then, for each data frame inside the list,
I use &lt;code&gt;filter()&lt;/code&gt; to only keep the rows from position 6 to 58. Then, I only select the &lt;code&gt;character&lt;/code&gt;
column, which actually contains the text inside the cell. Then, using &lt;code&gt;map2()&lt;/code&gt;, I add the values
inside the &lt;code&gt;headers&lt;/code&gt; object as a new column, called &lt;code&gt;population&lt;/code&gt;. Then, I bind the &lt;code&gt;activities&lt;/code&gt;
column to the data frame and bind all the rows together.&lt;/p&gt;
&lt;p&gt;Time to use this function! Let’s see:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_data &amp;lt;- sheets %&amp;gt;%
    map(~filter(time_use_xl, sheet %in% .)) %&amp;gt;%
    set_names(sheets) %&amp;gt;%
    map(extract_data) %&amp;gt;%
    map2(.x = ., .y = sheets, ~mutate(.x, &amp;quot;day&amp;quot; = .y)) %&amp;gt;%
    bind_rows() %&amp;gt;%
    select(day, population, activities, time = character)

glimpse(clean_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 2,968
## Variables: 4
## $ day        &amp;lt;chr&amp;gt; &amp;quot;Year 2014_Monday til Friday&amp;quot;, &amp;quot;Year 2014_Monday til …
## $ population &amp;lt;chr&amp;gt; &amp;quot;All&amp;quot;, &amp;quot;All&amp;quot;, &amp;quot;All&amp;quot;, &amp;quot;All&amp;quot;, &amp;quot;All&amp;quot;, &amp;quot;All&amp;quot;, &amp;quot;All&amp;quot;, &amp;quot;All…
## $ activities &amp;lt;chr&amp;gt; &amp;quot;Personal care&amp;quot;, &amp;quot;Sleep&amp;quot;, &amp;quot;Eating&amp;quot;, &amp;quot;Other personal c…
## $ time       &amp;lt;chr&amp;gt; &amp;quot;11:07&amp;quot;, &amp;quot;08:26&amp;quot;, &amp;quot;01:47&amp;quot;, &amp;quot;00:56&amp;quot;, &amp;quot;07:37&amp;quot;, &amp;quot;07:47&amp;quot;,…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So I map my list of sheets to the &lt;code&gt;tibble&lt;/code&gt; I imported with &lt;code&gt;readxl&lt;/code&gt;, use &lt;code&gt;set_names&lt;/code&gt; to
name the elements of my list (which is superfluous, but I wanted to show this; might interest you!)
and then map this result to my little function. I could stop here,
but I then add a new column to each data frame that contains the day on which the data was
measured, bind the rows together and reorder the columns. Done!&lt;/p&gt;
&lt;p&gt;Now, how did I come up with this function? I did not start with a function. I started by writing
some code that did what I wanted for one table only, inside one sheet only. Only when I got
something that worked, did I start to generalize to several tables and then to several sheets. Most
of the time spent was actually in trying to find patterns in the Excel sheet that I could use
to write my function (for example noticing that the headers I wanted where always one column before
the column I was interested in). This is my advice when working with function programming; always
solve the issue for one element, wrap this code inside a function, and then simply map this function
to a list of elements!&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The year of the GNU&#43;Linux desktop is upon us: using user ratings of Steam Play compatibility to play around with regex and the tidyverse</title>
      <link>/blog/2018-09-08-steam_linux/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-09-08-steam_linux/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4QokOwvPxrE&#34;&gt;
&lt;img src=&#34;/img/want_to_believe.jpg&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I’ve been using GNU+Linux distros for about 10 years now, and have settled for openSUSE as my main operating system around 3 years ago, perhaps even more. If you’re a gamer, you might have heard about SteamOS
and how more and more games are available on GNU+Linux. I don’t really care about
games, I play the occasional one (currently &lt;a href=&#34;http://www.tangledeep.com/&#34;&gt;Tangledeep&lt;/a&gt;) when I find
the time, but still follow the news about gaming on GNU+Linux. Last week,
Valve announced something quite big; it is now possible to run Windows games on GNU+Linux directly
from Steam, using a modified version of &lt;a href=&#34;https://en.wikipedia.org/wiki/Wine_(software)&#34;&gt;Wine&lt;/a&gt;
they call Proton. The feature is still in Beta, and Valve announced that they guarantee around
30 games to work already flawlessly. Of course, people have tried running a lot of other games, and,
as was to be expected from Free Software and Open Source fans, GNU+Linux gamers created a Google Sheet
that lists which games were tried and how they run. You can take a look at the sheet &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1DcZZQ4HL_Ol969UbXJmFG8TzOHNnHoj8Q1f8DIFe8-8/htmlview?sle=true&amp;amp;pru=AAABZbqTTkc*IvT11ShwA2kjoe_4lPefiQ#gid=1003113831&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post, I will play around with this sheet. This blog post lists some &lt;code&gt;{tidyverse}&lt;/code&gt; tricks
I find useful and use often. Perhaps these tricks will be useful to you too! Let’s start by loading
the needed packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(magrittr)
library(readxl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since I’m lazy and don’t want to type the whole name of the file I’ll be using some little regex:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam &amp;lt;- read_excel(Sys.glob(&amp;quot;Steam*&amp;quot;), sheet = &amp;quot;Main&amp;quot;, skip = 2)

glimpse(steam)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 8,570
## Variables: 9
## $ SteamDB   &amp;lt;chr&amp;gt; &amp;quot;LINK&amp;quot;, &amp;quot;LINK&amp;quot;, &amp;quot;LINK&amp;quot;, &amp;quot;LINK&amp;quot;, &amp;quot;LINK&amp;quot;, &amp;quot;LINK&amp;quot;, &amp;quot;LINK&amp;quot;…
## $ Game      &amp;lt;chr&amp;gt; &amp;quot;64&amp;quot;, &amp;quot;1849&amp;quot;, &amp;quot;1982&amp;quot;, &amp;quot;1982&amp;quot;, &amp;quot;am Weapon: Revival&amp;quot;, &amp;quot;.…
## $ Submitted &amp;lt;chr&amp;gt; &amp;quot;5 days ago&amp;quot;, &amp;quot;12 days ago&amp;quot;, &amp;quot;11 days ago&amp;quot;, &amp;quot;11 days a…
## $ Status    &amp;lt;chr&amp;gt; &amp;quot;Garbage&amp;quot;, &amp;quot;Platinum&amp;quot;, &amp;quot;Gold&amp;quot;, &amp;quot;Platinum&amp;quot;, &amp;quot;Platinum&amp;quot;,…
## $ Notes     &amp;lt;chr&amp;gt; &amp;quot;Crashes with a debug log&amp;quot;, &amp;quot;Plays OK.&amp;quot;, &amp;quot;Gamepad supp…
## $ Distro    &amp;lt;chr&amp;gt; &amp;quot;Arch (4.18.5)&amp;quot;, &amp;quot;Manjaro XFCE&amp;quot;, &amp;quot;Gentoo AMD64 (Kernel…
## $ Driver    &amp;lt;chr&amp;gt; &amp;quot;Nvidia 396.54 / Intel xf86-video-intel (1:2.99.917+83…
## $ Specs     &amp;lt;chr&amp;gt; &amp;quot;Intel Core i7-7700HQ / Nvidia GTX 1050 (Mobile)&amp;quot;, &amp;quot;Ry…
## $ Proton    &amp;lt;chr&amp;gt; &amp;quot;3.7 Beta&amp;quot;, &amp;quot;3.7-4 Beta&amp;quot;, &amp;quot;3.7-4 Beta&amp;quot;, &amp;quot;Default&amp;quot;, &amp;quot;3.…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s count how many unique games are in the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;gt;%
    count(Game)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,855 x 2
##    Game                                                                   n
##    &amp;lt;chr&amp;gt;                                                              &amp;lt;int&amp;gt;
##  1 .hack//G.U. Last Recode                                                2
##  2 $1 Ride                                                                1
##  3 0rbitalis                                                              1
##  4 10 Second Ninja                                                        4
##  5 100% Orange Juice                                                     17
##  6 1000 Amps                                                              3
##  7 12 Labours of Hercules VII: Fleecing the Fleece (Platinum Edition)     1
##  8 16bit trader                                                           1
##  9 1849                                                                   1
## 10 1953 - KGB Unleased                                                    1
## # … with 3,845 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s quite a lot of games! However, not everyone of them is playable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;gt;%
    count(Status)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 2
##   Status       n
##   &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
## 1 Borked     205
## 2 bronze       1
## 3 Bronze     423
## 4 Garbage   2705
## 5 Gold       969
## 6 Platinum  2596
## 7 Primary      1
## 8 Silver    1670&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Around 2500 have the status “Platinum”, but some games might have more than one status:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;gt;%
    filter(Game == &amp;quot;100% Orange Juice&amp;quot;) %&amp;gt;%
    count(Status)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   Status       n
##   &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt;
## 1 Bronze       5
## 2 Garbage      3
## 3 Gold         2
## 4 Platinum     6
## 5 Silver       1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More games run like &lt;em&gt;Garbage&lt;/em&gt; than &lt;em&gt;Platinum&lt;/em&gt;. But perhaps we can dig a little deeper and see if
we find some patterns.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the GNU+Linux distros:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;gt;%
    count(Distro) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,085 x 2
##    Distro                                         n
##    &amp;lt;chr&amp;gt;                                      &amp;lt;int&amp;gt;
##  1 &amp;lt;NA&amp;gt;                                           1
##  2 ?                                              2
##  3 &amp;quot;\&amp;quot;Arch Linux\&amp;quot; (64 bit)&amp;quot;                      1
##  4 &amp;quot;\&amp;quot;Linux Mint 18.3 Sylvia 64bit&amp;quot;               1
##  5 &amp;quot;\&amp;quot;Manjaro Stable 64-bit (Kernel 4.14.66)&amp;quot;     1
##  6 &amp;quot;\&amp;quot;Solus\&amp;quot; (64 bit)&amp;quot;                           2
##  7 (K)ubuntu 18.04 64-bit (Kernel 4.15.0)         2
##  8 (L)Ubuntu 18.04.1 LTS                          1
##  9 18.04.1                                        1
## 10 18.04.1 LTS                                    2
## # … with 2,075 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok the distro column is pretty messy. Let’s try to bring some order to it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;lt;&amp;gt;%
    mutate(distribution = as_factor(case_when(
        grepl(&amp;quot;buntu|lementary|antergos|steam|mint|18.|pop|neon&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;Ubuntu&amp;quot;,
        grepl(&amp;quot;arch|manjaro&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;Arch Linux&amp;quot;,
        grepl(&amp;quot;gentoo&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;Gentoo&amp;quot;,
        grepl(&amp;quot;fedora&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;Fedora&amp;quot;,
        grepl(&amp;quot;suse&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;openSUSE&amp;quot;,
        grepl(&amp;quot;debian|sid|stretch|lmde&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;Debian&amp;quot;,
        grepl(&amp;quot;solus&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;Solus&amp;quot;,
        grepl(&amp;quot;slackware&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;Slackware&amp;quot;,
        grepl(&amp;quot;void&amp;quot;, Distro, ignore.case = TRUE) ~ &amp;quot;Void Linux&amp;quot;,
        TRUE ~ &amp;quot;Other&amp;quot;
    )))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;%&amp;lt;&amp;gt;%&lt;/code&gt; operator is shorthand for &lt;code&gt;a &amp;lt;- a %&amp;gt;% f()&lt;/code&gt;. It passes &lt;code&gt;a&lt;/code&gt; to &lt;code&gt;f()&lt;/code&gt; and assigns the
result back to &lt;code&gt;a&lt;/code&gt;. Anyways, let’s take a look at the &lt;code&gt;distribution&lt;/code&gt; column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;gt;%
    count(distribution)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    distribution     n
##    &amp;lt;fct&amp;gt;        &amp;lt;int&amp;gt;
##  1 Ubuntu        6632
##  2 Arch Linux     805
##  3 Solus          175
##  4 Debian         359
##  5 Fedora         355
##  6 Gentoo          42
##  7 Void Linux      38
##  8 Other           76
##  9 openSUSE        66
## 10 Slackware       22&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will group distributions that have less than 100 occurrences into a single category
(meaning I will keep the 5 more common values):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;lt;&amp;gt;%
    mutate(distribution = fct_lump(distribution, n = 5, other_level = &amp;quot;Other&amp;quot;)) 

steam %&amp;gt;%
    count(distribution)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   distribution     n
##   &amp;lt;fct&amp;gt;        &amp;lt;int&amp;gt;
## 1 Ubuntu        6632
## 2 Arch Linux     805
## 3 Solus          175
## 4 Debian         359
## 5 Fedora         355
## 6 Other          244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s do the same for the CPUs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;lt;&amp;gt;%
    mutate(CPU = as_factor(case_when(
        grepl(&amp;quot;intel|i\\d|xeon|core2|\\d{4}k|q\\d{4}|pentium&amp;quot;, Specs, ignore.case = TRUE) ~ &amp;quot;Intel&amp;quot;,
        grepl(&amp;quot;ryzen|threadripper|tr|amd|fx|r\\d|\\d{4}x|phenom&amp;quot;, Specs, ignore.case = TRUE) ~ &amp;quot;AMD&amp;quot;,
        TRUE ~ NA_character_
    )))

steam %&amp;gt;%
    count(CPU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Factor `CPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   CPU       n
##   &amp;lt;fct&amp;gt; &amp;lt;int&amp;gt;
## 1 Intel  5768
## 2 AMD    2319
## 3 &amp;lt;NA&amp;gt;    483&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the same for the GPUs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;lt;&amp;gt;%
    mutate(GPU = as_factor(case_when(
        grepl(&amp;quot;nvidia|geforce|3\\d{2}|nouveau|gtx|gt\\s?\\d{1,}|9\\d0|1060|1070|1080&amp;quot;, Specs, ignore.case = TRUE) ~ &amp;quot;Nvidia&amp;quot;,
        grepl(&amp;quot;amd|radeon|ati|rx|vega|r9&amp;quot;, Specs, ignore.case = TRUE) ~ &amp;quot;AMD&amp;quot;,
        grepl(&amp;quot;intel|igpu|integrated|hd\\d{4}|hd\\sgraphics&amp;quot;, Specs, ignore.case = TRUE) ~ &amp;quot;Intel&amp;quot;,
        TRUE ~ NA_character_
    )))

steam %&amp;gt;%
    count(GPU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Factor `GPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   GPU        n
##   &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;
## 1 Nvidia  6086
## 2 AMD     1374
## 3 Intel    413
## 4 &amp;lt;NA&amp;gt;     697&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will also add a rank for the &lt;code&gt;Status&lt;/code&gt; column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;lt;&amp;gt;%
    mutate(rank_status = case_when(
        Status == &amp;quot;Platinum&amp;quot; ~ 5,
        Status == &amp;quot;Gold&amp;quot; ~ 4,
        Status == &amp;quot;Silver&amp;quot; ~ 3,
        Status == &amp;quot;Bronze&amp;quot; ~ 2,
        Status == &amp;quot;Garbage&amp;quot; ~ 1
    ))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, what are the top 5 most frequent combinations of Status, distribution, CPU and GPU?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;gt;%
    filter(!is.na(CPU), !is.na(GPU)) %&amp;gt;%
    count(Status, distribution, CPU, GPU) %&amp;gt;%
    mutate(total = sum(n)) %&amp;gt;%
    mutate(freq = n / total) %&amp;gt;%
    top_n(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Selecting by freq&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 7
##   Status   distribution CPU   GPU        n total   freq
##   &amp;lt;chr&amp;gt;    &amp;lt;fct&amp;gt;        &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 Garbage  Ubuntu       Intel Nvidia  1025  7443 0.138 
## 2 Gold     Ubuntu       Intel Nvidia   361  7443 0.0485
## 3 Platinum Ubuntu       Intel Nvidia  1046  7443 0.141 
## 4 Platinum Ubuntu       AMD   Nvidia   338  7443 0.0454
## 5 Silver   Ubuntu       Intel Nvidia   650  7443 0.0873&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, Ubuntu, or distributions using Ubuntu as a base, are the most popular ones. Nvidia
is the most popular GPU, Intel for CPUs and in most cases, this combo of hardware and distribution
is associated with positive ratings (even though there are almost as many “Garbage” ratings than
“Platinum” ratings).&lt;/p&gt;
&lt;p&gt;Now let’s compute some dumb averages of Statuses by distribution, CPU and GPU. Since I’m going
to run the same computation three times, I’ll write a function to do that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_avg &amp;lt;- function(dataset, var){
    var &amp;lt;- enquo(var)
    dataset %&amp;gt;%
        select(rank_status, (!!var)) %&amp;gt;%
        group_by((!!var)) %&amp;gt;%
        mutate(wt = n()) %&amp;gt;%
        summarise(average_rating = weighted.mean(rank_status, (!!var), wt, na.rm = TRUE))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see now if we can rank distribution by Steam play rating:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_avg(steam, distribution)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   distribution average_rating
##   &amp;lt;fct&amp;gt;                 &amp;lt;dbl&amp;gt;
## 1 Ubuntu                 3.03
## 2 Arch Linux             3.05
## 3 Solus                  3.03
## 4 Debian                 3.01
## 5 Fedora                 3.07
## 6 Other                  3.16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How about for hardware?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_avg(steam, GPU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Factor `GPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`

## Warning: Factor `GPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 2
##   GPU    average_rating
##   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 Nvidia           3.07
## 2 AMD              2.90
## 3 Intel            3.01
## 4 &amp;lt;NA&amp;gt;            NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;compute_avg(steam, CPU)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Factor `CPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`

## Warning: Factor `CPU` contains implicit NA, consider using
## `forcats::fct_explicit_na`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   CPU   average_rating
##   &amp;lt;fct&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 Intel           3.03
## 2 AMD             3.06
## 3 &amp;lt;NA&amp;gt;           NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To wrap this up, what are the games with the most ratings? Perhaps this can give us a hint about which
games GNU+Linux users prefer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;steam %&amp;gt;%
    count(Game) %&amp;gt;%
    top_n(10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Selecting by n&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##    Game                              n
##    &amp;lt;chr&amp;gt;                         &amp;lt;int&amp;gt;
##  1 Age of Empires II: HD Edition    43
##  2 Borderlands                      39
##  3 DiRT 3 Complete Edition          32
##  4 DOOM                             62
##  5 Fallout: New Vegas               45
##  6 Grim Dawn                        34
##  7 No Man&amp;#39;s Sky                     40
##  8 Path of Exile                    35
##  9 Quake Champions                  32
## 10 The Elder Scrolls V: Skyrim      46&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I actually laughed out loud when I saw that DOOM was the game with the most ratings! What else
was I expecting, really.&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dealing with heteroskedasticity; regression with robust standard errors using R</title>
      <link>/blog/2018-07-08-rob_stderr/</link>
      <pubDate>Sun, 08 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07-08-rob_stderr/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/sandwich/index.html&#34;&gt;
&lt;img src=&#34;/img/bread-breakfast-bun-5678.jpg&#34; width=&#34;640&#34; height=&#34;360&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;First of all, is it heteros&lt;strong&gt;k&lt;/strong&gt;edasticity or heteros&lt;strong&gt;c&lt;/strong&gt;edasticity? According to
&lt;a href=&#34;https://www.jstor.org/stable/1911250&#34;&gt;McCulloch (1985)&lt;/a&gt;,
heteros&lt;strong&gt;k&lt;/strong&gt;edasticity is the proper spelling, because when transliterating Greek words, scientists
use the Latin letter k in place of the Greek letter κ (kappa). κ sometimes is transliterated as
the Latin letter c, but only when these words entered the English language through French, such
as scepter.&lt;/p&gt;
&lt;p&gt;Now that this is out of the way, we can get to the meat of this blogpost (foreshadowing pun).
A random variable is said to be heteroskedastic, if its variance is not constant. For example,
the variability of expenditures may increase with income. Richer families may spend a similar
amount on groceries as poorer people, but some rich families will sometimes buy expensive
items such as lobster. The variability of expenditures for rich families is thus quite large.
However, the expenditures on food of poorer families, who cannot afford lobster, will not vary much.
Heteroskedasticity can also appear when data is clustered; for example, variability of
expenditures on food may vary from city to city, but is quite constant within a city.&lt;/p&gt;
&lt;p&gt;To illustrate this, let’s first load all the packages needed for this blog post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(robustbase)
library(tidyverse)
library(sandwich)
library(lmtest)
library(modelr)
library(broom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, let’s load and prepare the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;education&amp;quot;)

education &amp;lt;- education %&amp;gt;% 
    rename(residents = X1,
           per_capita_income = X2,
           young_residents = X3,
           per_capita_exp = Y,
           state = State) %&amp;gt;% 
    mutate(region = case_when(
        Region == 1 ~ &amp;quot;northeast&amp;quot;,
        Region == 2 ~ &amp;quot;northcenter&amp;quot;,
        Region == 3 ~ &amp;quot;south&amp;quot;,
        Region == 4 ~ &amp;quot;west&amp;quot;
    )) %&amp;gt;% 
    select(-Region)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will be using the &lt;code&gt;education&lt;/code&gt; data set from the &lt;code&gt;{robustbase}&lt;/code&gt; package. I renamed some columns
and changed the values of the &lt;code&gt;Region&lt;/code&gt; column. Now, let’s do a scatterplot of per capita expenditures
on per capita income:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(education, aes(per_capita_income, per_capita_exp)) + 
    geom_point() +
    theme_dark()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-07-08-rob_stderr_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It would seem that, as income increases, variability of expenditures increases too. Let’s look
at the same plot by &lt;code&gt;region&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(education, aes(per_capita_income, per_capita_exp)) + 
    geom_point() + 
    facet_wrap(~region) + 
    theme_dark()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-07-08-rob_stderr_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I don’t think this shows much; it would seem that observations might be clustered, but there are
not enough observations to draw any conclusion from this plot (in any case, drawing conclusions
from only plots is dangerous).&lt;/p&gt;
&lt;p&gt;Let’s first run a good ol’ linear regression:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmfit &amp;lt;- lm(per_capita_exp ~ region + residents + young_residents + per_capita_income, data = education)

summary(lmfit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = per_capita_exp ~ region + residents + young_residents + 
##     per_capita_income, data = education)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -77.963 -25.499  -2.214  17.618  89.106 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)       -467.40283  142.57669  -3.278 0.002073 ** 
## regionnortheast     15.72741   18.16260   0.866 0.391338    
## regionsouth          7.08742   17.29950   0.410 0.684068    
## regionwest          34.32416   17.49460   1.962 0.056258 .  
## residents           -0.03456    0.05319  -0.650 0.519325    
## young_residents      1.30146    0.35717   3.644 0.000719 ***
## per_capita_income    0.07204    0.01305   5.520 1.82e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 39.88 on 43 degrees of freedom
## Multiple R-squared:  0.6292, Adjusted R-squared:  0.5774 
## F-statistic: 12.16 on 6 and 43 DF,  p-value: 6.025e-08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s test for heteroskedasticity using the Breusch-Pagan test that you can find in the &lt;code&gt;{lmtest}&lt;/code&gt;
package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bptest(lmfit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  studentized Breusch-Pagan test
## 
## data:  lmfit
## BP = 17.921, df = 6, p-value = 0.006432&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This test shows that we can reject the null that the variance of the residuals is constant,
thus heteroskedacity is present. To get the correct standard errors, we can use the &lt;code&gt;vcovHC()&lt;/code&gt;
function from the &lt;code&gt;{sandwich}&lt;/code&gt; package (hence the choice for the header picture of this post):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmfit %&amp;gt;% 
    vcovHC() %&amp;gt;% 
    diag() %&amp;gt;% 
    sqrt()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       (Intercept)   regionnortheast       regionsouth        regionwest 
##      311.31088691       25.30778221       23.56106307       24.12258706 
##         residents   young_residents per_capita_income 
##        0.09184368        0.68829667        0.02999882&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default &lt;code&gt;vcovHC()&lt;/code&gt; estimates a heteroskedasticity consistent (HC) variance covariance
matrix for the parameters. There are several ways to estimate such a HC matrix, and by default
&lt;code&gt;vcovHC()&lt;/code&gt; estimates the “HC3” one. You can refer to &lt;a href=&#34;https://www.jstatsoft.org/article/view/v011i10&#34;&gt;Zeileis (2004)&lt;/a&gt;
for more details.&lt;/p&gt;
&lt;p&gt;We see that the standard errors are much larger than before! The intercept and &lt;code&gt;regionwest&lt;/code&gt; variables
are not statistically significant anymore.&lt;/p&gt;
&lt;p&gt;You can achieve the same in one single step:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coeftest(lmfit, vcov = vcovHC(lmfit))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## t test of coefficients:
## 
##                      Estimate  Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)       -467.402827  311.310887 -1.5014  0.14056  
## regionnortheast     15.727405   25.307782  0.6214  0.53759  
## regionsouth          7.087424   23.561063  0.3008  0.76501  
## regionwest          34.324157   24.122587  1.4229  0.16198  
## residents           -0.034558    0.091844 -0.3763  0.70857  
## young_residents      1.301458    0.688297  1.8908  0.06540 .
## per_capita_income    0.072036    0.029999  2.4013  0.02073 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s is also easy to change the estimation method for the variance-covariance matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coeftest(lmfit, vcov = vcovHC(lmfit, type = &amp;quot;HC0&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## t test of coefficients:
## 
##                      Estimate  Std. Error t value  Pr(&amp;gt;|t|)    
## (Intercept)       -467.402827  172.577569 -2.7084  0.009666 ** 
## regionnortheast     15.727405   20.488148  0.7676  0.446899    
## regionsouth          7.087424   17.755889  0.3992  0.691752    
## regionwest          34.324157   19.308578  1.7777  0.082532 .  
## residents           -0.034558    0.054145 -0.6382  0.526703    
## young_residents      1.301458    0.387743  3.3565  0.001659 ** 
## per_capita_income    0.072036    0.016638  4.3296 8.773e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I wrote above, by default, the &lt;code&gt;type&lt;/code&gt; argument is equal to “HC3”.&lt;/p&gt;
&lt;p&gt;Another way of dealing with heteroskedasticity is to use the &lt;code&gt;lmrob()&lt;/code&gt; function from the
&lt;code&gt;{robustbase}&lt;/code&gt; package. This package is quite interesting, and offers quite a lot of functions
for robust linear, and nonlinear, regression models. Running a robust linear regression
is just the same as with &lt;code&gt;lm()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmrobfit &amp;lt;- lmrob(per_capita_exp ~ region + residents + young_residents + per_capita_income, 
                  data = education)

summary(lmrobfit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lmrob(formula = per_capita_exp ~ region + residents + young_residents + per_capita_income, 
##     data = education)
##  \--&amp;gt; method = &amp;quot;MM&amp;quot;
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -57.074 -14.803  -0.853  24.154 174.279 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)       -156.37169  132.73828  -1.178  0.24526   
## regionnortheast     20.64576   26.45378   0.780  0.43940   
## regionsouth         10.79695   29.42746   0.367  0.71549   
## regionwest          45.22589   33.07950   1.367  0.17867   
## residents            0.03406    0.04412   0.772  0.44435   
## young_residents      0.57896    0.25512   2.269  0.02832 * 
## per_capita_income    0.04328    0.01442   3.000  0.00447 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Robust residual standard error: 26.4 
## Multiple R-squared:  0.6235, Adjusted R-squared:  0.571 
## Convergence in 24 IRWLS iterations
## 
## Robustness weights: 
##  observation 50 is an outlier with |weight| = 0 ( &amp;lt; 0.002); 
##  7 weights are ~= 1. The remaining 42 ones are summarized as
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## 0.05827 0.85200 0.93870 0.85250 0.98700 0.99790 
## Algorithmic parameters: 
##        tuning.chi                bb        tuning.psi        refine.tol 
##         1.548e+00         5.000e-01         4.685e+00         1.000e-07 
##           rel.tol         scale.tol         solve.tol       eps.outlier 
##         1.000e-07         1.000e-10         1.000e-07         2.000e-03 
##             eps.x warn.limit.reject warn.limit.meanrw 
##         1.071e-08         5.000e-01         5.000e-01 
##      nResample         max.it       best.r.s       k.fast.s          k.max 
##            500             50              2              1            200 
##    maxit.scale      trace.lev            mts     compute.rd fast.s.large.n 
##            200              0           1000              0           2000 
##                   psi           subsampling                   cov 
##            &amp;quot;bisquare&amp;quot;         &amp;quot;nonsingular&amp;quot;         &amp;quot;.vcov.avar1&amp;quot; 
## compute.outlier.stats 
##                  &amp;quot;SM&amp;quot; 
## seed : int(0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This however, gives you different estimates than when fitting a linear regression model.
The estimates should be the same, only the standard errors should be different. This is because
the estimation method is different, and is also robust to outliers (at least that’s my understanding,
I haven’t read the theoretical papers behind the package yet).&lt;/p&gt;
&lt;p&gt;Finally, it is also possible to bootstrap the standard errors. For this I will use the
&lt;code&gt;bootstrap()&lt;/code&gt; function from the &lt;code&gt;{modelr}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;resamples &amp;lt;- 100

boot_education &amp;lt;- education %&amp;gt;% 
 modelr::bootstrap(resamples)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the &lt;code&gt;boot_education&lt;/code&gt; object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boot_education&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 100 x 2
##    strap      .id  
##    &amp;lt;list&amp;gt;     &amp;lt;chr&amp;gt;
##  1 &amp;lt;resample&amp;gt; 001  
##  2 &amp;lt;resample&amp;gt; 002  
##  3 &amp;lt;resample&amp;gt; 003  
##  4 &amp;lt;resample&amp;gt; 004  
##  5 &amp;lt;resample&amp;gt; 005  
##  6 &amp;lt;resample&amp;gt; 006  
##  7 &amp;lt;resample&amp;gt; 007  
##  8 &amp;lt;resample&amp;gt; 008  
##  9 &amp;lt;resample&amp;gt; 009  
## 10 &amp;lt;resample&amp;gt; 010  
## # … with 90 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column &lt;code&gt;strap&lt;/code&gt; contains resamples of the original data. I will run my linear regression
from before on each of the resamples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
    boot_lin_reg &amp;lt;- boot_education %&amp;gt;% 
        mutate(regressions = 
                   map(strap, 
                       ~lm(per_capita_exp ~ region + residents + 
                               young_residents + per_capita_income, 
                           data = .))) 
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 100 x 3
##    strap      .id   regressions
##    &amp;lt;list&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;list&amp;gt;     
##  1 &amp;lt;resample&amp;gt; 001   &amp;lt;lm&amp;gt;       
##  2 &amp;lt;resample&amp;gt; 002   &amp;lt;lm&amp;gt;       
##  3 &amp;lt;resample&amp;gt; 003   &amp;lt;lm&amp;gt;       
##  4 &amp;lt;resample&amp;gt; 004   &amp;lt;lm&amp;gt;       
##  5 &amp;lt;resample&amp;gt; 005   &amp;lt;lm&amp;gt;       
##  6 &amp;lt;resample&amp;gt; 006   &amp;lt;lm&amp;gt;       
##  7 &amp;lt;resample&amp;gt; 007   &amp;lt;lm&amp;gt;       
##  8 &amp;lt;resample&amp;gt; 008   &amp;lt;lm&amp;gt;       
##  9 &amp;lt;resample&amp;gt; 009   &amp;lt;lm&amp;gt;       
## 10 &amp;lt;resample&amp;gt; 010   &amp;lt;lm&amp;gt;       
## # … with 90 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have added a new column called &lt;code&gt;regressions&lt;/code&gt; which contains the linear regressions on each
bootstrapped sample. Now, I will create a list of tidied regression results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
    tidied &amp;lt;- boot_lin_reg %&amp;gt;% 
        mutate(tidy_lm = 
                   map(regressions, broom::tidy))
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 100 x 4
##    strap      .id   regressions tidy_lm         
##    &amp;lt;list&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;list&amp;gt;      &amp;lt;list&amp;gt;          
##  1 &amp;lt;resample&amp;gt; 001   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
##  2 &amp;lt;resample&amp;gt; 002   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
##  3 &amp;lt;resample&amp;gt; 003   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
##  4 &amp;lt;resample&amp;gt; 004   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
##  5 &amp;lt;resample&amp;gt; 005   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
##  6 &amp;lt;resample&amp;gt; 006   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
##  7 &amp;lt;resample&amp;gt; 007   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
##  8 &amp;lt;resample&amp;gt; 008   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
##  9 &amp;lt;resample&amp;gt; 009   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
## 10 &amp;lt;resample&amp;gt; 010   &amp;lt;lm&amp;gt;        &amp;lt;tibble [7 × 5]&amp;gt;
## # … with 90 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;broom::tidy()&lt;/code&gt; creates a data frame of the regression results. Let’s look at one of these:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidied$tidy_lm[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 5
##   term              estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)       -571.     109.        -5.22  4.92e- 6
## 2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3
## 3 regionsouth        -21.3     15.1       -1.41  1.66e- 1
## 4 regionwest           1.88    13.9        0.135 8.93e- 1
## 5 residents           -0.134    0.0608    -2.21  3.28e- 2
## 6 young_residents      1.50     0.308      4.89  1.47e- 5
## 7 per_capita_income    0.100    0.0125     8.06  3.85e-10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This format is easier to handle than the standard &lt;code&gt;lm()&lt;/code&gt; output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidied$regressions[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = per_capita_exp ~ region + residents + young_residents + 
##     per_capita_income, data = .)
## 
## Coefficients:
##       (Intercept)    regionnortheast        regionsouth  
##         -571.0568           -48.0018           -21.3019  
##        regionwest          residents    young_residents  
##            1.8808            -0.1341             1.5042  
## per_capita_income  
##            0.1005&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I have all these regression results, I can compute any statistic I need. But first,
let’s transform the data even further:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_mods &amp;lt;- tidied %&amp;gt;% 
    pull(tidy_lm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;list_mods&lt;/code&gt; is a list of the &lt;code&gt;tidy_lm&lt;/code&gt; data frames. I now add an index and
bind the rows together (by using &lt;code&gt;map2_df()&lt;/code&gt; instead of &lt;code&gt;map2()&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mods_df &amp;lt;- map2_df(list_mods, 
                   seq(1, resamples), 
                   ~mutate(.x, resample = .y))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the final object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(mods_df, 25)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 25 x 6
##    term              estimate std.error statistic  p.value resample
##    &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;
##  1 (Intercept)       -571.     109.        -5.22  4.92e- 6        1
##  2 regionnortheast    -48.0     17.2       -2.80  7.71e- 3        1
##  3 regionsouth        -21.3     15.1       -1.41  1.66e- 1        1
##  4 regionwest           1.88    13.9        0.135 8.93e- 1        1
##  5 residents           -0.134    0.0608    -2.21  3.28e- 2        1
##  6 young_residents      1.50     0.308      4.89  1.47e- 5        1
##  7 per_capita_income    0.100    0.0125     8.06  3.85e-10        1
##  8 (Intercept)        -97.2    145.        -0.672 5.05e- 1        2
##  9 regionnortheast     -1.48    10.8       -0.136 8.92e- 1        2
## 10 regionsouth         12.5     11.4        1.09  2.82e- 1        2
## # … with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now this is a very useful format, because I now can group by the &lt;code&gt;term&lt;/code&gt; column and compute any
statistics I need, in the present case the standard deviation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(
    r.std.error &amp;lt;- mods_df %&amp;gt;% 
        group_by(term) %&amp;gt;% 
        summarise(r.std.error = sd(estimate))
)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   term              r.std.error
##   &amp;lt;chr&amp;gt;                   &amp;lt;dbl&amp;gt;
## 1 (Intercept)          220.    
## 2 per_capita_income      0.0197
## 3 regionnortheast       24.5   
## 4 regionsouth           21.1   
## 5 regionwest            22.7   
## 6 residents              0.0607
## 7 young_residents        0.498&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can append this column to the linear regression model result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmfit %&amp;gt;% 
    broom::tidy() %&amp;gt;% 
    full_join(r.std.error) %&amp;gt;% 
    select(term, estimate, std.error, r.std.error)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;term&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 4
##   term               estimate std.error r.std.error
##   &amp;lt;chr&amp;gt;                 &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1 (Intercept)       -467.      143.        220.    
## 2 regionnortheast     15.7      18.2        24.5   
## 3 regionsouth          7.09     17.3        21.1   
## 4 regionwest          34.3      17.5        22.7   
## 5 residents           -0.0346    0.0532      0.0607
## 6 young_residents      1.30      0.357       0.498 
## 7 per_capita_income    0.0720    0.0131      0.0197&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, using the whole bootstrapping procedure is longer than simply using either one of
the first two methods. However, this procedure is very flexible and can thus be adapted to a very
large range of situations. Either way, in the case of heteroskedasticity, you can see that
results vary a lot depending on the procedure you use, so I would advise to use them all as
robustness tests and discuss the differences.&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Missing data imputation and instrumental variables regression: the tidy approach</title>
      <link>/blog/2018-07-01-tidy_ive/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-07-01-tidy_ive/</guid>
      <description>&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=o5S7CreWiBY/&#34;&gt;
&lt;img src=&#34;/img/trumpet_boy.jpg&#34; width=&#34;640&#34; height=&#34;360&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this blog post I will discuss missing data imputation and instrumental variables regression. This
is based on a short presentation I will give at my job. You can find the data used here on this
website: &lt;a href=&#34;http://eclr.humanities.manchester.ac.uk/index.php/IV_in_R&#34; class=&#34;uri&#34;&gt;http://eclr.humanities.manchester.ac.uk/index.php/IV_in_R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The data is used is from Wooldridge’s book, &lt;em&gt;Econometrics: A modern Approach&lt;/em&gt;.
You can download the data by clicking &lt;a href=&#34;http://eclr.humanities.manchester.ac.uk/images/5/5f/Mroz.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is the variable description:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; 1. inlf                     =1 if in labor force, 1975
 2. hours                    hours worked, 1975
 3. kidslt6                  # kids &amp;lt; 6 years
 4. kidsge6                  # kids 6-18
 5. age                      woman&amp;#39;s age in yrs
 6. educ                     years of schooling
 7. wage                     estimated wage from earns., hours
 8. repwage                  reported wage at interview in 1976
 9. hushrs                   hours worked by husband, 1975
10. husage                   husband&amp;#39;s age
11. huseduc                  husband&amp;#39;s years of schooling
12. huswage                  husband&amp;#39;s hourly wage, 1975
13. faminc                   family income, 1975
14. mtr                      fed. marginal tax rate facing woman
15. motheduc                 mother&amp;#39;s years of schooling
16. fatheduc                 father&amp;#39;s years of schooling
17. unem                     unem. rate in county of resid.
18. city                     =1 if live in SMSA
19. exper                    actual labor mkt exper
20. nwifeinc                 (faminc - wage*hours)/1000
21. lwage                    log(wage)
22. expersq                  exper^2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The goal is to first impute missing data in the data set, and then determine the impact of one added
year of education on wages. If one simply ignores missing values, bias can be introduced depending on
the missingness mechanism. The second problem here is that education is likely to be endogeneous
(and thus be correlated to the error term), as it is not randomly assigned. This causes biased estimates
and may lead to seriously wrong conclusions. So missingness and endogeneity should be dealt with, but
dealing with both issues is more of a programming challenge than an econometrics challenge.
Thankfully, the packages contained in the &lt;code&gt;{tidyverse}&lt;/code&gt; as well as &lt;code&gt;{mice}&lt;/code&gt; will save the day!&lt;/p&gt;
&lt;p&gt;If you inspect the data, you will see that there are no missing values. So I will use the &lt;code&gt;{mice}&lt;/code&gt;
package to first &lt;em&gt;ampute&lt;/em&gt; the data (which means adding missing values). This, of course, is done
for education purposes. If you’re lucky enough to not have missing values in your data, you shouldn’t
add them!&lt;/p&gt;
&lt;p&gt;Let’s load all the packages needed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(AER)
library(naniar)
library(mice)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So first, let’s read in the data, and ampute it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wages_data &amp;lt;- read_csv(&amp;quot;http://eclr.humanities.manchester.ac.uk/images/5/5f/Mroz.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   .default = col_integer(),
##   wage = col_character(),
##   repwage = col_double(),
##   huswage = col_double(),
##   mtr = col_double(),
##   unem = col_double(),
##   nwifeinc = col_double(),
##   lwage = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See spec(...) for full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, I only select the variables I want to use and convert them to the correct class:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wages_data &amp;lt;- wages_data %&amp;gt;% 
    select(wage, educ, fatheduc, motheduc, inlf, hours, 
               kidslt6, kidsge6, age, huswage, 
               mtr, unem, city, exper) %&amp;gt;% 
    mutate_at(vars(kidslt6, kidsge6, hours, educ, age, wage, huswage, mtr,
                    motheduc, fatheduc, unem, exper), as.numeric) %&amp;gt;% 
    mutate_at(vars(city, inlf), as.character)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in evalq(as.numeric(wage), &amp;lt;environment&amp;gt;): NAs introduced by
## coercion&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the data, some women are not in the labour force, and thus do not have any wages; meaning they
should have a 0 there. Instead, this is represented with the following symbol: “.”. So I convert
these dots to 0. One could argue that the wages should not be 0, but that they’re truly missing.
This is true, and there are ways to deal with such questions (Heckman’s selection model for instance),
but this is not the point here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wages_data &amp;lt;- wages_data %&amp;gt;% 
    mutate(wage = ifelse(is.na(wage), 0, wage))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s double check if there are any missing values in the data, using &lt;code&gt;naniar::vis_miss()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vis_miss(wages_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-07-01-tidy_ive_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Nope! Let’s ampute it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wages_mis &amp;lt;- ampute(wages_data)$amp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Data is made numeric because the calculation of weights requires
## numeric data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ampute()&lt;/code&gt; returns an object where the &lt;code&gt;amp&lt;/code&gt; element is the amputed data. This is what I save into
the new variable &lt;code&gt;wages_mis&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vis_miss(wages_mis)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-07-01-tidy_ive_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, so now we have missing values. Let’s use the recently added &lt;code&gt;mice::parlmice()&lt;/code&gt; function to
impute the dataset, in parallel:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_wages &amp;lt;- parlmice(data = wages_mis, m = 10, maxit = 20, cl.type = &amp;quot;FORK&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For reproducibility, I save these objects to disk:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(wages_mis, &amp;quot;wages_mis.csv&amp;quot;)

saveRDS(imp_wages, &amp;quot;imp_wages.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a sanity check, let’s look at the missingness pattern for the first completed dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vis_miss(complete(imp_wages))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-07-01-tidy_ive_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;mice::parlmice()&lt;/code&gt; was able to impute the dataset. I imputed it 10 times, so now I have 10 imputed
datasets. If I want to estimate a model using this data, I will need to do so 10 times.
This is where the tidyverse comes into play. First, let’s combine all the 10 imputed datasets into
one long dataset, with an index to differentiate them. This is done easily with &lt;code&gt;mice::complete()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_wages_df &amp;lt;- mice::complete(imp_wages, &amp;quot;long&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(imp_wages_df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   .imp .id   wage educ fatheduc motheduc inlf hours kidslt6 kidsge6 age
## 1    1   1 3.3540   12        7       12    1  1610       1       0  32
## 2    1   2 1.3889   12        7        7    1  1656       0       2  30
## 3    1   3 4.5455   12        7       12    1  1980       0       3  35
## 4    1   4 1.0965   12        7        7    1   456       0       3  34
## 5    1   5 4.5918   14       14       12    1  1568       1       2  31
## 6    1   6 4.7421   12        7       14    1  2032       0       0  54
##   huswage    mtr unem city exper
## 1  4.0288 0.7215  5.0    0    14
## 2  8.4416 0.6615 11.0    1     5
## 3  3.5807 0.6915  5.0    0    15
## 4  3.5417 0.7815  5.0    0     6
## 5 10.0000 0.6215  9.5    1    14
## 6  4.7364 0.6915  7.5    1    33&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, there are two new columns, &lt;code&gt;.id&lt;/code&gt; and &lt;code&gt;.imp&lt;/code&gt;. &lt;code&gt;.imp&lt;/code&gt; equals &lt;code&gt;i&lt;/code&gt; means that it is the
&lt;code&gt;i&lt;/code&gt;th imputed dataset.&lt;/p&gt;
&lt;p&gt;Because I have 0’s in my dependent variable, I will not log the wages but instead use the Inverse
Hyperbolic Sine transformation. Marc F. Bellemare wrote a nice post about
it &lt;a href=&#34;http://marcfbellemare.com/wordpress/12856&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ihs &amp;lt;- function(x){
    log(x + sqrt(x**2 + 1))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can now apply this function, but first I have to group by &lt;code&gt;.imp&lt;/code&gt;. Remember, these are 10 separated
datasets. I also create the experience squared:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_wages_df &amp;lt;- imp_wages_df %&amp;gt;% 
    group_by(.imp) %&amp;gt;% 
    mutate(ihs_wage = ihs(wage),
           exper2 = exper**2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now comes some tidyverse magic. I will create a new dataset by using the &lt;code&gt;nest()&lt;/code&gt; function from &lt;code&gt;tidyr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(imp_wages &amp;lt;- imp_wages_df %&amp;gt;% 
    group_by(.imp) %&amp;gt;% 
    nest())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 2
##     .imp data               
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;             
##  1     1 &amp;lt;tibble [753 × 17]&amp;gt;
##  2     2 &amp;lt;tibble [753 × 17]&amp;gt;
##  3     3 &amp;lt;tibble [753 × 17]&amp;gt;
##  4     4 &amp;lt;tibble [753 × 17]&amp;gt;
##  5     5 &amp;lt;tibble [753 × 17]&amp;gt;
##  6     6 &amp;lt;tibble [753 × 17]&amp;gt;
##  7     7 &amp;lt;tibble [753 × 17]&amp;gt;
##  8     8 &amp;lt;tibble [753 × 17]&amp;gt;
##  9     9 &amp;lt;tibble [753 × 17]&amp;gt;
## 10    10 &amp;lt;tibble [753 × 17]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, &lt;code&gt;imp_wages&lt;/code&gt; is now a dataset with two columns: &lt;code&gt;.imp&lt;/code&gt;, indexing the imputed datasets,
and a column called &lt;code&gt;data&lt;/code&gt;, where each element is itself a tibble! &lt;code&gt;data&lt;/code&gt; is a so-called list-column.
You can read more about it on the
&lt;a href=&#34;https://jennybc.github.io/purrr-tutorial/ls13_list-columns.html&#34;&gt;&lt;code&gt;purrr&lt;/code&gt; tutorial&lt;/a&gt; written by
&lt;a href=&#34;https://twitter.com/JennyBryan&#34;&gt;Jenny Bryan&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Estimating a model now is easy, if you’re familiar with &lt;code&gt;purrr&lt;/code&gt;. This is how you do it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_wages_reg = imp_wages %&amp;gt;% 
    mutate(lin_reg = map(data, 
                         ~lm(ihs_wage ~ educ + inlf + hours + 
                                 kidslt6 + kidsge6 + age + huswage + 
                                 mtr + unem + city + exper + exper2, 
                             data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, so what happened here? &lt;code&gt;imp_wages&lt;/code&gt; is a data frame, so it’s possible to add a column to it
with &lt;code&gt;mutate()&lt;/code&gt;. I call that column &lt;code&gt;lin_reg&lt;/code&gt; and use &lt;code&gt;map()&lt;/code&gt; on the column called &lt;code&gt;data&lt;/code&gt; (remember,
this column is actually a list of data frame objects, and &lt;code&gt;map()&lt;/code&gt; takes a list as an argument, and then a
function or formula) with the following formula:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;~lm(ihs_wage ~ educ + inlf + hours + 
        kidslt6 + kidsge6 + age + huswage + 
        mtr + unem + city + exper + exper2, 
    data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This formula is nothing more that a good old linear regression. The last line &lt;code&gt;data = .&lt;/code&gt; means that
the data to be used inside &lt;code&gt;lm()&lt;/code&gt; should be coming from the list called &lt;code&gt;data&lt;/code&gt;, which is the second
column of &lt;code&gt;imp_wages&lt;/code&gt;. As I’m writing these lines, I realize it is confusing as hell. But I promise
you that learning to use &lt;code&gt;purrr&lt;/code&gt; is a bit like learning how to use a bicycle. Very difficult to explain,
but once you know how to do it, it feels super natural. Take some time to play with the lines above
to really understand what happened.&lt;/p&gt;
&lt;p&gt;Now, let’s take a look at the result:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_wages_reg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##     .imp data                lin_reg
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;              &amp;lt;list&amp;gt; 
##  1     1 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
##  2     2 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
##  3     3 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
##  4     4 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
##  5     5 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
##  6     6 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
##  7     7 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
##  8     8 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
##  9     9 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;   
## 10    10 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;imp_wages_reg&lt;/code&gt; now has a third column called &lt;code&gt;lin_reg&lt;/code&gt; where each element is a linear model, estimated
on the data from the &lt;code&gt;data&lt;/code&gt; column! We can now pool the results of these 10 regressions using
&lt;code&gt;mice::pool()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pool_lin_reg &amp;lt;- pool(imp_wages_reg$lin_reg)

summary(pool_lin_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  estimate    std.error  statistic       df      p.value
## (Intercept)  1.2868701172 3.214473e-01  4.0033628 737.9337 6.876133e-05
## educ         0.0385310276 8.231906e-03  4.6806931 737.9337 3.401935e-06
## inlf         1.8845418354 5.078235e-02 37.1101707 737.9337 0.000000e+00
## hours       -0.0001164143 3.011378e-05 -3.8658143 737.9337 1.204773e-04
## kidslt6     -0.0438925013 3.793152e-02 -1.1571510 737.9337 2.475851e-01
## kidsge6     -0.0117978229 1.405226e-02 -0.8395678 737.9337 4.014227e-01
## age         -0.0030084595 2.666614e-03 -1.1281946 737.9337 2.596044e-01
## huswage     -0.0231736955 5.607364e-03 -4.1327255 737.9337 3.995866e-05
## mtr         -2.2109176781 3.188827e-01 -6.9333267 737.9337 8.982592e-12
## unem         0.0028775444 5.462973e-03  0.5267360 737.9337 5.985352e-01
## city         0.0157414671 3.633755e-02  0.4332011 737.9337 6.649953e-01
## exper        0.0164364027 6.118875e-03  2.6861806 737.9337 7.389936e-03
## exper2      -0.0002022602 1.916146e-04 -1.0555575 737.9337 2.915159e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function averages the results from the 10 regressions and computes correct standard errors. This
is based on Rubin’s rules (Rubin, 1987, p. 76). As you can see, the linear regression indicates that
one year of added education has a positive, significant effect of log wages (they’re not log wages,
I used the IHS transformation, but &lt;em&gt;log wages&lt;/em&gt; just sounds better than &lt;em&gt;inverted hyperbolic sined wages&lt;/em&gt;).
This effect is almost 4%.&lt;/p&gt;
&lt;p&gt;But education is not randomly assigned, and as such might be endogenous. This is where instrumental
variables come into play. An instrument is a variables that impacts the dependent variable only through
the endogenous variable (here, education). For example, the education of the parents do not have
a direct impact over one’s wage, but having college-educated parents means that you are likely
college-educated yourself, and thus have a higher wage that if you only have a high school diploma.&lt;/p&gt;
&lt;p&gt;I am thus going to instrument education with both parents’ education:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_wages_reg = imp_wages_reg %&amp;gt;% 
    mutate(iv_reg = map(data, 
                         ~ivreg(ihs_wage ~ educ + inlf + hours + 
                                 kidslt6 + kidsge6 + age + huswage + 
                                 mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, 
                             data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only difference from before is the formula:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;~ivreg(ihs_wage ~ educ + inlf + hours + 
           kidslt6 + kidsge6 + age + huswage + 
           mtr + unem + city + exper + exper2 |.-educ + fatheduc + motheduc, 
       data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ~ivreg(ihs_wage ~ educ + inlf + hours + kidslt6 + kidsge6 + age + 
##     huswage + mtr + unem + city + exper + exper2 | . - educ + 
##     fatheduc + motheduc, data = .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Instead of &lt;code&gt;lm()&lt;/code&gt; I use &lt;code&gt;AER::ivreg()&lt;/code&gt; and the formula has a second part, after the &lt;code&gt;|&lt;/code&gt; symbol. This
is where I specify that I instrument education with the parents’ education.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;imp_wages_reg&lt;/code&gt; now looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_wages_reg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 4
##     .imp data                lin_reg iv_reg 
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;              &amp;lt;list&amp;gt;  &amp;lt;list&amp;gt; 
##  1     1 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
##  2     2 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
##  3     3 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
##  4     4 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
##  5     5 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
##  6     6 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
##  7     7 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
##  8     8 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
##  9     9 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;
## 10    10 &amp;lt;tibble [753 × 17]&amp;gt; &amp;lt;lm&amp;gt;    &amp;lt;ivreg&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pool_iv_reg &amp;lt;- pool(imp_wages_reg$iv_reg)

summary(pool_iv_reg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  estimate    std.error  statistic       df      p.value
## (Intercept)  2.0091904157 5.146812e-01  3.9037568 737.9337 1.033832e-04
## educ         0.0038859137 2.086592e-02  0.1862326 737.9337 8.523136e-01
## inlf         1.9200207113 5.499457e-02 34.9129122 737.9337 0.000000e+00
## hours       -0.0001313866 3.157375e-05 -4.1612608 737.9337 3.537881e-05
## kidslt6     -0.0234593391 4.000689e-02 -0.5863824 737.9337 5.577979e-01
## kidsge6     -0.0123239220 1.422241e-02 -0.8665145 737.9337 3.864897e-01
## age         -0.0040874625 2.763340e-03 -1.4791748 737.9337 1.395203e-01
## huswage     -0.0242737100 5.706497e-03 -4.2536970 737.9337 2.373189e-05
## mtr         -2.6385172445 3.998419e-01 -6.5989008 737.9337 7.907430e-11
## unem         0.0047331976 5.622137e-03  0.8418859 737.9337 4.001246e-01
## city         0.0255647706 3.716783e-02  0.6878197 737.9337 4.917824e-01
## exper        0.0180917073 6.258779e-03  2.8906127 737.9337 3.957817e-03
## exper2      -0.0002291007 1.944599e-04 -1.1781381 737.9337 2.391213e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, education is not statistically significant anymore! This is why it is quite important
to think about endogeneity issues. However, it is not always very easy to find suitable instruments.
A series of tests exist to determine if you have relevant and strong instruments, but this blog post
is already long enough. I will leave this for a future blog post.&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
&lt;style&gt;.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#ffffff !important;background-color:#272b30 !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing:0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:&#39;Cookie&#39;, cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#82518c !important;}&lt;/style&gt;
&lt;p&gt;&lt;link href=&#34;https://fonts.googleapis.com/css?family=Cookie&#34; rel=&#34;stylesheet&#34;&gt;&lt;a class=&#34;bmc-button&#34; target=&#34;_blank&#34; href=&#34;https://www.buymeacoffee.com/brodriguesco&#34;&gt;&lt;img src=&#34;https://www.buymeacoffee.com/assets/img/BMC-btn-logo.svg&#34; alt=&#34;Buy me an Espresso&#34;&gt;&lt;span style=&#34;margin-left:5px&#34;&gt;Buy me an Espresso&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Forecasting my weight with R</title>
      <link>/blog/2018-06-24-fun_ts/</link>
      <pubDate>Sun, 24 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-06-24-fun_ts/</guid>
      <description>&lt;p&gt;I’ve been measuring my weight almost daily for almost 2 years now; I actually started earlier, but
not as consistently. The goal of this blog post is to get re-acquaiented with time series; I haven’t
had the opportunity to work with time series for a long time now and I have seen that quite a few
packages that deal with time series have been released on CRAN. In this blog post, I will explore
my weight measurements using some functions from the &lt;code&gt;{tsibble}&lt;/code&gt; and &lt;code&gt;{tibbletime}&lt;/code&gt; packages,
and then do some predictions with the &lt;code&gt;{forecast}&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;First, let’s load the needed packages, read in the data and convert it to a &lt;code&gt;tsibble&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;tidyverse&amp;quot;)
library(&amp;quot;readr&amp;quot;)
library(&amp;quot;forecast&amp;quot;)
library(&amp;quot;tsibble&amp;quot;)
library(&amp;quot;tibbletime&amp;quot;)
library(&amp;quot;mice&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weight &amp;lt;- read_csv(&amp;quot;https://gist.githubusercontent.com/b-rodrigues/ea60679135f8dbed448ccf66a216811f/raw/18b469f3b0720f76ce5ee2715d0f9574b615f170/gistfile1.txt&amp;quot;) %&amp;gt;% 
    as_tsibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   Date = col_date(format = &amp;quot;&amp;quot;),
##   Poids = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The `index` is `Date`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can read more about &lt;code&gt;{tsibble}&lt;/code&gt; &lt;a href=&#34;https://pkg.earo.me/tsibble/&#34;&gt;here&lt;/a&gt;. Here, I use &lt;code&gt;{tsibble}&lt;/code&gt; mostly
for the next step, which is using the function &lt;code&gt;fill_na()&lt;/code&gt; on the tsibble. &lt;code&gt;fill_na()&lt;/code&gt; turns
implicit missing values into explicit missing values. These are implicit missing values:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;          Date Poids
1   2013-01-01 84.10
2   2013-01-04 85.60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and this is the same view, but with explicit missing values:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;          Date Poids
1   2013-01-01 84.10
2   2013-01-02 NA
3   2013-01-03 NA
4   2013-01-04 85.60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is useful to do, because I want to impute the missing values using the &lt;code&gt;{mice}&lt;/code&gt; package.
Let’s do this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weight &amp;lt;- weight %&amp;gt;% 
    fill_na()

imp_weight &amp;lt;- mice(data = weight) %&amp;gt;% 
    mice::complete(&amp;quot;long&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  iter imp variable
##   1   1  Poids
##   1   2  Poids
##   1   3  Poids
##   1   4  Poids
##   1   5  Poids
##   2   1  Poids
##   2   2  Poids
##   2   3  Poids
##   2   4  Poids
##   2   5  Poids
##   3   1  Poids
##   3   2  Poids
##   3   3  Poids
##   3   4  Poids
##   3   5  Poids
##   4   1  Poids
##   4   2  Poids
##   4   3  Poids
##   4   4  Poids
##   4   5  Poids
##   5   1  Poids
##   5   2  Poids
##   5   3  Poids
##   5   4  Poids
##   5   5  Poids&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;imp_weight&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(imp_weight)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   .imp .id       Date Poids
## 1    1   1 2013-10-28  84.1
## 2    1   2 2013-10-29  84.4
## 3    1   3 2013-10-30  83.5
## 4    1   4 2013-10-31  84.1
## 5    1   5 2013-11-01  85.6
## 6    1   6 2013-11-02  85.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s select the relevant data. I filter from the 11th of July 2016, which is where I started
weighing myself almost every day, to the 31st of May 2018. I want to predict my weight for the
month of June (you might think of the month of June 2018 as the test data, and the rest as training
data):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_weight_train &amp;lt;- imp_weight %&amp;gt;% 
    filter(Date &amp;gt;= &amp;quot;2016-07-11&amp;quot;, Date &amp;lt;= &amp;quot;2018-05-31&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the next lines, I create a column called &lt;code&gt;imputation&lt;/code&gt; which is simply the same as the column
&lt;code&gt;.imp&lt;/code&gt; but of character class, remove unneeded columns and rename some other columns (“Poids” is
French for weight):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_weight_train &amp;lt;- imp_weight_train %&amp;gt;% 
    mutate(imputation = as.character(.imp)) %&amp;gt;% 
    select(-.id, -.imp) %&amp;gt;% 
    rename(date = Date) %&amp;gt;% 
    rename(weight = Poids)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(imp_weight_train, aes(date, weight, colour = imputation)) +
    geom_line() + 
    theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-06-24-fun_ts_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plots gives some info, but it might be better to smooth the lines. This is possible by
computing a rolling mean. For this I will use the &lt;code&gt;rollify()&lt;/code&gt; function of the &lt;code&gt;{tibbletime}&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_roll_5 &amp;lt;- rollify(mean, window = 5)
mean_roll_10 &amp;lt;- rollify(mean, window = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;rollify()&lt;/code&gt; can be seen as an adverb, pretty much like &lt;code&gt;purrr::safely()&lt;/code&gt;; &lt;code&gt;rollify()&lt;/code&gt; is a higher
order function that literally rollifies a function, in this case &lt;code&gt;mean()&lt;/code&gt; which means that
rollifying the mean creates a function that returns the rolling mean. The &lt;code&gt;window&lt;/code&gt; argument lets
you decide how smooth you want the curve to be: the higher the smoother. However, you will lose
some observations. Let’s use this functions to add the rolling means to the data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_weight_train &amp;lt;- imp_weight_train %&amp;gt;% 
    group_by(imputation) %&amp;gt;% 
    mutate(roll_5 = mean_roll_5(weight),
           roll_10 = mean_roll_10(weight))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s plot these new curves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(imp_weight_train, aes(date, roll_5, colour = imputation)) +
    geom_line() + 
    theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 20 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-06-24-fun_ts_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(imp_weight_train, aes(date, roll_10, colour = imputation)) +
    geom_line() + 
    theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 45 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-06-24-fun_ts_files/figure-html/unnamed-chunk-11-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s easier to read, isn’t it?&lt;/p&gt;
&lt;p&gt;Now, I will use the &lt;code&gt;auto.arima()&lt;/code&gt; function to train a model on the data to forecast my weight for
the month of June. However, my data, &lt;code&gt;imp_weight_train&lt;/code&gt; is a list of datasets. &lt;code&gt;auto.arima()&lt;/code&gt; does
not take a data frame as an argument, much less so a list of datasets. I’ll create a wrapper around
&lt;code&gt;auto.arima()&lt;/code&gt; that works on a dataset, and then map it to the list of datasets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;auto.arima.df &amp;lt;- function(data, y, ...){

    y &amp;lt;- enquo(y)

    yts &amp;lt;- data %&amp;gt;% 
        pull(!!y) %&amp;gt;% 
        as.ts()

    auto.arima(yts, ...)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;auto.arima.df()&lt;/code&gt; takes a data frame as argument, and then &lt;code&gt;y&lt;/code&gt;, which is the column that contains the
univariate time series. This column then gets pulled out of the data frame, converted to a time
series object with &lt;code&gt;as.ts()&lt;/code&gt;, and then passed down to &lt;code&gt;auto.arima()&lt;/code&gt;. I can now use this function
on my list of data sets. The first step is to nest the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested_data &amp;lt;- imp_weight_train %&amp;gt;% 
    group_by(imputation) %&amp;gt;% 
    nest() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;nested_data&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nested_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
##   imputation data              
##   &amp;lt;chr&amp;gt;      &amp;lt;list&amp;gt;            
## 1 1          &amp;lt;tibble [690 × 4]&amp;gt;
## 2 2          &amp;lt;tibble [690 × 4]&amp;gt;
## 3 3          &amp;lt;tibble [690 × 4]&amp;gt;
## 4 4          &amp;lt;tibble [690 × 4]&amp;gt;
## 5 5          &amp;lt;tibble [690 × 4]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;nested_data&lt;/code&gt; is a tibble with a column called &lt;code&gt;data&lt;/code&gt;, which is a so-called list-column. Each
element of &lt;code&gt;data&lt;/code&gt; is itself a tibble. This is a useful structure, because now I can map &lt;code&gt;auto.arima.df()&lt;/code&gt;
to the data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models &amp;lt;- nested_data %&amp;gt;% 
    mutate(model = map(data, auto.arima.df, y = weight))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This trick can be a bit difficult to follow the first time you see it. The idea is the following:
&lt;code&gt;nested_data&lt;/code&gt; is a tibble. Thus, I can add a column to it using &lt;code&gt;mutate()&lt;/code&gt;. So far so good.
Now that I am “inside” the mutate call, I can use &lt;code&gt;purrr::map()&lt;/code&gt;. Why? &lt;code&gt;purrr::map()&lt;/code&gt; takes a list
and then a function as arguments. Remember that &lt;code&gt;data&lt;/code&gt; is a list column; you can see it above,
the type of the column &lt;code&gt;data&lt;/code&gt; is list. So &lt;code&gt;data&lt;/code&gt; is a list, and thus can be used inside &lt;code&gt;purrr::map()&lt;/code&gt;.
Great. Now, what is inside &lt;code&gt;data&lt;/code&gt;? tibbles, where inside each of them is a column
called &lt;code&gt;weight&lt;/code&gt;. This is the column that contains my univariate time series I want to model. Let’s
take a look at &lt;code&gt;models&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
##   imputation data               model      
##   &amp;lt;chr&amp;gt;      &amp;lt;list&amp;gt;             &amp;lt;list&amp;gt;     
## 1 1          &amp;lt;tibble [690 × 4]&amp;gt; &amp;lt;S3: ARIMA&amp;gt;
## 2 2          &amp;lt;tibble [690 × 4]&amp;gt; &amp;lt;S3: ARIMA&amp;gt;
## 3 3          &amp;lt;tibble [690 × 4]&amp;gt; &amp;lt;S3: ARIMA&amp;gt;
## 4 4          &amp;lt;tibble [690 × 4]&amp;gt; &amp;lt;S3: ARIMA&amp;gt;
## 5 5          &amp;lt;tibble [690 × 4]&amp;gt; &amp;lt;S3: ARIMA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;models&lt;/code&gt; is a tibble with a column called &lt;code&gt;model&lt;/code&gt;, where each element is a model of type &lt;code&gt;ARIMA&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Adding forecasts is based on the same trick as above, and we use the &lt;code&gt;forecast()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecasts &amp;lt;- models %&amp;gt;% 
    mutate(predictions = map(model, forecast, h = 24)) %&amp;gt;% 
    mutate(predictions = map(predictions, as_tibble)) %&amp;gt;% 
    pull(predictions) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I forecast 24 days (I am writing this on the 24th of June), and convert the predictions to tibbles,
and then pull only the predictions tibble:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecasts&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1             71.5    70.7    72.3    70.2    72.8
##  2             71.5    70.7    72.4    70.3    72.8
##  3             71.5    70.6    72.3    70.1    72.8
##  4             71.5    70.6    72.4    70.1    72.9
##  5             71.4    70.5    72.4    70.0    72.9
##  6             71.5    70.5    72.4    70.0    72.9
##  7             71.4    70.5    72.4    69.9    72.9
##  8             71.4    70.4    72.4    69.9    72.9
##  9             71.4    70.4    72.4    69.9    72.9
## 10             71.4    70.4    72.4    69.8    73.0
## # ... with 14 more rows
## 
## [[2]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1             71.6    70.8    72.3    70.3    72.8
##  2             71.6    70.8    72.5    70.3    72.9
##  3             71.5    70.6    72.4    70.2    72.9
##  4             71.5    70.6    72.5    70.1    72.9
##  5             71.5    70.5    72.5    70.0    73.0
##  6             71.5    70.5    72.5    70.0    73.0
##  7             71.5    70.5    72.5    69.9    73.0
##  8             71.5    70.4    72.5    69.9    73.1
##  9             71.5    70.4    72.5    69.8    73.1
## 10             71.4    70.3    72.6    69.7    73.1
## # ... with 14 more rows
## 
## [[3]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1             71.6    70.8    72.4    70.4    72.8
##  2             71.5    70.7    72.4    70.2    72.8
##  3             71.5    70.6    72.4    70.2    72.9
##  4             71.5    70.6    72.4    70.1    72.9
##  5             71.5    70.5    72.4    70.0    72.9
##  6             71.5    70.5    72.4    70.0    73.0
##  7             71.5    70.5    72.5    69.9    73.0
##  8             71.4    70.4    72.5    69.9    73.0
##  9             71.4    70.4    72.5    69.8    73.0
## 10             71.4    70.4    72.5    69.8    73.1
## # ... with 14 more rows
## 
## [[4]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1             71.5    70.8    72.3    70.3    72.8
##  2             71.5    70.7    72.4    70.3    72.8
##  3             71.5    70.7    72.4    70.2    72.8
##  4             71.5    70.6    72.4    70.1    72.9
##  5             71.5    70.6    72.4    70.1    72.9
##  6             71.5    70.5    72.5    70.0    73.0
##  7             71.5    70.5    72.5    69.9    73.0
##  8             71.5    70.4    72.5    69.9    73.0
##  9             71.4    70.4    72.5    69.8    73.1
## 10             71.4    70.3    72.5    69.8    73.1
## # ... with 14 more rows
## 
## [[5]]
## # A tibble: 24 x 5
##    `Point Forecast` `Lo 80` `Hi 80` `Lo 95` `Hi 95`
##  *            &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1             71.5    70.8    72.3    70.3    72.8
##  2             71.5    70.7    72.4    70.3    72.8
##  3             71.5    70.7    72.4    70.2    72.8
##  4             71.5    70.6    72.4    70.1    72.9
##  5             71.5    70.6    72.4    70.1    72.9
##  6             71.5    70.5    72.4    70.0    73.0
##  7             71.5    70.5    72.5    69.9    73.0
##  8             71.5    70.4    72.5    69.9    73.0
##  9             71.4    70.4    72.5    69.8    73.1
## 10             71.4    70.3    72.5    69.8    73.1
## # ... with 14 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So &lt;code&gt;forecasts&lt;/code&gt; is a list of tibble, each containing a forecast. Remember that I have 5 tibbles, because
I imputed the data 5 times. I will merge this list of data sets together into one, but before I need
to add a column that indices the forecasts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecasts &amp;lt;- map2(.x = forecasts, .y = as.character(seq(1, 5)), 
     ~mutate(.x, id = .y)) %&amp;gt;% 
    bind_rows() %&amp;gt;% 
    select(-c(`Lo 80`, `Hi 80`))

colnames(forecasts) &amp;lt;- c(&amp;quot;point_forecast&amp;quot;, &amp;quot;low_95&amp;quot;, &amp;quot;hi_95&amp;quot;, &amp;quot;id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look again at &lt;code&gt;forecasts&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecasts&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 120 x 4
##    point_forecast low_95 hi_95 id   
##             &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
##  1           71.5   70.2  72.8 1    
##  2           71.5   70.3  72.8 1    
##  3           71.5   70.1  72.8 1    
##  4           71.5   70.1  72.9 1    
##  5           71.4   70.0  72.9 1    
##  6           71.5   70.0  72.9 1    
##  7           71.4   69.9  72.9 1    
##  8           71.4   69.9  72.9 1    
##  9           71.4   69.9  72.9 1    
## 10           71.4   69.8  73.0 1    
## # ... with 110 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I now select the true values for the month of June. I also imputed this data, but here I will
simply keep the average of the imputations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weight_june &amp;lt;- imp_weight %&amp;gt;% 
    filter(Date &amp;gt;= &amp;quot;2018-06-01&amp;quot;) %&amp;gt;% 
    select(-.id) %&amp;gt;% 
    group_by(Date) %&amp;gt;% 
    summarise(true_weight = mean(Poids)) %&amp;gt;% 
    rename(date = Date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at &lt;code&gt;weight_june&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weight_june&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 24 x 2
##    date       true_weight
##    &amp;lt;date&amp;gt;           &amp;lt;dbl&amp;gt;
##  1 2018-06-01        71.8
##  2 2018-06-02        70.8
##  3 2018-06-03        71.2
##  4 2018-06-04        71.4
##  5 2018-06-05        70.9
##  6 2018-06-06        70.8
##  7 2018-06-07        70.5
##  8 2018-06-08        70.1
##  9 2018-06-09        70.3
## 10 2018-06-10        71.0
## # ... with 14 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s repeat &lt;code&gt;weight_june&lt;/code&gt; 5 times, and add the index 1 to 5. Why? Because I want to merge the
true data with the forecasts, and having the data in this form makes things easier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weight_june &amp;lt;- modify(list_along(1:5), ~`&amp;lt;-`(., weight_june)) %&amp;gt;% 
    map2(.y = as.character(seq(1, 5)), 
         ~mutate(.x, id = .y)) %&amp;gt;% 
    bind_rows()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;modify(list_along(1:5), ~`&amp;lt;-`(., weight_june)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;looks quite complicated, but you will see that it is not, once we break it apart. &lt;code&gt;modify()&lt;/code&gt;
modifies a list. The list to modify is &lt;code&gt;list_along(1:5)&lt;/code&gt;, which create a list of &lt;code&gt;NULL&lt;/code&gt;s:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_along(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## NULL
## 
## [[2]]
## NULL
## 
## [[3]]
## NULL
## 
## [[4]]
## NULL
## 
## [[5]]
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second argument of &lt;code&gt;modify()&lt;/code&gt; is either a function or a formula. I created the following
formula:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~`&amp;lt;-`(., weight_june)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We all know the function &lt;code&gt;&amp;lt;-()&lt;/code&gt;, but are not used to see it that way. But consider the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;`&amp;lt;-`(a, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These two formulations are equivalent. So these lines fill the empty element of the list of &lt;code&gt;NULL&lt;/code&gt;s
with the data frame &lt;code&gt;weight_june&lt;/code&gt;. Then I add the &lt;code&gt;id&lt;/code&gt; column and then bind the rows together: &lt;code&gt;bind_rows()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s bind the columns of &lt;code&gt;weight_june&lt;/code&gt; and &lt;code&gt;forecasts&lt;/code&gt; and take a look at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecasts &amp;lt;- bind_cols(weight_june, forecasts) %&amp;gt;% 
    select(-id1)

forecasts&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 120 x 6
##    date       true_weight id    point_forecast low_95 hi_95
##    &amp;lt;date&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 2018-06-01        71.8 1               71.5   70.2  72.8
##  2 2018-06-02        70.8 1               71.5   70.3  72.8
##  3 2018-06-03        71.2 1               71.5   70.1  72.8
##  4 2018-06-04        71.4 1               71.5   70.1  72.9
##  5 2018-06-05        70.9 1               71.4   70.0  72.9
##  6 2018-06-06        70.8 1               71.5   70.0  72.9
##  7 2018-06-07        70.5 1               71.4   69.9  72.9
##  8 2018-06-08        70.1 1               71.4   69.9  72.9
##  9 2018-06-09        70.3 1               71.4   69.9  72.9
## 10 2018-06-10        71.0 1               71.4   69.8  73.0
## # ... with 110 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, for the last plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(forecasts, aes(x = date, colour = id)) +
    geom_line(aes(y = true_weight), size = 2) + 
    geom_line(aes(y = hi_95)) + 
    geom_line(aes(y = low_95)) + 
    theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-06-24-fun_ts_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The true data fall within all the confidence intervals, but I am a bit surprised by the intervals,
especially the upper confidence intervals; they all are way above 72kg, however my true weight
has been fluctuating around 71kg for quite some months now. I think I have to refresh my memory
on time series, because I am certainly missing something!&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting data from pdfs using the pdftools package</title>
      <link>/blog/2018-06-10-scraping_pdfs/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-06-10-scraping_pdfs/</guid>
      <description>&lt;p&gt;It is often the case that data is trapped inside pdfs, but thankfully there are ways to extract
it from the pdfs. A very nice package for this task is
&lt;code&gt;pdftools&lt;/code&gt; (&lt;a href=&#34;https://github.com/ropensci/pdftools&#34;&gt;Github link&lt;/a&gt;)
and this blog post will describe some basic functionality from that package.&lt;/p&gt;
&lt;p&gt;First, let’s find some pdfs that contain interesting data. For this post, I’m using the diabetes
country profiles from the World Health Organization. You can find them &lt;a href=&#34;http://www.who.int/diabetes/country-profiles/en/#U&#34;&gt;here&lt;/a&gt;.
If you open one of these pdfs, you are going to see this:&lt;/p&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1&#34;&gt;
&lt;img src=&#34;/img/diabetes_lux.png&#34; width=&#34;499&#34; height=&#34;680&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I’m interested in this table here in the middle:&lt;/p&gt;
&lt;div style=&#34;text-align:center;&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1&#34;&gt;
&lt;img src=&#34;/img/diabetes_table.png&#34; width=&#34;499&#34; height=&#34;680&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I want to get the data from different countries, put it all into a nice data frame and make a
simple plot.&lt;/p&gt;
&lt;p&gt;Let’s first start by loading the needed packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;pdftools&amp;quot;)
library(&amp;quot;glue&amp;quot;)
library(&amp;quot;tidyverse&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 2.2.1     ✔ purrr   0.2.5
## ✔ tibble  1.4.2     ✔ dplyr   0.7.5
## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
## ✔ readr   1.1.1     ✔ forcats 0.3.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::collapse() masks glue::collapse()
## ✖ dplyr::filter()   masks stats::filter()
## ✖ dplyr::lag()      masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;ggthemes&amp;quot;)

country &amp;lt;- c(&amp;quot;lux&amp;quot;, &amp;quot;fra&amp;quot;, &amp;quot;deu&amp;quot;, &amp;quot;usa&amp;quot;, &amp;quot;prt&amp;quot;, &amp;quot;gbr&amp;quot;)

url &amp;lt;- &amp;quot;http://www.who.int/diabetes/country-profiles/{country}_en.pdf?ua=1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first 4 lines load the needed packages for this exercise: &lt;code&gt;pdftools&lt;/code&gt; is the package that I
described in the beginning of the post, &lt;code&gt;glue&lt;/code&gt; is optional but offers a nice alternative to the
&lt;code&gt;paste()&lt;/code&gt; and &lt;code&gt;paste0()&lt;/code&gt; functions. Take a closer look at the url: you’ll see that I wrote &lt;code&gt;{country}&lt;/code&gt;.
This is not in the original links; the original links look like this (for example for the USA):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So because I’m interested in several countries, I created a vector with the country codes
of the countries I’m interested in. Now, using the &lt;code&gt;glue()&lt;/code&gt; function, something magical happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(urls &amp;lt;- glue(url))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## http://www.who.int/diabetes/country-profiles/lux_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/fra_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/deu_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/usa_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/prt_en.pdf?ua=1
## http://www.who.int/diabetes/country-profiles/gbr_en.pdf?ua=1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This created a vector with all the links where &lt;code&gt;{country}&lt;/code&gt; is replaced by each of the codes
contained in the variable &lt;code&gt;country&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I use the same trick to create the names of the pdfs that I will download:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf_names &amp;lt;- glue(&amp;quot;report_{country}.pdf&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now I can download them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;walk2(urls, pdf_names, download.file, mode = &amp;quot;wb&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;walk2()&lt;/code&gt; is a function from the &lt;code&gt;purrr&lt;/code&gt; package that is similar to &lt;code&gt;map2()&lt;/code&gt;. You could use &lt;code&gt;map2()&lt;/code&gt;
for this, but &lt;code&gt;walk2()&lt;/code&gt; is cleaner here, because &lt;code&gt;dowload.file()&lt;/code&gt; is a function with a so-called
side effect; it downloads files. &lt;code&gt;map2()&lt;/code&gt; is used for functions without side effects.&lt;/p&gt;
&lt;p&gt;Now, I can finally use the &lt;code&gt;pdf_text()&lt;/code&gt; function from the &lt;code&gt;pdftools&lt;/code&gt; function to get the text
from the pdfs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raw_text &amp;lt;- map(pdf_names, pdf_text)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;raw_text&lt;/code&gt; is a list of where each element is the text from one of the pdfs. Let’s take a look:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(raw_text)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 6
##  $ : chr &amp;quot;Luxembourg                                                                                                     &amp;quot;| __truncated__
##  $ : chr &amp;quot;France                                                                                                         &amp;quot;| __truncated__
##  $ : chr &amp;quot;Germany                                                                                                        &amp;quot;| __truncated__
##  $ : chr &amp;quot;United States Of America                                                                                       &amp;quot;| __truncated__
##  $ : chr &amp;quot;Portugal                                                                                                       &amp;quot;| __truncated__
##  $ : chr &amp;quot;United Kingdom                                                                                                 &amp;quot;| __truncated__&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at one of these elements, which is nothing but a very long character:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;raw_text[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Luxembourg                                                                                                                                          Total population: 567 000\n                                                                                                                                                         Income group: High\nMortality\nNumber of diabetes deaths                                                                     Number of deaths attributable to high blood glucose\n                                                                     males         females                                                            males       females\nages 30–69                                                           &amp;lt;100            &amp;lt;100     ages 30–69                                              &amp;lt;100          &amp;lt;100\nages 70+                                                             &amp;lt;100            &amp;lt;100     ages 70+                                                &amp;lt;100          &amp;lt;100\nProportional mortality (% of total deaths, all ages)                                          Trends in age-standardized prevalence of diabetes\n                    Communicable,\n                   maternal, perinatal              Injuries                                                    35%\n                    and nutritional                   6%                     Cardiovascular\n                      conditions                                               diseases\n                          6%                                                      33%\n                                                                                                                30%\n                                                                                                                25%\n                                                                                              % of population\n               Other NCDs\n                  16%                                                                                           20%\n                                     No data available                                                          15%           No data available\n              Diabetes                                                                                          10%\n                 2%\n                                                                                                                5%\n                   Respiratory\n                    diseases\n                       6%                                                                                       0%\n                                                           Cancers\n                                                            31%\n                                                                                                                                  males     females\nPrevalence of diabetes and related risk factors\n                                                                                                                      males               females               total\nDiabetes                                                                                                              8.3%                 5.3%                 6.8%\nOverweight                                                                                                            70.7%               51.5%                61.0%\nObesity                                                                                                               28.3%               21.3%                24.8%\nPhysical inactivity                                                                                                   28.2%               31.7%                30.0%\nNational response to diabetes\nPolicies, guidelines and monitoring\nOperational policy/strategy/action plan for diabetes                                                                                                ND\nOperational policy/strategy/action plan to reduce overweight and obesity                                                                            ND\nOperational policy/strategy/action plan to reduce physical inactivity                                                                               ND\nEvidence-based national diabetes guidelines/protocols/standards                                                                                     ND\nStandard criteria for referral of patients from primary care to higher level of care                                                                ND\nDiabetes registry                                                                                                                                   ND\nRecent national risk factor survey in which blood glucose was measured                                                                              ND\nAvailability of medicines, basic technologies and procedures in the public health sector\nMedicines in primary care facilities                                                          Basic technologies in primary care facilities\nInsulin                                                                               ND      Blood glucose measurement                                             ND\nMetformin                                                                             ND      Oral glucose tolerance test                                           ND\nSulphonylurea                                                                         ND      HbA1c test                                                            ND\nProcedures                                                                                    Dilated fundus examination                                            ND\nRetinal photocoagulation                                                              ND      Foot vibration perception by tuning fork                              ND\nRenal replacement therapy by dialysis                                                 ND      Foot vascular status by Doppler                                       ND\nRenal replacement therapy by transplantation                                          ND      Urine strips for glucose and ketone measurement                       ND\nND = country did not respond to country capacity survey\n〇 = not generally available   ● = generally available\nWorld Health Organization – Diabetes country profiles, 2016.\n&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, this is a very long character string with some line breaks (the &lt;code&gt;&amp;quot;\n&amp;quot;&lt;/code&gt; character).
So first, we need to split this string into a character vector by the &lt;code&gt;&amp;quot;\n&amp;quot;&lt;/code&gt; character. Also, it might
be difficult to see, but the table starts at the line with the following string:
&lt;code&gt;&amp;quot;Prevalence of diabetes&amp;quot;&lt;/code&gt; and ends with &lt;code&gt;&amp;quot;National response to diabetes&amp;quot;&lt;/code&gt;. Also, we need to get
the name of the country from the text and add it as a column. As you can see, a whole lot
of operations are needed, so what I do is put all these operations into a function that I will apply
to each element of &lt;code&gt;raw_text&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;clean_table &amp;lt;- function(table){
    table &amp;lt;- str_split(table, &amp;quot;\n&amp;quot;, simplify = TRUE)
    country_name &amp;lt;- table[1, 1] %&amp;gt;% 
        stringr::str_squish() %&amp;gt;% 
        stringr::str_extract(&amp;quot;.+?(?=\\sTotal)&amp;quot;)
    table_start &amp;lt;- stringr::str_which(table, &amp;quot;Prevalence of diabetes&amp;quot;)
    table_end &amp;lt;- stringr::str_which(table, &amp;quot;National response to diabetes&amp;quot;)
    table &amp;lt;- table[1, (table_start +1 ):(table_end - 1)]
    table &amp;lt;- str_replace_all(table, &amp;quot;\\s{2,}&amp;quot;, &amp;quot;|&amp;quot;)
    text_con &amp;lt;- textConnection(table)
    data_table &amp;lt;- read.csv(text_con, sep = &amp;quot;|&amp;quot;)
    colnames(data_table) &amp;lt;- c(&amp;quot;Condition&amp;quot;, &amp;quot;Males&amp;quot;, &amp;quot;Females&amp;quot;, &amp;quot;Total&amp;quot;)
    dplyr::mutate(data_table, Country = country_name)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I advise you to go through all these operations and understand what each does. However, I will
describe some of the lines, such as this one:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;stringr::str_extract(&amp;quot;.+?(?=\\sTotal)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This uses a very bizarre looking regular expression: &lt;code&gt;&amp;quot;.+?(?=\\sTotal)&amp;quot;&lt;/code&gt;. This extracts everything
before a space, followed by the string &lt;code&gt;&amp;quot;Total&amp;quot;&lt;/code&gt;. This is because the first line, the one that contains
the name of the country looks like this: &lt;code&gt;&amp;quot;Luxembourg Total population: 567 000\n&amp;quot;&lt;/code&gt;. So everything
before a space followed by the word &lt;code&gt;&amp;quot;Total&amp;quot;&lt;/code&gt; is the country name. Then there’s these lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;table &amp;lt;- str_replace_all(table, &amp;quot;\\s{2,}&amp;quot;, &amp;quot;|&amp;quot;)
text_con &amp;lt;- textConnection(table)
data_table &amp;lt;- read.csv(text_con, sep = &amp;quot;|&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first lines replaces 2 spaces or more (“&lt;code&gt;\\s{2,}&lt;/code&gt;”) with &lt;code&gt;&amp;quot;|&amp;quot;&lt;/code&gt;. The reason I do this is because
then I can read the table back into R as a data frame by specifying the separator as the “|” character.
On the second line, I define &lt;code&gt;table&lt;/code&gt; as a text connection, that I can then read back into R using
&lt;code&gt;read.csv()&lt;/code&gt;. On the second to the last line I change the column names and then I add a column
called &lt;code&gt;&amp;quot;Country&amp;quot;&lt;/code&gt; to the data frame.&lt;/p&gt;
&lt;p&gt;Now, I can map this useful function to the list of raw text extracted from the pdfs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diabetes &amp;lt;- map_df(raw_text, clean_table) %&amp;gt;% 
    gather(Sex, Share, Males, Females, Total) %&amp;gt;% 
    mutate(Share = as.numeric(str_extract(Share, &amp;quot;\\d{1,}\\.\\d{1,}&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I reshape the data with the &lt;code&gt;gather()&lt;/code&gt; function (see what the data looks like before and after
reshaping). I then convert the &lt;code&gt;&amp;quot;Share&amp;quot;&lt;/code&gt; column into a numeric (it goes from something that looks
like &lt;code&gt;&amp;quot;12.3 %&amp;quot;&lt;/code&gt; into &lt;code&gt;12.3&lt;/code&gt;) and then I can create a nice plot. But first let’s take a look at
the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diabetes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Condition                  Country     Sex Share
## 1             Diabetes               Luxembourg   Males   8.3
## 2           Overweight               Luxembourg   Males  70.7
## 3              Obesity               Luxembourg   Males  28.3
## 4  Physical inactivity               Luxembourg   Males  28.2
## 5             Diabetes                   France   Males   9.5
## 6           Overweight                   France   Males  69.9
## 7              Obesity                   France   Males  25.3
## 8  Physical inactivity                   France   Males  21.2
## 9             Diabetes                  Germany   Males   8.4
## 10          Overweight                  Germany   Males  67.0
## 11             Obesity                  Germany   Males  24.1
## 12 Physical inactivity                  Germany   Males  20.1
## 13            Diabetes United States Of America   Males   9.8
## 14          Overweight United States Of America   Males  74.1
## 15             Obesity United States Of America   Males  33.7
## 16 Physical inactivity United States Of America   Males  27.6
## 17            Diabetes                 Portugal   Males  10.7
## 18          Overweight                 Portugal   Males  65.0
## 19             Obesity                 Portugal   Males  21.4
## 20 Physical inactivity                 Portugal   Males  33.5
## 21            Diabetes           United Kingdom   Males   8.4
## 22          Overweight           United Kingdom   Males  71.1
## 23             Obesity           United Kingdom   Males  28.5
## 24 Physical inactivity           United Kingdom   Males  35.4
## 25            Diabetes               Luxembourg Females   5.3
## 26          Overweight               Luxembourg Females  51.5
## 27             Obesity               Luxembourg Females  21.3
## 28 Physical inactivity               Luxembourg Females  31.7
## 29            Diabetes                   France Females   6.6
## 30          Overweight                   France Females  58.6
## 31             Obesity                   France Females  26.1
## 32 Physical inactivity                   France Females  31.2
## 33            Diabetes                  Germany Females   6.4
## 34          Overweight                  Germany Females  52.7
## 35             Obesity                  Germany Females  21.4
## 36 Physical inactivity                  Germany Females  26.5
## 37            Diabetes United States Of America Females   8.3
## 38          Overweight United States Of America Females  65.3
## 39             Obesity United States Of America Females  36.3
## 40 Physical inactivity United States Of America Females  42.1
## 41            Diabetes                 Portugal Females   7.8
## 42          Overweight                 Portugal Females  55.0
## 43             Obesity                 Portugal Females  22.8
## 44 Physical inactivity                 Portugal Females  40.8
## 45            Diabetes           United Kingdom Females   6.9
## 46          Overweight           United Kingdom Females  62.4
## 47             Obesity           United Kingdom Females  31.1
## 48 Physical inactivity           United Kingdom Females  44.3
## 49            Diabetes               Luxembourg   Total   6.8
## 50          Overweight               Luxembourg   Total  61.0
## 51             Obesity               Luxembourg   Total  24.8
## 52 Physical inactivity               Luxembourg   Total  30.0
## 53            Diabetes                   France   Total   8.0
## 54          Overweight                   France   Total  64.1
## 55             Obesity                   France   Total  25.7
## 56 Physical inactivity                   France   Total  26.4
## 57            Diabetes                  Germany   Total   7.4
## 58          Overweight                  Germany   Total  59.7
## 59             Obesity                  Germany   Total  22.7
## 60 Physical inactivity                  Germany   Total  23.4
## 61            Diabetes United States Of America   Total   9.1
## 62          Overweight United States Of America   Total  69.6
## 63             Obesity United States Of America   Total  35.0
## 64 Physical inactivity United States Of America   Total  35.0
## 65            Diabetes                 Portugal   Total   9.2
## 66          Overweight                 Portugal   Total  59.8
## 67             Obesity                 Portugal   Total  22.1
## 68 Physical inactivity                 Portugal   Total  37.3
## 69            Diabetes           United Kingdom   Total   7.7
## 70          Overweight           United Kingdom   Total  66.7
## 71             Obesity           United Kingdom   Total  29.8
## 72 Physical inactivity           United Kingdom   Total  40.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s go for the plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(diabetes) + theme_fivethirtyeight() + scale_fill_hc() +
    geom_bar(aes(y = Share, x = Sex, fill = Country), 
             stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
    facet_wrap(~Condition)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-06-10-scraping_pdfs_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That was a whole lot of work for such a simple plot!&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>{pmice}, an experimental package for missing data imputation in parallel using {mice} and {furrr}</title>
      <link>/blog/2018-04-15-announcing_pmice/</link>
      <pubDate>Sun, 15 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-15-announcing_pmice/</guid>
      <description>&lt;p&gt;Yesterday I wrote &lt;a href=&#34;http://www.brodrigues.co/blog/2018-04-14-playing_with_furrr/&#34;&gt;this blog post&lt;/a&gt;
which showed how one could use &lt;code&gt;{furrr}&lt;/code&gt; and &lt;code&gt;{mice}&lt;/code&gt; to impute missing data in parallel, thus
speeding up the process tremendously.&lt;/p&gt;
&lt;p&gt;To make using this snippet of code easier, I quickly cobbled together an experimental package
called &lt;code&gt;{pmice}&lt;/code&gt; that you can install from Github:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/pmice&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For now, it returns a list of &lt;code&gt;mids&lt;/code&gt; objects and not a &lt;code&gt;mids&lt;/code&gt; object like &lt;code&gt;mice::mice()&lt;/code&gt; does,
but I’ll be working on it. Contributions welcome!&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Imputing missing values in parallel using {furrr}</title>
      <link>/blog/2018-04-14-playing_with_furrr/</link>
      <pubDate>Sat, 14 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-14-playing_with_furrr/</guid>
      <description>&lt;p&gt;Today I saw this tweet on my timeline:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;For those of us that just can&amp;#39;t wait until RStudio officially supports parallel purrr in &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt;, boy have I got something for you. &lt;br&gt;&lt;br&gt;Introducing `furrr`, parallel purrr through the use of futures. Go ahead, break things, you know you want to:&lt;a href=&#34;https://t.co/l9z1UC2Tew&#34;&gt;https://t.co/l9z1UC2Tew&lt;/a&gt;&lt;/p&gt;&amp;mdash; Davis Vaughan (@dvaughan32) &lt;a href=&#34;https://twitter.com/dvaughan32/status/984828716181319680?ref_src=twsrc%5Etfw&#34;&gt;April 13, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;and as a heavy &lt;code&gt;{purrr}&lt;/code&gt; user, as well as the happy owner of a 6-core AMD Ryzen 5 1600X cpu,
I was very excited to try out &lt;code&gt;{furrr}&lt;/code&gt;. For those unfamiliar with &lt;code&gt;{purrr}&lt;/code&gt;, you can
read some of my previous blog posts on it &lt;a href=&#34;http://www.brodrigues.co/blog/2017-03-24-lesser_known_purrr/&#34;&gt;here&lt;/a&gt;,
&lt;a href=&#34;http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/&#34;&gt;here&lt;/a&gt; or
&lt;a href=&#34;http://www.brodrigues.co/blog/2018-01-19-mapping_functions_with_any_cols/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To summarize very quickly: &lt;code&gt;{purrr}&lt;/code&gt; contains so-called higher order functions, which are functions
that take other functions as argument. One such function is &lt;code&gt;map()&lt;/code&gt;. Consider the following simple example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers &amp;lt;- seq(1, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want the square root of this numbers, you can of course simply use the &lt;code&gt;sqrt()&lt;/code&gt; function,
because it is vectorized:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(numbers)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751
##  [8] 2.828427 3.000000 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But in a lot of situations, the solution is not so simple. Sometimes you have to loop over the
values. This is what we would need to do if &lt;code&gt;sqrt()&lt;/code&gt; was not vectorized:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt_numbers &amp;lt;- rep(0, 10)

for(i in length(numbers)){
  sqrt_numbers[i] &amp;lt;- sqrt(numbers[i])
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First, you need to initialize a container, and then you have to populate the &lt;code&gt;sqrt_numbers&lt;/code&gt; list with the results.
Using, &lt;code&gt;{purrr}&lt;/code&gt; is way easier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
map(numbers, sqrt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051
## 
## [[4]]
## [1] 2
## 
## [[5]]
## [1] 2.236068
## 
## [[6]]
## [1] 2.44949
## 
## [[7]]
## [1] 2.645751
## 
## [[8]]
## [1] 2.828427
## 
## [[9]]
## [1] 3
## 
## [[10]]
## [1] 3.162278&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;map()&lt;/code&gt; is only one of the nice functions that are bundled inside &lt;code&gt;{purrr}&lt;/code&gt;. Mastering &lt;code&gt;{purrr}&lt;/code&gt; can really make you a much
more efficient R programmer. Anyways, recently, I have been playing around with imputation and the &lt;code&gt;{mice}&lt;/code&gt; package.
&lt;code&gt;{mice}&lt;/code&gt; comes with an example dataset called &lt;code&gt;boys&lt;/code&gt;, let’s take a look at it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mice)

data(boys)

brotools::describe(boys) %&amp;gt;%
  select(variable, type, n_missing, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 13
##   variable type    n_missing  nobs   mean    sd mode     min   max   q25
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numeric         0   748   9.16  6.89 0.035  0.035  21.2  1.58
## 2 bmi      Numeric        21   748  18.1   3.05 14.54 11.8    31.7 15.9 
## 3 hc       Numeric        46   748  51.5   5.91 33.7  33.7    65   48.1 
## 4 hgt      Numeric        20   748 132.   46.5  50.1  50     198   84.9 
## 5 tv       Numeric       522   748  11.9   7.99 &amp;lt;NA&amp;gt;   1      25    4   
## 6 wgt      Numeric         4   748  37.2  26.0  3.65   3.14  117.  11.7 
## 7 gen      Factor        503   748  NA    NA    &amp;lt;NA&amp;gt;  NA      NA   NA   
## 8 phb      Factor        503   748  NA    NA    &amp;lt;NA&amp;gt;  NA      NA   NA   
## 9 reg      Factor          3   748  NA    NA    south NA      NA   NA   
## # ... with 3 more variables: median &amp;lt;dbl&amp;gt;, q75 &amp;lt;dbl&amp;gt;, n_unique &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the code above I use the &lt;code&gt;describe()&lt;/code&gt; function from my personal package to get some summary
statistics of the &lt;code&gt;boys&lt;/code&gt; dataset (you can read more about this function
&lt;a href=&#34;http://www.brodrigues.co/blog/2018-04-10-brotools_describe&#34;&gt;here&lt;/a&gt;). I am especially interested in the number of
missing values, which is why I re-order the columns. If I did not re-order the columns, it would not appear in
the output on my blog.&lt;/p&gt;
&lt;p&gt;We see that some columns have a lot of missing values. Using the &lt;code&gt;mice&lt;/code&gt; function, it is very
easy to impute them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;start &amp;lt;- Sys.time()
imp_boys &amp;lt;- mice(boys, m = 10, maxit = 100, printFlag = FALSE)
end &amp;lt;- Sys.time() - start

print(end)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 3.290611 mins&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imputation on a single core took around 3 minutes on my computer. This might seem ok, but if you
have a larger data set with more variables, 3 minutes can become 3 hours. And if you increase &lt;code&gt;maxit&lt;/code&gt;,
which helps convergence, or the number of imputations, 3 hours can become 30 hours. With a 6-core CPU
this could potentially be brought down to 5 hours (in theory). Let’s see if we can go faster,
but first let’s take a look at the imputed data.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;mice()&lt;/code&gt; function returns a &lt;code&gt;mids&lt;/code&gt; object. If you want to look at the data, you have to use
the &lt;code&gt;complete()&lt;/code&gt; function (careful, there is also a &lt;code&gt;complete()&lt;/code&gt; function in the &lt;code&gt;{tidyr}&lt;/code&gt; package,
so to avoid problems, I suggest you explicitely call &lt;code&gt;mice::complete()&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_boys &amp;lt;- mice::complete(imp_boys, &amp;quot;long&amp;quot;)

brotools::describe(imp_boys) %&amp;gt;%
  select(variable, type, n_missing, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 11 x 13
##    variable type   n_missing  nobs   mean     sd mode     min   max    q25
##    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
##  1 .id      Numer…         0  7480 374.   216.   1      1     748   188.  
##  2 .imp     Numer…         0  7480   5.5    2.87 1      1      10     3   
##  3 age      Numer…         0  7480   9.16   6.89 0.035  0.035  21.2   1.58
##  4 bmi      Numer…         0  7480  18.0    3.03 14.54 11.8    31.7  15.9 
##  5 hc       Numer…         0  7480  51.6    5.89 33.7  33.7    65    48.3 
##  6 hgt      Numer…         0  7480 131.    46.5  50.1  50     198    83   
##  7 tv       Numer…         0  7480   8.39   8.09 2      1      25     2   
##  8 wgt      Numer…         0  7480  37.1   26.0  3.65   3.14  117.   11.7 
##  9 gen      Factor         0  7480  NA     NA    G1    NA      NA    NA   
## 10 phb      Factor         0  7480  NA     NA    P1    NA      NA    NA   
## 11 reg      Factor         0  7480  NA     NA    south NA      NA    NA   
## # ... with 3 more variables: median &amp;lt;dbl&amp;gt;, q75 &amp;lt;dbl&amp;gt;, n_unique &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, no more missing values. The “long” argument inside &lt;code&gt;mice::complete()&lt;/code&gt; is needed if you want the &lt;code&gt;complete()&lt;/code&gt;
function to return a long dataset. Doing the above “manually” using &lt;code&gt;{purrr}&lt;/code&gt; is possible with the following
code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;start &amp;lt;- Sys.time()
imp_boys_purrr &amp;lt;- map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))
end &amp;lt;- Sys.time() - start

print(end)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 3.393966 mins&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What this does is map the function &lt;code&gt;~mice(data = boys, m = ., maxit = 100, printFlag = FALSE)&lt;/code&gt;
to a list of &lt;code&gt;1&lt;/code&gt;s, and creates 10 imputed data sets. &lt;code&gt;m = .&lt;/code&gt; means that &lt;code&gt;m&lt;/code&gt; will be equal to whatever is inside
the list we are mapping our function over, so &lt;code&gt;1&lt;/code&gt;, then &lt;code&gt;1&lt;/code&gt; then another &lt;code&gt;1&lt;/code&gt; etc….
It took around the same amount of time as using &lt;code&gt;mice()&lt;/code&gt; directly.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;imp_boys_purrr&lt;/code&gt; is now a list of 10 &lt;code&gt;mids&lt;/code&gt; objects. We thus need to map &lt;code&gt;mice::complete()&lt;/code&gt;
to &lt;code&gt;imp_boys_purrr&lt;/code&gt; to get the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_boys_purrr_complete &amp;lt;- map(imp_boys_purrr, mice::complete)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, &lt;code&gt;imp_boys_purrr_complete&lt;/code&gt; is a list of 10 datasets. Let’s map &lt;code&gt;brotools::describe()&lt;/code&gt; to it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(imp_boys_purrr_complete, brotools::describe)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.7   5.90 33.7  33.7    65   48.3    53.1  56  
## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   84     146.  175. 
## 5 tv       Numer…   748   8.35  8.00 3      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[2]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5
## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.3    53.2  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   145.  175  
## 5 tv       Numer…   748   8.37  8.02 1      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.9    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P2    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[3]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.5  19.5
## 3 hc       Numer…   748  51.6   5.87 33.7  33.7    65   48.5    53.3  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   145.  175  
## 5 tv       Numer…   748   8.46  8.14 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[4]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.1   3.02 14.54 11.8    31.7 15.9    17.5  19.4
## 3 hc       Numer…   748  51.7   5.93 33.7  33.7    65   48.5    53.4  56  
## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   82.9   145.  175  
## 5 tv       Numer…   748   8.45  8.11 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.0  3.65   3.14  117.  11.7    34.7  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[5]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.03 14.54 11.8    31.7 15.9    17.5  19.5
## 3 hc       Numer…   748  51.6   5.91 33.7  33.7    65   48.3    53.2  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175. 
## 5 tv       Numer…   748   8.21  8.02 3      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[6]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.7   5.89 33.7  33.7    65   48.3    53.2  56  
## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  
## 5 tv       Numer…   748   8.44  8.24 3      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.0  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[7]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.1   3.04 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.6   5.88 33.7  33.7    65   48.2    53.2  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.5   146.  175  
## 5 tv       Numer…   748   8.47  8.15 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[8]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.4
## 3 hc       Numer…   748  51.6   5.85 33.7  33.7    65   48.2    53.3  56  
## 4 hgt      Numer…   748 131.   46.5  50.1  50     198   83.0   146.  175  
## 5 tv       Numer…   748   8.36  8.06 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.2  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[9]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.05 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.6   5.90 33.7  33.7    65   48.3    53.2  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.9   146.  175  
## 5 tv       Numer…   748   8.57  8.25 1      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;
## 
## [[10]]
## # A tibble: 9 x 13
##   variable type    nobs   mean    sd mode     min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 age      Numer…   748   9.16  6.89 0.035  0.035  21.2  1.58   10.5  15.3
## 2 bmi      Numer…   748  18.0   3.04 14.54 11.8    31.7 15.9    17.4  19.5
## 3 hc       Numer…   748  51.6   5.89 33.7  33.7    65   48.3    53.1  56  
## 4 hgt      Numer…   748 131.   46.6  50.1  50     198   83.0   146.  175  
## 5 tv       Numer…   748   8.49  8.18 2      1      25    2       3    15  
## 6 wgt      Numer…   748  37.1  26.1  3.65   3.14  117.  11.7    34.6  59.6
## 7 gen      Factor   748  NA    NA    G1    NA      NA   NA      NA    NA  
## 8 phb      Factor   748  NA    NA    P1    NA      NA   NA      NA    NA  
## 9 reg      Factor   748  NA    NA    south NA      NA   NA      NA    NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before merging this 10 datasets together into one, it would be nice to have a column with the id of the datasets.
This can easily be done with a variant of &lt;code&gt;purrr::map()&lt;/code&gt;, called &lt;code&gt;map2()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_boys_purrr &amp;lt;- map2(.x = seq(1,10), .y = imp_boys_purrr_complete, ~mutate(.y, imp_id = as.character(.x)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;map2()&lt;/code&gt; applies a function, say &lt;code&gt;f()&lt;/code&gt;, to 2 lists sequentially: &lt;code&gt;f(x_1, y_1)&lt;/code&gt;, then &lt;code&gt;f(x_2, y_2)&lt;/code&gt;, etc…
So here I map &lt;code&gt;mutate()&lt;/code&gt; to create a new column, &lt;code&gt;imp_id&lt;/code&gt; in each dataset. Now let’s bind the rows and
take a look at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_boys_purrr &amp;lt;- bind_rows(imp_boys_purrr)

imp_boys_purrr %&amp;gt;%
  brotools::describe() %&amp;gt;%
  select(variable, type, n_missing, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 13
##    variable type     n_missing  nobs   mean    sd mode     min   max   q25
##    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58
##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 
##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.3 
##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   
##  5 tv       Numeric          0  7480   8.42  8.11 3      1      25    2   
##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 
##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   
##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   
##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   
## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   
## # ... with 3 more variables: median &amp;lt;dbl&amp;gt;, q75 &amp;lt;dbl&amp;gt;, n_unique &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may ask yourself why I am bothering with all this. This will become apparent now. We can now use
the code we wrote to get our 10 imputed datasets using &lt;code&gt;purrr::map()&lt;/code&gt; and simply use &lt;code&gt;furrr::future_map()&lt;/code&gt;
to parallelize the imputation process:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(furrr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: future&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plan(multiprocess)

start &amp;lt;- Sys.time()
imp_boys_future &amp;lt;- future_map(rep(1, 10), ~mice(data = boys, m = ., maxit = 100, printFlag = FALSE))
end &amp;lt;- Sys.time() - start

print(end)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 33.73772 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Boooom! Much faster! And simply by loading &lt;code&gt;{furrr}&lt;/code&gt;, then using &lt;code&gt;plan(multiprocess)&lt;/code&gt; to run the code in
parallel (if you forget that, the code will run on a single core) and using &lt;code&gt;future_map()&lt;/code&gt; instead of &lt;code&gt;map()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;imp_boys_future_complete &amp;lt;- map(imp_boys_future, mice::complete)

imp_boys_future &amp;lt;- map2(.x = seq(1,10), .y = imp_boys_future_complete, ~mutate(.y, imp_id = as.character(.x)))

imp_boys_future &amp;lt;- bind_rows(imp_boys_future)

imp_boys_future %&amp;gt;%
  brotools::describe() %&amp;gt;%
  select(variable, type, n_missing, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 13
##    variable type     n_missing  nobs   mean    sd mode     min   max   q25
##    &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 age      Numeric          0  7480   9.16  6.89 0.035  0.035  21.2  1.58
##  2 bmi      Numeric          0  7480  18.0   3.04 14.54 11.8    31.7 15.9 
##  3 hc       Numeric          0  7480  51.6   5.89 33.7  33.7    65   48.4 
##  4 hgt      Numeric          0  7480 131.   46.5  50.1  50     198   83   
##  5 tv       Numeric          0  7480   8.35  8.09 3      1      25    2   
##  6 wgt      Numeric          0  7480  37.1  26.0  3.65   3.14  117.  11.7 
##  7 imp_id   Charact…         0  7480  NA    NA    1     NA      NA   NA   
##  8 gen      Factor           0  7480  NA    NA    G1    NA      NA   NA   
##  9 phb      Factor           0  7480  NA    NA    P1    NA      NA   NA   
## 10 reg      Factor           0  7480  NA    NA    south NA      NA   NA   
## # ... with 3 more variables: median &amp;lt;dbl&amp;gt;, q75 &amp;lt;dbl&amp;gt;, n_unique &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So imputation went from 3.4 minutes (around 200 seconds) to 30 seconds. How cool is that? If you want to play around
with &lt;code&gt;{furrr}&lt;/code&gt; you must install it from Github, as it is not yet available on CRAN:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;DavisVaughan/furrr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are not comfortable with &lt;code&gt;map()&lt;/code&gt; (and thus &lt;code&gt;future_map()&lt;/code&gt;) but still want to impute in parallel, there is this
very nice script &lt;a href=&#34;https://github.com/gerkovink/parlMICE&#34;&gt;here&lt;/a&gt; to do just that. I created a package around this script,
called &lt;a href=&#34;https://github.com/b-rodrigues/parlMICE&#34;&gt;parlMICE&lt;/a&gt; (the same name as the script), to make installation and
usage easier. You can install it like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/parlMICE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Get basic summary statistics for all the variables in a data frame</title>
      <link>/blog/2018-04-10-brotools_describe/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-10-brotools_describe/</guid>
      <description>&lt;p&gt;I have added a new function to my &lt;code&gt;{brotools}&lt;/code&gt; package, called &lt;code&gt;describe()&lt;/code&gt;,
which takes a data frame as an argument, and returns another data frame with descriptive
statistics. It is very much inspired by the &lt;a href=&#34;https://github.com/ropenscilabs/skimr&#34;&gt;&lt;code&gt;{skmir}&lt;/code&gt;&lt;/a&gt;
package but also by
&lt;a href=&#34;https://github.com/bjornerstedt/assist/blob/master/R/describe.R&#34;&gt;&lt;code&gt;assist::describe()&lt;/code&gt;&lt;/a&gt; (click
on the packages to be redirected to the respective Github repos)
but I wanted to write my own for two reasons: first, as an exercice, and second
I really only needed the function &lt;code&gt;skim_to_wide()&lt;/code&gt; from &lt;code&gt;{skimr}&lt;/code&gt;. So instead of installing a
whole package for a single function, I decided to write my own (since I use &lt;code&gt;{brotools}&lt;/code&gt; daily).&lt;/p&gt;
&lt;p&gt;Below you can see it in action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
data(starwars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brotools::describe(starwars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 13
##    variable  type   nobs  mean    sd mode     min   max   q25 median   q75
##    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 birth_ye… Nume…    87  87.6 155.  19         8   896  35       52  72  
##  2 height    Nume…    87 174.   34.8 172       66   264 167      180 191  
##  3 mass      Nume…    87  97.3 169.  77        15  1358  55.6     79  84.5
##  4 eye_color Char…    87  NA    NA   blue      NA    NA  NA       NA  NA  
##  5 gender    Char…    87  NA    NA   male      NA    NA  NA       NA  NA  
##  6 hair_col… Char…    87  NA    NA   blond     NA    NA  NA       NA  NA  
##  7 homeworld Char…    87  NA    NA   Tatoo…    NA    NA  NA       NA  NA  
##  8 name      Char…    87  NA    NA   Luke …    NA    NA  NA       NA  NA  
##  9 skin_col… Char…    87  NA    NA   fair      NA    NA  NA       NA  NA  
## 10 species   Char…    87  NA    NA   Human     NA    NA  NA       NA  NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the object that is returned by &lt;code&gt;describe()&lt;/code&gt; is a &lt;code&gt;tibble&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For now, this function does not handle dates, but it’s in the pipeline.&lt;/p&gt;
&lt;p&gt;You can also only describe certain columns:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brotools::describe(starwars, height, mass, name)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 13
##   variable type    nobs  mean    sd mode      min   max   q25 median   q75
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 height   Numer…    87 174.   34.8 172        66   264 167      180 191  
## 2 mass     Numer…    87  97.3 169.  77         15  1358  55.6     79  84.5
## 3 name     Chara…    87  NA    NA   Luke S…    NA    NA  NA       NA  NA  
## # ... with 2 more variables: n_missing &amp;lt;int&amp;gt;, n_unique &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to try it out,
you can install &lt;code&gt;{brotools}&lt;/code&gt; from Github:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;devtools::install_github(&amp;quot;b-rodrigues/brotools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting {sparklyr}, {h2o}, {rsparkling} to work together and some fun with bash</title>
      <link>/blog/2018-03-03-sparklyr_h2o_rsparkling/</link>
      <pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-03-sparklyr_h2o_rsparkling/</guid>
      <description>&lt;p&gt;This is going to be the type of blog posts that would perhaps be better as a gist, but it is easier for me to use my blog as my own personal collection of gists. Plus, someone else might find this useful, so here it is! In this blog post I am going to show a little trick to randomly sample rows from a text file using bash, and then train a model using the &lt;code&gt;{h2o}&lt;/code&gt; package. I will also use the &lt;code&gt;{rsparkling}&lt;/code&gt; package. From &lt;code&gt;{rsparkling}&lt;/code&gt;’s documentation: &lt;em&gt;&lt;code&gt;{rsparkling}&lt;/code&gt; is a package that provides an R interface to the &lt;code&gt;H2O&lt;/code&gt; Sparkling Water machine learning library.&lt;/em&gt; and will be needed to transfer the data from Spark to H2O.&lt;/p&gt;
&lt;p&gt;In a &lt;a href=&#34;http://www.brodrigues.co/blog/2018-02-16-importing_30gb_of_data/&#34;&gt;previous blog post&lt;/a&gt; I used the &lt;code&gt;{sparklyr}&lt;/code&gt; package to load a 30GB csv file into R. I created the file by combining around 300 csv files, each around 80MB big. Here, I would like to use the machine learning functions included in the &lt;code&gt;{h2o}&lt;/code&gt; packages to train a random forest on this data. However, I only want to have a simple prototype that simply runs, and check if all the packages work well together. If everything is ok, I’ll keep iterating to make the model better (in a possible subsequent post).&lt;/p&gt;
&lt;p&gt;For fast prototyping, using 30GB of data is not a good idea, so I am going to sample 500000 from this file using the linux command line (works on macOS too and also on Windows if you installed the linux subsystem). Why not use R to sample 500000 rows? Because on my machine, loading the 30GB file takes 25 minutes. Sampling half a million lines from it would take quite long too. So here are some bash lines that do that directly on the file, without needing to load it into R beforehand:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ get_seeded_random()
{
  seed=&amp;quot;$1&amp;quot;
  openssl enc -aes-256-ctr -pass pass:&amp;quot;$seed&amp;quot; -nosalt \
  &amp;lt;/dev/zero 2&amp;gt;/dev/null
}

[18-03-03 21:50] brodriguesco in /Documents/AirOnTimeCSV ➤ sed &amp;quot;1 d&amp;quot; combined.csv | shuf --random-source=&amp;lt;(get_seeded_random 42) -n 500000 &amp;gt; small_combined_temp.csv

[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ head -1 combined.csv &amp;gt; colnames.csv

[18-03-03 21:56] brodriguesco in /Documents/AirOnTimeCSV ➤ cat colnames.csv small_combined_temp.csv &amp;gt; small_combined.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first function I took from the &lt;a href=&#34;https://www.gnu.org/software/coreutils/manual/html_node/Random-sources.html&#34;&gt;gnu coreutils manual&lt;/a&gt; which allows me to fix the random seed to reproduce the same sampling of the file. Then I use &lt;code&gt;&amp;quot;sed 1 d&amp;quot; cobmined.csv&lt;/code&gt; to remove the first line of &lt;code&gt;combined.csv&lt;/code&gt; which is the header of the file. Then, I pipe the result of &lt;code&gt;sed&lt;/code&gt; using &lt;code&gt;|&lt;/code&gt; to &lt;code&gt;shuf&lt;/code&gt; which does the shuffling. The option &lt;code&gt;--random-source=&amp;lt;(get_seeded_random 42)&lt;/code&gt; fixes the seed, and &lt;code&gt;-n 500000&lt;/code&gt; only shuffles 500000 and not the whole file. The final bit of the line, &lt;code&gt;&amp;gt; small_combined_temp.csv&lt;/code&gt;, saves the result to &lt;code&gt;small_cobmined_temp.csv&lt;/code&gt;. Because I need to add back the header, I use &lt;code&gt;head -1&lt;/code&gt; to extract the first line of &lt;code&gt;combined.csv&lt;/code&gt; and save it into &lt;code&gt;colnames.csv&lt;/code&gt;. Finally, I bind the rows of both files using &lt;code&gt;cat colnames.csv small_combined_temp.csv&lt;/code&gt; and save the result into &lt;code&gt;small_combined.cvs&lt;/code&gt;. Taken together, all these steps took about 5 minutes (without counting the googling around for finding how to pass a fixed seed to &lt;code&gt;shuf&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Now that I have this small dataset, I can write a small prototype:&lt;/p&gt;
&lt;p&gt;First, you need to install &lt;code&gt;{sparklyr}&lt;/code&gt;, &lt;code&gt;{rsparkling}&lt;/code&gt; and &lt;code&gt;{h2o}&lt;/code&gt;. Refer to &lt;a href=&#34;https://github.com/h2oai/rsparkling&#34;&gt;this&lt;/a&gt; to know how to install the packages. I had a mismatch between the version of H2O that was automatically installed when I installed the &lt;code&gt;{h2o}&lt;/code&gt; package, and the version of Spark that &lt;code&gt;{sparklyr}&lt;/code&gt; installed but thankfully the &lt;code&gt;{h2o}&lt;/code&gt; package returns a very helpful error message with the following lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;detach(&amp;quot;package:rsparkling&amp;quot;, unload = TRUE)
                       if (&amp;quot;package:h2o&amp;quot; %in% search()) { detach(&amp;quot;package:h2o&amp;quot;, unload = TRUE) }
                       if (isNamespaceLoaded(&amp;quot;h2o&amp;quot;)){ unloadNamespace(&amp;quot;h2o&amp;quot;) }
                       remove.packages(&amp;quot;h2o&amp;quot;)
                       install.packages(&amp;quot;h2o&amp;quot;, type = &amp;quot;source&amp;quot;, repos = &amp;quot;https://h2o-release.s3.amazonaws.com/h2o/rel-weierstrass/2/R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which tells you which version to install.&lt;/p&gt;
&lt;p&gt;So now, let’s load everything:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sparklyr)
library(rsparkling)
library(h2o)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ----------------------------------------------------------------------
## 
## Your next step is to start H2O:
##     &amp;gt; h2o.init()
## 
## For H2O package documentation, ask for help:
##     &amp;gt; ??h2o
## 
## After starting H2O, you can use the Web UI at http://localhost:54321
## For more information visit http://docs.h2o.ai
## 
## ----------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;h2o&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     cor, sd, var&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     &amp;amp;&amp;amp;, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,
##     colnames&amp;lt;-, ifelse, is.character, is.factor, is.numeric, log,
##     log10, log1p, log2, round, signif, trunc&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;h2o.init()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## H2O is not running yet, starting it now...
## 
## Note:  In case of errors look at the following log files:
##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.out
##     /tmp/Rtmph48vf9/h2o_cbrunos_started_from_r.err
## 
## 
## Starting H2O JVM and connecting: .. Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         1 seconds 944 milliseconds 
##     H2O cluster version:        3.16.0.2 
##     H2O cluster version age:    4 months and 15 days !!! 
##     H2O cluster name:           H2O_started_from_R_cbrunos_bpn152 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   6.98 GB 
##     H2O cluster total cores:    12 
##     H2O cluster allowed cores:  12 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.4.4 (2018-03-15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in h2o.clusterInfo(): 
## Your H2O cluster version is too old (4 months and 15 days)!
## Please download and install the latest version from http://h2o.ai/download/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I left all the startup messages because they’re quite helpful. Especially that bit telling you to start &lt;code&gt;H2O&lt;/code&gt; with &lt;code&gt;h2o.init()&lt;/code&gt;. If something’s wrong, &lt;code&gt;h2o.init()&lt;/code&gt; will give you helpful information.&lt;/p&gt;
&lt;p&gt;Now that all this is loaded, I can start working on the data (the steps below are explained in detail in my &lt;a href=&#34;http://www.brodrigues.co/blog/2018-02-16-importing_30gb_of_data/&#34;&gt;previous blog post&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;spark_dir = &amp;quot;/my_2_to_disk/spark/&amp;quot;

config = spark_config()

config$`sparklyr.shell.driver-memory` &amp;lt;- &amp;quot;4G&amp;quot;
config$`sparklyr.shell.executor-memory` &amp;lt;- &amp;quot;4G&amp;quot;
config$`spark.yarn.executor.memoryOverhead` &amp;lt;- &amp;quot;512&amp;quot;
config$`sparklyr.shell.driver-java-options` = paste0(&amp;quot;-Djava.io.tmpdir=&amp;quot;, spark_dir)

sc = spark_connect(master = &amp;quot;local&amp;quot;, config = config)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another useful function that allows you to check if everything is alright is &lt;code&gt;h2o_context()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;h2o_context(sc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;jobj[12]&amp;gt;
  org.apache.spark.h2o.H2OContext

Sparkling Water Context:
 * H2O name: sparkling-water-cbrunos_local-1520111879840
 * cluster size: 1
 * list of used nodes:
  (executorId, host, port)
  ------------------------
  (driver,127.0.0.1,54323)
  ------------------------

  Open H2O Flow in browser: http://127.0.0.1:54323 (CMD + click in Mac OSX)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s load the data into R with &lt;code&gt;{sparklyr}&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;air = spark_read_csv(sc, name = &amp;quot;air&amp;quot;, path = &amp;quot;small_combined.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, here, using Spark is overkill, because &lt;code&gt;small_combined.csv&lt;/code&gt; is only around 100MB big, so no need for &lt;code&gt;{sparklyr}&lt;/code&gt; but as stated in the beginning this is only to have a quick and dirty prototype. Once all the pieces are working together, I can iterate on the real data, for which &lt;code&gt;{sparklyr}&lt;/code&gt; will be needed. Now, if I needed to use &lt;code&gt;{dplyr}&lt;/code&gt; I could use it on &lt;code&gt;air&lt;/code&gt;, but I don’t want to do anything on it, so I convert it to a &lt;code&gt;h2o&lt;/code&gt; data frame. &lt;code&gt;h2o&lt;/code&gt; data frames are needed as arguments for the machine learning algorithms included in the &lt;code&gt;{h2o}&lt;/code&gt; package. &lt;code&gt;as_h2o_frame()&lt;/code&gt; is a function included in &lt;code&gt;{rsparkling}&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;air_hf = as_h2o_frame(sc, air)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, I convert the columns I need to factors (I am only using factors here):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;air_hf$ORIGIN = as.factor(air_hf$ORIGIN)
air_hf$UNIQUE_CARRIER = as.factor(air_hf$UNIQUE_CARRIER)
air_hf$DEST = as.factor(air_hf$DEST)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;{h2o}&lt;/code&gt; functions need the names of the predictors and of the target columns, so let’s define that:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;target = &amp;quot;ARR_DELAY&amp;quot;
predictors = c(&amp;quot;UNIQUE_CARRIER&amp;quot;, &amp;quot;ORIGIN&amp;quot;, &amp;quot;DEST&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s train a random Forest, without any hyper parameter tweaking:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model = h2o.randomForest(predictors, target, training_frame = air_hf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that this runs, I will in the future split the data into training, validation and test set, and train a model with better hyper parameters. For now, let’s take a look at the summary of &lt;code&gt;model&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;summary(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Model Details:
==============

H2ORegressionModel: drf
Model Key:  DRF_model_R_1520111880605_1
Model Summary:
  number_of_trees number_of_internal_trees model_size_in_bytes min_depth
1              50                       50            11055998        20
  max_depth mean_depth min_leaves max_leaves mean_leaves
1        20   20.00000       1856       6129  4763.42000

H2ORegressionMetrics: drf
** Reported on training data. **
** Metrics reported on Out-Of-Bag training samples **

MSE:  964.9246
RMSE:  31.06324
MAE:  17.65517
RMSLE:  NaN
Mean Residual Deviance :  964.9246





Scoring History:
             timestamp   duration number_of_trees training_rmse training_mae
1  2018-03-03 22:52:24  0.035 sec               0
2  2018-03-03 22:52:25  1.275 sec               1      30.93581     17.78216
3  2018-03-03 22:52:25  1.927 sec               2      31.36998     17.78867
4  2018-03-03 22:52:26  2.272 sec               3      31.36880     17.80359
5  2018-03-03 22:52:26  2.564 sec               4      31.29683     17.79467
6  2018-03-03 22:52:26  2.854 sec               5      31.31226     17.79467
7  2018-03-03 22:52:27  3.121 sec               6      31.26214     17.78542
8  2018-03-03 22:52:27  3.395 sec               7      31.20749     17.75703
9  2018-03-03 22:52:27  3.666 sec               8      31.19706     17.74753
10 2018-03-03 22:52:27  3.935 sec               9      31.16108     17.73547
11 2018-03-03 22:52:28  4.198 sec              10      31.13725     17.72493
12 2018-03-03 22:52:32  8.252 sec              27      31.07608     17.66648
13 2018-03-03 22:52:36 12.462 sec              44      31.06325     17.65474
14 2018-03-03 22:52:38 14.035 sec              50      31.06324     17.65517
   training_deviance
1
2          957.02450
3          984.07580
4          984.00150
5          979.49147
6          980.45794
7          977.32166
8          973.90720
9          973.25655
10         971.01272
11         969.52856
12         965.72249
13         964.92530
14         964.92462

Variable Importances: (Extract with `h2o.varimp`)
=================================================

Variable Importances:
        variable relative_importance scaled_importance percentage
1         ORIGIN    291883392.000000          1.000000   0.432470
2           DEST    266749168.000000          0.913890   0.395230
3 UNIQUE_CARRIER    116289536.000000          0.398411   0.172301
&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Keep trying that api call with purrr::possibly()</title>
      <link>/blog/2018-03-12-keep_trying/</link>
      <pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-12-keep_trying/</guid>
      <description>&lt;p&gt;Sometimes you need to call an api to get some result from a web service, but sometimes this call might
fail. You might get an error 500 for example, or maybe you’re making too many calls too fast. Regarding
this last point, I really encourage you to read &lt;a href=&#34;https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01&#34;&gt;Ethics in Web Scraping&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post I will show you how you can keep trying to make this api call using &lt;code&gt;purrr::possibly()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this, let’s use this function that will simulate an api call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_data = function(){
  number = rbinom(1, 1, 0.9)
  ifelse(number == 0, &amp;quot;OK&amp;quot;, stop(&amp;quot;Error: too many calls!&amp;quot;))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function simply returns a random draw from a binomial distribution. If this number equals 0
with probability 0.1, the function returns “OK”, if not, it throws an error. Because the probability
of success is only 10%, your api call might be unsuccessful:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_data()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in ifelse(number == 0, &amp;quot;OK&amp;quot;, stop(&amp;quot;Error: too many calls!&amp;quot;)) :
  Error: too many calls!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How to keep trying until it works? For this, we’re going to use &lt;code&gt;purrr::possibly()&lt;/code&gt;; this function
takes another function as argument and either returns the result, or another output in case of error,
that the user can define:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;possibly_get_data = purrr::possibly(get_data, otherwise = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s try it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(12)
possibly_get_data()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;set.seed(12)&lt;/code&gt;, the function returns a number different from 0, and thus throws an error: but
because we’re wrapping the function around &lt;code&gt;purrr::possibly()&lt;/code&gt;, the function now returns &lt;code&gt;NULL&lt;/code&gt;. The
first step is done; now we can use this to our advantage:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;definitely_get_data = function(func, n_tries, sleep, ...){

  possibly_func = purrr::possibly(func, otherwise = NULL)

  result = NULL
  try_number = 1

  while(is.null(result) &amp;amp;&amp;amp; try_number &amp;lt;= n_tries){
    print(paste(&amp;quot;Try number: &amp;quot;, try_number))
    try_number = try_number + 1
    result = possibly_func(...)
    Sys.sleep(sleep)
  }

  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;definitely_get_data()&lt;/code&gt; is a function that takes any function as argument, as well as a user provided
number of tries (as well as &lt;code&gt;...&lt;/code&gt; to pass further arguments to &lt;code&gt;func()&lt;/code&gt;). Remember, if &lt;code&gt;func()&lt;/code&gt; fails,
it will return &lt;code&gt;NULL&lt;/code&gt;; the while loop ensures that while the result is &lt;code&gt;NULL&lt;/code&gt;, and the number of tries
is below what you provided, the function will keep getting called. I didn’t talk about &lt;code&gt;sleep&lt;/code&gt;; this
argument is provided to &lt;code&gt;Sys.sleep()&lt;/code&gt; which introduces a break between calls that is equal to &lt;code&gt;sleep&lt;/code&gt;
seconds. This ensures you don’t make too many calls too fast. Let’s try it out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
definitely_get_data(get_data, 10, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Try number:  1&amp;quot;
## [1] &amp;quot;Try number:  2&amp;quot;
## [1] &amp;quot;Try number:  3&amp;quot;
## [1] &amp;quot;Try number:  4&amp;quot;
## [1] &amp;quot;Try number:  5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;OK&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It took 5 tries to get the result! However, if after 10 tries &lt;code&gt;get_data()&lt;/code&gt; fails to return
what you need it will stop (but you can increase the number of tries…).&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Importing 30GB of data into R with sparklyr</title>
      <link>/blog/2018-02-16-importing_30gb_of_data/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-02-16-importing_30gb_of_data/</guid>
      <description>&lt;p&gt;Disclaimer: the first part of this blog post draws heavily from &lt;a href=&#34;http://bconnelly.net/working-with-csvs-on-the-command-line/&#34;&gt;Working with CSVs on the Command
Line&lt;/a&gt;, which is a beautiful resource
that lists very nice tips and tricks to work with CSV files before having to load them into R, or
any other statistical software. I highly recommend it! Also, if you find this interesting, read
also &lt;a href=&#34;https://www.datascienceatthecommandline.com/&#34;&gt;Data Science at the Command Line&lt;/a&gt; another great
resource!&lt;/p&gt;
&lt;p&gt;In this blog post I am going to show you how to analyze 30GB of data. 30GB of data does not qualify
as big data, but it’s large enough that you cannot simply import it into R and start working on it,
unless you have a machine with &lt;em&gt;a lot&lt;/em&gt; of RAM.&lt;/p&gt;
&lt;p&gt;Let’s start by downloading some data. I am going to import and analyze (very briefly) the airline
dataset that you can download from Microsoft
&lt;a href=&#34;https://packages.revolutionanalytics.com/datasets/&#34;&gt;here&lt;/a&gt;. I downloaded the file
&lt;code&gt;AirOnTimeCSV.zip&lt;/code&gt; from &lt;code&gt;AirOnTime87to12&lt;/code&gt;. Once you decompress it, you’ll end up with 303 csv
files, each around 80MB. Before importing them into R, I will use command line tools to bind the
rows together. But first, let’s make sure that the datasets all have the same columns. I am using
Linux, and if you are too, or if you are using macOS, you can follow along. Windows users that
installed the Linux Subsystem can also use the commands I am going to show! First, I’ll use
the &lt;code&gt;head&lt;/code&gt; command in bash. If you’re familiar with &lt;code&gt;head()&lt;/code&gt; from R, the &lt;code&gt;head&lt;/code&gt;
command in bash works exactly the same:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[18-02-15 21:12] brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT198710.csv
&amp;quot;YEAR&amp;quot;,&amp;quot;MONTH&amp;quot;,&amp;quot;DAY_OF_MONTH&amp;quot;,&amp;quot;DAY_OF_WEEK&amp;quot;,&amp;quot;FL_DATE&amp;quot;,&amp;quot;UNIQUE_CARRIER&amp;quot;,&amp;quot;TAIL_NUM&amp;quot;,&amp;quot;FL_NUM&amp;quot;,
1987,10,1,4,1987-10-01,&amp;quot;AA&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;1&amp;quot;,12478,&amp;quot;JFK&amp;quot;,&amp;quot;NY&amp;quot;,12892,&amp;quot;LAX&amp;quot;,&amp;quot;CA&amp;quot;,&amp;quot;0900&amp;quot;,&amp;quot;0901&amp;quot;,1.00,
1987,10,2,5,1987-10-02,&amp;quot;AA&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;1&amp;quot;,12478,&amp;quot;JFK&amp;quot;,&amp;quot;NY&amp;quot;,12892,&amp;quot;LAX&amp;quot;,&amp;quot;CA&amp;quot;,&amp;quot;0900&amp;quot;,&amp;quot;0901&amp;quot;,1.00
1987,10,3,6,1987-10-03,&amp;quot;AA&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;1&amp;quot;,12478,&amp;quot;JFK&amp;quot;,&amp;quot;NY&amp;quot;,12892,&amp;quot;LAX&amp;quot;,&amp;quot;CA&amp;quot;,&amp;quot;0900&amp;quot;,&amp;quot;0859&amp;quot;,-1.00
1987,10,4,7,1987-10-04,&amp;quot;AA&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;1&amp;quot;,12478,&amp;quot;JFK&amp;quot;,&amp;quot;NY&amp;quot;,12892,&amp;quot;LAX&amp;quot;,&amp;quot;CA&amp;quot;,&amp;quot;0900&amp;quot;,&amp;quot;0900&amp;quot;,0.00,&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;let’s also check the 5 first lines of the last file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[18-02-15 21:13] cbrunos in brodriguesco in /Documents/AirOnTimeCSV ➤ head -5 airOT201212.csv
&amp;quot;YEAR&amp;quot;,&amp;quot;MONTH&amp;quot;,&amp;quot;DAY_OF_MONTH&amp;quot;,&amp;quot;DAY_OF_WEEK&amp;quot;,&amp;quot;FL_DATE&amp;quot;,&amp;quot;UNIQUE_CARRIER&amp;quot;,&amp;quot;TAIL_NUM&amp;quot;,&amp;quot;FL_NUM&amp;quot;,
2012,12,1,6,2012-12-01,&amp;quot;AA&amp;quot;,&amp;quot;N322AA&amp;quot;,&amp;quot;1&amp;quot;,12478,&amp;quot;JFK&amp;quot;,&amp;quot;NY&amp;quot;,12892,&amp;quot;LAX&amp;quot;,&amp;quot;CA&amp;quot;,&amp;quot;0900&amp;quot;,&amp;quot;0852&amp;quot;,
2012,12,2,7,2012-12-02,&amp;quot;AA&amp;quot;,&amp;quot;N327AA&amp;quot;,&amp;quot;1&amp;quot;,12478,&amp;quot;JFK&amp;quot;,&amp;quot;NY&amp;quot;,12892,&amp;quot;LAX&amp;quot;,&amp;quot;CA&amp;quot;,&amp;quot;0900&amp;quot;,&amp;quot;0853&amp;quot;,
2012,12,3,1,2012-12-03,&amp;quot;AA&amp;quot;,&amp;quot;N319AA&amp;quot;,&amp;quot;1&amp;quot;,12478,&amp;quot;JFK&amp;quot;,&amp;quot;NY&amp;quot;,12892,&amp;quot;LAX&amp;quot;,&amp;quot;CA&amp;quot;,&amp;quot;0900&amp;quot;,&amp;quot;0856&amp;quot;
2012,12,4,2,2012-12-04,&amp;quot;AA&amp;quot;,&amp;quot;N329AA&amp;quot;,&amp;quot;1&amp;quot;,12478,&amp;quot;JFK&amp;quot;,&amp;quot;NY&amp;quot;,12892,&amp;quot;LAX&amp;quot;,&amp;quot;CA&amp;quot;,&amp;quot;0900&amp;quot;,&amp;quot;1006&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why do that in bash instead of R? This way, I don’t need to import the data into R before checking
its contents!&lt;/p&gt;
&lt;p&gt;It does look like the structure did not change. Before importing the data into R, I am going to
bind the rows of the datasets using other command line tools. Again, the reason I don’t import all the files
into R is because I would need around 30GB of RAM to do so. So it’s easier
to do it with bash:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;head -1 airOT198710.csv &amp;gt; combined.csv
for file in $(ls airOT*); do cat $file | sed &amp;quot;1 d&amp;quot; &amp;gt;&amp;gt; combined.csv; done&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the first line I use &lt;code&gt;head&lt;/code&gt; again to only copy the column names (the first line of the first
file) into a new file called &lt;code&gt;combined.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This &lt;code&gt;&amp;gt;&lt;/code&gt; operator looks like the now well known pipe operator in R, &lt;code&gt;%&amp;gt;%&lt;/code&gt;, but in
bash, &lt;code&gt;%&amp;gt;%&lt;/code&gt; is actually &lt;code&gt;|&lt;/code&gt;, not &lt;code&gt;&amp;gt;&lt;/code&gt;. &lt;code&gt;&amp;gt;&lt;/code&gt; redirects the output of the left hand side to a file on
the right hand side, not to another command. On the second line, I loop over the files. I
list the files with &lt;code&gt;ls&lt;/code&gt;, and because I want only to loop over those that are named &lt;code&gt;airOTxxxxx&lt;/code&gt; I
use a regular expression, &lt;code&gt;airOT*&lt;/code&gt; to only list those. The second part is &lt;code&gt;do cat $file&lt;/code&gt;. &lt;code&gt;do&lt;/code&gt; is
self-explanatory, and &lt;code&gt;cat&lt;/code&gt; stands for &lt;code&gt;catenate&lt;/code&gt;. Think of it as &lt;code&gt;head&lt;/code&gt;, but on all rows instead
of just 5; it prints &lt;code&gt;$file&lt;/code&gt; to the terminal. &lt;code&gt;$file&lt;/code&gt; one element of the list of files I am looping over.
But because I don’t want to see the contents of &lt;code&gt;$file&lt;/code&gt; on my terminal, I redirect the output with
the pipe, &lt;code&gt;|&lt;/code&gt; to another command, &lt;code&gt;sed&lt;/code&gt;. &lt;code&gt;sed&lt;/code&gt; has an option, &lt;code&gt;&amp;quot;1 d&amp;quot;&lt;/code&gt;, and what this does is filtering
out the first line, containing the header, from &lt;code&gt;$file&lt;/code&gt; before appending it with
&lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; to &lt;code&gt;combined.csv&lt;/code&gt;. If you found this interesting, read more about it
&lt;a href=&#34;http://bconnelly.net/working-with-csvs-on-the-command-line/#combining-rows-from-two-or-more-csvs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This creates a 30GB CSV file that you can then import. But how? There seems to be different ways to
import and work with larger than memory data in R using your personal computer. I chose to use
&lt;code&gt;{sparklyr}&lt;/code&gt;, an R package that allows you to work with Apache Spark from R. Apache Spark is a &lt;em&gt;fast
and general engine for large-scale data processing&lt;/em&gt;, and &lt;code&gt;{sparklyr}&lt;/code&gt; not only offers bindings to it,
but also provides a complete &lt;code&gt;{dplyr}&lt;/code&gt; backend. Let’s start:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sparklyr)
library(tidyverse)

spark_dir = &amp;quot;/my_2_to_disk/spark/&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I first load &lt;code&gt;{sparklyr}&lt;/code&gt; and the &lt;code&gt;{tidyverse}&lt;/code&gt; and also define a &lt;code&gt;spark_dir&lt;/code&gt;. This is because
Spark creates a lot of temporary files that I want to save there instead of my root partition,
which is on my SSD. My root partition only has around 20GO of space left, so whenever I tried to
import the data I would get the following error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java.io.IOException: No space left on device&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to avoid this error, I define this directory on my 2TO hard disk.
I then define the temporary directory using the two lines below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;config = spark_config()

config$`sparklyr.shell.driver-java-options` &amp;lt;-  paste0(&amp;quot;-Djava.io.tmpdir=&amp;quot;, spark_dir)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is not sufficient however; when I tried to read in the data, I got another error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java.lang.OutOfMemoryError: Java heap space&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The solution for this one is to add the following lines to your &lt;code&gt;config()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;config$`sparklyr.shell.driver-memory` &amp;lt;- &amp;quot;4G&amp;quot;
config$`sparklyr.shell.executor-memory` &amp;lt;- &amp;quot;4G&amp;quot;
config$`spark.yarn.executor.memoryOverhead` &amp;lt;- &amp;quot;512&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I can load the data. Because I am working on my machine, I &lt;em&gt;connect&lt;/em&gt; to a &lt;code&gt;&amp;quot;local&amp;quot;&lt;/code&gt; Spark
instance. Then, using &lt;code&gt;spark_read_csv()&lt;/code&gt;, I specify the Spark connection, &lt;code&gt;sc&lt;/code&gt;, I give a name to the
data that will be inside the database and the path to it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sc = spark_connect(master = &amp;quot;local&amp;quot;, config = config)

air = spark_read_csv(sc, name = &amp;quot;air&amp;quot;, path = &amp;quot;combined.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On my machine, this took around 25 minutes, and RAM usage was around 6GO.&lt;/p&gt;
&lt;p&gt;It is possible to use standard &lt;code&gt;{dplyr}&lt;/code&gt; verbs with &lt;code&gt;{sparklyr}&lt;/code&gt; objects, so if I want the mean
delay at departure per day, I can simply write:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic = Sys.time()
mean_dep_delay = air %&amp;gt;%
  group_by(YEAR, MONTH, DAY_OF_MONTH) %&amp;gt;%
  summarise(mean_delay = mean(DEP_DELAY))
(toc = Sys.time() - tic)
Time difference of 0.05634999 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s amazing, only 0.06 seconds to compute these means! Wait a minute, that’s weird… I mean my computer
is brand new and quite powerful but still… Let’s take a look at &lt;code&gt;mean_dep_delay&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(mean_dep_delay)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Source:   lazy query [?? x 4]
# Database: spark_connection
# Groups:   YEAR, MONTH
   YEAR MONTH DAY_OF_MONTH mean_delay
  &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;
1  1987    10            9       6.71
2  1987    10           10       3.72
3  1987    10           12       4.95
4  1987    10           14       4.53
5  1987    10           23       6.48
6  1987    10           29       5.77
Warning messages:
1: Missing values are always removed in SQL.
Use `AVG(x, na.rm = TRUE)` to silence this warning
2: Missing values are always removed in SQL.
Use `AVG(x, na.rm = TRUE)` to silence this warning&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Surprisingly, this takes around 5 minutes to print? Why? Look at the class of &lt;code&gt;mean_dep_delay&lt;/code&gt;:
it’s a lazy query that only gets evaluated once I need it. Look at the first line; &lt;code&gt;lazy query [?? x 4]&lt;/code&gt;.
This means that I don’t even know how many rows are in &lt;code&gt;mean_dep_delay&lt;/code&gt;!
The contents of &lt;code&gt;mean_dep_delay&lt;/code&gt; only get computed once I explicitly ask for them. I do so
with the &lt;code&gt;collect()&lt;/code&gt; function, which transfers the Spark object into R’s memory:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tic = Sys.time()
r_mean_dep_delay = collect(mean_dep_delay)
(toc = Sys.time() - tic)
Time difference of 5.2399 mins&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, because it took such a long time to compute: I save it to disk:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;saveRDS(r_mean_dep_delay, &amp;quot;mean_dep_delay.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now that I &lt;em&gt;transferred&lt;/em&gt; this sparklyr table to a standard tibble in R, I can create a nice plot
of departure delays:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)

dep_delay =  r_mean_dep_delay %&amp;gt;%
  arrange(YEAR, MONTH, DAY_OF_MONTH) %&amp;gt;%
  mutate(date = ymd(paste(YEAR, MONTH, DAY_OF_MONTH, sep = &amp;quot;-&amp;quot;)))

ggplot(dep_delay, aes(date, mean_delay)) + geom_smooth()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;gam&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-02-16-importing_30gb_of_data_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That’s it for now, but in a future blog post I will continue to explore this data!&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting job search by training a random forest on an unbalanced dataset</title>
      <link>/blog/2018-02-11-census-random_forest/</link>
      <pubDate>Sun, 11 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-02-11-census-random_forest/</guid>
      <description>&lt;p&gt;In this blog post, I am going to train a random forest on census data from the US to predict
the probability that someone is looking for a job. To this end, I downloaded the US 1990 census
data from the UCI &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/US+Census+Data+%281990%29&#34;&gt;Machine Learning Repository&lt;/a&gt;.
Having a background in economics, I am always quite interested by such datasets. I downloaded the raw
data which is around 820mb uncompressed. You can download it from this folder
&lt;a href=&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/census1990-mld/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Before training a random forest on it, some preprocessing is needed. First problem: the columns
in the data do not have names. Actually, training a random forest on unamed variables is possible,
but I like my columns to have names. The names are on a separate file, called &lt;code&gt;USCensus1990raw.attributes.txt&lt;/code&gt;.
This is how this file looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VAR:        TYP:   DES:    LEN:   CAT:    VARIABLE/CATEGORY LABEL:
__________________________________________________________________________________
HISPANIC     C       X      3             Detailed Hispanic Origin Code See Append
                                  000     Not Hispanic 006 199
                                  001     Mexican, Mex Am 210 220
                                  002     Puerto Rican 261 270
                                  003     Cuban 271 274
                                  004     Other Hispanic 200 209, 250 260, 290 401

VAR:        TYP:   DES:    LEN:   CAT:    VARIABLE/CATEGORY LABEL:
__________________________________________________________________________________
HOUR89       C       X      2             Usual Hrs. Worked Per Week Last Yr. 1989
                                  00      N/a Less Than 16 Yrs. Old/did Not Work i
                                  99      99 or More Usual Hrs.

VAR:        TYP:   DES:    LEN:   CAT:    VARIABLE/CATEGORY LABEL:
__________________________________________________________________________________
HOURS        C       X      2             Hrs. Worked Last Week
                                  00      N/a Less Than 16 Yrs. Old/not At Work/un
                                  99      99 or More Hrs. Worked Last Week

VAR:        TYP:   DES:    LEN:   CAT:    VARIABLE/CATEGORY LABEL:
__________________________________________________________________________________
IMMIGR       C       X      2             Yr. of Entry
                                  00      Born in the U.S.
                                  01      1987 to 1990
                                  02      1985 to 1986
                                  03      1982 to 1984


                                  04      1980 or 1981
                                  05      1975 to 1979
                                  06      1970 to 1974
                                  07      1965 to 1969
                                  08      1960 to 1964
                                  09      1950 to 1959
                                  10      Before 1950&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variable names are always written in upper case and sometimes end with some numbers.
Regular expressions will help extract these column names:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

census_raw = import(&amp;quot;USCensus1990raw.data.txt&amp;quot;)

attributes_raw = readLines(&amp;quot;USCensus1990raw.attributes.txt&amp;quot;)

column_names = str_extract_all(attributes_raw, &amp;quot;^[A-Z]+(\\d{1,}|[A-Z])\\s+&amp;quot;) %&amp;gt;%
  flatten %&amp;gt;%
  str_trim %&amp;gt;%
  tolower&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;readLines&lt;/code&gt; I load this text file into R. Then with &lt;code&gt;stringr::str_extract_all&lt;/code&gt;, I can extract
the variable names from this text file. The regular expression, &lt;code&gt;^[A-Z]+(\\d{1,}|[A-Z])\\s+&lt;/code&gt; can
seem complicated, but by breaking it up, it’ll be clear:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;^[A-Z]+&lt;/code&gt;: matches one or more uppercase letter, at the beginning of the line (hence the &lt;code&gt;^&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;\\d{1,}&lt;/code&gt;: matches one or more digits&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[A-Z]\\s+&lt;/code&gt;: matches one uppercase letter, followed by one or more spaces&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(\\d{1,}|[A-Z])\\s+&lt;/code&gt;: matches one or more digits OR (the &lt;code&gt;|&lt;/code&gt;) matches one uppercase letter, followed by one or more spaces&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This regular expression matches only the variable names. By using &lt;code&gt;^&lt;/code&gt; I only limit myself to the
uppercase letters at the start of the line, which already removes a lot of unneeded lines from the
text. Then, by matching numbers or letters, followed by spaces, I avoid matching strings such as
&lt;code&gt;VAR:&lt;/code&gt;. There’s probably a shorter way to write this regular expression, but since this one works,
I stopped looking for another solution.&lt;/p&gt;
&lt;p&gt;Now that I have a vector called &lt;code&gt;column_names&lt;/code&gt;, I can baptize the columns in my dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(census_raw) &amp;lt;- column_names&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also add a column called &lt;code&gt;caseid&lt;/code&gt; to the dataset, but it’s actually not really needed. But it
made me look for and find &lt;code&gt;rownames_to_column()&lt;/code&gt;, which can be useful:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census = census_raw %&amp;gt;%
  rownames_to_column(&amp;quot;caseid&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I select the variables I need. I use &lt;code&gt;dplyr::select()&lt;/code&gt; to select the columns I need (actually,
I will remove some of these later for the purposes of the blog post, but will continue exploring
them. Maybe write a part 2?):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census %&amp;lt;&amp;gt;%
  select(caseid, age, citizen, class, disabl1, disabl2, lang1, looking, fertil, hour89, hours, immigr,
         industry, means, occup, powpuma, powstate, pwgt1, race, ragechld, rearning,
         relat1, relat2, remplpar, rlabor, rpincome, rpob, rspouse, rvetserv, school, sex, tmpabsnt,
         travtime, week89, work89, worklwk, yearsch, yearwrk, yrsserv)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I convert factor variables to factors and only relevel the &lt;code&gt;race&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;census %&amp;lt;&amp;gt;%
  mutate(race = case_when(race == 1 ~ &amp;quot;white&amp;quot;,
                          race == 2 ~ &amp;quot;black&amp;quot;,
                          !(race %in% c(1, 2)) ~ &amp;quot;other&amp;quot;,
                          is.na(race) ~ NA_character_)) %&amp;gt;%
  filter(looking != 0) %&amp;gt;%
  mutate_at(vars(class, disabl1, disabl2, lang1, looking, fertil, immigr, industry, means,
                 occup, powstate, race, ragechld, remplpar, rlabor, rpob, rspouse,
                 rvetserv, school, sex, tmpabsnt, work89, worklwk, yearwrk),
            as.factor) %&amp;gt;%
  select(looking, age, class, disabl1, disabl2, lang1, fertil, immigr,
         race, ragechld, remplpar, rlabor, rpob, rspouse,
         rvetserv, school, sex, tmpabsnt, work89, worklwk, yearwrk, rpincome, rearning,
         travtime, week89, work89, hours, yearsch, yrsserv) %&amp;gt;%
  as_tibble

export(census, &amp;quot;regression_data.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the variable I want to predict is &lt;code&gt;looking&lt;/code&gt; which has 2 levels (I removed the level &lt;code&gt;0&lt;/code&gt;, which
stands for &lt;code&gt;NA&lt;/code&gt;). I convert all the variables that are supposed to be factors into factors using
&lt;code&gt;mutate_at()&lt;/code&gt; and then reselect a subsample of the columns. &lt;code&gt;census&lt;/code&gt; is now a tibble with 39
columns and 2458285 rows. I will train the forest on a subsample only, because with cross validation
it would take forever on the whole dataset.&lt;/p&gt;
&lt;p&gt;I run the training on another script, that I will then run using the &lt;code&gt;Rscript&lt;/code&gt; command instead of
running it from Spacemacs (yes, I don’t use RStudio at home but Spacemacs + ESS). Here’s the script:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
library(doParallel)
library(rio)

reg_data = import(&amp;quot;regression_data.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;janitor::tabyl(reg_data$looking)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reg_data$looking      n   percent
1                1  75792 0.1089562
2                2 619827 0.8910438&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;90% of the individuals in the sample are not looking for a new job. For training purposes, I will
only use 50000 observations instead of the whole sample. I’m already thinking about writing another
blog post where I show how to use the whole data. But 50000 observations should be more than enough
to have a pretty nice model. However, having 90% of observations belonging to a single class can
cause problems with the model; the model might predict that everyone should belong to class 2 and in
doing so, the model would be 90% accurate! Let’s ignore this for now, but later I am going to
tackle this issue with a procedure calleds SMOTE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
sample_df = sample_n(reg_data, 50000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, using &lt;code&gt;caret::trainIndex()&lt;/code&gt;, I partition the data into a training sample and a testing
sample:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainIndex = createDataPartition(sample_df$looking, p = 0.8,
                                 list = FALSE,
                                 times = 1)

train_data = sample_df[trainIndex, ]
test_data = sample_df[-trainIndex, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also save the testing data to disk, because when the training is done I’ll lose my R session
(remember, I’ll run the training using Rscript):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;saveRDS(test_data, &amp;quot;test_data.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before training the model, I’ll change some options; I’ll do 5-fold cross validation that I repeat
5 times. This will further split the training set into training/testing sets which will increase
my confidence in the metrics that I get from the training. This will ensure that the best model
really is the best, and not a fluke resulting from the splitting of the data that I did beforehand.
Then, I will test the best model on the testing data from above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitControl &amp;lt;- trainControl(
  method = &amp;quot;repeatedcv&amp;quot;,
  number = 5,
  repeats = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A very nice feature from the &lt;code&gt;caret&lt;/code&gt; package is the possibility to make the training in parallel.
For this, load the &lt;code&gt;doParallel&lt;/code&gt; package (which I did above), and then register the number of cores
you want to use for training with &lt;code&gt;makeCluster()&lt;/code&gt;. You can replace &lt;code&gt;detectCores()&lt;/code&gt; by the number of
cores you want to use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cl = makeCluster(detectCores())
registerDoParallel(cl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can train the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit_caret = train(looking ~ .,
                  data = train_data,
                  trainControl = fitControl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because it takes around 1 and a half hours to train, I save the model to disk using &lt;code&gt;saveRDS()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;saveRDS(fit_caret, &amp;quot;model_unbalanced.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The picture below shows all the cores from my computer running and RAM usage being around 20gb during
the training process:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/training_cpu.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;And this the results of training the random forest on the unbalanced data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_unbalanced = readRDS(&amp;quot;model_unbalanced.rds&amp;quot;)

test_data = readRDS(&amp;quot;test_data.rds&amp;quot;)

plot(model_unbalanced)

preds = predict.train(model_unbalanced, newdata = test_data)

confusionMatrix(preds, reference = test_data$looking)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/plot_acc_unbalanced.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Confusion Matrix and Statistics

Reference
Prediction     1     2
1  1287   112
2   253 12348

Accuracy : 0.9739
95% CI : (0.9712, 0.9765)
    No Information Rate : 0.89
    P-Value [Acc &amp;gt; NIR] : &amp;lt; 2.2e-16

                  Kappa : 0.8613
 Mcnemar&amp;#39;s Test P-Value : 2.337e-13

            Sensitivity : 0.83571
            Specificity : 0.99101
         Pos Pred Value : 0.91994
         Neg Pred Value : 0.97992
             Prevalence : 0.11000
         Detection Rate : 0.09193
   Detection Prevalence : 0.09993
      Balanced Accuracy : 0.91336

       &amp;#39;Positive&amp;#39; Class : 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If someone really is looking for a job, the model is able to predict it correctly 92% of the times
and 98% of the times if that person is not looking for a job. It’s slightly better than simply saying
than no one is looking for a job, which would be right 90% of the times, but not great either.&lt;/p&gt;
&lt;p&gt;To train to make the model more accurate in predicting class 1, I will resample the training set, but by
downsampling class 2 and upsampling class 1. This can be done with the function &lt;code&gt;SMOTE()&lt;/code&gt; from the
&lt;code&gt;{DMwR}&lt;/code&gt; package. However, the testing set should have the same distribution as the population,
so I should not apply &lt;code&gt;SMOTE()&lt;/code&gt; to the testing set. I will resplit the data, but this time with a 95/5 % percent
split; this way I have 5% of the original dataset used for testing, I can use &lt;code&gt;SMOTE()&lt;/code&gt; on the
95% remaining training set. Because &lt;code&gt;SMOTE&lt;/code&gt;ing takes some time, I save the &lt;em&gt;SMOTE&lt;/em&gt;d training set
using &lt;code&gt;readRDS()&lt;/code&gt; for later use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reg_data = import(&amp;quot;regression_data.rds&amp;quot;)


set.seed(1234)
trainIndex = createDataPartition(reg_data$looking, p = 0.95,
                                 list = FALSE,
                                 times = 1)

test_data = reg_data[-trainIndex, ]

saveRDS(test_data, &amp;quot;test_smote.rds&amp;quot;)


# Balance training set
train_data = reg_data[trainIndex, ]

train_smote = DMwR::SMOTE(looking ~ ., train_data, perc.over = 100, perc.under=200)

saveRDS(train_smote, &amp;quot;train_smote.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The testing set has 34780 observations and below you can see the distribution of the target variable,
&lt;code&gt;looking&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;janitor::tabyl(test_data$looking)
  test_data$looking     n   percent
1                 1  3789 0.1089419
2                 2 30991 0.8910581&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_smote = readRDS(&amp;quot;model_smote.rds&amp;quot;)

test_smote = readRDS(&amp;quot;test_smote.rds&amp;quot;)

plot(model_smote)

preds = predict.train(model_smote, newdata = test_smote)

confusionMatrix(preds, reference = test_smote$looking)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Confusion Matrix and Statistics

Reference
Prediction     1     2
1  3328  1142
2   461 29849

Accuracy : 0.9539
95% CI : (0.9517, 0.9561)
    No Information Rate : 0.8911
    P-Value [Acc &amp;gt; NIR] : &amp;lt; 2.2e-16

                  Kappa : 0.78
 Mcnemar&amp;#39;s Test P-Value : &amp;lt; 2.2e-16

            Sensitivity : 0.87833
            Specificity : 0.96315
         Pos Pred Value : 0.74452
         Neg Pred Value : 0.98479
             Prevalence : 0.10894
         Detection Rate : 0.09569
   Detection Prevalence : 0.12852
      Balanced Accuracy : 0.92074

       &amp;#39;Positive&amp;#39; Class : 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/plot_acc_smote.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The balanced accuracy is higher, but unlike what I expected (and hoped), this model is worse in
predicting class 1! I will be trying one last thing; since I have a lot of data at my disposal,
I will simply sample 25000 observations where the target variable &lt;code&gt;looking&lt;/code&gt; equals 1, and then sample
another 25000 observations where the target variable equals 2 (without using &lt;code&gt;SMOTE()&lt;/code&gt;). Then I’ll
simply bind the rows and train the model on that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reg_data = import(&amp;quot;regression_data.rds&amp;quot;)


set.seed(1234)
trainIndex = createDataPartition(reg_data$looking, p = 0.95,
                                 list = FALSE,
                                 times = 1)

test_data = reg_data[-trainIndex, ]

saveRDS(test_data, &amp;quot;test_up_down.rds&amp;quot;)


# Balance training set
train_data = reg_data[trainIndex, ]

train_data1 = train_data %&amp;gt;%
  filter(looking == 1)

set.seed(1234)
train_data1 = sample_n(train_data1, 25000)


train_data2 = train_data %&amp;gt;%
  filter(looking == 2)

set.seed(1234)
train_data2 = sample_n(train_data2, 25000)

train_up_down = bind_rows(train_data1, train_data2)


fitControl &amp;lt;- trainControl(
  method = &amp;quot;repeatedcv&amp;quot;,
  number = 5,
  repeats = 5)

cl = makeCluster(detectCores())
registerDoParallel(cl)

fit_caret = train(looking ~ .,
                  data = train_up_down,
                  trControl = fitControl,
                  preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;))

saveRDS(fit_caret, &amp;quot;model_up_down.rds&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here are the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_up_down = readRDS(&amp;quot;model_up_down.rds&amp;quot;)

test_up_down = readRDS(&amp;quot;test_up_down.rds&amp;quot;)

plot(model_up_down)

preds = predict.train(model_up_down, newdata = test_up_down)

confusionMatrix(preds, reference = test_up_down$looking)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Confusion Matrix and Statistics

Reference
Prediction     1     2
1  3403  1629
2   386 29362

Accuracy : 0.9421
95% CI : (0.9396, 0.9445)
    No Information Rate : 0.8911
    P-Value [Acc &amp;gt; NIR] : &amp;lt; 2.2e-16

                  Kappa : 0.7391
 Mcnemar&amp;#39;s Test P-Value : &amp;lt; 2.2e-16

            Sensitivity : 0.89813
            Specificity : 0.94744
         Pos Pred Value : 0.67627
         Neg Pred Value : 0.98702
             Prevalence : 0.10894
         Detection Rate : 0.09784
   Detection Prevalence : 0.14468
      Balanced Accuracy : 0.92278

       &amp;#39;Positive&amp;#39; Class : 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/plot_acc_smote.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Looks like it’s not much better than using &lt;code&gt;SMOTE()&lt;/code&gt;!&lt;/p&gt;
&lt;p&gt;There are several ways I could achieve better predictions; tuning the model is one possibility,
or perhaps going with another type of model altogether. I will certainly come back to this dataset
in future blog posts!&lt;/p&gt;
&lt;p&gt;Using the best model, let’s take a look at which variables are the most important for predicting job search:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; varImp(model_unbalanced)
rf variable importance

only 20 most important variables shown (out of 109)

Overall
rlabor3   100.0000
rlabor6    35.2702
age         6.3758
rpincome    6.2964
tmpabsnt1   5.8047
rearning    5.3560
week89      5.2863
tmpabsnt2   4.0195
yearsch     3.4892
tmpabsnt3   1.7434
work892     1.3231
racewhite   0.9002
class1      0.7866
school2     0.7117
yearwrk2    0.6970
sex1        0.6955
disabl12    0.6809
lang12      0.6619
rpob23      0.6507
rspouse6    0.6330&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s also possible to have a plot of the above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(varImp(model_unbalanced))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/img/varimp.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;To make sense of this, we have to read the description of the features &lt;a href=&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/census1990-mld/USCensus1990raw.attributes.txt&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rlabor3&lt;/code&gt; is the most important variable, and means that the individual is unemployed. &lt;code&gt;rlabor6&lt;/code&gt;
means not in the labour force. Then the age of the individual as well as the individual’s income
play a role. &lt;code&gt;tmpabsnt&lt;/code&gt; is a variable that equals 1 if the individual is temporary absent from work,
due to a layoff. All these variables having an influence on the probability of looking
for a job make sense, but looks like a very simple model focusing on just a couple of variables
would make as good a job as the random forest.&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mapping a list of functions to a list of datasets with a list of columns as arguments</title>
      <link>/blog/2018-01-19-mapping_functions_with_any_cols/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-01-19-mapping_functions_with_any_cols/</guid>
      <description>&lt;p&gt;This week I had the opportunity to teach R at my workplace, again. This course was the “advanced
R” course, and unlike the one I taught at the end of last year, I had one more day (so 3 days in total)
where I could show my colleagues the joys of the &lt;code&gt;tidyverse&lt;/code&gt; and R.&lt;/p&gt;
&lt;p&gt;To finish the section on programming with R, which was the very last section of the whole 3 day course
I wanted to blow their minds; I had already shown them packages from the &lt;code&gt;tidyverse&lt;/code&gt; in the previous
days, such as &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;purrr&lt;/code&gt; and &lt;code&gt;stringr&lt;/code&gt;, among others. I taught them how to use &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;broom&lt;/code&gt;
and &lt;code&gt;modelr&lt;/code&gt;. They also liked &lt;code&gt;janitor&lt;/code&gt; and &lt;code&gt;rio&lt;/code&gt; very much. I noticed that it took them a bit more
time and effort for them to digest &lt;code&gt;purrr::map()&lt;/code&gt; and &lt;code&gt;purrr::reduce()&lt;/code&gt;, but they all seemed to see
how powerful these functions were. To finish on a very high note, I showed them the ultimate
&lt;code&gt;purrr::map()&lt;/code&gt; use case.&lt;/p&gt;
&lt;p&gt;Consider the following; imagine you have a situation where you are working on a list of datasets.
These datasets might be the same, but for different years, or for different countries, or they might
be completely different datasets entirely. If you used &lt;code&gt;rio::import_list()&lt;/code&gt; to read them into R,
you will have them in a nice list. Let’s consider the following list as an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)
data(iris)

data_list = list(mtcars, iris)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I made the choice to have completely different datasets. Now, I would like to map some functions
to the columns of these datasets. If I only worked on one, for example on &lt;code&gt;mtcars&lt;/code&gt;, I would do
something like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_summarise_f = function(dataset, cols, funcs){
  dataset %&amp;gt;%
    summarise_at(vars(!!!cols), funs(!!!funcs))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then I would use my function like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
  my_summarise_f(quos(mpg, drat, hp), quos(mean, sd, max))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max
## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93
##   hp_max
## 1    335&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;my_summarise_f()&lt;/code&gt; takes a dataset, a list of columns and a list of functions as arguments and uses
tidy evaluation to apply &lt;code&gt;mean()&lt;/code&gt;, &lt;code&gt;sd()&lt;/code&gt;, and &lt;code&gt;max()&lt;/code&gt; to the columns &lt;code&gt;mpg&lt;/code&gt;, &lt;code&gt;drat&lt;/code&gt; and &lt;code&gt;hp&lt;/code&gt;
of &lt;code&gt;mtcars&lt;/code&gt;. That’s pretty useful, but not useful enough! Now I want to apply this to the list of
datasets I defined above. For this, let’s define the list of columns I want to work on:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cols_mtcars = quos(mpg, drat, hp)
cols_iris = quos(Sepal.Length, Sepal.Width)

cols_list = list(cols_mtcars, cols_iris)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s use some &lt;code&gt;purrr&lt;/code&gt; magic to apply the functions I want to the columns I have defined in
&lt;code&gt;list_cols&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map2(data_list,
     cols_list,
     my_summarise_f, funcs = quos(mean, sd, max))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max
## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93
##   hp_max
## 1    335
## 
## [[2]]
##   Sepal.Length_mean Sepal.Width_mean Sepal.Length_sd Sepal.Width_sd
## 1          5.843333         3.057333       0.8280661      0.4358663
##   Sepal.Length_max Sepal.Width_max
## 1              7.9             4.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s pretty useful, but not useful enough! I want to also use different functions to different datasets!&lt;/p&gt;
&lt;p&gt;Well, let’s define a list of functions then:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;funcs_mtcars = quos(mean, sd, max)
funcs_iris = quos(median, min)

funcs_list = list(funcs_mtcars, funcs_iris)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because there is no &lt;code&gt;map3()&lt;/code&gt;, we need to use &lt;code&gt;pmap()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pmap(
  list(
    dataset = data_list,
    cols = cols_list,
    funcs = funcs_list
  ),
  my_summarise_f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##   mpg_mean drat_mean  hp_mean   mpg_sd   drat_sd    hp_sd mpg_max drat_max
## 1 20.09062  3.596563 146.6875 6.026948 0.5346787 68.56287    33.9     4.93
##   hp_max
## 1    335
## 
## [[2]]
##   Sepal.Length_median Sepal.Width_median Sepal.Length_min Sepal.Width_min
## 1                 5.8                  3              4.3               2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I’m satisfied! Let me tell you, this blew their minds 😄!&lt;/p&gt;
&lt;p&gt;To be able to use things like that, I told them to always solve a problem for a single example, and
from there, try to generalize their solution using functional programming tools found in &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you found this blog post useful, you might want to follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;
for blog post updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>It&#39;s lists all the way down, part 2: We need to go deeper</title>
      <link>/blog/2018-01-05-lists_all_the_way2/</link>
      <pubDate>Fri, 05 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-01-05-lists_all_the_way2/</guid>
      <description>&lt;p&gt;Shortly after my &lt;a href=&#34;http://www.brodrigues.co/blog/2018-01-03-lists_all_the_way/&#34;&gt;previous blog post&lt;/a&gt;,
I saw this tweet on my timeline:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;The purrr resolution for 2018 -  learn at least one purrr function per week - is officially launched with encouragement and inspiration from &lt;a href=&#34;https://twitter.com/statwonk?ref_src=twsrc%5Etfw&#34;&gt;@statwonk&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/hadleywickham?ref_src=twsrc%5Etfw&#34;&gt;@hadleywickham&lt;/a&gt;.  We start with modify_depth: &lt;a href=&#34;https://t.co/dCMnSHP7Pl&#34;&gt;https://t.co/dCMnSHP7Pl&lt;/a&gt;. Please join to learn and share.  &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt;&lt;/p&gt;&amp;mdash; Isabella R. Ghement (@IsabellaGhement) &lt;a href=&#34;https://twitter.com/IsabellaGhement/status/948685418731487232?ref_src=twsrc%5Etfw&#34;&gt;January 3, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This is a great initiative, and a big coincidence, as I just had blogged about nested lists and how
to map over them. I also said this in my previous blog post:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There is also another function that you might want to study, modify_depth() which solves related
issues but I will end the blog post here. I might talk about it in a future blog post.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And so after I got this reply from &lt;a href=&#34;https://twitter.com/IsabellaGhement&#34;&gt;&lt;code&gt;@IsabellaGhement&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Bruno, I would love it if you would chime in with an explicit contrast between nested map calls (which I personally find a bit clunky) and alternatives.  In other words, present solutions side-by-side and highlight pros and cons.  That would be very useful! 🤗&lt;/p&gt;&amp;mdash; Isabella R. Ghement (@IsabellaGhement) &lt;a href=&#34;https://twitter.com/IsabellaGhement/status/949029796788367361?ref_src=twsrc%5Etfw&#34;&gt;January 4, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;What else was I supposed to do than blog about &lt;code&gt;purrr::modify_depth()&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;Bear in mind that I was not really familiar with this function before writing my last blog post;
and even then, I decided to keep it for another blog post, which is this one. Which came much faster
than what I had originally planned. So I might have missed some functionality; if that’s the case
don’t hesitate to tweet me an example or send me an email! (bruno at brodrigues dot co)&lt;/p&gt;
&lt;p&gt;So what is this blog post about? It’s about lists, nested lists, and some things that you can do with
them. Let’s use the same example as in my last post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)

nice_function = function(df, param1, param2){
  df = df %&amp;gt;%
    filter(cyl == param1, am == param2) %&amp;gt;%
    mutate(result = mpg * param1 * (2 - param2))

  return(df)
}

nice_function(mtcars, 4, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;values_cyl = c(4, 6, 8)

values_am = c(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’re here, we would like to apply &lt;code&gt;nice_function()&lt;/code&gt; to each element of &lt;code&gt;values_cyl&lt;/code&gt; and
&lt;code&gt;values_am&lt;/code&gt;. In essence, loop over these values. But because loops are not really easy to manipulate,
(as explained, in part, &lt;a href=&#34;http://blog.rdata.lu/post/2017-12-21-skip-errors-in-r-by-not-writing-loops/&#34;&gt;here&lt;/a&gt;)
I use the &lt;code&gt;map*&lt;/code&gt; family of functions included in &lt;code&gt;purrr&lt;/code&gt; (When I teach R, I only show loops in the
&lt;em&gt;advanced topics&lt;/em&gt; chapter of my notes). So let’s “loop” over &lt;code&gt;values_cyl&lt;/code&gt; and &lt;code&gt;values_am&lt;/code&gt; with &lt;code&gt;map()&lt;/code&gt;
(and not &lt;code&gt;map_df()&lt;/code&gt;; there is a reason for this, bear with me):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [[1]][[1]]
##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0
## 
## [[1]][[2]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8
## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2
## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4
## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6
## 
## [[1]][[3]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2
## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8
## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4
## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8
## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2
## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4
## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4
## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2
## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0
## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2
## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8
## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2
## 
## 
## [[2]]
## [[2]][[1]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2
## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6
## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6
## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6
## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2
## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0
## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6
## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6
## 
## [[2]][[2]]
##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result
## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0
## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0
## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2
## 
## [[2]][[3]]
##    mpg cyl disp  hp drat   wt qsec vs am gear carb result
## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4
## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Until now, nothing new compared to my previous post (so if you have a hard time to follow what I’m
doing here, go read it &lt;a href=&#34;http://www.brodrigues.co/blog/2018-01-03-lists_all_the_way/&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;As far as I know, there is no way, in this example, to avoid this nested map call. However,
suppose now that you want to apply a function to each single data frame contained in the list &lt;code&gt;result&lt;/code&gt;.
Of course, here, you could simply use &lt;code&gt;bind_rows()&lt;/code&gt; to have a single data frame and then apply your
function to it. But suppose that you want to keep this list structure; at the end, I will give an
example of why you might want that, using another &lt;code&gt;purrr&lt;/code&gt; function, &lt;code&gt;walk()&lt;/code&gt; and Thomas’ J. Leeper
brilliant &lt;a href=&#34;https://github.com/leeper/rio&#34;&gt;&lt;code&gt;rio&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;So suppose you want to use this function here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;double_col = function(dataset, col){
  col = enquo(col)
  col_name = paste0(&amp;quot;double_&amp;quot;, quo_name(col))
  dataset %&amp;gt;%
    mutate(!!col_name := 2*(!!col))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to double the values of a column of a dataset. It uses &lt;code&gt;tidyeval&lt;/code&gt;’s &lt;code&gt;enquo()&lt;/code&gt;, &lt;code&gt;quo_name()&lt;/code&gt; and &lt;code&gt;!!()&lt;/code&gt;
functions to make it work with &lt;code&gt;tidyverse&lt;/code&gt; functions such as &lt;code&gt;mutate()&lt;/code&gt;. You can use it like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;double_col(mtcars, hp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb double_hp
## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4       220
## 2  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4       220
## 3  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1       186
## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1       220
## 5  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2       350
## 6  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1       210
## 7  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4       490
## 8  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2       124
## 9  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2       190
## 10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4       246
## 11 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4       246
## 12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3       360
## 13 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3       360
## 14 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3       360
## 15 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4       410
## 16 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4       430
## 17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4       460
## 18 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1       132
## 19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2       104
## 20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1       130
## 21 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1       194
## 22 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2       300
## 23 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2       300
## 24 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4       490
## 25 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2       350
## 26 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1       132
## 27 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2       182
## 28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2       226
## 29 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4       528
## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6       350
## 31 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8       670
## 32 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2       218&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice, but you want to use this function on all of the data frames contained in your &lt;code&gt;result&lt;/code&gt; list.
You can use a nested &lt;code&gt;map()&lt;/code&gt; as before:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(result, ~map(., .f = double_col, col = disp))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [[1]][[1]]
##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2
## 
## [[1]][[2]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0
## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0
## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2
## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2
## 
## [[1]][[3]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0
## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0
## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6
## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6
## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6
## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0
## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0
## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0
## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0
## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0
## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0
## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0
## 
## 
## [[2]]
## [[2]][[1]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0
## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4
## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4
## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2
## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0
## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6
## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2
## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0
## 
## [[2]][[2]]
##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320
## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320
## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290
## 
## [[2]][[3]]
##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp
## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702
## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but there’s an easier solution, which is using &lt;code&gt;modify_depth()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(result = modify_depth(result, .depth = 2, double_col, col = disp))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [[1]][[1]]
##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2
## 
## [[1]][[2]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0
## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0
## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2
## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2
## 
## [[1]][[3]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0
## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0
## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6
## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6
## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6
## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0
## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0
## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0
## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0
## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0
## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0
## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0
## 
## 
## [[2]]
## [[2]][[1]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2       216.0
## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6       157.4
## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6       151.4
## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6       142.2
## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2       158.0
## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0       240.6
## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6       190.2
## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6       242.0
## 
## [[2]][[2]]
##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0         320
## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0         320
## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2         290
## 
## [[2]][[3]]
##    mpg cyl disp  hp drat   wt qsec vs am gear carb result double_disp
## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4         702
## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0         602&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So how does it work? &lt;code&gt;modify_depth()&lt;/code&gt; needs a list and a &lt;code&gt;.depth&lt;/code&gt; argument, which corresponds to
where you you want to apply your function. The following lines of code might help you understand:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Depth of 1:

result[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2
## 
## [[2]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8       516.0
## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2       450.0
## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4       335.2
## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6       335.2
## 
## [[3]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result double_disp
## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2       720.0
## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8       720.0
## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4       551.6
## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8       551.6
## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2       551.6
## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4       944.0
## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4       920.0
## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2       880.0
## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0       636.0
## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2       608.0
## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8       700.0
## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2       800.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, a depth of 1 corresponds to a list of three data frame. Can you use your function
&lt;code&gt;double_col()&lt;/code&gt; on a list of three data frames? No, because the domain of &lt;code&gt;double_col()&lt;/code&gt; is the set
of data frames, not the set of lists of data frames. So you need to go deeper:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Depth of 2:

result[[1]][[1]] # or try result[[1]][[2]] or result[[1]][[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result double_disp
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2       293.4
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4       281.6
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0       240.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At the depth of 2, you’re dealing with data frames! So you can use your function &lt;code&gt;double_col()&lt;/code&gt;.
With a depth of 2, one might not see the added value of &lt;code&gt;modify_depth()&lt;/code&gt; over nested map calls, but
if you have to go even deeper, nested map calls are very confusing and verbose.&lt;/p&gt;
&lt;p&gt;Now for the last part; why doing all this, and not simply bind all the rows, apply &lt;code&gt;double_col()&lt;/code&gt;
and call it a day? Well, suppose that there is a reason you have these data frames inside lists; for
example, the first element, i.e., &lt;code&gt;result[[1]]&lt;/code&gt; might be data for, say, Portugal, for 3 different years.
&lt;code&gt;result[[2]]&lt;/code&gt; however, is data for France, for the same years. Suppose also that you have to give
this data, after having worked on it, to a colleague (or to another institution) in the Excel format;
one Excel workbook per country, one sheet per year. This example might seem contrived, but I have
been confronted to this exact situation very often. Well, if you bind all the rows together, how are
you going to save the data in the workbooks like you are required to?&lt;/p&gt;
&lt;p&gt;Well, thanks to &lt;code&gt;rio&lt;/code&gt;, one line of code is enough:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rio)

walk2(result, list(&amp;quot;portugal.xlsx&amp;quot;, &amp;quot;france.xlsx&amp;quot;), export)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I know what you’re thinking; &lt;em&gt;Bruno, that’s two lines of code!&lt;/em&gt;. Yes, but I had to load &lt;code&gt;rio&lt;/code&gt;. Also,
&lt;code&gt;walk()&lt;/code&gt; (and &lt;code&gt;walk2()&lt;/code&gt;) are basically the same as &lt;code&gt;map()&lt;/code&gt;, but you use &lt;code&gt;walk()&lt;/code&gt; over &lt;code&gt;map()&lt;/code&gt; when
you are only interested in the side effect of the function you are applying over your list; here, &lt;code&gt;export()&lt;/code&gt;
which is &lt;code&gt;rio&lt;/code&gt;’s function to write data to disk. The side effect of this function is… writing data to disk!
You could have used &lt;code&gt;map2()&lt;/code&gt; just the same, but I wanted to show you &lt;code&gt;walk2()&lt;/code&gt; (however, you cannot
replace &lt;code&gt;map()&lt;/code&gt; by &lt;code&gt;walk()&lt;/code&gt; in most cases; try it and see what happens).&lt;/p&gt;
&lt;p&gt;Here’s what it looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/deeper_xlsx.gif&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I have two Excel workbooks, (one per list), where each sheet is a data frame!&lt;/p&gt;
&lt;p&gt;If you enjoy these blog posts, you can follow me on &lt;a href=&#34;https://twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>It&#39;s lists all the way down</title>
      <link>/blog/2018-01-03-lists_all_the_way/</link>
      <pubDate>Wed, 03 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-01-03-lists_all_the_way/</guid>
      <description>&lt;p&gt;&lt;em&gt;There’s a part 2 to this post: read it &lt;a href=&#34;http://www.brodrigues.co/blog/2018-01-05-lists_all_the_way2/&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Today, I had the opportunity to help someone over at the R for Data Science Slack group (read more
about this group &lt;a href=&#34;https://medium.com/@kierisi/r4ds-the-next-iteration-d51e0a1b0b82&#34;&gt;here&lt;/a&gt;) and I
thought that the question asked could make for an interesting blog post, so here it is!&lt;/p&gt;
&lt;p&gt;Disclaimer: the way I’m doing things here is totally not optimal, but I want to illustrate how to map
functions over nested lists. But I show the optimal way at the end, so for the people that are
familiar with &lt;code&gt;purrr&lt;/code&gt; don’t get mad at me.&lt;/p&gt;
&lt;p&gt;Suppose you have to do certain data transformation tasks on a data frame, and you write a nice function
that does that for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)

nice_function = function(df, param1, param2){
  df = df %&amp;gt;%
    filter(cyl == param1, am == param2) %&amp;gt;%
    mutate(result = mpg * param1 * (2 - param2))

  return(df)
}

nice_function(mtcars, 4, 0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This might seem like a silly function and not a nice function, but it will illustrate the point I want
to make (and the question that was asked) very well. This function is completely useless, but bear
with me. Now, suppose that you want to do these operations for each value of &lt;code&gt;cyl&lt;/code&gt; and &lt;code&gt;am&lt;/code&gt; (of course
you can do that without using &lt;code&gt;nice_function()&lt;/code&gt;…). First, you might want to fix the value of &lt;code&gt;am&lt;/code&gt;
to 0, and then loop over the values of &lt;code&gt;cyl&lt;/code&gt;. But as I have explained in this
&lt;a href=&#34;http://blog.rdata.lu/post/2017-12-21-skip-errors-in-r-by-not-writing-loops/&#34;&gt;other blog post&lt;/a&gt; I
prefer using the &lt;code&gt;map()&lt;/code&gt; functions included in &lt;code&gt;purrr&lt;/code&gt;. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;values_cyl = c(4, 6, 8)

(result = map(values_cyl, nice_function, df = mtcars, param2 = 0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0
## 
## [[2]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8
## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2
## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4
## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6
## 
## [[3]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2
## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8
## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4
## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8
## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2
## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4
## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4
## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2
## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0
## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2
## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8
## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What you get here is a list for each value in &lt;code&gt;values_cyl&lt;/code&gt;; so one list for &lt;code&gt;4&lt;/code&gt;, one for &lt;code&gt;6&lt;/code&gt; and
one for &lt;code&gt;8&lt;/code&gt;. Suppose now that you are feeling adventurous, and want to loop over the values of &lt;code&gt;am&lt;/code&gt; too:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;values_am = c(0, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So first, we need to map a function to each element of &lt;code&gt;values_am&lt;/code&gt;. But which function? Well, for
&lt;em&gt;given&lt;/em&gt; value of &lt;code&gt;am&lt;/code&gt;, our problem is the same as before; we need to map &lt;code&gt;nice_function()&lt;/code&gt; to each
value of &lt;code&gt;cyl&lt;/code&gt;. So, that’s what we’re going to do:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(result = map(values_am, ~map(values_cyl, nice_function, df = mtcars, param2 = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [[1]][[1]]
##    mpg cyl  disp hp drat    wt  qsec vs am gear carb result
## 1 24.4   4 146.7 62 3.69 3.190 20.00  1  0    4    2  195.2
## 2 22.8   4 140.8 95 3.92 3.150 22.90  1  0    4    2  182.4
## 3 21.5   4 120.1 97 3.70 2.465 20.01  1  0    3    1  172.0
## 
## [[1]][[2]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8
## 2 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2
## 3 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4
## 4 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6
## 
## [[1]][[3]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2
## 2  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8
## 3  16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4
## 4  17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8
## 5  15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2
## 6  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4
## 7  10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4
## 8  14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2
## 9  15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0
## 10 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2
## 11 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8
## 12 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2
## 
## 
## [[2]]
## [[2]][[1]]
##    mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2
## 2 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6
## 3 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6
## 4 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6
## 5 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2
## 6 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0
## 7 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6
## 8 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6
## 
## [[2]][[2]]
##    mpg cyl disp  hp drat    wt  qsec vs am gear carb result
## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4  126.0
## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4  126.0
## 3 19.7   6  145 175 3.62 2.770 15.50  0  1    5    6  118.2
## 
## [[2]][[3]]
##    mpg cyl disp  hp drat   wt qsec vs am gear carb result
## 1 15.8   8  351 264 4.22 3.17 14.5  0  1    5    4  126.4
## 2 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8  120.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a list of size 2 (for each value of &lt;code&gt;am&lt;/code&gt;) where each element is itself a list of size
3 (for each value of &lt;code&gt;cyl&lt;/code&gt;) where each element is a data frame. Are you still with me? Also, notice
that the second map is given as a formula (notice the &lt;code&gt;~&lt;/code&gt; in front of the second map). This creates
an anonymous function, where the parameter is given by the &lt;code&gt;.&lt;/code&gt; (think of the &lt;code&gt;.&lt;/code&gt; as being the &lt;code&gt;x&lt;/code&gt;
in &lt;code&gt;f(x)&lt;/code&gt;). So the &lt;code&gt;.&lt;/code&gt; is the stand-in for the values contained inside &lt;code&gt;values_am&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The people that are familiar with the &lt;code&gt;map()&lt;/code&gt; functions must be fuming right now; there is a way
to avoid this nested hell. I will talk about it soon, but first I want to play around with this list of lists.&lt;/p&gt;
&lt;p&gt;If you have a list of data frames, you can bind their rows together with &lt;code&gt;reduce(list_of_dfs, rbind)&lt;/code&gt;.
You would like to this here, but because your lists of data frames are contained inside another list…
you guessed it, you have to map over it!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(result2 = map(result, ~reduce(., rbind)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2
## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4
## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0
## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8
## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2
## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4
## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6
## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2
## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8
## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4
## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8
## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2
## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4
## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4
## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2
## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0
## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2
## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8
## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2
## 
## [[2]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2
## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6
## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6
## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6
## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2
## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0
## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6
## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6
## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0
## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0
## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2
## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4
## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here again, I pass &lt;code&gt;reduce()&lt;/code&gt; as a formula to &lt;code&gt;map()&lt;/code&gt; to create an anonymous function. Again, the &lt;code&gt;.&lt;/code&gt;
is used as the stand-in for each element contained in &lt;code&gt;result&lt;/code&gt;; a list of data frames, where &lt;code&gt;reduce(., rbind)&lt;/code&gt;
knows what to do. Now that we have this we can use &lt;code&gt;reduce()&lt;/code&gt; with &lt;code&gt;rbind()&lt;/code&gt; again to get a single
data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(result3 = reduce(result2, rbind))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2
## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4
## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0
## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8
## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2
## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4
## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6
## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2
## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8
## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4
## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8
## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2
## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4
## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4
## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2
## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0
## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2
## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8
## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2
## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2
## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6
## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6
## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6
## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2
## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0
## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6
## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6
## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0
## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0
## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2
## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4
## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, since &lt;code&gt;reduce(list_of_dfs, rbind)&lt;/code&gt; is such a common operation, you could have simply used
&lt;code&gt;dplyr::bind_rows&lt;/code&gt;, which does exactly this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(result2 = map(result, bind_rows))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2
## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4
## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0
## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8
## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2
## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4
## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6
## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2
## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8
## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4
## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8
## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2
## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4
## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4
## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2
## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0
## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2
## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8
## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2
## 
## [[2]]
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2
## 2  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6
## 3  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6
## 4  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6
## 5  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2
## 6  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0
## 7  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6
## 8  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6
## 9  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0
## 10 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0
## 11 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2
## 12 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4
## 13 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(result3 = bind_rows(result2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2
## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4
## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0
## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8
## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2
## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4
## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6
## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2
## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8
## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4
## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8
## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2
## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4
## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4
## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2
## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0
## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2
## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8
## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2
## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2
## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6
## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6
## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6
## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2
## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0
## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6
## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6
## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0
## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0
## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2
## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4
## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, things are even simpler: you can avoid this deeply nested monstrosity by using &lt;code&gt;map_df()&lt;/code&gt;
instead of &lt;code&gt;map()&lt;/code&gt;! &lt;code&gt;map_df()&lt;/code&gt; works just like &lt;code&gt;map()&lt;/code&gt; but return a data frame (hence the &lt;code&gt;_df&lt;/code&gt;
in the name) instead of a list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(result_df = map_df(values_am, ~map_df(values_cyl, nice_function, df = mtcars, param2 = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb result
## 1  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2  195.2
## 2  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  182.4
## 3  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  172.0
## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  256.8
## 5  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  217.2
## 6  19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  230.4
## 7  17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  213.6
## 8  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  299.2
## 9  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  228.8
## 10 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  262.4
## 11 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  276.8
## 12 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  243.2
## 13 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  166.4
## 14 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  166.4
## 15 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  235.2
## 16 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  248.0
## 17 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  243.2
## 18 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  212.8
## 19 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  307.2
## 20 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1   91.2
## 21 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1  129.6
## 22 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2  121.6
## 23 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1  135.6
## 24 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1  109.2
## 25 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2  104.0
## 26 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2  121.6
## 27 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2   85.6
## 28 21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  126.0
## 29 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  126.0
## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  118.2
## 31 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  126.4
## 32 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  120.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you look at the source code of &lt;code&gt;map_df()&lt;/code&gt; you see that &lt;code&gt;dplyr::bind_rows&lt;/code&gt; gets called at the end:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (.x, .f, ..., .id = NULL) 
## {
##     if (!is_installed(&amp;quot;dplyr&amp;quot;)) {
##         abort(&amp;quot;`map_df()` requires dplyr&amp;quot;)
##     }
##     .f &amp;lt;- as_mapper(.f, ...)
##     res &amp;lt;- map(.x, .f, ...)
##     dplyr::bind_rows(res, .id = .id)
## }
## &amp;lt;bytecode: 0x55dad486e6a0&amp;gt;
## &amp;lt;environment: namespace:purrr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So moral of the story? There are a lot of variants of the common &lt;code&gt;purrr::map()&lt;/code&gt; functions (as well
as of &lt;code&gt;dplyr&lt;/code&gt; verbs, such as &lt;code&gt;filter_at&lt;/code&gt;, &lt;code&gt;select_if&lt;/code&gt;, etc…) and learning about them can save you
from a lot of pain! However, if you need to apply a function to nested lists this is still possible;
you just have to think about the structure of the nested list for a bit. There is also another function
that you might want to study, &lt;code&gt;modify_depth()&lt;/code&gt; which solves related issues but I will end the
blog post here. I might talk about it in a future blog post.&lt;/p&gt;
&lt;p&gt;Also, if you want to learn more about R and the tidyverse, do read the link I posted in the introduction
of the post and join the R4ds slack group! There are a lot of very nice people there that want to help you
get better with your R-fu. Also, this is where I got the inspiration to write this blog post and I
am thankful to the people there for the discussions; I feel comfortable with R, but I still learn
new tips and tricks every day!&lt;/p&gt;
&lt;p&gt;If you enjoy these blog posts, you can follow me on &lt;a href=&#34;https://twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;.
And happy new yeaR!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building formulae</title>
      <link>/blog/2017-12-27-build_formulae/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-12-27-build_formulae/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/47957081/k-fold-cross-validation-in-purr-and-model&#34;&gt;This&lt;/a&gt;
Stackoverflow question made me think about how to build formulae. For example, you might want to
programmatically build linear model formulae and then map these models on data. For example,
suppose the following (output suppressed):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)

lm(mpg ~ hp, data = mtcars)
lm(mpg ~I(hp^2), data = mtcars)
lm(mpg ~I(hp^3), data = mtcars)
lm(mpg ~I(hp^4), data = mtcars)
lm(mpg ~I(hp^5), data = mtcars)
lm(mpg ~I(hp^6), data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To avoid doing this, one can write a function that builds the formulae:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;create_form = function(power){
  rhs = substitute(I(hp^pow), list(pow=power))
  rlang::new_formula(quote(mpg), rhs)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you are not familiar with &lt;code&gt;substitute()&lt;/code&gt;, try the following to understand what it does:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;substitute(y ~ x, list(x = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## y ~ 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then using &lt;code&gt;rlang::new_formula()&lt;/code&gt; I build a formula by providing the left hand side, which is
&lt;code&gt;quote(mpg)&lt;/code&gt; here, and the right hand side, which I built using &lt;code&gt;substitute()&lt;/code&gt;. Now I can create a
list of formulae:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

list_formulae = map(seq(1, 6), create_form)

str(list_formulae)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 6
##  $ :Class &amp;#39;formula&amp;#39;  language mpg ~ I(hp^1L)
##   .. ..- attr(*, &amp;quot;.Environment&amp;quot;)=&amp;lt;environment: 0x55605f897ca0&amp;gt; 
##  $ :Class &amp;#39;formula&amp;#39;  language mpg ~ I(hp^2L)
##   .. ..- attr(*, &amp;quot;.Environment&amp;quot;)=&amp;lt;environment: 0x55605f891418&amp;gt; 
##  $ :Class &amp;#39;formula&amp;#39;  language mpg ~ I(hp^3L)
##   .. ..- attr(*, &amp;quot;.Environment&amp;quot;)=&amp;lt;environment: 0x55605da76098&amp;gt; 
##  $ :Class &amp;#39;formula&amp;#39;  language mpg ~ I(hp^4L)
##   .. ..- attr(*, &amp;quot;.Environment&amp;quot;)=&amp;lt;environment: 0x55605da6a600&amp;gt; 
##  $ :Class &amp;#39;formula&amp;#39;  language mpg ~ I(hp^5L)
##   .. ..- attr(*, &amp;quot;.Environment&amp;quot;)=&amp;lt;environment: 0x55605da68980&amp;gt; 
##  $ :Class &amp;#39;formula&amp;#39;  language mpg ~ I(hp^6L)
##   .. ..- attr(*, &amp;quot;.Environment&amp;quot;)=&amp;lt;environment: 0x55605da66d38&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, &lt;code&gt;power&lt;/code&gt; got replaced by 1, 2, 3,… and each element of the list is a nice formula.
Exactly what &lt;code&gt;lm()&lt;/code&gt; needs. So now it’s easy to map &lt;code&gt;lm()&lt;/code&gt; to this list of formulae:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(mtcars)

map(list_formulae, lm, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## 
## Call:
## .f(formula = .x[[i]], data = ..1)
## 
## Coefficients:
## (Intercept)      I(hp^1)  
##    30.09886     -0.06823  
## 
## 
## [[2]]
## 
## Call:
## .f(formula = .x[[i]], data = ..1)
## 
## Coefficients:
## (Intercept)      I(hp^2)  
##  24.3887252   -0.0001649  
## 
## 
## [[3]]
## 
## Call:
## .f(formula = .x[[i]], data = ..1)
## 
## Coefficients:
## (Intercept)      I(hp^3)  
##   2.242e+01   -4.312e-07  
## 
## 
## [[4]]
## 
## Call:
## .f(formula = .x[[i]], data = ..1)
## 
## Coefficients:
## (Intercept)      I(hp^4)  
##   2.147e+01   -1.106e-09  
## 
## 
## [[5]]
## 
## Call:
## .f(formula = .x[[i]], data = ..1)
## 
## Coefficients:
## (Intercept)      I(hp^5)  
##   2.098e+01   -2.801e-12  
## 
## 
## [[6]]
## 
## Call:
## .f(formula = .x[[i]], data = ..1)
## 
## Coefficients:
## (Intercept)      I(hp^6)  
##   2.070e+01   -7.139e-15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is still a new topic for me there might be more elegant ways to do that, using tidyeval to remove
the hardcoding of the columns in &lt;code&gt;create_form()&lt;/code&gt;. I might continue exploring this.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching the tidyverse to beginners</title>
      <link>/blog/2017-12-17-teaching_tidyverse/</link>
      <pubDate>Sun, 17 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-12-17-teaching_tidyverse/</guid>
      <description>&lt;p&gt;End October I tweeted this:&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;will teach &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; soon again but this time following &lt;a href=&#34;https://twitter.com/drob?ref_src=twsrc%5Etfw&#34;&gt;@drob&lt;/a&gt; &amp;#39;s suggestion of the tidyverse first as laid out here: &lt;a href=&#34;https://t.co/js8SsUs8Nv&#34;&gt;https://t.co/js8SsUs8Nv&lt;/a&gt;&lt;/p&gt;&amp;mdash; Bruno Rodrigues (@brodriguesco) &lt;a href=&#34;https://twitter.com/brodriguesco/status/922741554992812032?ref_src=twsrc%5Etfw&#34;&gt;October 24, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;and it generated some discussion. Some people believe that this is the right approach, and some
others think that one should first present &lt;em&gt;base&lt;/em&gt; R and then show how the &lt;code&gt;tidyverse&lt;/code&gt; complements
it. This year, I taught three classes; a 12-hour class to colleagues that work with me, a 15-hour
class to master’s students and 3 hours again to some of my colleagues. Each time, I decided to
focus on the &lt;code&gt;tidyverse&lt;/code&gt;(almost) entirely, and must say that I am not disappointed with the results!&lt;/p&gt;
&lt;p&gt;The 12 hour class was divided in two 6 hours days. It was a bit intense, especially the last 3 hours
that took place Friday afternoon. The crowd was composed of some economists that had experience
with STATA, some others that were mostly using Excel and finally some colleagues from the IT
department that sometimes need to dig into some data themselves. Apart from 2 people, all the other
never had any experience with R.&lt;/p&gt;
&lt;p&gt;We went from 0 to being able to do the plot below after the end of the first day
(so 6 hours in). Keep in mind that practically none of them even had opened RStudio before. I
show the code so you can see the progress made in just a few hours:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(Ecdat)
library(tidyverse)
library(ggthemes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(Bwages)
bwages = Bwages %&amp;gt;%
  mutate(educ_level = case_when(educ == 1 ~ &amp;quot;Primary School&amp;quot;,
                                educ == 2 ~ &amp;quot;High School&amp;quot;,
                                educ == 3 ~ &amp;quot;Some university&amp;quot;,
                                educ == 4 ~ &amp;quot;Master&amp;#39;s degree&amp;quot;,
                                educ == 5 ~ &amp;quot;Doctoral degree&amp;quot;))

ggplot(bwages) +
  geom_smooth(aes(exper, wage, colour = educ_level)) +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;, legend.title = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-12-17-teaching_tidyverse_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course some of them needed some help here and there, and I also gave them hints (for example I told
them about &lt;code&gt;case_when()&lt;/code&gt; and try to use it inside &lt;code&gt;mutate()&lt;/code&gt; instead of nested ifs)
but it was mostly due to lack of experience and because they hadn’t had the time to fully digest
R’s syntax which was for most people involved completely new.&lt;/p&gt;
&lt;p&gt;On the second day I showed &lt;code&gt;purrr::map()&lt;/code&gt; and &lt;code&gt;purrr::reduce()&lt;/code&gt; and overall it went quite well too.
I even showed list-columns, and this is where I started losing some of them; I did not insist too
much on it though, only wanted to show them the flexibility of &lt;code&gt;data.frame&lt;/code&gt; objects. Some of them
were quite impressed by list-columns! Then I started showing (for and while) loops and writing
functions. I even showed them &lt;code&gt;tidyeval&lt;/code&gt; and again, it went relatively well. Once they had the
opportunity to play a bit around with it, I think it clicked (plus they have lots of code examples
to go back too).&lt;/p&gt;
&lt;p&gt;At the end, people seemed to have enjoyed the course, but told me that Friday was heavy; indeed it
was, but I feel that it was mostly because 12 hours spread on 2 days is not the best format for this
type of material, but we all had time constraints.&lt;/p&gt;
&lt;p&gt;The 15 hour Master’s course was spread over 4 days, and covered basically the same. I just
used the last 3 hours to show the students some basic functions for model estimation
(linear, count, logit/probit and survival models). Again, the students were quite impressed by how
easily they could get descriptive statistics by first grouping by some variables. Through their
questions, I even got to show them scoped versions of &lt;code&gt;dplyr&lt;/code&gt; verbs, such as &lt;code&gt;select_if()&lt;/code&gt; and
&lt;code&gt;summarise_at()&lt;/code&gt;. I was expecting to lose them there, but actually most of them got these scoped
versions quite fast. These students already had some experience with R though, but none with
the &lt;code&gt;tidyverse&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally the 3 hour course was perhaps the most interesting; I only had 100% total beginners. Some
just knew R by name and had never heard/seen/opened RStudio (with the exception of one person)!
I did not show them any loops, function definitions and no plots. I only showed them how RStudio
looked and worked, what were (and how to install) packages (as well as the CRAN Task Views) and
then how to import data with &lt;code&gt;rio&lt;/code&gt; and do descriptive statistics only with &lt;code&gt;dplyr&lt;/code&gt;. They were
really interested and quite impressed by &lt;code&gt;rio&lt;/code&gt; (“what do you mean I can use the same code for
importing any dataset, in any format?”) but also by the simplicity of &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In all the courses, I did show the &lt;code&gt;$&lt;/code&gt; primitive to refer to columns inside a &lt;code&gt;data.frame&lt;/code&gt;. First I
showed them lists which is where I introduced &lt;code&gt;$&lt;/code&gt;. Then it was easy to explain to them why it was
the same for a column inside a &lt;code&gt;data.frame&lt;/code&gt;; a &lt;code&gt;data.frame&lt;/code&gt; is simply a list! This is also the
distinction I made from the previous years; I simply mentioned (and showed really quickly) matrices
and focused almost entirely on lists. Most participants, if not all, had learned to program
statistics by thinking about linear algebra and matrices. Nothing wrong with that, but I feel that R
really shines when you focus on lists and on how to work with them.&lt;/p&gt;
&lt;p&gt;Overall as the teacher, I think that focusing on the &lt;code&gt;tidyverse&lt;/code&gt; might be a very good strategy. I
might have to do some adjustments here and there for the future courses, but my hunch is that the
difficulties that some participants had were not necessarily due to the &lt;code&gt;tidyverse&lt;/code&gt; but simply to
lack of time to digest what was shown, as well as a total lack of experience with R.
I do not think that these participants would have better understood a more traditional, &lt;code&gt;base&lt;/code&gt;,
matrix-oriented course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functional peace of mind</title>
      <link>/blog/2017-11-14-peace_r/</link>
      <pubDate>Tue, 14 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-11-14-peace_r/</guid>
      <description>&lt;p&gt;I think what I enjoy the most about functional programming is the peace of mind that comes with it.
With functional programming, there’s a lot of stuff you don’t need to think about. You can write
functions that are general enough so that they solve a variety of problems. For example, imagine
for a second that R does not have the &lt;code&gt;sum()&lt;/code&gt; function anymore. If you want to compute the sum of,
say, the first 100 integers, you could write a loop that would do that for you:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers = 0

for (i in 1:100){
  numbers = numbers + i
}

print(numbers)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5050&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem with this approach, is that you cannot reuse any of the code there, even if you put it
inside a function. For instance, what if you want to merge 4 datasets together? You would need
something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
data(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars1 = mtcars %&amp;gt;%
  mutate(id = &amp;quot;1&amp;quot;)

mtcars2 = mtcars %&amp;gt;%
  mutate(id = &amp;quot;2&amp;quot;)

mtcars3 = mtcars %&amp;gt;%
  mutate(id = &amp;quot;3&amp;quot;)

mtcars4 = mtcars %&amp;gt;%
  mutate(id = &amp;quot;4&amp;quot;)

datasets = list(mtcars1, mtcars2, mtcars3, mtcars4)

temp = datasets[[1]]

for(i in 1:3){
  temp = full_join(temp, datasets[[i+1]])
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;drat&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;qsec&amp;quot;, &amp;quot;vs&amp;quot;, &amp;quot;am&amp;quot;, &amp;quot;gear&amp;quot;, &amp;quot;carb&amp;quot;, &amp;quot;id&amp;quot;)
## Joining, by = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;drat&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;qsec&amp;quot;, &amp;quot;vs&amp;quot;, &amp;quot;am&amp;quot;, &amp;quot;gear&amp;quot;, &amp;quot;carb&amp;quot;, &amp;quot;id&amp;quot;)
## Joining, by = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;drat&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;qsec&amp;quot;, &amp;quot;vs&amp;quot;, &amp;quot;am&amp;quot;, &amp;quot;gear&amp;quot;, &amp;quot;carb&amp;quot;, &amp;quot;id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(temp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 128
## Variables: 12
## $ mpg  &amp;lt;dbl&amp;gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19....
## $ cyl  &amp;lt;dbl&amp;gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, ...
## $ disp &amp;lt;dbl&amp;gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1...
## $ hp   &amp;lt;dbl&amp;gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, ...
## $ drat &amp;lt;dbl&amp;gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9...
## $ wt   &amp;lt;dbl&amp;gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3...
## $ qsec &amp;lt;dbl&amp;gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2...
## $ vs   &amp;lt;dbl&amp;gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ...
## $ am   &amp;lt;dbl&amp;gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...
## $ gear &amp;lt;dbl&amp;gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, ...
## $ carb &amp;lt;dbl&amp;gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, ...
## $ id   &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, the logic is very similar as before, but you need to think carefully about the structure
holding your elements (which can be numbers, datasets, characters, etc…) as well as be careful
about indexing correctly… and depending on the type of objects you are working on, you might need
to tweak the code further.&lt;/p&gt;
&lt;p&gt;How would a functional programming approach make this easier? Of course, you could use
&lt;code&gt;purrr::reduce()&lt;/code&gt; to solve these problems. However, since I assumed that &lt;code&gt;sum()&lt;/code&gt; does not exist,
I will also assume that &lt;code&gt;purrr::reduce()&lt;/code&gt; does not exist either and write my own, clumsy
implementation. Here’s the code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_reduce = function(a_list, a_func, init = NULL, ...){

  if(is.null(init)){
    init = `[[`(a_list, 1)
    a_list = tail(a_list, -1)
  }

  car = `[[`(a_list, 1)
  cdr = tail(a_list, -1)
  init = a_func(init, car, ...)

  if(length(cdr) != 0){
    my_reduce(cdr, a_func, init, ...)
  }
  else {
    init
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can look much more complicated than before, but the idea is quite simple; &lt;em&gt;if you know about
recursive functions&lt;/em&gt; (recursive functions are functions that call themselves). I won’t explain how
the function works, because it is not the main point of the article (but if
you’re curious, I encourage you to play around with it). The point is that now, I can do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_reduce(list(1,2,3,4,5), `+`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_reduce(datasets, full_join) %&amp;gt;% glimpse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;drat&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;qsec&amp;quot;, &amp;quot;vs&amp;quot;, &amp;quot;am&amp;quot;, &amp;quot;gear&amp;quot;, &amp;quot;carb&amp;quot;, &amp;quot;id&amp;quot;)
## Joining, by = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;drat&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;qsec&amp;quot;, &amp;quot;vs&amp;quot;, &amp;quot;am&amp;quot;, &amp;quot;gear&amp;quot;, &amp;quot;carb&amp;quot;, &amp;quot;id&amp;quot;)
## Joining, by = c(&amp;quot;mpg&amp;quot;, &amp;quot;cyl&amp;quot;, &amp;quot;disp&amp;quot;, &amp;quot;hp&amp;quot;, &amp;quot;drat&amp;quot;, &amp;quot;wt&amp;quot;, &amp;quot;qsec&amp;quot;, &amp;quot;vs&amp;quot;, &amp;quot;am&amp;quot;, &amp;quot;gear&amp;quot;, &amp;quot;carb&amp;quot;, &amp;quot;id&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 128
## Variables: 12
## $ mpg  &amp;lt;dbl&amp;gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19....
## $ cyl  &amp;lt;dbl&amp;gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, ...
## $ disp &amp;lt;dbl&amp;gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1...
## $ hp   &amp;lt;dbl&amp;gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, ...
## $ drat &amp;lt;dbl&amp;gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9...
## $ wt   &amp;lt;dbl&amp;gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3...
## $ qsec &amp;lt;dbl&amp;gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2...
## $ vs   &amp;lt;dbl&amp;gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ...
## $ am   &amp;lt;dbl&amp;gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...
## $ gear &amp;lt;dbl&amp;gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, ...
## $ carb &amp;lt;dbl&amp;gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, ...
## $ id   &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if I need to merge another dataset, I don’t need to change anything at all. Plus, because &lt;code&gt;my_reduce()&lt;/code&gt;
is very general, I can even use it for situation I didn’t write it for in the first place:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_reduce(list(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;e&amp;quot;), paste)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a b c d e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, &lt;code&gt;paste()&lt;/code&gt; is vectorized, so you could just as well do &lt;code&gt;paste(1, 2, 3, 4, 5)&lt;/code&gt;, but again, I want
to insist on the fact that writing or using such functions allows you to abstract over a lot of thing.
There is nothing specific to any type of object in &lt;code&gt;my_reduce()&lt;/code&gt;, whereas the loops have to be tailored
for the kind of object you’re working with. As long as the &lt;code&gt;a_func&lt;/code&gt; argument is a binary operator
that combines the elements inside &lt;code&gt;a_list&lt;/code&gt;, it’s going to work. And I don’t need to think about
indexing, about having temporary variables or thinking about the structure that will hold my
results.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Easy peasy STATA-like marginal effects with R</title>
      <link>/blog/2017-10-26-margins_r/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-10-26-margins_r/</guid>
      <description>&lt;p&gt;Model interpretation is essential in the social sciences. If one wants to know the effect of
variable &lt;code&gt;x&lt;/code&gt; on the dependent variable &lt;code&gt;y&lt;/code&gt;, marginal effects are an easy way to get the answer.
STATA includes a &lt;code&gt;margins&lt;/code&gt; command that has been ported to R
by &lt;a href=&#34;http://thomasleeper.com/&#34;&gt;Thomas J. Leeper&lt;/a&gt;
of the London School of Economics and Political Science.
You can find the source code of the package
&lt;a href=&#34;https://github.com/leeper/margins&#34;&gt;on github&lt;/a&gt;. In this short blog post, I demo some of the
functionality of &lt;code&gt;margins&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;First, let’s load some packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tibble)
library(broom)
library(margins)
library(Ecdat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an example, we are going to use the &lt;code&gt;Participation&lt;/code&gt; data from the &lt;code&gt;Ecdat&lt;/code&gt; package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(Participation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;?Participation&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Labor Force Participation

Description

a cross-section

number of observations : 872

observation : individuals

country : Switzerland

Usage

data(Participation)
Format

A dataframe containing :

lfp
labour force participation ?

lnnlinc
the log of nonlabour income

age
age in years divided by 10

educ
years of formal education

nyc
the number of young children (younger than 7)

noc
number of older children

foreign
foreigner ?

Source

Gerfin, Michael (1996) “Parametric and semiparametric estimation of the binary response”, Journal of Applied Econometrics, 11(3), 321-340.

References

Davidson, R. and James G. MacKinnon (2004) Econometric Theory and Methods, New York, Oxford University Press, http://www.econ.queensu.ca/ETM/, chapter 11.

Journal of Applied Econometrics data archive : http://qed.econ.queensu.ca/jae/.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variable of interest is &lt;code&gt;lfp&lt;/code&gt;: whether the individual participates in the labour force or not.
To know which variables are relevant in the decision to participate in the labour force,
one could estimate a logit model, using &lt;code&gt;glm()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit_participation = glm(lfp ~ ., data = Participation, family = &amp;quot;binomial&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we ran the regression, we can take a look at the results. I like to use &lt;code&gt;broom::tidy()&lt;/code&gt;
to look at the results of regressions, as &lt;code&gt;tidy()&lt;/code&gt; returns a nice
&lt;code&gt;data.frame&lt;/code&gt;, but you could use &lt;code&gt;summary()&lt;/code&gt; if you’re only interested in reading the output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(logit_participation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          term    estimate  std.error  statistic      p.value
## 1 (Intercept) 10.37434616 2.16685216  4.7877499 1.686617e-06
## 2     lnnlinc -0.81504064 0.20550116 -3.9661122 7.305449e-05
## 3         age -0.51032975 0.09051783 -5.6378920 1.721444e-08
## 4        educ  0.03172803 0.02903580  1.0927211 2.745163e-01
## 5         nyc -1.33072362 0.18017027 -7.3859224 1.514000e-13
## 6         noc -0.02198573 0.07376636 -0.2980454 7.656685e-01
## 7  foreignyes  1.31040497 0.19975784  6.5599678 5.381941e-11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the results above, one can only interpret the sign of the coefficients. To know how much a
variable influences the labour force participation, one has to use &lt;code&gt;margins()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;effects_logit_participation = margins(logit_participation) 

print(effects_logit_participation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Average marginal effects&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## glm(formula = lfp ~ ., family = &amp;quot;binomial&amp;quot;, data = Participation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  lnnlinc     age     educ     nyc       noc foreignyes
##  -0.1699 -0.1064 0.006616 -0.2775 -0.004584     0.2834&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;summary()&lt;/code&gt; on the object returned by &lt;code&gt;margins()&lt;/code&gt; provides more details:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(effects_logit_participation)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      factor     AME     SE       z      p   lower   upper
##         age -0.1064 0.0176 -6.0494 0.0000 -0.1409 -0.0719
##        educ  0.0066 0.0060  1.0955 0.2733 -0.0052  0.0185
##  foreignyes  0.2834 0.0399  7.1102 0.0000  0.2053  0.3615
##     lnnlinc -0.1699 0.0415 -4.0994 0.0000 -0.2512 -0.0887
##         noc -0.0046 0.0154 -0.2981 0.7656 -0.0347  0.0256
##         nyc -0.2775 0.0333 -8.3433 0.0000 -0.3426 -0.2123&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And it is also possible to plot the effects with base graphics:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(effects_logit_participation)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-26-margins_r_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This uses the basic R plotting capabilities, which is useful because it is a simple call to the
function &lt;code&gt;plot()&lt;/code&gt; but if you’ve been using &lt;code&gt;ggplot2&lt;/code&gt; and want this graph to have the same look as
the others made with &lt;code&gt;ggplot2&lt;/code&gt; you first need to save the summary in a variable.
Let’s overwrite this &lt;code&gt;effects_logit_participation&lt;/code&gt; variable with its summary:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;effects_logit_participation = summary(effects_logit_participation)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now it is possible to use &lt;code&gt;ggplot2&lt;/code&gt; to create the same plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = effects_logit_participation) +
  geom_point(aes(factor, AME)) +
  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = 0) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-26-margins_r_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So an infinitesimal increase, in say, non-labour income (&lt;code&gt;lnnlinc&lt;/code&gt;) of 0.001 is associated with a
decrease of the probability of labour force participation by 0.001*17 percentage points.&lt;/p&gt;
&lt;p&gt;You can also extract the marginal effects of a single variable, with &lt;code&gt;dydx()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(dydx(Participation, logit_participation, &amp;quot;lnnlinc&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   dydx_lnnlinc
## 1  -0.15667764
## 2  -0.20014487
## 3  -0.18495109
## 4  -0.05377262
## 5  -0.18710476
## 6  -0.19586986&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which makes it possible to extract the effects for a list of individuals that you can create yourself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_subjects = tribble(
    ~lfp,  ~lnnlinc, ~age, ~educ, ~nyc, ~noc, ~foreign,
    &amp;quot;yes&amp;quot;,   10.780,  7.0,     4,    1,    1,    &amp;quot;yes&amp;quot;,
     &amp;quot;no&amp;quot;,     1.30,  9.0,     1,    4,    1,    &amp;quot;yes&amp;quot;
)

dydx(my_subjects, logit_participation, &amp;quot;lnnlinc&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   dydx_lnnlinc
## 1  -0.09228119
## 2  -0.17953451&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I used the &lt;code&gt;tribble()&lt;/code&gt; function from the &lt;code&gt;tibble&lt;/code&gt; package to create this test data set, row by
row. Then, using &lt;code&gt;dydx()&lt;/code&gt;, I get the marginal effect of variable &lt;code&gt;lnnlinc&lt;/code&gt; for these two individuals.
No doubt that this package will be a huge help convincing more social scientists to try out R and
make a potential transition from STATA easier.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why I find tidyeval useful</title>
      <link>/blog/2017-08-27-why_tidyeval/</link>
      <pubDate>Sun, 27 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-08-27-why_tidyeval/</guid>
      <description>&lt;p&gt;First thing’s first: maybe you shouldn’t care about &lt;code&gt;tidyeval&lt;/code&gt;. Maybe you don’t need it. If you
exclusively work interactively, I don’t think that learning about &lt;code&gt;tidyeval&lt;/code&gt; is important. I can
only speak for me, and explain to you why I personally find &lt;code&gt;tidyeval&lt;/code&gt; useful.&lt;/p&gt;
&lt;p&gt;I wanted to write this blog post after reading this
&lt;a href=&#34;https://twitter.com/dataandme/status/901429535266267136&#34;&gt;twitter thread&lt;/a&gt;
and specifically &lt;a href=&#34;https://twitter.com/Kwarizmi/status/901457435948236801&#34;&gt;this question&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/dataandme&#34;&gt;Mara Averick&lt;/a&gt; then wrote
&lt;a href=&#34;http://maraaverick.rbind.io/2017/08/tidyeval-resource-roundup/&#34;&gt;this blogpost&lt;/a&gt; linking to 6 other blog
posts that give some &lt;code&gt;tidyeval&lt;/code&gt; examples. Reading them, plus the
&lt;a href=&#34;http://dplyr.tidyverse.org/articles/programming.html&#34;&gt;Programming with dplyr&lt;/a&gt; vignette should help you
get started with &lt;code&gt;tidyeval&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But maybe now you know how to use it, but not why and when you should use it… Basically, whenever
you want to write a function that looks something like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_function(my_data, one_column_inside_data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;is when you want to use the power of &lt;code&gt;tidyeval&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I work at &lt;a href=&#34;http://www.statistiques.public.lu/en/index.html&#34;&gt;STATEC&lt;/a&gt;,
Luxembourg’s national institute of statistics. I work on all kinds of different projects, and when
data gets updated (for example because a new round of data collection for some survey finished),
I run my own scripts on the fresh data to make the data nice and tidy for analysis. Because surveys
get updated, sometimes column names change a little bit, and this can cause some issues.&lt;/p&gt;
&lt;p&gt;Very recently, a dataset I work with got updated. Data collection was finished, so I
just loaded my hombrewed package written for this project, changed the path from last year’s script
to this year’s fresh data path, ran the code, and watched as the folders got populated with new
&lt;code&gt;ggplot2&lt;/code&gt; graphs and LaTeX tables with descriptive statistics and regression
results. This is then used to generate this year’s report. However, by looking at the graphs, I
noticed something weird; some graphs were showing some very strange patterns. It turns out that one
column got its name changed, and also one of its values got changed too.&lt;/p&gt;
&lt;p&gt;Last year, this column, let’s call it &lt;code&gt;spam&lt;/code&gt;, had values &lt;code&gt;1&lt;/code&gt; for &lt;code&gt;good&lt;/code&gt; and &lt;code&gt;0&lt;/code&gt; for &lt;code&gt;bad&lt;/code&gt;.
This year the column is called &lt;code&gt;Spam&lt;/code&gt; and the values are &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt;. When I found out that this was
the source of the problem, I just had to change the arguments of my functions from&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;generate_spam_plot(dataset = data2016, column = spam, value = 1)
generate_spam_plot(dataset = data2016, column = spam, value = 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;generate_spam_plot(dataset = data2017, column = Spam, value = 1)
generate_spam_plot(dataset = data2017, column = Spam, value = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;without needing to change anything else. This is why I use &lt;code&gt;tidyeval&lt;/code&gt;; without it, writing a
function such as &lt;code&gt;genereta_spam_plot&lt;/code&gt; would not be easy. It would be possible, but not easy.&lt;/p&gt;
&lt;p&gt;If you want to know more about &lt;code&gt;tidyeval&lt;/code&gt; and working programmatically with R, I shamelessly
invite you to read a book I’ve been working on: &lt;a href=&#34;https://b-rodrigues.github.io/fput/&#34; class=&#34;uri&#34;&gt;https://b-rodrigues.github.io/fput/&lt;/a&gt;
It’s still a WIP, but maybe you’ll find it useful. I plan on finishing it by the end of the year,
but there’s already some content to keep you busy!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>tidyr::spread() and dplyr::rename_at() in action</title>
      <link>/blog/2017-07-27-spread_rename_at/</link>
      <pubDate>Thu, 27 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-07-27-spread_rename_at/</guid>
      <description>&lt;p&gt;I was recently confronted to a situation that required going from a long dataset to a wide dataset,
but with a small twist: there were two datasets, which I had to merge into one. You might wonder
what kinda crappy twist that is, right? Well, let’s take a look at the data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data1; data2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 4
##    country date       variable_1       value
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt;
##  1 lu      01/01/2005 maybe               22
##  2 lu      01/07/2005 maybe               13
##  3 lu      01/01/2006 maybe               40
##  4 lu      01/07/2006 maybe               25
##  5 lu      01/01/2005 totally_agree       42
##  6 lu      01/07/2005 totally_agree       17
##  7 lu      01/01/2006 totally_agree       25
##  8 lu      01/07/2006 totally_agree       16
##  9 lu      01/01/2005 totally_disagree    39
## 10 lu      01/07/2005 totally_disagree    17
## 11 lu      01/01/2006 totally_disagree    23
## 12 lu      01/07/2006 totally_disagree    21
## 13 lu      01/01/2005 kinda_disagree      69
## 14 lu      01/07/2005 kinda_disagree      12
## 15 lu      01/01/2006 kinda_disagree      10
## 16 lu      01/07/2006 kinda_disagree       9
## 17 lu      01/01/2005 kinda_agree         38
## 18 lu      01/07/2005 kinda_agree         31
## 19 lu      01/01/2006 kinda_agree         19
## 20 lu      01/07/2006 kinda_agree         12&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 4
##    country date       variable_2       value
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt;
##  1 lu      01/01/2005 kinda_agree         22
##  2 lu      01/07/2005 kinda_agree         13
##  3 lu      01/01/2006 kinda_agree         40
##  4 lu      01/07/2006 kinda_agree         25
##  5 lu      01/01/2005 totally_agree       42
##  6 lu      01/07/2005 totally_agree       17
##  7 lu      01/01/2006 totally_agree       25
##  8 lu      01/07/2006 totally_agree       16
##  9 lu      01/01/2005 totally_disagree    39
## 10 lu      01/07/2005 totally_disagree    17
## 11 lu      01/01/2006 totally_disagree    23
## 12 lu      01/07/2006 totally_disagree    21
## 13 lu      01/01/2005 maybe               69
## 14 lu      01/07/2005 maybe               12
## 15 lu      01/01/2006 maybe               10
## 16 lu      01/07/2006 maybe                9
## 17 lu      01/01/2005 kinda_disagree      38
## 18 lu      01/07/2005 kinda_disagree      31
## 19 lu      01/01/2006 kinda_disagree      19
## 20 lu      01/07/2006 kinda_disagree      12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As explained in &lt;a href=&#34;http://vita.had.co.nz/papers/tidy-data.html&#34;&gt;Hadley (2014)&lt;/a&gt;, this is how you should keep your data… But for a particular
purpose, I had to transform these datasets. What I was asked to do was to merge these into a single
wide data frame. Doing this for one dataset is easy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data1 %&amp;gt;%
  spread(variable_1, value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 7
##   country date       kinda_agree kinda_disagree maybe totally_agree
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt;          &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;         &amp;lt;int&amp;gt;
## 1 lu      01/01/2005          38             69    22            42
## 2 lu      01/01/2006          19             10    40            25
## 3 lu      01/07/2005          31             12    13            17
## 4 lu      01/07/2006          12              9    25            16
## # ... with 1 more variable: totally_disagree &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But because &lt;code&gt;data1&lt;/code&gt; and &lt;code&gt;data2&lt;/code&gt; have the same levels for &lt;code&gt;variable_1&lt;/code&gt; and &lt;code&gt;variable_2&lt;/code&gt;, this would not
work. So the solution I found online, in this &lt;a href=&#34;https://stackoverflow.com/questions/43578723/conditional-replacement-of-column-name-in-tibble-using-dplyr&#34;&gt;SO thread&lt;/a&gt; was to use &lt;code&gt;tidyr::spread()&lt;/code&gt; with
&lt;code&gt;dplyr::rename_at()&lt;/code&gt; like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data1 &amp;lt;- data1 %&amp;gt;%
  spread(variable_1, value) %&amp;gt;%
  rename_at(vars(-country, -date), funs(paste0(&amp;quot;variable1:&amp;quot;, .)))

glimpse(data1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4
## Variables: 7
## $ country                      &amp;lt;chr&amp;gt; &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;
## $ date                         &amp;lt;chr&amp;gt; &amp;quot;01/01/2005&amp;quot;, &amp;quot;01/01/2006&amp;quot;, &amp;quot;01/0...
## $ `variable1:kinda_agree`      &amp;lt;int&amp;gt; 38, 19, 31, 12
## $ `variable1:kinda_disagree`   &amp;lt;int&amp;gt; 69, 10, 12, 9
## $ `variable1:maybe`            &amp;lt;int&amp;gt; 22, 40, 13, 25
## $ `variable1:totally_agree`    &amp;lt;int&amp;gt; 42, 25, 17, 16
## $ `variable1:totally_disagree` &amp;lt;int&amp;gt; 39, 23, 17, 21&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data2 &amp;lt;- data2 %&amp;gt;%
  spread(variable_2, value) %&amp;gt;%
  rename_at(vars(-country, -date), funs(paste0(&amp;quot;variable2:&amp;quot;, .)))

glimpse(data2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4
## Variables: 7
## $ country                      &amp;lt;chr&amp;gt; &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;
## $ date                         &amp;lt;chr&amp;gt; &amp;quot;01/01/2005&amp;quot;, &amp;quot;01/01/2006&amp;quot;, &amp;quot;01/0...
## $ `variable2:kinda_agree`      &amp;lt;int&amp;gt; 22, 40, 13, 25
## $ `variable2:kinda_disagree`   &amp;lt;int&amp;gt; 38, 19, 31, 12
## $ `variable2:maybe`            &amp;lt;int&amp;gt; 69, 10, 12, 9
## $ `variable2:totally_agree`    &amp;lt;int&amp;gt; 42, 25, 17, 16
## $ `variable2:totally_disagree` &amp;lt;int&amp;gt; 39, 23, 17, 21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;rename_at()&lt;/code&gt; needs variables which you pass to &lt;code&gt;vars()&lt;/code&gt;, a helper function to select variables, and
a function that will do the renaming, passed to &lt;code&gt;funs()&lt;/code&gt;. The function I use is simply &lt;code&gt;paste0()&lt;/code&gt;,
which pastes a string, for example “variable1:” with the name of the columns, given by the single ‘.’,
a dummy argument. Now these datasets can be merged:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data1 %&amp;gt;%
  full_join(data2) %&amp;gt;%
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;country&amp;quot;, &amp;quot;date&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4
## Variables: 12
## $ country                      &amp;lt;chr&amp;gt; &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;, &amp;quot;lu&amp;quot;
## $ date                         &amp;lt;chr&amp;gt; &amp;quot;01/01/2005&amp;quot;, &amp;quot;01/01/2006&amp;quot;, &amp;quot;01/0...
## $ `variable1:kinda_agree`      &amp;lt;int&amp;gt; 38, 19, 31, 12
## $ `variable1:kinda_disagree`   &amp;lt;int&amp;gt; 69, 10, 12, 9
## $ `variable1:maybe`            &amp;lt;int&amp;gt; 22, 40, 13, 25
## $ `variable1:totally_agree`    &amp;lt;int&amp;gt; 42, 25, 17, 16
## $ `variable1:totally_disagree` &amp;lt;int&amp;gt; 39, 23, 17, 21
## $ `variable2:kinda_agree`      &amp;lt;int&amp;gt; 22, 40, 13, 25
## $ `variable2:kinda_disagree`   &amp;lt;int&amp;gt; 38, 19, 31, 12
## $ `variable2:maybe`            &amp;lt;int&amp;gt; 69, 10, 12, 9
## $ `variable2:totally_agree`    &amp;lt;int&amp;gt; 42, 25, 17, 16
## $ `variable2:totally_disagree` &amp;lt;int&amp;gt; 39, 23, 17, 21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope this post helps you understand the difference between long and wide datasets better, as well
as &lt;code&gt;dplyr::rename_at()&lt;/code&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lesser known dplyr 0.7* tricks</title>
      <link>/blog/2017-06-19-dplyr-0-70-tutorial/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-06-19-dplyr-0-70-tutorial/</guid>
      <description>&lt;p&gt;This blog post is an update to an older &lt;a href=&#34;http://www.brodrigues.co/blog/2017-02-17-lesser_known_tricks/&#34;&gt;one&lt;/a&gt;
I wrote in March.
In the post from March, &lt;code&gt;dplyr&lt;/code&gt; was at version 0.50, but since then a major update introduced some
changes that make some of the tips in that post obsolete. So here I revisit the blog post from March
by using &lt;code&gt;dplyr&lt;/code&gt; 0.70.&lt;/p&gt;
&lt;div id=&#34;create-new-columns-with-mutate-and-case_when&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create new columns with &lt;code&gt;mutate()&lt;/code&gt; and &lt;code&gt;case_when()&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The basic things such as selecting columns, renaming them, filtering, etc did not change with this new
version. What did change however is creating new columns using &lt;code&gt;case_when()&lt;/code&gt;.
First, load &lt;code&gt;dplyr&lt;/code&gt; and the &lt;code&gt;mtcars&lt;/code&gt; dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;dplyr&amp;quot;)
data(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This was how it was done in version 0.50 (notice the ‘.$’ symbol before the variable ‘carb’):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
    mutate(carb_new = case_when(.$carb == 1 ~ &amp;quot;one&amp;quot;,
                                .$carb == 2 ~ &amp;quot;two&amp;quot;,
                                .$carb == 4 ~ &amp;quot;four&amp;quot;,
                                 TRUE ~ &amp;quot;other&amp;quot;)) %&amp;gt;%
    head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new
## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four
## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four
## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one
## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one
## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This has been simplified to:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
    mutate(carb_new = case_when(carb == 1 ~ &amp;quot;one&amp;quot;,
                                carb == 2 ~ &amp;quot;two&amp;quot;,
                                carb == 4 ~ &amp;quot;four&amp;quot;,
                                TRUE ~ &amp;quot;other&amp;quot;)) %&amp;gt;%
    head(5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    mpg cyl disp  hp drat    wt  qsec vs am gear carb carb_new
## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4     four
## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4     four
## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1      one
## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1      one
## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2      two&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No need for &lt;code&gt;.$&lt;/code&gt; anymore.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;apply-a-function-to-certain-columns-only-by-rows-with-purrrlyr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply a function to certain columns only, by rows, with &lt;code&gt;purrrlyr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;dplyr&lt;/code&gt; wasn’t the only package to get an overhaul, &lt;code&gt;purrr&lt;/code&gt; also got the same treatment.&lt;/p&gt;
&lt;p&gt;In the past, I applied a function to certains columns like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
    select(am, gear, carb) %&amp;gt;%
    purrr::by_row(sum, .collate = &amp;quot;cols&amp;quot;, .to = &amp;quot;sum_am_gear_carb&amp;quot;) -&amp;gt; mtcars2
head(mtcars2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, &lt;code&gt;by_row()&lt;/code&gt; does not exist in &lt;code&gt;purrr&lt;/code&gt; anymore, but instead a new package called &lt;code&gt;purrrlyr&lt;/code&gt;
was introduced with functions that don’t really fit inside &lt;code&gt;purrr&lt;/code&gt; nor &lt;code&gt;dplyr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
    select(am, gear, carb) %&amp;gt;%
    purrrlyr::by_row(sum, .collate = &amp;quot;cols&amp;quot;, .to = &amp;quot;sum_am_gear_carb&amp;quot;) -&amp;gt; mtcars2
head(mtcars2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##      am  gear  carb sum_am_gear_carb
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1     1     4     4                9
## 2     1     4     4                9
## 3     1     4     1                6
## 4     0     3     1                4
## 5     0     3     2                5
## 6     0     3     1                4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Think of &lt;code&gt;purrrlyr&lt;/code&gt; as &lt;code&gt;purrr&lt;/code&gt;s and &lt;code&gt;dplyr&lt;/code&gt;s love child.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-dplyr-functions-inside-your-own-functions-or-what-is-tidyeval&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;dplyr&lt;/code&gt; functions inside your own functions, or what is &lt;code&gt;tidyeval&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Programming with &lt;code&gt;dplyr&lt;/code&gt; has been simplified a lot. Before version &lt;code&gt;0.70&lt;/code&gt;, one needed to use
&lt;code&gt;dplyr&lt;/code&gt; in conjuction with &lt;code&gt;lazyeval&lt;/code&gt; to use &lt;code&gt;dplyr&lt;/code&gt; functions inside one’s own fuctions. It was
not always very easy, especially if you mixed columns and values inside your functions. Here’s the
example from the March blog post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_vars &amp;lt;- function(data, some_string){

  data %&amp;gt;%
    select_(lazyeval::interp(~contains(some_string))) -&amp;gt; data

  return(data)
}

extract_vars(mtcars, &amp;quot;spam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More examples are available in &lt;a href=&#34;http://www.brodrigues.co/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/&#34;&gt;this other blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will revisit them now with &lt;code&gt;dplyr&lt;/code&gt;’s new &lt;code&gt;tidyeval&lt;/code&gt; syntax. I’d recommend you read the &lt;em&gt;Tidy evaluation&lt;/em&gt;
vignette &lt;a href=&#34;https://cran.r-project.org/web/packages/rlang/vignettes/tidy-evaluation.html&#34;&gt;here&lt;/a&gt;. This vignette
is part of the &lt;code&gt;rlang&lt;/code&gt; package, which gets used under the hood by &lt;code&gt;dplyr&lt;/code&gt; for all your programming needs.
Here is the function I called &lt;code&gt;simpleFunction()&lt;/code&gt;, written with the old &lt;code&gt;dplyr&lt;/code&gt; syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name){
  dataset %&amp;gt;%
    group_by_(col_name) %&amp;gt;%
    summarise(mean_mpg = mean(mpg)) -&amp;gt; dataset
  return(dataset)
}


simpleFunction(mtcars, &amp;quot;cyl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##     cyl mean_mpg
##   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     4     26.7
## 2     6     19.7
## 3     8     15.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the new synax, it must be rewritten a little bit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name){
  col_name &amp;lt;- enquo(col_name)
  dataset %&amp;gt;%
    group_by(!!col_name) %&amp;gt;%
    summarise(mean_mpg = mean(mpg)) -&amp;gt; dataset
  return(dataset)
}


simpleFunction(mtcars, cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##     cyl mean_mpg
##   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     4     26.7
## 2     6     19.7
## 3     8     15.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What has changed? Forget the underscore versions of the usual functions such as &lt;code&gt;select_()&lt;/code&gt;,
&lt;code&gt;group_by_()&lt;/code&gt;, etc. Now, you must quote the column name using &lt;code&gt;enquo()&lt;/code&gt; (or just &lt;code&gt;quo()&lt;/code&gt; if working
interactively, outside a function), which returns a &lt;strong&gt;quosure&lt;/strong&gt;. This &lt;strong&gt;quosure&lt;/strong&gt; can then be
evaluated using &lt;code&gt;!!&lt;/code&gt; in front of the quosure and inside the usual &lt;code&gt;dplyr&lt;/code&gt; functions.&lt;/p&gt;
&lt;p&gt;Let’s look at another example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name, value){
  filter_criteria &amp;lt;- lazyeval::interp(~y == x, .values=list(y = as.name(col_name), x = value))
  dataset %&amp;gt;%
    filter_(filter_criteria) %&amp;gt;%
    summarise(mean_cyl = mean(cyl)) -&amp;gt; dataset
  return(dataset)
}


simpleFunction(mtcars, &amp;quot;am&amp;quot;, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mean_cyl
## 1 5.076923&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, it’s a bit more complicated, as you needed to use &lt;code&gt;lazyeval::interp()&lt;/code&gt; to make it work.
With the improved &lt;code&gt;dplyr&lt;/code&gt;, here’s how it’s done:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name, value){
  col_name &amp;lt;- enquo(col_name)
  dataset %&amp;gt;%
    filter((!!col_name) == value) %&amp;gt;%
    summarise(mean_cyl = mean(cyl)) -&amp;gt; dataset
  return(dataset)
}


simpleFunction(mtcars, am, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mean_cyl
## 1 5.076923&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Much, much easier! There is something that you must pay attention to though. Notice that I’ve written:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter((!!col_name) == value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and not:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(!!col_name == value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have enclosed &lt;code&gt;!!col_name&lt;/code&gt; inside parentheses. I struggled with this, but thanks to help
from &lt;a href=&#34;https://twitter.com/dmi3k/status/880374506291953664&#34;&gt;@dmi3k&lt;/a&gt; and
&lt;a href=&#34;https://twitter.com/_lionelhenry/status/880380691078361090&#34;&gt;@_lionelhenry&lt;/a&gt; I was able to understand
what was happening (isn’t the #rstats community on twitter great?).&lt;/p&gt;
&lt;p&gt;One last thing: let’s make this function a bit more general. I hard-coded the variable &lt;code&gt;cyl&lt;/code&gt; inside the
body of the function, but maybe you’d like the mean of another variable? Easy:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, group_col, mean_col, value){
  group_col &amp;lt;- enquo(group_col)
  mean_col &amp;lt;- enquo(mean_col)
  dataset %&amp;gt;%
    filter((!!group_col) == value) %&amp;gt;%
    summarise(mean((!!mean_col))) -&amp;gt; dataset
  return(dataset)
}


simpleFunction(mtcars, am, cyl, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mean(cyl)
## 1  5.076923&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;«That’s very nice Bruno, but &lt;code&gt;mean((cyl))&lt;/code&gt; in the output looks ugly as sin»&lt;/em&gt; you might think, and you’d be
right. It is possible to set the name of the column in the output using &lt;code&gt;:=&lt;/code&gt; instead of &lt;code&gt;=&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, group_col, mean_col, value){
  group_col &amp;lt;- enquo(group_col)
  mean_col &amp;lt;- enquo(mean_col)
  mean_name &amp;lt;- paste0(&amp;quot;mean_&amp;quot;, mean_col)[2]
  dataset %&amp;gt;%
    filter((!!group_col) == value) %&amp;gt;%
    summarise(!!mean_name := mean((!!mean_col))) -&amp;gt; dataset
  return(dataset)
}


simpleFunction(mtcars, am, cyl, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   mean_cyl
## 1 5.076923&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get the name of the column I added this line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_name &amp;lt;- paste0(&amp;quot;mean_&amp;quot;, mean_col)[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see what it does, try the following inside an R interpreter (remember to us &lt;code&gt;quo()&lt;/code&gt; instead of &lt;code&gt;enquo()&lt;/code&gt;
outside functions!):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste0(&amp;quot;mean_&amp;quot;, quo(cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;mean_~&amp;quot;   &amp;quot;mean_cyl&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;enquo()&lt;/code&gt; quotes the input, and with &lt;code&gt;paste0()&lt;/code&gt; it gets converted to a string that can be used as a column
name. However, the &lt;code&gt;~&lt;/code&gt; is in the way and the output of &lt;code&gt;paste0()&lt;/code&gt; is a vector of two strings: the correct
name is contained in the second element, hence the &lt;code&gt;[2]&lt;/code&gt;. There might be a more elegant way of doing that,
but for now this has been working well for me.&lt;/p&gt;
&lt;p&gt;That was it folks! I do recommend you read the &lt;em&gt;Programming with dplyr&lt;/em&gt; vignette
&lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html&#34;&gt;here&lt;/a&gt; as well as other blog posts,
such as the one recommended to me by &lt;a href=&#34;https://twitter.com/dmi3k&#34;&gt;@dmi3k&lt;/a&gt;
&lt;a href=&#34;http://www.win-vector.com/blog/2017/06/non-standard-evaluation-and-function-composition-in-r/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Have fun with &lt;code&gt;dplyr 0.70&lt;/code&gt;!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Make ggplot2 purrr</title>
      <link>/blog/2017-03-29-make-ggplot2-purrr/</link>
      <pubDate>Wed, 29 Mar 2017 06:45:48 +0200</pubDate>
      
      <guid>/blog/2017-03-29-make-ggplot2-purrr/</guid>
      <description>&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: I’ve included another way of saving a separate plot by group in this article, as pointed out
by &lt;a href=&#34;https://twitter.com/monitus/status/849033025631297536&#34;&gt;&lt;code&gt;@monitus&lt;/code&gt;&lt;/a&gt;. Actually, this is the preferred
solution; using &lt;code&gt;dplyr::do()&lt;/code&gt; is deprecated, according to Hadley Wickham &lt;a href=&#34;https://twitter.com/hadleywickham/status/719542847045636096&#34;&gt;himself&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ll be honest: the title is a bit misleading. I will not use &lt;code&gt;purrr&lt;/code&gt; that much in this blog post.
Actually, I will use one single &lt;code&gt;purrr&lt;/code&gt; function, at the very end. I use &lt;code&gt;dplyr&lt;/code&gt; much more.
However &lt;em&gt;Make ggplot2 purrr&lt;/em&gt; sounds better than &lt;em&gt;Make ggplot dplyr&lt;/em&gt; or whatever the verb for &lt;code&gt;dplyr&lt;/code&gt; would be.&lt;/p&gt;
&lt;p&gt;Also, this blog post was inspired by a stackoverflow question and in particular one of the
&lt;a href=&#34;http://stackoverflow.com/a/29035145/1298051&#34;&gt;answers&lt;/a&gt;. So I don’t bring anything new to the table,
but I found this stackoverflow answer so useful and so underrated (only 16 upvotes as I’m writing
this!) that I wanted to write something about it.&lt;/p&gt;
&lt;p&gt;Basically the idea of this blog post is to show how to create graphs using &lt;code&gt;ggplot2&lt;/code&gt;, but by
grouping by a factor variable beforehand. To illustrate this idea, let’s use the data from the &lt;a href=&#34;http://www.rug.nl/ggdc/productivity/pwt/&#34;&gt;Penn World
Tables 9.0&lt;/a&gt;. The easiest way to get this data is to
install the package called &lt;code&gt;pwt9&lt;/code&gt; with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;pwt9&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then load the data with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;pwt9.0&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s load the needed packages. I am also using &lt;code&gt;ggthemes&lt;/code&gt; which makes themeing your ggplots
very easy. I’ll be making &lt;a href=&#34;https://en.wikipedia.org/wiki/Edward_Tufte&#34;&gt;Tufte&lt;/a&gt;-style plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(ggthemes)
library(dplyr)
library(tidyr)
library(purrr)
library(pwt9)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First let’s select a list of countries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country_list &amp;lt;- c(&amp;quot;France&amp;quot;, &amp;quot;Germany&amp;quot;, &amp;quot;United States of America&amp;quot;, &amp;quot;Luxembourg&amp;quot;, &amp;quot;Switzerland&amp;quot;, &amp;quot;Greece&amp;quot;)

small_pwt &amp;lt;- pwt9.0 %&amp;gt;%
  filter(country %in% country_list)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s us also order the countries in the data frame as I have written them in &lt;code&gt;country_list&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_pwt &amp;lt;- small_pwt %&amp;gt;%
  mutate(country = factor(country, levels = country_list, ordered = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might be wondering why this is important. At the end of the article, we are going to save the
plots to disk. If we do not re-order the countries inside the data frame as in &lt;code&gt;country_list&lt;/code&gt;, the
name of the files will not correspond to the correct plots!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: While this can still be interesting to know, especially if you want to order the bars
of a barplot made with &lt;code&gt;ggplot2&lt;/code&gt;, I included a suggestion by &lt;a href=&#34;https://twitter.com/expersso/status/846986357792739328&#34;&gt;&lt;code&gt;@expersso&lt;/code&gt;&lt;/a&gt;
that does not require your data to be ordered!&lt;/p&gt;
&lt;p&gt;Now when you want to plot the same variable by countries, say &lt;code&gt;avh&lt;/code&gt; (&lt;em&gt;Average annual hours worked by
persons engaged&lt;/em&gt;), the usual way to do this is with one of &lt;code&gt;facet_wrap()&lt;/code&gt; or &lt;code&gt;facet_grid()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = small_pwt) + theme_tufte() +
  geom_line(aes(y = avh, x = year)) +
  facet_wrap(~country)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-03-29-make-ggplot2-purrr_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = small_pwt) + theme_tufte() +
  geom_line(aes(y = avh, x = year)) +
  facet_grid(country~.)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-03-29-make-ggplot2-purrr_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, for this particular example, &lt;code&gt;facet_grid()&lt;/code&gt; is not very useful, but do notice its
argument, &lt;code&gt;country~.&lt;/code&gt;, which is different from &lt;code&gt;facet_wrap()&lt;/code&gt;’s argument. This way, I get the graphs
stacked horizontally. If I had used &lt;code&gt;facet_grid(~country)&lt;/code&gt; the graphs would be side by side and completely
unreadable.&lt;/p&gt;
&lt;p&gt;Now, let’s go to the meat of this post: what if you would like to have one single graph for each
country? You’d probably think of using &lt;code&gt;dplyr::group_by()&lt;/code&gt; to form the groups and then the graphs. This
is the way to go, but you also have to use &lt;code&gt;dplyr::do()&lt;/code&gt;. This is because as far as I understand,
&lt;code&gt;ggplot2&lt;/code&gt; is not &lt;code&gt;dplyr&lt;/code&gt;-aware, and using an arbitrary function with groups is only possible with
&lt;code&gt;dplyr::do()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: As explained in the intro above, I also added the solution that uses &lt;code&gt;tidyr::nest()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Ancient, deprecated way of doing this
plots &amp;lt;- small_pwt %&amp;gt;%
  group_by(country) %&amp;gt;%
  do(plot = ggplot(data = .) + theme_tufte() +
       geom_line(aes(y = avh, x = year)) +
       ggtitle(unique(.$country)) +
       ylab(&amp;quot;Year&amp;quot;) +
       xlab(&amp;quot;Average annual hours worked by persons engaged&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And this is the approach that uses &lt;code&gt;tidyr::nest()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Preferred approach
plots &amp;lt;- small_pwt %&amp;gt;%
  group_by(country) %&amp;gt;%
  nest() %&amp;gt;%
  mutate(plot = map2(data, country, ~ggplot(data = .x) + theme_tufte() +
       geom_line(aes(y = avh, x = year)) +
       ggtitle(.y) +
       ylab(&amp;quot;Year&amp;quot;) +
       xlab(&amp;quot;Average annual hours worked by persons engaged&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you know &lt;code&gt;dplyr&lt;/code&gt; at least a little bit, the above lines should be easy for you to understand.
But notice how we get the title of the graphs, with &lt;code&gt;ggtitle(unique(.$country))&lt;/code&gt;, which was actually
the point of the stackoverflow question.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; The modern version uses &lt;code&gt;tidyr::nest()&lt;/code&gt;. Its documentation tells us:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;There are many possible ways one could choose to nest columns inside a data frame.
&lt;code&gt;nest()&lt;/code&gt; creates a list of data frames containing all the nested variables: this seems to be the most useful form in practice.&lt;/em&gt;
Let’s take a closer look at what it does exactly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;small_pwt %&amp;gt;%
  group_by(country) %&amp;gt;%
  nest() %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   country                  data              
##   &amp;lt;ord&amp;gt;                    &amp;lt;list&amp;gt;            
## 1 Switzerland              &amp;lt;tibble [65 × 46]&amp;gt;
## 2 Germany                  &amp;lt;tibble [65 × 46]&amp;gt;
## 3 France                   &amp;lt;tibble [65 × 46]&amp;gt;
## 4 Greece                   &amp;lt;tibble [65 × 46]&amp;gt;
## 5 Luxembourg               &amp;lt;tibble [65 × 46]&amp;gt;
## 6 United States of America &amp;lt;tibble [65 × 46]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is why I love lists in R; we get a &lt;code&gt;tibble&lt;/code&gt; where each element of the column &lt;code&gt;data&lt;/code&gt; is itself a &lt;code&gt;tibble&lt;/code&gt;.
We can now apply any function that we know works on lists.&lt;/p&gt;
&lt;p&gt;What might be surprising though, is the object that is created by this code. Let’s take a look at
&lt;code&gt;plots&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(plots)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   country                  data               plot    
##   &amp;lt;ord&amp;gt;                    &amp;lt;list&amp;gt;             &amp;lt;list&amp;gt;  
## 1 Switzerland              &amp;lt;tibble [65 × 46]&amp;gt; &amp;lt;S3: gg&amp;gt;
## 2 Germany                  &amp;lt;tibble [65 × 46]&amp;gt; &amp;lt;S3: gg&amp;gt;
## 3 France                   &amp;lt;tibble [65 × 46]&amp;gt; &amp;lt;S3: gg&amp;gt;
## 4 Greece                   &amp;lt;tibble [65 × 46]&amp;gt; &amp;lt;S3: gg&amp;gt;
## 5 Luxembourg               &amp;lt;tibble [65 × 46]&amp;gt; &amp;lt;S3: gg&amp;gt;
## 6 United States of America &amp;lt;tibble [65 × 46]&amp;gt; &amp;lt;S3: gg&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As &lt;code&gt;dplyr::do()&lt;/code&gt;’s documentation tells us, the return values get stored inside a list. And this is
exactly what we get back; a list of plots! Lists are a very flexible and useful class, and you cannot
spell &lt;em&gt;list&lt;/em&gt; without &lt;code&gt;purrr&lt;/code&gt; (at least not when you’re a ne&lt;code&gt;R&lt;/code&gt;d).&lt;/p&gt;
&lt;p&gt;Here are the final lines that use &lt;code&gt;purrr::map2()&lt;/code&gt; to save all these plots at once inside your working directory:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: I have changed the code below which does not require your data frame to be ordered according
to the variable &lt;code&gt;country_list&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# file_names &amp;lt;- paste0(country_list, &amp;quot;.pdf&amp;quot;)

map2(paste0(plots$country, &amp;quot;.pdf&amp;quot;), plots$plot, ggsave)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As I said before, if you do not re-order the countries inside the data frame, the names of the files
and the plots will not match. Try running all the code without re-ordering, you’ll see!&lt;/p&gt;
&lt;p&gt;I hope you found this post useful. You can follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt; for
blog updates.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: Many thanks to the readers of this article and for their useful suggestions. I love the R
community; everyday I learn something new and useful!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing brotools</title>
      <link>/blog/2017-03-27-introducing_brotools/</link>
      <pubDate>Mon, 27 Mar 2017 09:23:56 +0200</pubDate>
      
      <guid>/blog/2017-03-27-introducing_brotools/</guid>
      <description>&lt;p&gt;I’m happy to announce my first R package, called &lt;code&gt;brotools&lt;/code&gt;. This is a package that contains
functions that are specific to my needs but that you might find also useful. I blogged about some
of these functions, so if you follow my blog you might already be familiar with some of them. It is
not on CRAN and might very well never be. The code is hosted on &lt;a href=&#34;https://bitbucket.org/b-rodrigues/brotools&#34;&gt;bitbucket&lt;/a&gt;
and you can install the package with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_bitbucket(&amp;quot;b-rodrigues/brotools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope you’ll find the &lt;code&gt;brotools&lt;/code&gt; useful!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lesser known purrr tricks</title>
      <link>/blog/2017-03-24-lesser_known_purrr/</link>
      <pubDate>Fri, 24 Mar 2017 12:00:00 +0100</pubDate>
      
      <guid>/blog/2017-03-24-lesser_known_purrr/</guid>
      <description>&lt;p&gt;&lt;code&gt;purrr&lt;/code&gt; is a package that extends R’s functional programming capabilities. It brings a lot of new stuff to
the table and in this post I show you some of the most useful (at least to me) functions included in &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;getting-rid-of-loops-with-map&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting rid of loops with &lt;code&gt;map()&lt;/code&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)

numbers &amp;lt;- list(11, 12, 13, 14)

map_dbl(numbers, sqrt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.316625 3.464102 3.605551 3.741657&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might wonder why this might be preferred to a for loop? It’s a lot less verbose, and you do not need to
initialise any kind of structure to hold the result. If you google “create empty list in R” you will see that
this is very common. However, with the &lt;code&gt;map()&lt;/code&gt; family of functions, there is no need for an initial structure.
&lt;code&gt;map_dbl()&lt;/code&gt; returns an atomic list of real numbers, but if you use &lt;code&gt;map()&lt;/code&gt; you will get a list back. Try them all out!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;map-conditionally&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Map conditionally&lt;/h2&gt;
&lt;div id=&#34;map_if&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;map_if()&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a helper function that returns TRUE if a number is even
is_even &amp;lt;- function(x){
  !as.logical(x %% 2)
}

map_if(numbers, is_even, sqrt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 11
## 
## [[2]]
## [1] 3.464102
## 
## [[3]]
## [1] 13
## 
## [[4]]
## [1] 3.741657&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;map_at&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;map_at()&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_at(numbers, c(1,3), sqrt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 3.316625
## 
## [[2]]
## [1] 12
## 
## [[3]]
## [1] 3.605551
## 
## [[4]]
## [1] 14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;map_if()&lt;/code&gt; and &lt;code&gt;map_at()&lt;/code&gt; have a further argument than &lt;code&gt;map()&lt;/code&gt;; in the case of &lt;code&gt;map_if()&lt;/code&gt;, a predicate function (
a function that returns &lt;code&gt;TRUE&lt;/code&gt; or &lt;code&gt;FALSE&lt;/code&gt;) and a vector of positions for &lt;code&gt;map_at()&lt;/code&gt;. This allows you to map your
function only when certain conditions are met, which is also something that a lot of people google for.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;map-a-function-with-multiple-arguments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Map a function with multiple arguments&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;numbers2 &amp;lt;- list(1, 2, 3, 4)

map2(numbers, numbers2, `+`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 12
## 
## [[2]]
## [1] 14
## 
## [[3]]
## [1] 16
## 
## [[4]]
## [1] 18&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can map two lists to a function which takes two arguments using &lt;code&gt;map_2()&lt;/code&gt;. You can even map an arbitrary number
of lists to any function using &lt;code&gt;pmap()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By the way, try this in: &lt;code&gt;`+`(1,3)&lt;/code&gt; and see what happens.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dont-stop-execution-of-your-function-if-something-goes-wrong&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Don’t stop execution of your function if something goes wrong&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;possible_sqrt &amp;lt;- possibly(sqrt, otherwise = NA_real_)

numbers_with_error &amp;lt;- list(1, 2, 3, &amp;quot;spam&amp;quot;, 4)

map(numbers_with_error, possible_sqrt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1.414214
## 
## [[3]]
## [1] 1.732051
## 
## [[4]]
## [1] NA
## 
## [[5]]
## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another very common issue is to keep running your loop even when something goes wrong. In most cases the loop simply stops
at the error, but you would like it to continue and see where it failed. Try to google “skip error in a loop”
or some variation of it and you’ll see that a lot of people really just want that.
This is possible by combining &lt;code&gt;map()&lt;/code&gt; and &lt;code&gt;possibly()&lt;/code&gt;. Most solutions involve the use of
&lt;code&gt;tryCatch()&lt;/code&gt; which I personally do not find very easy to use.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dont-stop-execution-of-your-function-if-something-goes-wrong-and-capture-the-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Don’t stop execution of your function if something goes wrong and capture the error&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;safe_sqrt &amp;lt;- safely(sqrt, otherwise = NA_real_)

map(numbers_with_error, safe_sqrt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [[1]]$result
## [1] 1
## 
## [[1]]$error
## NULL
## 
## 
## [[2]]
## [[2]]$result
## [1] 1.414214
## 
## [[2]]$error
## NULL
## 
## 
## [[3]]
## [[3]]$result
## [1] 1.732051
## 
## [[3]]$error
## NULL
## 
## 
## [[4]]
## [[4]]$result
## [1] NA
## 
## [[4]]$error
## &amp;lt;simpleError in sqrt(x = x): non-numeric argument to mathematical function&amp;gt;
## 
## 
## [[5]]
## [[5]]$result
## [1] 2
## 
## [[5]]$error
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;safely()&lt;/code&gt; is very similar to &lt;code&gt;possibly()&lt;/code&gt; but it returns a list of lists. An element is thus a list of the result
and the accompagnying error message. If there is no error, the error component is &lt;code&gt;NULL&lt;/code&gt; if there is an error, it
returns the error message.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;transpose-a-list&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transpose a list&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;safe_result_list &amp;lt;- map(numbers_with_error, safe_sqrt)

transpose(safe_result_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $result
## $result[[1]]
## [1] 1
## 
## $result[[2]]
## [1] 1.414214
## 
## $result[[3]]
## [1] 1.732051
## 
## $result[[4]]
## [1] NA
## 
## $result[[5]]
## [1] 2
## 
## 
## $error
## $error[[1]]
## NULL
## 
## $error[[2]]
## NULL
## 
## $error[[3]]
## NULL
## 
## $error[[4]]
## &amp;lt;simpleError in sqrt(x = x): non-numeric argument to mathematical function&amp;gt;
## 
## $error[[5]]
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we transposed the above list. This means that we still have a list of lists, but where the first list holds
all the results (which you can then access with &lt;code&gt;safe_result_list$result&lt;/code&gt;) and the second list holds all the errors
(which you can access with &lt;code&gt;safe_result_list$error&lt;/code&gt;). This can be quite useful!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;apply-a-function-to-a-lower-depth-of-a-list&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply a function to a lower depth of a list&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transposed_list &amp;lt;- transpose(safe_result_list)

transposed_list %&amp;gt;%
    at_depth(2, is_null)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: at_depth() is deprecated, please use `modify_depth()` instead&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $result
## $result[[1]]
## [1] FALSE
## 
## $result[[2]]
## [1] FALSE
## 
## $result[[3]]
## [1] FALSE
## 
## $result[[4]]
## [1] FALSE
## 
## $result[[5]]
## [1] FALSE
## 
## 
## $error
## $error[[1]]
## [1] TRUE
## 
## $error[[2]]
## [1] TRUE
## 
## $error[[3]]
## [1] TRUE
## 
## $error[[4]]
## [1] FALSE
## 
## $error[[5]]
## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sometimes working with lists of lists can be tricky, especially when we want to apply a function to the sub-lists. This
is easily done with &lt;code&gt;at_depth()&lt;/code&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-names-of-list-elements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set names of list elements&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;name_element &amp;lt;- c(&amp;quot;sqrt()&amp;quot;, &amp;quot;ok?&amp;quot;)

set_names(transposed_list, name_element)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $`sqrt()`
## $`sqrt()`[[1]]
## [1] 1
## 
## $`sqrt()`[[2]]
## [1] 1.414214
## 
## $`sqrt()`[[3]]
## [1] 1.732051
## 
## $`sqrt()`[[4]]
## [1] NA
## 
## $`sqrt()`[[5]]
## [1] 2
## 
## 
## $`ok?`
## $`ok?`[[1]]
## NULL
## 
## $`ok?`[[2]]
## NULL
## 
## $`ok?`[[3]]
## NULL
## 
## $`ok?`[[4]]
## &amp;lt;simpleError in sqrt(x = x): non-numeric argument to mathematical function&amp;gt;
## 
## $`ok?`[[5]]
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reduce-a-list-to-a-single-value&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reduce a list to a single value&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reduce(numbers, `*`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24024&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;reduce()&lt;/code&gt; applies the function &lt;code&gt;*&lt;/code&gt; iteratively to the list of numbers. There’s also &lt;code&gt;accumulate()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;accumulate(numbers, `*`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]    11   132  1716 24024&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which keeps the intermediary results.&lt;/p&gt;
&lt;p&gt;This function is very general, and you can reduce anything:&lt;/p&gt;
&lt;p&gt;Matrices:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mat1 &amp;lt;- matrix(rnorm(10), nrow = 2)
mat2 &amp;lt;- matrix(rnorm(10), nrow = 2)
mat3 &amp;lt;- matrix(rnorm(10), nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_mat &amp;lt;- list(mat1, mat2, mat3)

reduce(list_mat, `+`)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             [,1]       [,2]       [,3]     [,4]      [,5]
## [1,] -2.48530177  1.0110049  0.4450388 1.280802 1.3413979
## [2,]  0.07596679 -0.6872268 -0.6579242 1.615237 0.8231933&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;even data frames:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1 &amp;lt;- as.data.frame(mat1)
df2 &amp;lt;- as.data.frame(mat2)
df3 &amp;lt;- as.data.frame(mat3)

list_df &amp;lt;- list(df1, df2, df3)

reduce(list_df, dplyr::full_join)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;V1&amp;quot;, &amp;quot;V2&amp;quot;, &amp;quot;V3&amp;quot;, &amp;quot;V4&amp;quot;, &amp;quot;V5&amp;quot;)
## Joining, by = c(&amp;quot;V1&amp;quot;, &amp;quot;V2&amp;quot;, &amp;quot;V3&amp;quot;, &amp;quot;V4&amp;quot;, &amp;quot;V5&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           V1         V2          V3          V4         V5
## 1 -0.6264538 -0.8356286  0.32950777  0.48742905  0.5757814
## 2  0.1836433  1.5952808 -0.82046838  0.73832471 -0.3053884
## 3 -0.8969145  1.5878453 -0.08025176  0.70795473  1.9844739
## 4  0.1848492 -1.1303757  0.13242028 -0.23969802 -0.1387870
## 5 -0.9619334  0.2587882  0.19578283  0.08541773 -1.2188574
## 6 -0.2925257 -1.1521319  0.03012394  1.11661021  1.2673687&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hope you enjoyed this list of useful functions! If you enjoy the content of my blog, you can follow me on &lt;a href=&#34;https://www.twitter.com/brodriguesco&#34;&gt;twitter&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Lesser known dplyr tricks</title>
      <link>/blog/2017-02-17-lesser_known_tricks/</link>
      <pubDate>Wed, 08 Mar 2017 12:00:00 +0100</pubDate>
      
      <guid>/blog/2017-02-17-lesser_known_tricks/</guid>
      <description>&lt;p&gt;In this blog post I share some lesser-known (at least I believe they are) tricks that use mainly functions from &lt;code&gt;dplyr&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;removing-unneeded-columns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Removing unneeded columns&lt;/h2&gt;
&lt;p&gt;Did you know that you can use &lt;code&gt;-&lt;/code&gt; in front of a column name to remove it from a data frame?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
    select(-disp) %&amp;gt;% 
    head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    mpg cyl  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6 105 2.76 3.460 20.22  1  0    3    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;re-ordering-columns&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Re-ordering columns&lt;/h2&gt;
&lt;p&gt;Still using &lt;code&gt;select()&lt;/code&gt;, it is easy te re-order columns in your data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
    select(cyl, disp, hp, everything()) %&amp;gt;% 
    head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   cyl disp  hp  mpg drat    wt  qsec vs am gear carb
## Mazda RX4           6  160 110 21.0 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag       6  160 110 21.0 3.90 2.875 17.02  0  1    4    4
## Datsun 710          4  108  93 22.8 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive      6  258 110 21.4 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout   8  360 175 18.7 3.15 3.440 17.02  0  0    3    2
## Valiant             6  225 105 18.1 2.76 3.460 20.22  1  0    3    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As its name implies &lt;code&gt;everything()&lt;/code&gt; simply means all the other columns.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;renaming-columns-with-rename&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Renaming columns with &lt;code&gt;rename()&lt;/code&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars &amp;lt;- rename(mtcars, spam_mpg = mpg)
mtcars &amp;lt;- rename(mtcars, spam_disp = disp)
mtcars &amp;lt;- rename(mtcars, spam_hp = hp)

head(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am
## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1
## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1
## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1
## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0
## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0
## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0
##                   gear carb
## Mazda RX4            4    4
## Mazda RX4 Wag        4    4
## Datsun 710           4    1
## Hornet 4 Drive       3    1
## Hornet Sportabout    3    2
## Valiant              3    1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;selecting-columns-with-a-regexp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Selecting columns with a regexp&lt;/h2&gt;
&lt;p&gt;It is easy to select the columns that start with “spam” with some helper functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
    select(contains(&amp;quot;spam&amp;quot;)) %&amp;gt;% 
    head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   spam_mpg spam_disp spam_hp
## Mazda RX4             21.0       160     110
## Mazda RX4 Wag         21.0       160     110
## Datsun 710            22.8       108      93
## Hornet 4 Drive        21.4       258     110
## Hornet Sportabout     18.7       360     175
## Valiant               18.1       225     105&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;take also a look at &lt;code&gt;starts_with()&lt;/code&gt;, &lt;code&gt;ends_with()&lt;/code&gt;, &lt;code&gt;contains()&lt;/code&gt;, &lt;code&gt;matches()&lt;/code&gt;, &lt;code&gt;num_range()&lt;/code&gt;, &lt;code&gt;one_of()&lt;/code&gt; and &lt;code&gt;everything()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-new-columns-with-mutate-and-if_else&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create new columns with &lt;code&gt;mutate()&lt;/code&gt; and &lt;code&gt;if_else()&lt;/code&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
    mutate(vs_new = if_else(
        vs == 1, 
        &amp;quot;one&amp;quot;, 
        &amp;quot;zero&amp;quot;, 
        NA_character_)) %&amp;gt;% 
    head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb vs_new
## 1     21.0   6       160     110 3.90 2.620 16.46  0  1    4    4   zero
## 2     21.0   6       160     110 3.90 2.875 17.02  0  1    4    4   zero
## 3     22.8   4       108      93 3.85 2.320 18.61  1  1    4    1    one
## 4     21.4   6       258     110 3.08 3.215 19.44  1  0    3    1    one
## 5     18.7   8       360     175 3.15 3.440 17.02  0  0    3    2   zero
## 6     18.1   6       225     105 2.76 3.460 20.22  1  0    3    1    one&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might want to create a new variable conditionally on several values of another column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
    mutate(carb_new = case_when(.$carb == 1 ~ &amp;quot;one&amp;quot;,
                                .$carb == 2 ~ &amp;quot;two&amp;quot;,
                                .$carb == 4 ~ &amp;quot;four&amp;quot;,
                                 TRUE ~ &amp;quot;other&amp;quot;)) %&amp;gt;% 
    head(15)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am gear carb
## 1      21.0   6     160.0     110 3.90 2.620 16.46  0  1    4    4
## 2      21.0   6     160.0     110 3.90 2.875 17.02  0  1    4    4
## 3      22.8   4     108.0      93 3.85 2.320 18.61  1  1    4    1
## 4      21.4   6     258.0     110 3.08 3.215 19.44  1  0    3    1
## 5      18.7   8     360.0     175 3.15 3.440 17.02  0  0    3    2
## 6      18.1   6     225.0     105 2.76 3.460 20.22  1  0    3    1
## 7      14.3   8     360.0     245 3.21 3.570 15.84  0  0    3    4
## 8      24.4   4     146.7      62 3.69 3.190 20.00  1  0    4    2
## 9      22.8   4     140.8      95 3.92 3.150 22.90  1  0    4    2
## 10     19.2   6     167.6     123 3.92 3.440 18.30  1  0    4    4
## 11     17.8   6     167.6     123 3.92 3.440 18.90  1  0    4    4
## 12     16.4   8     275.8     180 3.07 4.070 17.40  0  0    3    3
## 13     17.3   8     275.8     180 3.07 3.730 17.60  0  0    3    3
## 14     15.2   8     275.8     180 3.07 3.780 18.00  0  0    3    3
## 15     10.4   8     472.0     205 2.93 5.250 17.98  0  0    3    4
##    carb_new
## 1      four
## 2      four
## 3       one
## 4       one
## 5       two
## 6       one
## 7      four
## 8       two
## 9       two
## 10     four
## 11     four
## 12    other
## 13    other
## 14    other
## 15     four&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mind the &lt;code&gt;.$&lt;/code&gt; before the variable &lt;code&gt;carb&lt;/code&gt;. There is a &lt;a href=&#34;https://github.com/hadley/dplyr/issues/1965&#34;&gt;github issue&lt;/a&gt;
about this, and it is already fixed in the development version of &lt;code&gt;dplyr&lt;/code&gt;, which means that in the next version
of &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;case_when()&lt;/code&gt; will work as any other specialized &lt;code&gt;dplyr&lt;/code&gt; function inside &lt;code&gt;mutate()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;apply-a-function-to-certain-columns-only-by-rows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply a function to certain columns only, by rows&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;%
    select(am, gear, carb) %&amp;gt;%
    purrr::by_row(sum, .collate = &amp;quot;cols&amp;quot;, .to = &amp;quot;sum_am_gear_carb&amp;quot;) -&amp;gt; mtcars2
head(mtcars2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this, I had to use &lt;code&gt;purrr&lt;/code&gt;’s &lt;code&gt;by_row()&lt;/code&gt; function. You can then add this column to your original data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars &amp;lt;- cbind(mtcars, &amp;quot;sum_am_gear_carb&amp;quot; = mtcars2$sum_am_gear_carb)
head(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   spam_mpg cyl spam_disp spam_hp drat    wt  qsec vs am
## Mazda RX4             21.0   6       160     110 3.90 2.620 16.46  0  1
## Mazda RX4 Wag         21.0   6       160     110 3.90 2.875 17.02  0  1
## Datsun 710            22.8   4       108      93 3.85 2.320 18.61  1  1
## Hornet 4 Drive        21.4   6       258     110 3.08 3.215 19.44  1  0
## Hornet Sportabout     18.7   8       360     175 3.15 3.440 17.02  0  0
## Valiant               18.1   6       225     105 2.76 3.460 20.22  1  0
##                   gear carb sum_am_gear_carb
## Mazda RX4            4    4                9
## Mazda RX4 Wag        4    4                9
## Datsun 710           4    1                6
## Hornet 4 Drive       3    1                4
## Hornet Sportabout    3    2                5
## Valiant              3    1                4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;use-do-to-do-any-arbitrary-operation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use &lt;code&gt;do()&lt;/code&gt; to do any arbitrary operation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars %&amp;gt;% 
    group_by(cyl) %&amp;gt;% 
    do(models = lm(spam_mpg ~ drat + wt, data = .)) %&amp;gt;% 
    broom::tidy(models)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 6
## # Groups:   cyl [3]
##     cyl term        estimate std.error statistic p.value
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     4 (Intercept)   33.2      17.1       1.94  0.0877 
## 2     4 drat           1.32      3.45      0.384 0.711  
## 3     4 wt            -5.24      2.22     -2.37  0.0456 
## 4     6 (Intercept)   30.7       7.51      4.08  0.0151 
## 5     6 drat          -0.444     1.17     -0.378 0.725  
## 6     6 wt            -2.99      1.57     -1.91  0.129  
## 7     8 (Intercept)   29.7       7.09      4.18  0.00153
## 8     8 drat          -1.47      1.63     -0.903 0.386  
## 9     8 wt            -2.45      0.799    -3.07  0.0107&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;do()&lt;/code&gt; is useful when you want to use any R function (user defined functions work too!) with &lt;code&gt;dplyr&lt;/code&gt; functions.
First I grouped the observations by &lt;code&gt;cyl&lt;/code&gt; and then ran a linear model for each group. Then I converted the output
to a tidy data frame using &lt;code&gt;broom::tidy()&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-dplyr-functions-inside-your-own-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;dplyr&lt;/code&gt; functions inside your own functions&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_vars &amp;lt;- function(data, some_string){
    
  data %&amp;gt;%
    select_(lazyeval::interp(~contains(some_string))) -&amp;gt; data
    
  return(data)
}

extract_vars(mtcars, &amp;quot;spam&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     spam_mpg spam_disp spam_hp
## Mazda RX4               21.0     160.0     110
## Mazda RX4 Wag           21.0     160.0     110
## Datsun 710              22.8     108.0      93
## Hornet 4 Drive          21.4     258.0     110
## Hornet Sportabout       18.7     360.0     175
## Valiant                 18.1     225.0     105
## Duster 360              14.3     360.0     245
## Merc 240D               24.4     146.7      62
## Merc 230                22.8     140.8      95
## Merc 280                19.2     167.6     123
## Merc 280C               17.8     167.6     123
## Merc 450SE              16.4     275.8     180
## Merc 450SL              17.3     275.8     180
## Merc 450SLC             15.2     275.8     180
## Cadillac Fleetwood      10.4     472.0     205
## Lincoln Continental     10.4     460.0     215
## Chrysler Imperial       14.7     440.0     230
## Fiat 128                32.4      78.7      66
## Honda Civic             30.4      75.7      52
## Toyota Corolla          33.9      71.1      65
## Toyota Corona           21.5     120.1      97
## Dodge Challenger        15.5     318.0     150
## AMC Javelin             15.2     304.0     150
## Camaro Z28              13.3     350.0     245
## Pontiac Firebird        19.2     400.0     175
## Fiat X1-9               27.3      79.0      66
## Porsche 914-2           26.0     120.3      91
## Lotus Europa            30.4      95.1     113
## Ford Pantera L          15.8     351.0     264
## Ferrari Dino            19.7     145.0     175
## Maserati Bora           15.0     301.0     335
## Volvo 142E              21.4     121.0     109&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;About this last point, you can read more about it &lt;a href=&#34;http://www.brodrigues.co/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope you liked this small list of tricks!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to use jailbreakr</title>
      <link>/blog/2017-02-17-how_to_use_jailbreakr/</link>
      <pubDate>Fri, 17 Feb 2017 12:51:00 +0100</pubDate>
      
      <guid>/blog/2017-02-17-how_to_use_jailbreakr/</guid>
      <description>&lt;div id=&#34;what-is-jailbreakr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is &lt;code&gt;jailbreakr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;jailbreakr&lt;/code&gt; package is probably one of the most interesting packages I came across recently.
This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I
am sure you already had to deal with spreadsheets that contained little tables inside a single
sheet for example. As far as I know, there is no simple way of extracting these tables without having
to fiddle around a lot. This is now over with &lt;code&gt;jailbreakr&lt;/code&gt;. Well not entirely, because &lt;code&gt;jailbreakr&lt;/code&gt;
is still in development, but it works well already. If you want to know more about the planned
features, you can watch the
following
&lt;a href=&#34;https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/jailbreakr-Get-out-of-Excel-free&#34;&gt;video&lt;/a&gt; by
Jenny Bryan, one of the package’s authors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installation-and-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installation and data&lt;/h2&gt;
&lt;p&gt;You will have to install the package from Github, as it is not on CRAN
yet. &lt;a href=&#34;https://github.com/rsheets/jailbreakr&#34;&gt;Here is the Github link&lt;/a&gt;. To install the package, just
run the following commands in an R console:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(c(&amp;quot;hadley/xml2&amp;quot;,
                           &amp;quot;rsheets/linen&amp;quot;,
                           &amp;quot;rsheets/cellranger&amp;quot;,
                           &amp;quot;rsheets/rexcel&amp;quot;,
                           &amp;quot;rsheets/jailbreakr&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you get the following error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;devtools::install_github(&amp;quot;hadley/xml2&amp;quot;)
Downloading GitHub repo hadley/xml2@master
from URL https://api.github.com/repos/hadley/xml2/zipball/master
Error in system(full, intern = quiet, ignore.stderr = quiet, ...) :
    error in running command&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and if you’re on a GNU+Linux distribution try to run the following command:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(unzip = &amp;quot;internal&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then run &lt;code&gt;github_install()&lt;/code&gt; again.&lt;/p&gt;
&lt;p&gt;As you can see, you need some other packages to make it work. Now we are going to get some data. We
are going to download some time series from the European Commission, data I had to deal with
recently. Download the data by clicking &lt;a href=&#34;http://ec.europa.eu/economy_finance/db_indicators/surveys/documents/series/nace2_ecfin_1701/investment_total_nsa_nace2.zip&#34;&gt;here&lt;/a&gt;
and look for the spreadsheet titled &lt;code&gt;Investment_total_factors_nace2.xlsx&lt;/code&gt;. The data we are interested
in is on the second sheet, named &lt;code&gt;TOT&lt;/code&gt;. You cannot import this sheet easily into R because there
are four tables on the same sheet. Let us use &lt;code&gt;jailbreakr&lt;/code&gt; to get these tables out of the sheet and
into nice, tidy, data frames.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;jailbreakr-to-the-rescue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;jailbreakr&lt;/code&gt; to the rescue&lt;/h2&gt;
&lt;p&gt;The first step is to read the data in. For this, we are going to use the &lt;code&gt;rexcel&lt;/code&gt; package, which is
also part of the &lt;code&gt;rsheets&lt;/code&gt; organization on Github that was set up by Jenny Brian and Rich Fitzjohn,
the authors of these packages. &lt;code&gt;rexcel&lt;/code&gt; imports the sheet you want but not in a way that is
immediately useful to you. It just gets the sheet into R, which makes it then possible to use
&lt;code&gt;jailbreakr&lt;/code&gt;’s magic on it. First, let’s import the packages we need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;rexcel&amp;quot;)
library(&amp;quot;jailbreakr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to check which sheet to import. There are two sheets, and we want to import the one called
&lt;code&gt;TOT&lt;/code&gt;, the second one. But is it really the second one? I have noticed that sometimes, there are
hidden sheets which makes importing the one you want impossible. So first, let use use another
package, &lt;code&gt;readxl&lt;/code&gt; and its function &lt;code&gt;excel_sheets()&lt;/code&gt; to make sure we are extracting the sheet we
really need:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sheets &amp;lt;- readxl::excel_sheets(path_to_data)

tot_sheet &amp;lt;- which(sheets == &amp;quot;TOT&amp;quot;)

print(tot_sheet)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the sheet we want is not the second, but the third! Let us import this sheet into R now
(this might take more time than you think; on my computer it takes around 10 seconds):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my_sheet &amp;lt;- rexcel_read(path_to_data, sheet = tot_sheet)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can start using &lt;code&gt;jailbreakr&lt;/code&gt;. The function &lt;code&gt;split_sheet()&lt;/code&gt; is the one that splits the sheet
into little tables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tables &amp;lt;- split_sheet(my_sheet)
str(tables)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ :Classes &amp;#39;worksheet_view&amp;#39;, &amp;#39;R6&amp;#39; &amp;lt;worksheet_view&amp;gt;
##   Public:
##     cells: active binding
##     clone: function (deep = FALSE) 
##     data: NULL
##     dim: 34 28
##     header: NULL
##     idx: list
##     initialize: function (sheet, xr, filter, header, data) 
##     lookup: active binding
##     lookup2: active binding
##     merged: active binding
##     sheet: worksheet, R6
##     table: function (col_names = TRUE, ...) 
##     values: function () 
##     xr: cell_limits, list 
##  $ :Classes &amp;#39;worksheet_view&amp;#39;, &amp;#39;R6&amp;#39; &amp;lt;worksheet_view&amp;gt;
##   Public:
##     cells: active binding
##     clone: function (deep = FALSE) 
##     data: NULL
##     dim: 33 28
##     header: NULL
##     idx: list
##     initialize: function (sheet, xr, filter, header, data) 
##     lookup: active binding
##     lookup2: active binding
##     merged: active binding
##     sheet: worksheet, R6
##     table: function (col_names = TRUE, ...) 
##     values: function () 
##     xr: cell_limits, list 
##  $ :Classes &amp;#39;worksheet_view&amp;#39;, &amp;#39;R6&amp;#39; &amp;lt;worksheet_view&amp;gt;
##   Public:
##     cells: active binding
##     clone: function (deep = FALSE) 
##     data: NULL
##     dim: 32 28
##     header: NULL
##     idx: list
##     initialize: function (sheet, xr, filter, header, data) 
##     lookup: active binding
##     lookup2: active binding
##     merged: active binding
##     sheet: worksheet, R6
##     table: function (col_names = TRUE, ...) 
##     values: function () 
##     xr: cell_limits, list 
##  $ :Classes &amp;#39;worksheet_view&amp;#39;, &amp;#39;R6&amp;#39; &amp;lt;worksheet_view&amp;gt;
##   Public:
##     cells: active binding
##     clone: function (deep = FALSE) 
##     data: NULL
##     dim: 33 28
##     header: NULL
##     idx: list
##     initialize: function (sheet, xr, filter, header, data) 
##     lookup: active binding
##     lookup2: active binding
##     merged: active binding
##     sheet: worksheet, R6
##     table: function (col_names = TRUE, ...) 
##     values: function () 
##     xr: cell_limits, list&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tables&lt;/code&gt; is actually a list containing &lt;code&gt;worksheet_view&lt;/code&gt; objects. Take a look at the &lt;code&gt;dim&lt;/code&gt;
attribute: you see the dimensions of the tables there. When I started using &lt;code&gt;jailbreakr&lt;/code&gt; I was
stuck here. I was looking for the function that would extract the data frames and could not find
it. Then I watched the video and I understood what I had to do: a &lt;code&gt;worksheet_view&lt;/code&gt; object has a
&lt;code&gt;values()&lt;/code&gt; method that does the extraction for you. This is a bit unusual in R (it made me feel
like I was using Python); maybe in future versions this &lt;code&gt;values()&lt;/code&gt; method will become a separate
function of its own in the package. What happens when we use &lt;code&gt;values()&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;purrr&amp;quot;)
list_of_data &amp;lt;-  map(tables, (function(x)(x$values())))
map(list_of_data, head)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
##      [,1]     [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
## [1,] &amp;quot;TOT&amp;quot;    NA      NA    NA    NA    NA    NA    NA    NA    NA   
## [2,] &amp;quot;DEMAND&amp;quot; 33603   33969 34334 34699 35064 35430 35795 36160 36525
## [3,] &amp;quot;FDEMT&amp;quot;  &amp;quot;FDEMN&amp;quot; NA    NA    NA    NA    NA    NA    NA    NA   
## [4,] &amp;quot;EU&amp;quot;     &amp;quot;:&amp;quot;     16.9  -1.4  20.2  34.5  31.4  37.5  39    37.3 
## [5,] &amp;quot;EA&amp;quot;     &amp;quot;:&amp;quot;     15.5  -13.1 14.8  30.9  25.1  35.2  39.2  37.1 
## [6,] &amp;quot;BE&amp;quot;     &amp;quot;:&amp;quot;     &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   42.3  43.1 
##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]
## [1,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
## [2,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543
## [3,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
## [4,] 39.2  27.5  20.6  21.4  29.8  26.4  32.5  47.1  19    -1.3  23.5 
## [5,] 39.5  25.3  18.2  18.9  27.4  23    28.2  46.1  12.3  -9.3  19.3 
## [6,] 45.8  42.2  42.9  43.8  45.8  47.4  49.1  50.9  48.2  46.9  46.3 
##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] 
## [1,] NA    NA    NA    NA    NA    NA    NA    
## [2,] 40908 41274 41639 42004 42369 42735 43100 
## [3,] NA    NA    NA    NA    NA    NA    NA    
## [4,] 29    22    21.1  25.6  31.8  22.9  &amp;quot;30.7&amp;quot;
## [5,] 26.2  18.6  15.7  21.7  28.8  17.3  26.6  
## [6,] 46.8  47.1  48.2  50.1  49.2  34.5  34.4  
## 
## [[2]]
##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
## [1,] &amp;quot;FINANCIAL&amp;quot; 33603   33969 34334 34699 35064 35430 35795 36160 36525
## [2,] &amp;quot;FFINT&amp;quot;     &amp;quot;FFINN&amp;quot; NA    NA    NA    NA    NA    NA    NA    NA   
## [3,] &amp;quot;EU&amp;quot;        &amp;quot;:&amp;quot;     -5.1  -6.2  2.7   6.7   9     14.4  13.9  14   
## [4,] &amp;quot;EA&amp;quot;        &amp;quot;:&amp;quot;     -8.8  -13.5 -3.4  2.6   5.7   12.5  13.2  13.1 
## [5,] &amp;quot;BE&amp;quot;        &amp;quot;:&amp;quot;     &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   21.5  22.4 
## [6,] &amp;quot;BG&amp;quot;        &amp;quot;:&amp;quot;     &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;  
##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]
## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543
## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
## [3,] 16.4  9.4   7.4   8.1   12.4  8.4   13.6  23.4  4.1   -4    10.9 
## [4,] 16.5  8     6.8   5.1   9.9   4.8   8.4   24.3  -2.8  -10.5 9.3  
## [5,] 20.9  22.3  32.2  33.5  33.8  34.8  35    34.5  37.2  33.5  32.7 
## [6,] &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   20.8  24    27.1  28.3  33.4  37.5  37.7  26.6  30.4 
##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] 
## [1,] 40908 41274 41639 42004 42369 42735 43100 
## [2,] NA    NA    NA    NA    NA    NA    NA    
## [3,] 12.4  10.2  8.8   13.4  17.4  6.2   &amp;quot;12.3&amp;quot;
## [4,] 9     7.2   5     11    13.1  -1    6.5   
## [5,] 31.5  32.3  33    31.7  32.2  19.9  20.5  
## [6,] 33.8  35.6  36    41.5  41.6  44.2  43.8  
## 
## [[3]]
##      [,1]        [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]
## [1,] &amp;quot;TECHNICAL&amp;quot; 33603   33969 34334 34699 35064 35430 35795 36160 36525
## [2,] &amp;quot;FTECT&amp;quot;     &amp;quot;FTECN&amp;quot; NA    NA    NA    NA    NA    NA    NA    NA   
## [3,] &amp;quot;EU&amp;quot;        &amp;quot;:&amp;quot;     39.2  37.6  38.3  40    40.7  42.8  43.5  43.8 
## [4,] &amp;quot;EA&amp;quot;        &amp;quot;:&amp;quot;     39.7  36.2  37.5  41.2  40    44    44.8  44.9 
## [5,] &amp;quot;BE&amp;quot;        &amp;quot;:&amp;quot;     &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   58.8  58.5 
## [6,] &amp;quot;BG&amp;quot;        &amp;quot;:&amp;quot;     &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;  
##      [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21]
## [1,] 36891 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543
## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
## [3,] 37    31.1  27.2  30.9  30.4  30.3  27.4  40.5  25.8  23.1  27.4 
## [4,] 37    30.3  27.4  31    29.9  29.7  24.8  41    23.4  19.5  26.4 
## [5,] 58.3  58.4  57.7  59.2  59.6  59.4  60.2  59.5  60.5  57.9  56.3 
## [6,] &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   17.3  17.5  21.1  21.5  25.3  28.2  26.1  21    25.3 
##      [,22] [,23] [,24] [,25] [,26] [,27] [,28] 
## [1,] 40908 41274 41639 42004 42369 42735 43100 
## [2,] NA    NA    NA    NA    NA    NA    NA    
## [3,] 28.9  26.3  31.3  32.1  32.1  30.2  &amp;quot;34.6&amp;quot;
## [4,] 28.5  25.9  32.1  32.4  33.1  30.2  36    
## [5,] 56.7  57.7  57.9  58.6  59.1  13.1  13.1  
## [6,] 24.6  26.8  30.4  31.9  34.1  34.8  33.7  
## 
## [[4]]
##      [,1]    [,2]    [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11]
## [1,] &amp;quot;OTHER&amp;quot; 33603   33969 34334 34699 35064 35430 35795 36160 36525 36891
## [2,] &amp;quot;FOTHT&amp;quot; &amp;quot;FOTHN&amp;quot; NA    NA    NA    NA    NA    NA    NA    NA    NA   
## [3,] &amp;quot;EU&amp;quot;    &amp;quot;:&amp;quot;     2.9   -0.5  3.9   3.9   1     4.1   4.7   7     7.2  
## [4,] &amp;quot;EA&amp;quot;    &amp;quot;:&amp;quot;     2.3   -4.9  1.4   1.3   -2.4  1.1   3.2   5.8   7    
## [5,] &amp;quot;BE&amp;quot;    &amp;quot;:&amp;quot;     &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   14    14.9  15.9 
## [6,] &amp;quot;BG&amp;quot;    &amp;quot;:&amp;quot;     &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;   &amp;quot;:&amp;quot;  
##      [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22]
## [1,] 37256 37621 37986 38352 38717 39082 39447 39813 40178 40543 40908
## [2,] NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   
## [3,] -1.5  6.2   8.1   7.6   1.4   2.4   13.7  -1.9  -3.2  1.1   1.1  
## [4,] -3.7  5.5   7.1   7.2   -2.2  0.4   15.5  -4.6  -8.4  0.3   -3.3 
## [5,] 16.3  22.8  23.1  22.4  24.5  25.3  25.5  26.6  26.6  24.7  24.6 
## [6,] &amp;quot;:&amp;quot;   -2.3  -0.8  2.4   2.9   3.5   4.8   5.5   2.2   3.3   3.2  
##      [,23] [,24] [,25] [,26] [,27] [,28]
## [1,] 41274 41639 42004 42369 42735 43100
## [2,] NA    NA    NA    NA    NA    NA   
## [3,] -1.6  0.9   2.7   1.9   -3.3  &amp;quot;2.1&amp;quot;
## [4,] -2.3  0.6   2.5   2.1   -5.4  1.7  
## [5,] 26.4  25.9  25    25.3  4.7   5.2  
## [6,] 5.9   7     8.2   9.6   9.4   9.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are getting really close to something useful! Now we can get the first table and do some basic
cleaning to have a tidy dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset1 &amp;lt;- list_of_data[[1]]

dataset1 &amp;lt;- dataset1[-c(1:3), ]
dataset1[dataset1 == &amp;quot;:&amp;quot;] &amp;lt;- NA
colnames(dataset1) &amp;lt;- c(&amp;quot;country&amp;quot;, seq(from = 1991, to = 2017))

head(dataset1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      country 1991 1992 1993  1994 1995 1996 1997 1998 1999 2000 2001 2002
## [1,] &amp;quot;EU&amp;quot;    NA   16.9 -1.4  20.2 34.5 31.4 37.5 39   37.3 39.2 27.5 20.6
## [2,] &amp;quot;EA&amp;quot;    NA   15.5 -13.1 14.8 30.9 25.1 35.2 39.2 37.1 39.5 25.3 18.2
## [3,] &amp;quot;BE&amp;quot;    NA   NA   NA    NA   NA   NA   NA   42.3 43.1 45.8 42.2 42.9
## [4,] &amp;quot;BG&amp;quot;    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   39.6
## [5,] &amp;quot;CZ&amp;quot;    NA   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   54.9
## [6,] &amp;quot;DK&amp;quot;    49.5 45   50    59.5 62.5 55.5 60.5 57.5 56   61.5 57.5 59.5
##      2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016
## [1,] 21.4 29.8 26.4 32.5 47.1 19   -1.3 23.5 29   22   21.1 25.6 31.8 22.9
## [2,] 18.9 27.4 23   28.2 46.1 12.3 -9.3 19.3 26.2 18.6 15.7 21.7 28.8 17.3
## [3,] 43.8 45.8 47.4 49.1 50.9 48.2 46.9 46.3 46.8 47.1 48.2 50.1 49.2 34.5
## [4,] 43   42.8 45.5 49.1 52.6 50.7 39.5 45.5 47.4 45.6 50.5 51.4 49.9 53.2
## [5,] 37   48.5 67.9 66.4 66.8 69.3 64.7 61   56   47.5 53   53.5 67.5 58  
## [6,] 53.5 50   59   64   63   56   33.5 57   47   48   52   45.5 40.5 36.5
##      2017  
## [1,] &amp;quot;30.7&amp;quot;
## [2,] 26.6  
## [3,] 34.4  
## [4,] 52.8  
## [5,] 59.5  
## [6,] 37.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Et voilà! We went from a messy spreadsheet to a tidy dataset in a matter of minutes. Even though
this package is still in early development and not all the features that are planned are available,
the basics are there and can save you a lot of pain!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Functional programming and unit testing for data munging with R available on Leanpub</title>
      <link>/blog/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub/</link>
      <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-12-24-functional-programming-and-unit-testing-for-data-munging-with-r-available-on-leanpub/</guid>
      <description>&lt;p&gt;The book I&amp;rsquo;ve been working on these pasts months (you can read about it &lt;a href=&#34;http://www.brodrigues.co/2016/11/04/ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r&#34;&gt;here&lt;/a&gt;, and read it for free &lt;a href=&#34;http://www.brodrigues.co/fput&#34;&gt;here&lt;/a&gt;) is now available on Leanpub! You can grab a copy and read it on your ebook reader or on your computer, and what&amp;rsquo;s even better is that it is available for free (but you can also decide to buy it if you really like it). Here is the link on &lt;a href=&#34;https://leanpub.com/fput&#34;&gt;Leanpub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the book, I show you the basics of functional programming, unit testing and package development for the R programming language. The end goal is to make your data tidy in a reproducible way!&lt;/p&gt;

&lt;p&gt;Just a heads up: as the book is right now, the formatting is not perfect and images are missing. This is because I use &lt;code&gt;bookdown&lt;/code&gt; to write the book and convert it to Leanpub&amp;rsquo;s markdown flavour is not trivial. I will find a solution to automate the conversion from &lt;code&gt;bookdown&lt;/code&gt;&amp;rsquo;s version to Leanpub&amp;rsquo;s markdown and try to keep both versions in sync. Of course, once the book will be finished, the version on Leanpub and on my &lt;a href=&#34;http://www.brodrigues.co/fput&#34;&gt;website&lt;/a&gt; are going to be completely identical. If you want to read it on your computer offline, you can also download a pdf from the book&amp;rsquo;s website, by clicking on the pdf icon in the top left corner.  Do not hesitate to send me an email if you want to give me feedback (just click on the red envelope in the top right corner) or tweet me &lt;a href=&#34;https://twitter.com/brodriguesco&#34;&gt;@brodriguesco&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My free book has a cover!</title>
      <link>/blog/2017-01-07-my-free-book-has-a-cover/</link>
      <pubDate>Sat, 24 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-01-07-my-free-book-has-a-cover/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m currently writing a book as a hobby. It&amp;rsquo;s titled &lt;em&gt;Functional programming and unit testing for data munging with R&lt;/em&gt; and you can get it for free &lt;a href=&#34;https://leanpub.com/fput/&#34;&gt;here&lt;/a&gt;. You can also read it online for free on my &lt;a href=&#34;http://www.brodrigues.co/fput&#34;&gt;webpage&lt;/a&gt; What&amp;rsquo;s the book about?&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the teaser text:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Learn the basics of functional programming, unit testing and package development for the R programming language in order to make your data tidy!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The book now has a beautiful cover thanks to &lt;a href=&#34;https://twitter.com/putosaure&#34;&gt;@putosaure&lt;/a&gt;. Putosaure is a Paris based graphic designer who also reviews video games. He is also a very good friend of mine and I am very happy he made this beautiful cover for my book:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/cover.png&#34; width=&#34;612&#34; height=&#34;792&#34;/&gt;&lt;/p&gt;

&lt;p&gt;In it, we see a guy holding a shield with the Greek letter lambda, which also happens to be the letter to designate functional programming. I&amp;rsquo;ve added the title with the &lt;a href=&#34;http://www.dafont.com/komika-title.font&#34;&gt;Komika Title&lt;/a&gt; font.&lt;/p&gt;

&lt;p&gt;Consider this cover in beta, it&amp;rsquo;ll probably evolve some more. But I couldn&amp;rsquo;t wait to use it!&lt;/p&gt;

&lt;p&gt;I love it. Hope you&amp;rsquo;ll love it too!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Work on lists of datasets instead of individual datasets by using functional programming</title>
      <link>/blog/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming/</link>
      <pubDate>Wed, 21 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-12-21-work-on-lists-of-datasets-instead-of-individual-datasets-by-using-functional-programming/</guid>
      <description>&lt;p&gt;&lt;p&gt;Analyzing a lot of datasets can be tedious. In my work, I often have to compute descriptive statistics, or plot some graphs for some variables for a lot of datasets. The variables in question have the same name accross the datasets but are measured for different years. As an example, imagine you have this situation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data2000 &amp;lt;- mtcars
data2001 &amp;lt;- mtcars&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of argument, imagine that &lt;code&gt;data2000&lt;/code&gt; is data from a survey conducted in the year 2000 and &lt;code&gt;data2001&lt;/code&gt; is the same survey but conducted in the year 2001. For illustration purposes, I use the &lt;code&gt;mtcars&lt;/code&gt; dataset, but I could have used any other example. In these sort of situations, the variables are named the same in both datasets. Now if I want to check the summary statistics of a variable, I might do it by running:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(data2000$cyl)
summary(data2001$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;but this can get quite tedious, especially if instead of only having two years of data, you have 20 years. Another possibility is to merge both datasets and then check the summary statistics of the variable of interest. But this might require a lot of preprocessing, and sometimes you really just want to do a quick check, or some dirty graphs. So you might be tempted to write a loop, which would require to put these two datasets in some kind of structure, such as a list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_data &amp;lt;- list(&amp;quot;data2000&amp;quot; = data2000, &amp;quot;data2001&amp;quot; = data2001)&lt;/p&gt;

&lt;p&gt;for (i in 1:2){
    print(summary(list_data[[i]]$cyl))
 }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  4.000   4.000   6.000   6.188   8.000   8.000
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  4.000   4.000   6.000   6.188   8.000   8.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this also might get tedious, especially if you want to do this for a lot of different variables, and want to use different functions than &lt;code&gt;summary()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Another, simpler way of doing this, is to use &lt;code&gt;purrr::map()&lt;/code&gt; or &lt;code&gt;lapply()&lt;/code&gt;. But there is a catch though: how do we specify the column we want to work on? Let’s try some things out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)&lt;/p&gt;

&lt;p&gt;map(list_data, summary(cyl))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Error in summary(cyl) : object &amp;lsquo;cyl&amp;rsquo; not found&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Maybe this will work:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(list_data, summary, cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $data2000
     mpg             cyl             disp             hp&lt;br /&gt;
Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0&lt;br /&gt;
1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5&lt;br /&gt;
Median :19.20   Median :6.000   Median :196.3   Median :123.0&lt;br /&gt;
Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7&lt;br /&gt;
3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0&lt;br /&gt;
Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0&lt;br /&gt;
     drat             wt             qsec             vs&lt;br /&gt;
Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000&lt;br /&gt;
1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000&lt;br /&gt;
Median :3.695   Median :3.325   Median :17.71   Median :0.0000&lt;br /&gt;
Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375&lt;br /&gt;
3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000&lt;br /&gt;
Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000&lt;br /&gt;
      am              gear            carb&lt;br /&gt;
Min.   :0.0000   Min.   :3.000   Min.   :1.000&lt;br /&gt;
1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000&lt;br /&gt;
Median :0.0000   Median :4.000   Median :2.000&lt;br /&gt;
Mean   :0.4062   Mean   :3.688   Mean   :2.812&lt;br /&gt;
3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000&lt;br /&gt;
Max.   :1.0000   Max.   :5.000   Max.   :8.000&lt;/p&gt;

&lt;p&gt;data2001
     mpg             cyl             disp             hp&lt;br /&gt;
Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0&lt;br /&gt;
1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5&lt;br /&gt;
Median :19.20   Median :6.000   Median :196.3   Median :123.0&lt;br /&gt;
Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7&lt;br /&gt;
3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0&lt;br /&gt;
Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0&lt;br /&gt;
     drat             wt             qsec             vs&lt;br /&gt;
Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000&lt;br /&gt;
1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000&lt;br /&gt;
Median :3.695   Median :3.325   Median :17.71   Median :0.0000&lt;br /&gt;
Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375&lt;br /&gt;
3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000&lt;br /&gt;
Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000&lt;br /&gt;
      am              gear            carb&lt;br /&gt;
Min.   :0.0000   Min.   :3.000   Min.   :1.000&lt;br /&gt;
1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000&lt;br /&gt;
Median :0.0000   Median :4.000   Median :2.000&lt;br /&gt;
Mean   :0.4062   Mean   :3.688   Mean   :2.812&lt;br /&gt;
3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000&lt;br /&gt;
Max.   :1.0000   Max.   :5.000   Max.   :8.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not quite! You get the summary statistics of every variable, &lt;code&gt;cyl&lt;/code&gt; simply gets ignored. This might be ok in our small toy example, but if you have dozens of datasets with hundreds of variables, the output becomes unreadable. The solution is to use an anonymous functions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(list_data, (function(x) summary(x$cyl)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $data2000
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  4.000   4.000   6.000   6.188   8.000   8.000&lt;/p&gt;

&lt;p&gt;$data2001
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  4.000   4.000   6.000   6.188   8.000   8.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is, in my opinion, much more readable than a loop, and the output of this is another list, so it’s easy to save it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary_cyl &amp;lt;- map(list_data, (function(x) summary(x$cyl)))
str(summary_cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 2
$ data2000:Classes &amp;lsquo;summaryDefault&amp;rsquo;, &amp;lsquo;table&amp;rsquo;  Named num [1:6] 4 4 6 6.19 8 &amp;hellip;
 .. ..- attr(&lt;em&gt;, &amp;quot;names&amp;quot;)= chr [1:6] &amp;quot;Min.&amp;quot; &amp;quot;1st Qu.&amp;quot; &amp;quot;Median&amp;quot; &amp;quot;Mean&amp;quot; &amp;hellip;
$ data2001:Classes &amp;lsquo;summaryDefault&amp;rsquo;, &amp;lsquo;table&amp;rsquo;  Named num [1:6] 4 4 6 6.19 8 &amp;hellip;
 .. ..- attr(&lt;/em&gt;, &amp;quot;names&amp;quot;)= chr [1:6] &amp;quot;Min.&amp;quot; &amp;quot;1st Qu.&amp;quot; &amp;quot;Median&amp;quot; &amp;quot;Mean&amp;quot; &amp;hellip;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the loop, you would need to “allocate” an empty list that you would fill at each iteration.&lt;/p&gt;
&lt;p&gt;So this is already nice, but wouldn’t it be nicer to simply have to type:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(list_data$cyl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and have the summary of variable &lt;code&gt;cyl&lt;/code&gt; for each dataset in the list? Well it is possible with the following function I wrote to make my life easier:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;to_map &amp;lt;- function(func){
  function(list, column, &amp;hellip;){
    if(missing(column)){
        res &amp;lt;- purrr::map(list, (function(x) func(x, &amp;hellip;)))
      } else {
        res &amp;lt;- purrr::map(list, (function(x) func(x[column], &amp;hellip;)))
             }
    res
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By following &lt;a href=&#34;http://adv-r.had.co.nz/Function-operators.html&#34;&gt;this&lt;/a&gt; chapter of Hadley Wickham’s book, &lt;i&gt;Advanced R&lt;/i&gt;, I was able to write this function. What does it do? It basically &lt;i&gt;generalizes&lt;/i&gt; a function to work on a list of datasets instead of just on a dataset. So for example, in the case of &lt;code&gt;summary()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summarymap &amp;lt;- to_map(summary)&lt;/p&gt;

&lt;p&gt;summarymap(list_data, &amp;quot;cyl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$data2000
     cyl&lt;br /&gt;
Min.   :4.000&lt;br /&gt;
1st Qu.:4.000&lt;br /&gt;
Median :6.000&lt;br /&gt;
Mean   :6.188&lt;br /&gt;
3rd Qu.:8.000&lt;br /&gt;
Max.   :8.000&lt;/p&gt;

&lt;p&gt;$data2001
     cyl&lt;br /&gt;
Min.   :4.000&lt;br /&gt;
1st Qu.:4.000&lt;br /&gt;
Median :6.000&lt;br /&gt;
Mean   :6.188&lt;br /&gt;
3rd Qu.:8.000&lt;br /&gt;
Max.   :8.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So now everytime I want to have summary statistics for a variable, I just need to use &lt;code&gt;summarymap()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summarymap(list_data, &amp;quot;mpg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $data2000
      mpg&lt;br /&gt;
 Min.   :10.40&lt;br /&gt;
 1st Qu.:15.43&lt;br /&gt;
 Median :19.20&lt;br /&gt;
 Mean   :20.09&lt;br /&gt;
 3rd Qu.:22.80&lt;br /&gt;
 Max.   :33.90&lt;/p&gt;

&lt;p&gt;$data2001
      mpg&lt;br /&gt;
 Min.   :10.40&lt;br /&gt;
 1st Qu.:15.43&lt;br /&gt;
 Median :19.20&lt;br /&gt;
 Mean   :20.09&lt;br /&gt;
 3rd Qu.:22.80&lt;br /&gt;
 Max.   :33.90&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I want the summary statistics for every variable, I simply omit the column name:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summarymap(list_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;$data2000
      mpg             cyl             disp             hp&lt;br /&gt;
 Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0&lt;br /&gt;
 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5&lt;br /&gt;
 Median :19.20   Median :6.000   Median :196.3   Median :123.0&lt;br /&gt;
 Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7&lt;br /&gt;
 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0&lt;br /&gt;
 Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0&lt;br /&gt;
      drat             wt             qsec             vs&lt;br /&gt;
 Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000&lt;br /&gt;
 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000&lt;br /&gt;
 Median :3.695   Median :3.325   Median :17.71   Median :0.0000&lt;br /&gt;
 Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375&lt;br /&gt;
 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000&lt;br /&gt;
 Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000&lt;br /&gt;
       am              gear            carb&lt;br /&gt;
 Min.   :0.0000   Min.   :3.000   Min.   :1.000&lt;br /&gt;
 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000&lt;br /&gt;
 Median :0.0000   Median :4.000   Median :2.000&lt;br /&gt;
 Mean   :0.4062   Mean   :3.688   Mean   :2.812&lt;br /&gt;
 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000&lt;br /&gt;
 Max.   :1.0000   Max.   :5.000   Max.   :8.000&lt;/p&gt;

&lt;p&gt;$data2001
      mpg             cyl             disp             hp&lt;br /&gt;
 Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0&lt;br /&gt;
 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5&lt;br /&gt;
 Median :19.20   Median :6.000   Median :196.3   Median :123.0&lt;br /&gt;
 Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7&lt;br /&gt;
 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0&lt;br /&gt;
 Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0&lt;br /&gt;
      drat             wt             qsec             vs&lt;br /&gt;
 Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000&lt;br /&gt;
 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000&lt;br /&gt;
 Median :3.695   Median :3.325   Median :17.71   Median :0.0000&lt;br /&gt;
 Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375&lt;br /&gt;
 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000&lt;br /&gt;
 Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000&lt;br /&gt;
       am              gear            carb&lt;br /&gt;
 Min.   :0.0000   Min.   :3.000   Min.   :1.000&lt;br /&gt;
 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000&lt;br /&gt;
 Median :0.0000   Median :4.000   Median :2.000&lt;br /&gt;
 Mean   :0.4062   Mean   :3.688   Mean   :2.812&lt;br /&gt;
 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000&lt;br /&gt;
 Max.   :1.0000   Max.   :5.000   Max.   :8.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can use any function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tablemap &amp;lt;- to_map(table)&lt;/p&gt;

&lt;p&gt;tablemap(list_data, &amp;quot;cyl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $data2000&lt;/p&gt;

&lt;p&gt;4  6  8
11  7 14&lt;/p&gt;

&lt;p&gt;$data2001&lt;/p&gt;

&lt;p&gt;4  6  8
11  7 14&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tablemap(list_data, &amp;quot;mpg&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $data2000&lt;/p&gt;

&lt;p&gt;10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7
   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1
  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9
   2    2    1    2    1    1    1    2    1    1&lt;/p&gt;

&lt;p&gt;$data2001&lt;/p&gt;

&lt;p&gt;10.4 13.3 14.3 14.7   15 15.2 15.5 15.8 16.4 17.3 17.8 18.1 18.7 19.2 19.7
   2    1    1    1    1    2    1    1    1    1    1    1    1    2    1
  21 21.4 21.5 22.8 24.4   26 27.3 30.4 32.4 33.9
   2    2    1    2    1    1    1    2    1    1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope you will find this little function useful, and as usual, for any comments just drop me an email by clicking the red enveloppe in the top right corner or &lt;a href=&#34;https://twitter.com/brodriguesco&#34;&gt;tweet me&lt;/a&gt;.&lt;/p&gt;
&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I&#39;ve started writing a &#39;book&#39;: Functional programming and unit testing for data munging with R</title>
      <link>/blog/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r/</link>
      <pubDate>Fri, 04 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-11-04-ive-started-writing-a-book-functional-programming-and-unit-testing-for-data-munging-with-r/</guid>
      <description>&lt;p&gt;I have started writing a &amp;lsquo;book&amp;rsquo; using the awesome &lt;code&gt;bookdown&lt;/code&gt; package. In the book I explain and show why using functional programming
and putting your functions in your own packages is the way to go when you want to clean, prepare and transform large data sets.
It makes testing and documenting your code easier. You don&amp;rsquo;t need to think about managing paths either. The book is far from complete,
but I plan on working on it steadily. For now, you can read an intro to functional programming, unit testing and creating your own packages
that will hold your code. I also show you can write documentation for your functions. I am also looking for feedback; so if you have any
suggestions, do not hesitate to shoot me an email or a tweet! You can read the book by clicking &lt;a href=&#34;http://www.brodrigues.co/fput/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Merge a list of datasets together</title>
      <link>/blog/2016-07-30-merge-a-list-of-datasets-together/</link>
      <pubDate>Sat, 30 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-07-30-merge-a-list-of-datasets-together/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.brodrigues.co/2016/07/26/read-a-lot-of-datasets-at-once-with-r&#34;&gt;Last week&lt;/a&gt; I showed how to read a lot of datasets at once with R, and this week I’ll continue from there and show a very simple function that uses this list of read datasets and merges them all together.&lt;/p&gt;
&lt;p&gt;First we’ll use &lt;code&gt;read_list()&lt;/code&gt; to read all the datasets at once (for more details read &lt;a href=&#34;http://www.brodrigues.co/2016/07/26/read-a-lot-of-datasets-at-once-with-r&#34;&gt;last week’s post&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;readr&amp;quot;)
library(&amp;quot;tibble&amp;quot;)

data_files &amp;lt;- list.files(pattern = &amp;quot;.csv&amp;quot;)

print(data_files)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data_1.csv&amp;quot; &amp;quot;data_2.csv&amp;quot; &amp;quot;data_3.csv&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_of_data_sets &amp;lt;- read_list(data_files, read_csv)

glimpse(list_of_data_sets)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 3
##  $ data_1:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:  19 obs. of  3 variables:
##   ..$ col1: chr [1:19] &amp;quot;0,018930679&amp;quot; &amp;quot;0,8748013128&amp;quot; &amp;quot;0,1025635934&amp;quot; &amp;quot;0,6246140983&amp;quot; ...
##   ..$ col2: chr [1:19] &amp;quot;0,0377725807&amp;quot; &amp;quot;0,5959457638&amp;quot; &amp;quot;0,4429121533&amp;quot; &amp;quot;0,558387159&amp;quot; ...
##   ..$ col3: chr [1:19] &amp;quot;0,6241767189&amp;quot; &amp;quot;0,031324594&amp;quot; &amp;quot;0,2238059868&amp;quot; &amp;quot;0,2773350732&amp;quot; ...
##  $ data_2:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:  19 obs. of  3 variables:
##   ..$ col1: chr [1:19] &amp;quot;0,9098418493&amp;quot; &amp;quot;0,1127788509&amp;quot; &amp;quot;0,5818891392&amp;quot; &amp;quot;0,1011773532&amp;quot; ...
##   ..$ col2: chr [1:19] &amp;quot;0,7455905887&amp;quot; &amp;quot;0,4015039612&amp;quot; &amp;quot;0,6625796605&amp;quot; &amp;quot;0,029955339&amp;quot; ...
##   ..$ col3: chr [1:19] &amp;quot;0,327232932&amp;quot; &amp;quot;0,2784035673&amp;quot; &amp;quot;0,8092386735&amp;quot; &amp;quot;0,1216045306&amp;quot; ...
##  $ data_3:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:  19 obs. of  3 variables:
##   ..$ col1: chr [1:19] &amp;quot;0,9236124896&amp;quot; &amp;quot;0,6303271761&amp;quot; &amp;quot;0,6413583054&amp;quot; &amp;quot;0,5573887416&amp;quot; ...
##   ..$ col2: chr [1:19] &amp;quot;0,2114708388&amp;quot; &amp;quot;0,6984538266&amp;quot; &amp;quot;0,0469865249&amp;quot; &amp;quot;0,9271510226&amp;quot; ...
##   ..$ col3: chr [1:19] &amp;quot;0,4941919971&amp;quot; &amp;quot;0,7391538511&amp;quot; &amp;quot;0,3876723797&amp;quot; &amp;quot;0,2815014394&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You see that all these datasets have the same column names. We can now merge them using this simple function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;multi_join &amp;lt;- function(list_of_loaded_data, join_func, ...){

    require(&amp;quot;dplyr&amp;quot;)

    output &amp;lt;- Reduce(function(x, y) {join_func(x, y, ...)}, list_of_loaded_data)

    return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function uses &lt;code&gt;Reduce()&lt;/code&gt;. &lt;code&gt;Reduce()&lt;/code&gt; is a very important function that can be found in all functional programming languages. What does &lt;code&gt;Reduce()&lt;/code&gt; do? Let’s take a look at the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Reduce(`+`, c(1, 2, 3, 4, 5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Reduce()&lt;/code&gt; has several arguments, but you need to specify at least two: a function, here &lt;code&gt;+&lt;/code&gt; and a list, here &lt;code&gt;c(1, 2, 3, 4, 5)&lt;/code&gt;. The next code block shows what &lt;code&gt;Reduce()&lt;/code&gt; basically does:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0 + c(1, 2, 3, 4, 5)
0 + 1 + c(2, 3, 4, 5)
0 + 1 + 2 + c(3, 4, 5)
0 + 1 + 2 + 3 + c(4, 5)
0 + 1 + 2 + 3 + 4 + c(5)
0 + 1 + 2 + 3 + 4 + 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;0&lt;/code&gt; had to be added as in “init”. You can also specify this “init” to &lt;code&gt;Reduce()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Reduce(`+`, c(1, 2, 3, 4, 5), init = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 35&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So what &lt;code&gt;multi_join()&lt;/code&gt; does, is the same operation as in the example above, but where the function is a user supplied join or merge function, and the list of datasets is the one read with &lt;code&gt;read_list()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s see what happens when we use &lt;code&gt;multi_join()&lt;/code&gt; on our list:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;merged_data &amp;lt;- multi_join(list_of_data_sets, full_join)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(merged_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;tbl_df&amp;quot;     &amp;quot;tbl&amp;quot;        &amp;quot;data.frame&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(merged_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 57
## Variables: 3
## $ col1 &amp;lt;chr&amp;gt; &amp;quot;0,018930679&amp;quot;, &amp;quot;0,8748013128&amp;quot;, &amp;quot;0,1025635934&amp;quot;, &amp;quot;0,6246140...
## $ col2 &amp;lt;chr&amp;gt; &amp;quot;0,0377725807&amp;quot;, &amp;quot;0,5959457638&amp;quot;, &amp;quot;0,4429121533&amp;quot;, &amp;quot;0,558387...
## $ col3 &amp;lt;chr&amp;gt; &amp;quot;0,6241767189&amp;quot;, &amp;quot;0,031324594&amp;quot;, &amp;quot;0,2238059868&amp;quot;, &amp;quot;0,2773350...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should make sure that all the data frames have the same column names but you can also join data frames with different column names if you give the argument &lt;code&gt;by&lt;/code&gt; to the join function. This is possible thanks to &lt;code&gt;...&lt;/code&gt; that allows you to pass further argument to &lt;code&gt;join_func()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This function was inspired by the one found on the blog &lt;a href=&#34;http://novicemetrics.blogspot.lu/2011/04/merging-multiple-data-files-into-one.html&#34;&gt;Coffee and Econometrics in the Morning&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Read a lot of datasets at once with R</title>
      <link>/blog/2016-07-26-read-a-lot-of-datasets-at-once-with-r/</link>
      <pubDate>Tue, 26 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-07-26-read-a-lot-of-datasets-at-once-with-r/</guid>
      <description>&lt;p&gt;I often have to read a lot of datasets at once using R. So I’ve wrote the following function to solve this issue:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_list &amp;lt;- function(list_of_datasets, read_func){

        read_and_assign &amp;lt;- function(dataset, read_func){
                dataset_name &amp;lt;- as.name(dataset)
                dataset_name &amp;lt;- read_func(dataset)
        }

        # invisible is used to suppress the unneeded output
        output &amp;lt;- invisible(
                sapply(list_of_datasets,
                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))

        # Remove the extension at the end of the data set names
        names_of_datasets &amp;lt;- c(unlist(strsplit(list_of_datasets, &amp;quot;[.]&amp;quot;))[c(T, F)])
        names(output) &amp;lt;- names_of_datasets
        return(output)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You need to supply a list of datasets as well as the function to read the datasets to &lt;code&gt;read_list&lt;/code&gt;. So for example to read in &lt;code&gt;.csv&lt;/code&gt; files, you could use &lt;code&gt;read.csv()&lt;/code&gt; (or &lt;code&gt;read_csv()&lt;/code&gt; from the &lt;code&gt;readr&lt;/code&gt; package, which I prefer to use), or &lt;code&gt;read_dta()&lt;/code&gt; from the package &lt;code&gt;haven&lt;/code&gt; for STATA files, and so on.&lt;/p&gt;
&lt;p&gt;Now imagine you have some data in your working directory. First start by saving the name of the datasets in a variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_files &amp;lt;- list.files(pattern = &amp;quot;.csv&amp;quot;)

print(data_files)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data_1.csv&amp;quot; &amp;quot;data_2.csv&amp;quot; &amp;quot;data_3.csv&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you can read all the data sets and save them in a list with &lt;code&gt;read_list()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;readr&amp;quot;)
library(&amp;quot;tibble&amp;quot;)

list_of_data_sets &amp;lt;- read_list(data_files, read_csv)


glimpse(list_of_data_sets)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 3
##  $ data_1:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:  19 obs. of  3 variables:
##   ..$ col1: chr [1:19] &amp;quot;0,018930679&amp;quot; &amp;quot;0,8748013128&amp;quot; &amp;quot;0,1025635934&amp;quot; &amp;quot;0,6246140983&amp;quot; ...
##   ..$ col2: chr [1:19] &amp;quot;0,0377725807&amp;quot; &amp;quot;0,5959457638&amp;quot; &amp;quot;0,4429121533&amp;quot; &amp;quot;0,558387159&amp;quot; ...
##   ..$ col3: chr [1:19] &amp;quot;0,6241767189&amp;quot; &amp;quot;0,031324594&amp;quot; &amp;quot;0,2238059868&amp;quot; &amp;quot;0,2773350732&amp;quot; ...
##  $ data_2:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:  19 obs. of  3 variables:
##   ..$ col1: chr [1:19] &amp;quot;0,9098418493&amp;quot; &amp;quot;0,1127788509&amp;quot; &amp;quot;0,5818891392&amp;quot; &amp;quot;0,1011773532&amp;quot; ...
##   ..$ col2: chr [1:19] &amp;quot;0,7455905887&amp;quot; &amp;quot;0,4015039612&amp;quot; &amp;quot;0,6625796605&amp;quot; &amp;quot;0,029955339&amp;quot; ...
##   ..$ col3: chr [1:19] &amp;quot;0,327232932&amp;quot; &amp;quot;0,2784035673&amp;quot; &amp;quot;0,8092386735&amp;quot; &amp;quot;0,1216045306&amp;quot; ...
##  $ data_3:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:  19 obs. of  3 variables:
##   ..$ col1: chr [1:19] &amp;quot;0,9236124896&amp;quot; &amp;quot;0,6303271761&amp;quot; &amp;quot;0,6413583054&amp;quot; &amp;quot;0,5573887416&amp;quot; ...
##   ..$ col2: chr [1:19] &amp;quot;0,2114708388&amp;quot; &amp;quot;0,6984538266&amp;quot; &amp;quot;0,0469865249&amp;quot; &amp;quot;0,9271510226&amp;quot; ...
##   ..$ col3: chr [1:19] &amp;quot;0,4941919971&amp;quot; &amp;quot;0,7391538511&amp;quot; &amp;quot;0,3876723797&amp;quot; &amp;quot;0,2815014394&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you prefer not to have the datasets in a list, but rather import them into the global environment, you can change the above function like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_list &amp;lt;- function(list_of_datasets, read_func){

        read_and_assign &amp;lt;- function(dataset, read_func){
                assign(dataset, read_func(dataset), envir = .GlobalEnv)
        }

        # invisible is used to suppress the unneeded output
        output &amp;lt;- invisible(
                sapply(list_of_datasets,
                           read_and_assign, read_func = read_func, simplify = FALSE, USE.NAMES = TRUE))

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But I personnally don’t like this second option, but I put it here for completeness.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data frame columns as arguments to dplyr functions</title>
      <link>/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/</link>
      <pubDate>Mon, 18 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;Suppose that you would like to create a function which does a series of computations on a data frame. You would like to pass a column as this function’s argument. Something like:&lt;/p&gt;

&lt;p&gt;&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(cars)
convertToKmh &amp;lt;- function(dataset, col_name){
  dataset$col_name &amp;lt;- dataset$speed * 1.609344
  return(dataset)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This example is obviously not very interesting (you don’t need a function for this), but it will illustrate the point. You would like to append a column called &lt;code&gt;speed_in_kmh&lt;/code&gt; with the speed in kilometers per hour to this dataset, but this is what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(convertToKmh(cars, &amp;quot;speed_in_kmh&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist  col_name
1     4    2  6.437376
2     4   10  6.437376
3     7    4 11.265408
4     7   22 11.265408
5     8   16 12.874752
6     9   10 14.484096&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your column is not called &lt;code&gt;speed_in_kmh&lt;/code&gt; but &lt;code&gt;col_name&lt;/code&gt;! It turns out that there is a very simple solution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;convertToKmh &amp;lt;- function(dataset, col_name){
  dataset[col_name] &amp;lt;- dataset$speed * 1.609344
  return(dataset)
}&lt;/p&gt;

&lt;p&gt;head(convertToKmh(cars, &amp;quot;speed_in_kmh&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   speed dist speed_in_kmh
1     4    2     6.437376
2     4   10     6.437376
3     7    4    11.265408
4     7   22    11.265408
5     8   16    12.874752
6     9   10    14.484096&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can access columns with &lt;code&gt;[]&lt;/code&gt; instead of &lt;code&gt;$&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But sometimes you want to do more complex things and for example have a function that groups by a variable and then computes new variables, filters by another and so on. You would like to avoid having to hard code these variables in your function, because then why write a function and of course you would like to use &lt;code&gt;dplyr&lt;/code&gt; to do it.&lt;/p&gt;
&lt;p&gt;I often use &lt;code&gt;dplyr&lt;/code&gt; functions in my functions. For illustration purposes, consider this very simple function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name){
  require(&amp;quot;dplyr&amp;quot;)
  dataset %&amp;gt;%
    group_by(col_name) %&amp;gt;%
    summarise(mean_speed = mean(speed)) -&amp;gt; dataset
  return(dataset)
}&lt;/p&gt;

&lt;p&gt;simpleFunction(cars, &amp;quot;dist&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function takes a dataset as an argument, as well as a column name. However, this does not work. You get this error:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error: unknown variable to group by : col_name &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variable &lt;code&gt;col_name&lt;/code&gt; is passed to &lt;code&gt;simpleFunction()&lt;/code&gt; as a string, but &lt;code&gt;group_by()&lt;/code&gt; requires a variable name. So why not try to convert &lt;code&gt;col_name&lt;/code&gt; to a name?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name){
  require(&amp;quot;dplyr&amp;quot;)
  col_name &amp;lt;- as.name(col_name)
  dataset %&amp;gt;%
    group_by(col_name) %&amp;gt;%
    summarise(mean_speed = mean(speed)) -&amp;gt; dataset
  return(dataset)
}&lt;/p&gt;

&lt;p&gt;simpleFunction(cars, &amp;quot;dist&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You get the same error as before:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error: unknown variable to group by : col_name &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So how can you pass a column name to &lt;code&gt;group_by()&lt;/code&gt;? Well, there is another version of &lt;code&gt;group_by()&lt;/code&gt; called &lt;code&gt;group_by_()&lt;/code&gt; that uses standard evaluation. You can learn more about it &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html&#34;&gt;here&lt;/a&gt;. Let’s take a look at what happens when we use &lt;code&gt;group_by_()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name){
  require(&amp;quot;dplyr&amp;quot;)
  dataset %&amp;gt;%
    group_by_(col_name) %&amp;gt;%
    summarise(mean_speed = mean(speed)) -&amp;gt; dataset
  return(dataset)
}&lt;/p&gt;

&lt;p&gt;simpleFunction(cars, &amp;quot;dist&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;A tibble: 35 x 2
 dist mean_speed
&amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
1      2        4.0
2      4        7.0
3     10        6.5
4     14       12.0
5     16        8.0
6     17       11.0
7     18       10.0
8     20       13.5
9     22        7.0
10    24       12.0
 &amp;hellip; with 25 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can even use a formula instead of a string:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction(cars, ~dist)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; A tibble: 35 x 2
    dist mean_speed
   &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
1      2        4.0
2      4        7.0
3     10        6.5
4     14       12.0
5     16        8.0
6     17       11.0
7     18       10.0
8     20       13.5
9     22        7.0
10    24       12.0
&amp;hellip; with 25 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if you want to pass column names and constants, for example to filter without hardcoding anything?&lt;/p&gt;
&lt;p&gt;Trying to do it naively will only yield pain and despair:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name, value){
  require(&amp;quot;dplyr&amp;quot;)
  dataset %&amp;gt;%
    filter_(col_name == value) %&amp;gt;%
    summarise(mean_speed = mean(speed)) -&amp;gt; dataset
  return(dataset)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; simpleFunction(cars, &amp;quot;dist&amp;quot;, 10)&lt;/p&gt;

&lt;p&gt;mean_speed
1        NaN&lt;/p&gt;

&lt;p&gt;&amp;gt; simpleFunction(cars, dist, 10)&lt;/p&gt;

&lt;p&gt;Error in col_name == value :
  comparison (1) is possible only for atomic and list types&lt;/p&gt;

&lt;p&gt;&amp;gt; simpleFunction(cars, ~dist, 10)&lt;/p&gt;

&lt;p&gt;mean_speed
1        NaN
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To solve this issue, we need to know a little bit about two concepts, &lt;em&gt;lazy evaluation&lt;/em&gt; and &lt;em&gt;non-standard evaluation&lt;/em&gt;. I recommend you read &lt;a href=&#34;http://adv-r.had.co.nz/Computing-on-the-language.html&#34;&gt;the following document&lt;/a&gt; from Hadley Wickham’s book &lt;em&gt;Advanced R&lt;/em&gt; as well as the part on lazy evaluation &lt;a href=&#34;http://adv-r.had.co.nz/Functions.html#function-arguments&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A nice package called &lt;code&gt;lazyeval&lt;/code&gt; can help us out. We would like to make R understand that the column name is not &lt;code&gt;col_name&lt;/code&gt; but the string inside it &lt;code&gt;&amp;quot;dist&amp;quot;&lt;/code&gt;, and now we would like to use &lt;code&gt;filter()&lt;/code&gt; for &lt;code&gt;dist&lt;/code&gt; equal to &lt;code&gt;10&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;lazyeval&lt;/code&gt; package, you’ll find the function &lt;code&gt;interp()&lt;/code&gt;. &lt;code&gt;interp()&lt;/code&gt; allows you to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;build an expression up from a mixture of constants and variables.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Take a look at this example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lazyeval)
interp(~x+y, x = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ~2 + y&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What you get back is this nice formula that you can then use within functions. To see why this is useful, let’s look at the above example again, and make it work using &lt;code&gt;interp()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simpleFunction &amp;lt;- function(dataset, col_name, value){
  require(&amp;quot;dplyr&amp;quot;)
  require(&amp;quot;lazyeval&amp;quot;)
  filter_criteria &amp;lt;- interp(~y == x, .values=list(y = as.name(col_name), x = value))
  dataset %&amp;gt;%
    filter_(filter_criteria) %&amp;gt;%
    summarise(mean_speed = mean(speed)) -&amp;gt; dataset
  return(dataset)
}&lt;/p&gt;

&lt;p&gt;simpleFunction(cars, &amp;quot;dist&amp;quot;, 10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;  mean_speed
1        6.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now it works! For some reason, you have to pass the column name as a string though.&lt;/p&gt;
&lt;p&gt;Sources: apart from the documents above, the following stackoverflow threads helped me out quite a lot: &lt;a href=&#34;http://stackoverflow.com/questions/28973056/in-r-pass-column-name-as-argument-and-use-it-in-function-with-dplyrmutate-a&#34;&gt;In R: pass column name as argument and use it in function with dplyr::mutate() and lazyeval::interp()&lt;/a&gt; and &lt;a href=&#34;http://stackoverflow.com/questions/26492280/non-standard-evaluation-nse-in-dplyrs-filter-pulling-data-from-mysql&#34;&gt;Non-standard evaluation (NSE) in dplyr’s filter_ &amp;amp; pulling data from MySQL&lt;/a&gt;.&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Careful with tryCatch</title>
      <link>/blog/2016-06-21-careful-with-trycatch/</link>
      <pubDate>Thu, 31 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-06-21-careful-with-trycatch/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;code&gt;tryCatch&lt;/code&gt; is one of the functions that allows the users to handle errors in a simple way. With it, you can do things like: &lt;code&gt;if(error), then(do this)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Take the following example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(&amp;quot;a&amp;quot;)
Error in sqrt(&amp;quot;a&amp;quot;) : non-numeric argument to mathematical function&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now maybe you’d want something to happen when such an error happens. You can achieve that with &lt;code&gt;tryCatch&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tryCatch(sqrt(&amp;quot;a&amp;quot;), error=function(e) print(&amp;quot;You can&#39;t take the square root of a character, silly!&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;You can&#39;t take the square root of a character, silly!&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why am I interested in &lt;code&gt;tryCatch&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;I am currently working with dates, specifically birthdays of people in my data sets. For a given mother, the birthday of her child is given in three distinct columns: a column for the child’s birth year, birth month and birth day respectively. I’ve wanted to put everything in a single column and convert the birthday to unix time (I have a very good reason to do that, but I won’t bore you with the details).&lt;/p&gt;
&lt;p&gt;Let’s create some data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mother &amp;lt;- as.data.frame(list(month=12, day=1, year=1988))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In my data, there’s a lot more columns of course, such as the mother’s wage, education level, etc, but for illustration purposes, this is all that’s needed.&lt;/p&gt;
&lt;p&gt;Now, to create this birthday column:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mother$birth1 &amp;lt;- as.POSIXct(paste0(as.character(mother$year), 
                                   &amp;quot;-&amp;quot;, as.character(mother$month), 
                                   &amp;quot;-&amp;quot;, as.character(mother$day)), 
                            origin=&amp;quot;1970-01-01&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and to convert it to unix time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mother$birth1 &amp;lt;- as.numeric(as.POSIXct(paste0(as.character(mother$year), 
                                              &amp;quot;-&amp;quot;, as.character(mother$month), 
                                              &amp;quot;-&amp;quot;, as.character(mother$day)),
                                       origin=&amp;quot;1970-01-01&amp;quot;))

print(mother)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   month day year    birth1
## 1    12   1 1988 596934000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s see what happens in this other example here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mother2 &amp;lt;- as.data.frame(list(month=2, day=30, year=1988))

mother2$birth1 &amp;lt;- as.POSIXct(paste0(as.character(mother2$year), 
                                    &amp;quot;-&amp;quot;, as.character(mother2$month), 
                                    &amp;quot;-&amp;quot;, as.character(mother2$day)), 
                             origin=&amp;quot;1970-01-01&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what happens:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Error in as.POSIXlt.character(x, tz, ...) : 
  character string is not in a standard unambiguous format&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This error is to be expected; there is no 30th of February! It turns out that in some rare cases, weird dates like this exist in my data. Probably some encoding errors. Not a problem I thought, I could use &lt;code&gt;tryCatch&lt;/code&gt; and return &lt;code&gt;NA&lt;/code&gt; in the case of an error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mother2 &amp;lt;- as.data.frame(list(month=2, day=30, year=1988))

mother2$birth1 &amp;lt;- tryCatch(as.POSIXct(paste0(as.character(mother2$year), 
                                    &amp;quot;-&amp;quot;, as.character(mother2$month), 
                                    &amp;quot;-&amp;quot;, as.character(mother2$day)), 
                             origin=&amp;quot;1970-01-01&amp;quot;), error=function(e) NA)

print(mother2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   month day year birth1
## 1     2  30 1988     NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pretty great, right? Well, no. Take a look at what happens in this case:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mother &amp;lt;- as.data.frame(list(month=c(12, 2), day=c(1, 30), year=c(1988, 1987)))
print(mother)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   month day year
## 1    12   1 1988
## 2     2  30 1987&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’d expect to have a correct date for the first mother and an &lt;code&gt;NA&lt;/code&gt; for the second. However, this is what happens&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mother$birth1 &amp;lt;- tryCatch(as.POSIXct(paste0(as.character(mother$year), 
                                    &amp;quot;-&amp;quot;, as.character(mother$month), 
                                    &amp;quot;-&amp;quot;, as.character(mother$day)), 
                             origin=&amp;quot;1970-01-01&amp;quot;), error=function(e) NA)

print(mother)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   month day year birth1
## 1    12   1 1988     NA
## 2     2  30 1987     NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we now have an &lt;code&gt;NA&lt;/code&gt; for both mothers! That’s actually to be expected. Indeed, this little example illustrates it well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(c(4, 9, &amp;quot;haha&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Error in sqrt(c(4, 9, &amp;quot;haha&amp;quot;)) : 
  non-numeric argument to mathematical function&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But you’d like to have this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[1]  2  3 NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So you could make the same mistake as myself and use tryCatch:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tryCatch(sqrt(c(4, 9, &amp;quot;haha&amp;quot;)), error=function(e) NA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But you only get &lt;code&gt;NA&lt;/code&gt; in return. That’s actually completely normal, but it took me off-guard and I spent quite some time to figure out what was happening. Especially because I had written unit tests to test my function &lt;code&gt;create_birthdays()&lt;/code&gt; that was doing the above computations and all tests were passing! The problem was that in my tests, I only had a single individual, so for a wrong date, having &lt;code&gt;NA&lt;/code&gt; for this individual was expected behaviour. But in a panel, only some individuals have a weird date like the 30th of February, but because of those, the whole column was filled with &lt;code&gt;NA&lt;/code&gt;’s! What I’m doing now is trying to either remove these weird birthdays (there are mothers whose children were born on the 99-99-9999. Documentation is lacking, but this probably means &lt;code&gt;missing value&lt;/code&gt;), or tyring to figure out how to only get &lt;code&gt;NA&lt;/code&gt;’s for the “weird” dates. I guess that the answer lies with &lt;code&gt;dplyr&lt;/code&gt;’s &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;mutate()&lt;/code&gt; to compute this birthdays for each individual separately.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unit testing with R</title>
      <link>/blog/2016-03-31-unit-testing-with-r/</link>
      <pubDate>Thu, 31 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-03-31-unit-testing-with-r/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;body&gt;
&lt;p&gt;I&amp;#39;ve been introduced to unit testing while working with colleagues on quite a big project for which
we use Python.&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;At first I was a bit skeptical about the need of writing unit tests, but now I must admit that I 
am seduced by the idea and by the huge time savings it allows. Naturally, I was wondering if the 
same could be achieved with R, and was quite happy to find out that it also possible to write unit
tests in R using a package called &lt;code&gt;testthat&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Unit tests (Not to be confused with unit root tests for time series) are small functions that test
your code and help you make sure everything is alright. I&amp;#39;m going to show how the &lt;code&gt;testthat&lt;/code&gt; 
packages works with a very trivial example, that might not do justice to the idea of
unit testing. But you&amp;#39;ll hopefully see why writing unit tests is not a waste of your time,
especially if your project gets very complex (if you&amp;#39;re writing a package for example).&lt;/p&gt;

&lt;p&gt;First, you&amp;#39;ll need to download and install &lt;code&gt;testthat&lt;/code&gt;. Some dependencies will also be installed.&lt;/p&gt;

&lt;p&gt;Now, you&amp;#39;ll need a function to test. Let&amp;#39;s suppose you&amp;#39;ve written a function that returns the
nth Fibonacci number:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;Fibonacci &amp;lt;- function(n){
    a &amp;lt;- 0
    b &amp;lt;- 1
    for (i in 1:n){
        temp &amp;lt;- b
        b &amp;lt;- a
        a &amp;lt;- a + temp
    }
    return(a)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You then save this function in a file, let&amp;#39;s call it &lt;code&gt;fibo.R&lt;/code&gt;. What you&amp;#39;ll probably do once you&amp;#39;ve
written this function, is to try it out:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;Fibonacci(5)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;#39;ll see that the function returns the right result and continue programming. The idea behind
unit testing is write a bunch of functions that you can run after you make changes to your code,
just to check that everything is still running as it should.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s create a script called &lt;code&gt;test_fibo.R&lt;/code&gt; and write the following code in it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;test_that(&amp;quot;Test Fibo(15)&amp;quot;,{
  phi &amp;lt;- (1 + sqrt(5))/2
  psi &amp;lt;- (1 - sqrt(5))/2
  expect_equal(Fibonacci(15), (phi**15 - psi**15)/sqrt(5))
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code above uses Binet&amp;#39;s formula, a closed form formula that gives the nth Fibonacci number and compares it 
our implementation of the algorithm. If you didn&amp;#39;t know about Binet&amp;#39;s formula, you could simply compute some numbers
by hand and compare them to what your function returns, for example. The function &lt;code&gt;expect_equal&lt;/code&gt; is a function from the 
package &lt;code&gt;testthat&lt;/code&gt; and does exactly what it tells. We expect the result of our implementation to be equal to the result of
Binet&amp;#39;s Formula. The file &lt;code&gt;test_fibo.R&lt;/code&gt; can contain as many tests as you need. 
Also, the file that contains the tests must start with the string &lt;code&gt;test&lt;/code&gt;, so that &lt;code&gt;testthat&lt;/code&gt; knows with files it has to run.&lt;/p&gt;

&lt;p&gt;Now, we&amp;#39;re almost done, create yet another script, let&amp;#39;s call it &lt;code&gt;run_tests.R&lt;/code&gt; and write the following code in it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;library(testthat) 

source(&amp;quot;path/to/fibo.R&amp;quot;)

test_results &amp;lt;- test_dir(&amp;quot;path/to/tests&amp;quot;, reporter=&amp;quot;summary&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running these lines, and if everything goes well, you should see a message like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; library(testthat)
&amp;gt; source(&amp;quot;path/to/fibo.R&amp;quot;)
&amp;gt; test_results &amp;lt;- test_dir(&amp;quot;path/to/tests&amp;quot;, reporter=&amp;quot;summary&amp;quot;)

.
Your tests are dandy! 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the small &lt;code&gt;.&lt;/code&gt; over the message? This means that one test was run successfully. You&amp;#39;ll get one dot per successful
test. If you take a look at &lt;code&gt;test_results&lt;/code&gt; you&amp;#39;ll see this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; test_results
         file context          test nb failed skipped error  user system  real
1 test_fibo.R         Test Fibo(15)  1      0   FALSE FALSE 0.004      0 0.006
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;#39;ll see each file and each function inside the files that were tested, and also whether the test was skipped, failed
etc. This may seem overkill for such a simple function, but imagine that you write dozens of functions that get more
and more complex over time. You might have to change a lot of lines because as time goes by you add new functionality,
but don&amp;#39;t want to break what was working. Running your unit tests each time you make changes can help you pinpoint 
regressions in your code. Unit tests can also help you start with your code. It can happen that sometimes you don&amp;#39;t
know exactly how to start; well you could start by writing a unit test that returns the result you want to have and 
then try to write the code to make that unit test pass. This is called test-driven development.&lt;/p&gt;

&lt;p&gt;I hope that this post motivated you to write unit tests and make you a better R programmer!&lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bootstrapping standard errors for difference-in-differences estimation with R</title>
      <link>/blog/2015-11-11-bootstrapping-did-with-r/</link>
      <pubDate>Wed, 11 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015-11-11-bootstrapping-did-with-r/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;body&gt;
&lt;p&gt;I’m currently working on a paper (with my colleague Vincent Vergnat who is also a Phd candidate at BETA) where I want to estimate the causal impact of the birth of a child on hourly and daily wages as well as yearly worked hours. For this we are using non-parametric difference-in-differences (henceforth DiD) and thus have to bootstrap the standard errors. In this post, I show how this is possible using the function &lt;code&gt;boot&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For this we are going to replicate the example from Wooldridge’s &lt;em&gt;Econometric Analysis of Cross Section and Panel Data&lt;/em&gt; and more specifically the example on page 415. You can download the data for R &lt;a href=&#34;/assets/files/kielmc.RData&#34;&gt;here&lt;/a&gt;. The question we are going to try to answer is &lt;em&gt;how much does the price of housing decrease due to the presence of an incinerator in the neighborhood?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;First put the data in a folder and set the correct working directory and load the &lt;code&gt;boot&lt;/code&gt; library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(boot)
setwd(&amp;quot;/home/path/to/data/kiel data/&amp;quot;)
load(&amp;quot;kielmc.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you need to write a function that takes the data as an argument, as well as an indices argument. This argument is used by the &lt;code&gt;boot&lt;/code&gt; function to select samples. This function should return the statistic you’re interested in, in our case, the DiD estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;run_DiD &amp;lt;- function(my_data, indices){
    d &amp;lt;- my_data[indices,]
    return(
        mean(d$rprice[d$year==1981 &amp;amp; d$nearinc==1]) -
        mean(d$rprice[d$year==1981 &amp;amp; d$nearinc==0]) -
        (mean(d$rprice[d$year==1978 &amp;amp; d$nearinc==1]) -
        mean(d$rprice[d$year==1978 &amp;amp; d$nearinc==0]))
    )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’re almost done! To bootstrap your DiD estimate you just need to use the boot function. If you have cpu with multiple cores (which you should, single core machines are quite outdated by now) you can even parallelize the bootstrapping.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boot_est &amp;lt;- boot(data, run_DiD, R=1000, parallel=&amp;quot;multicore&amp;quot;, ncpus = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now you should just take a look at your estimates:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boot_est&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
ORDINARY NONPARAMETRIC BOOTSTRAP&lt;/p&gt;

&lt;p&gt;Call:
boot(data = data, statistic = run_DiD, R = 1000, parallel = &amp;quot;multicore&amp;quot;,
 ncpus = 2)&lt;/p&gt;

&lt;p&gt;Bootstrap Statistics :
 original    bias    std. error
t1* -11863.9 -553.3393    8580.435&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These results are very similar to the ones in the book, only the standard error is higher.&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;You can get confidence intervals like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quantile(boot_est$t, c(0.025, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       2.5%      97.5% 
## -30186.397   3456.133&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or a t-statistic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boot_est$t0/sd(boot_est$t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1.382669&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or the density of the replications:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(density(boot_est$t))&lt;/code&gt;&lt;/pre&gt;

&lt;div style=&#34;text-align:center;&#34;&gt;
    &lt;img src=&#34;/img/density_did.png&#34; width=&#34;670&#34; height=&#34;450&#34; /&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;Just as in the book, we find that the DiD estimate is not significant to the 5% level.&lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Update to Introduction to programming econometrics with R</title>
      <link>/blog/2015-05-03-update-introduction-r-programming/</link>
      <pubDate>Sun, 03 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015-05-03-update-introduction-r-programming/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;This semester I taught a course on applied econometrics with the R programming language. For this, I created a document that I gave to my students and shared online. This is the kind of document I would have liked to read when I first started using R. I already had some programming experience in C and Pascal but this is not necessarily the case for everyone that is confronted to R when they start learning about econometrics.&lt;/p&gt;

&lt;p&gt;This is why the beginning of the document focuses more on general programming knowledge and techniques, and then only on econometrics. People online seemed to like the document, as I&amp;rsquo;ve received some positive comments by David Smith from Revolution R (read his blog post about the document &lt;a href=&#34;http://blog.revolutionanalytics.com/2015/01/introduction-to-applied-econometrics-with-r.html&#34;&gt;here&lt;/a&gt;) and Dave Giles which links to David&amp;rsquo;s blog post &lt;a href=&#34;http://davegiles.blogspot.fr/2015/04/introduction-to-applied-econometrics.html?spref=tw&#34;&gt;here&lt;/a&gt;. People on twitter have also retweeted David&amp;rsquo;s and Dave&amp;rsquo;s tweets to their blog posts and I&amp;rsquo;ve received some requests by people to send them the PDF by email (because they live in places where Dropbox is not accessible unfortunately).&lt;/p&gt;

&lt;p&gt;The document is still a work in progress (and will probably remain so for a long time), but I&amp;rsquo;ve added some new sections about reproducible research and thought that this update could warrant a new blog post.&lt;/p&gt;

&lt;p&gt;For now, only linear models are reviewed, but I think I&amp;rsquo;ll start adding some chapters about non-linear models soonish. The goal for these notes, however, is not to re-invent the wheel: there are lots of good books about econometrics with R out there and packages that estimate a very wide range of models. What I want for these notes, is to focus more on the programming knowledge an econometrician needs, in a very broad and general sense. I want my students to understand that R is a true programming language, and that they need to use every feature offered by such a language, and not think of R as a black box into which you only type pre-programmed commands, but also be able to program their own routines.&lt;/p&gt;

&lt;p&gt;Also, I&amp;rsquo;ve made it possible to create the PDF using a Makefile. This may be useful for people that do not have access to Dropbox, but are familiar with git.&lt;/p&gt;

&lt;p&gt;You can compile the book in two ways: first download the whole repository:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;git clone git@bitbucket.org:b-rodrigues/programmingeconometrics.git&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and then, with Rstudio, open the file &lt;em&gt;appliedEconometrics.Rnw&lt;/em&gt; and knit it. Another solution is to use the Makefile. Just type:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;make&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;inside a terminal (should work on GNU+Linux and OSX systems) and it should compile the document. You may get some message about &amp;ldquo;additional information&amp;rdquo; for some R packages. When these come up, just press Q on your keyboard to continue the compilation process.&lt;/p&gt;

&lt;p&gt;Get the notes &lt;a href=&#34;https://cloud.openmailbox.org/index.php/s/ghZwBxMb24tWGSL&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always, if you have questions or suggestions, do not hesitate to send me an  &lt;a href=&#34;mailto:contact@brodrigues.co&#34;&gt;email&lt;/a&gt; and I sure hope you&amp;rsquo;ll find these notes useful!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Export R output to a file</title>
      <link>/blog/2015-02-22-export-r-output-to-file/</link>
      <pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015-02-22-export-r-output-to-file/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;p&gt;Sometimes it is useful to export the output of a long-running R command. For example, you might want to run a time consuming regression just before leaving work on Friday night, but would like to get the output saved inside your Dropbox folder to take a look at the results before going back to work on Monday.&lt;/p&gt;
&lt;p&gt;This can be achieved very easily using &lt;code&gt;capture.output()&lt;/code&gt; and &lt;code&gt;cat()&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;out &amp;lt;- capture.output(summary(my_very_time_consuming_regression))

cat(&amp;quot;My title&amp;quot;, out, file=&amp;quot;summary_of_my_very_time_consuming_regression.txt&amp;quot;, sep=&amp;quot;\n&amp;quot;, append=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;my_very_time_consuming_regression&lt;/code&gt; is an object of class &lt;code&gt;lm&lt;/code&gt; for example. I save the output of &lt;code&gt;summary(my_very_time_consuming_regression)&lt;/code&gt; as text using &lt;code&gt;capture.output&lt;/code&gt; and save it in a variable called &lt;code&gt;out&lt;/code&gt;. Finally, I save &lt;code&gt;out&lt;/code&gt; to a file called &lt;code&gt;summary_of_my_very_time_consuming_regression.txt&lt;/code&gt; with the first sentence being &lt;code&gt;My title&lt;/code&gt; (you can put anything there). The file &lt;code&gt;summary_of_my_very_time_consuming_regression.txt&lt;/code&gt; doesn’t have to already exist in your working directory. The option &lt;code&gt;sep=&amp;quot;\n&amp;quot;&lt;/code&gt; is important or else the whole output will be written in a single line. Finally, &lt;code&gt;append=TRUE&lt;/code&gt; makes sure your file won’t be overwritten; additional output will be appended to the file, which can be nice if you want to compare different versions of your model.&lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to programming econometrics with R</title>
      <link>/blog/2015-01-12-introduction-to-programming-econometrics-with-r/</link>
      <pubDate>Mon, 12 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/blog/2015-01-12-introduction-to-programming-econometrics-with-r/</guid>
      <description>

&lt;p&gt;This semester, I&amp;rsquo;ll be teaching an introduction to applied econometrics with R, so I&amp;rsquo;ve decided to write a very small book called &amp;ldquo;Introduction to programming Econometrics with R&amp;rdquo;. This is primarily intended for bachelor students and the focus is not much on econometric theory, but more on how to implement econometric theory into computer code, using the R programming language. It&amp;rsquo;s very basic and doesn&amp;rsquo;t cover any advanced topics in econometrics and is intended for people with 0 previous programming knowledge. It is still very rough around the edges, and it&amp;rsquo;s missing the last chapter about reproducible research, and the references, but I think it&amp;rsquo;s time to put it out there; someone else than my students may find it useful. The book&amp;rsquo;s probably full of typos and mistakes, so don&amp;rsquo;t hesitate to drop me an e-mail if you find something fishy:
contact@brodrigues.co&lt;/p&gt;

&lt;p&gt;Also there might be some sections at the beginning that only concern my students. Just ignore that.&lt;/p&gt;

&lt;p&gt;Get it here: &lt;a href=&#34;https://cloud.openmailbox.org/index.php/s/ghZwBxMb24tWGSL&#34;&gt;download&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;update-2017-01-22&#34;&gt;Update (2017-01-22)&lt;/h3&gt;

&lt;p&gt;You might find the book useful as it is now, but I never had a chance to finish it. I might get back to
it once I&amp;rsquo;ll have more time, and port it to bookdown.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R, R with Atlas, R with OpenBLAS and Revolution R Open: which is fastest?</title>
      <link>/blog/2014-11-11-benchmarks-r-blas-atlas-rro/</link>
      <pubDate>Tue, 11 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/blog/2014-11-11-benchmarks-r-blas-atlas-rro/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;p&gt;In this short post, I benchmark different &amp;ldquo;versions&amp;rdquo; of R. I compare the execution speeds of R, R linked against OpenBLAS, R linked against ATLAS and Revolution R Open. Revolution R Open is a new open source version of R made by Revolution Analytics. It is linked against MKL and should offer huge speed improvements over vanilla R. Also, it uses every cores of your computer by default, without any change whatsoever to your code.&lt;/p&gt;

&lt;p&gt;TL;DR: Revolution R Open is the fastest of all the benchmarked versions (with R linked against OpenBLAS and ATLAS just behind), and easier to setup. &lt;/p&gt;

&lt;h2&gt;Setup&lt;/h2&gt;

&lt;p&gt;I benchmarked these different versions of R using &lt;code&gt;R-benchmark-25.R&lt;/code&gt; that you can download &lt;a href=&#34;http://r.research.att.com/benchmarks/R-benchmark-25.R&#34;&gt;here&lt;/a&gt;. This benchmark file was created by Simon Urbanek.&lt;/p&gt;

&lt;p&gt;I ran the benchmarks on my OpenSUSE 13.2 computer with a Pentium Dual-Core CPU E6500@2.93GHz with 4GB of Ram. It&amp;#39;s outdated, but it&amp;#39;s still quite fast for most of my numerical computation needs. I installed &amp;ldquo;vanilla&amp;rdquo; R from the official OpenSUSE repositories which is currently at version 3.1.2.&lt;/p&gt;

&lt;p&gt;Then, I downloaded OpenBLAS and ATLAS also from the official OpenSUSE repositories and made R use these libraries instead of its own implementation of BLAS. The way I did that is a bit hacky, but works: first, go to &lt;code&gt;/usr/lib64/R/lib&lt;/code&gt; and backup &lt;code&gt;libRblas.so&lt;/code&gt; (rename it to &lt;code&gt;libRblas.soBackup&lt;/code&gt; for instance). Then link &lt;code&gt;/usr/lib64/libopenblas.so.0&lt;/code&gt; to &lt;code&gt;/usr/lib64/R/lib/libRblas&lt;/code&gt;, and that&amp;#39;s it, R will use OpenBLAS. For ATLAS, you can do it in the same fashion, but you&amp;#39;ll find the library in &lt;code&gt;/usr/lib64/atlas/&lt;/code&gt;. These paths should be the same for any GNU/Linux distribution. For other operating systems, I&amp;#39;m sure you can find where these libraries are with Google.&lt;/p&gt;

&lt;p&gt;The last version I benchmarked was Revolution R Open. This is a new version of R released by Revolution Analytics. Revolution Analytics had their own version of R, called Revolution R, for quite some time now. They decided to release a completely free as in freedom and free as in free beer version of this product which they now renamed Revolution R Open. You can download Revolution R Open &lt;a href=&#34;http://mran.revolutionanalytics.com/download/#review&#34;&gt;here&lt;/a&gt;. You can have both &amp;ldquo;vanilla&amp;rdquo; R and Revolution R Open installed on your system. &lt;/p&gt;

&lt;h2&gt;Results&lt;/h2&gt;

&lt;p&gt;I ran the &lt;code&gt;R-benchmark-25.R&lt;/code&gt; 6 times for every version but will only discuss the 4 best runs.&lt;/p&gt;

&lt;p&gt;&lt;style type=&#34;text/css&#34;&gt;
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 11px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 11px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-0ord{text-align:right}
&lt;/style&gt;
&lt;table class=&#34;tg&#34;&gt;
&lt;tr&gt;
&lt;th class=&#34;tg-031e&#34;&gt;R version&lt;/th&gt;
&lt;th class=&#34;tg-0ord&#34;&gt;Fastest run&lt;/th&gt;
&lt;th class=&#34;tg-0ord&#34;&gt;Slowest run&lt;/th&gt;
&lt;th class=&#34;tg-0ord&#34;&gt;Mean Run&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-031e&#34;&gt;Vanilla R&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;63.65&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;66.21&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;64.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-031e&#34;&gt;OpenBLAS R&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;15.63&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;18.96&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;16.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-031e&#34;&gt;ATLAS R&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;16.92&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;21.57&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;18.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&#34;tg-031e&#34;&gt;RRO&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;14.96&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;16.08&lt;/td&gt;
&lt;td class=&#34;tg-0ord&#34;&gt;15.49&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;&lt;/p&gt;

&lt;p&gt;As you can read from the table above, Revolution R Open was the fastest of the four versions, but not significantly faster than BLAS or ATLAS R. However, RRO uses all the available cores by default, so if your code relies on a lot matrix algebra, RRO might be actually a lot more faster than OpenBLAS and ATLAS R. Another advantage of RRO is that it is very easy to install, and also works with Rstudio and is compatible with every R package to existence. &#34;Vanilla&#34; R is much slower than the other three versions, more than 3 times as slow! &lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;With other benchmarks, you could get other results, but I don&#39;t think that &#34;vanilla&#34; R could beat any of the other three versions. Whatever your choice, I recommend not using plain, &amp;ldquo;vanilla&amp;rdquo; R. The other options are much faster than standard R, and don&amp;#39;t require much work to set up. I&amp;#39;d personally recommend Revolution R Open, as it is free software and compatible with CRAN packages and Rstudio. &lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Object Oriented Programming with R: An example with a Cournot duopoly</title>
      <link>/blog/2014-04-23-r-s4-rootfinding/</link>
      <pubDate>Wed, 23 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/blog/2014-04-23-r-s4-rootfinding/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;body&gt;
&lt;p&gt;I started reading &lt;em&gt;Applied Computational Economics &amp;amp; Finance&lt;/em&gt; by Mario J. Miranda and Paul L. Fackler. It is a very interesting book that I recommend to every one of my colleagues. The only issue I have with this book, is that the programming language they use is Matlab, which is proprietary. While there is a free as in freedom implementation of the Matlab language, namely Octave, I still prefer using R. In this post, I will illustrate one example the authors present in the book with R, using the package &lt;code&gt;rootSolve&lt;/code&gt;. &lt;code&gt;rootSolve&lt;/code&gt; implements Newtonian algorithms to find roots of functions; to specify the functions for which I want the roots, I use R&amp;#39;s Object-Oriented Programming (OOP) capabilities to build a model that returns two functions. This is optional, but I found that it was a good example to illustrate OOP, even though simpler solutions exist, one of which was proposed by reddit user TheDrownedKraken (whom I thank) and will be presented at the end of the article.&lt;/p&gt;&lt;/p&gt;

&lt;h3&gt;Theoretical background&lt;/h3&gt;

&lt;p&gt;The example is taken from Miranda&amp;#39;s and Fackler&amp;#39;s book, on page 35. The authors present a Cournot duopoly model. In a Cournot duopoly model, two firms compete against each other by quantities. Both produce a certain quantity of an homogenous good, and take the quantity produce by their rival as given. &lt;/p&gt;

&lt;p&gt;The inverse demand of the good is :&lt;/p&gt;

&lt;p&gt;$$P(q) = q^{-\dfrac{1}{\eta}}$$&lt;/p&gt;

&lt;p&gt;the cost function for firm i is:&lt;/p&gt;

&lt;p&gt;$$C_i(q_i) = P(q_1+q_2)*q_i - C_i(q_i)$$&lt;/p&gt;

&lt;p&gt;and the profit for firm i:&lt;/p&gt;

&lt;p&gt;$$\pi_i(q1,q2) = P(q_1+q_2)q_i - C_i(q_i)$$&lt;/p&gt;

&lt;p&gt;The optimality condition for firm i is thus:&lt;/p&gt;

&lt;p&gt;$$\dfrac{\partial \pi_i}{\partial q_i} = (q1+q2)^{-\dfrac{1}{\eta}} - \dfrac{1}{\eta} (q_1+q_2)^{\dfrac{-1}{\eta-1}}q_i - c_iq_i=0.$$&lt;/p&gt;

&lt;h3&gt;Implementation in R&lt;/h3&gt;

&lt;p&gt;If we want to find the optimal quantities \(q_1\) and \(q_2\) we need to program the optimality condition and we could also use the jacobian of the optimality condition. The jacobian is generally useful to speed up the root finding routines. This is were OOP is useful. First let&amp;#39;s create a new class, called &lt;em&gt;Model&lt;/em&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;setClass(Class = &amp;quot;Model&amp;quot;, slots = list(OptimCond = &amp;quot;function&amp;quot;, JacobiOptimCond = &amp;quot;function&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This new class has two &lt;em&gt;slots&lt;/em&gt;, which here are functions (in general &lt;em&gt;slots&lt;/em&gt; are properties of your class); we need the model to return the optimality condition and the jacobian of the optimality condition.&lt;/p&gt;

&lt;p&gt;Now we can create a function which will return these two functions for certain values of the parameters, &lt;em&gt;c&lt;/em&gt; and  &lt;img src=&#34;http://latex.codecogs.com/png.latex?\inline \eta&#34; alt=&#34;\inline \eta&#34; /&gt; of the model:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;my_mod &amp;lt;- function(eta, c) {
    e = -1/eta

    OptimCond &amp;lt;- function(q) {
        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(c) %*% q)
    }

    JacobiOptimCond &amp;lt;- function(q) {
        return((e * sum(q)^e) * array(1, c(2, 2)) + (e * sum(q)^(e - 1)) * diag(1, 
            2) + (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(c))
    }

    return(new(&amp;quot;Model&amp;quot;, OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The function &lt;code&gt;my_mod&lt;/code&gt; takes two parameters, &lt;code&gt;eta&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; and returns two functions, the optimality condition and the jacobian of the optimality condition. Both are now accessible via &lt;code&gt;my_mod(eta=1.6,c = c(0.6,0.8))@OptimCond&lt;/code&gt; and &lt;code&gt;my_mod(eta=1.6,c = c(0.6,0.8))@JacobiOptimCond&lt;/code&gt; respectively (and by specifying values for &lt;code&gt;eta&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Now, we can use the &lt;code&gt;rootSolve&lt;/code&gt; package to get the optimal values \(q_1\) and \(q_2\) &lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;library(&amp;quot;rootSolve&amp;quot;)

multiroot(f = my_mod(eta = 1.6, c = c(0.6, 0.8))@OptimCond, start = c(1, 1), 
    maxiter = 100, jacfunc = my_mod(eta = 1.6, c = c(0.6, 0.8))@JacobiOptimCond)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $root
## [1] 0.8396 0.6888
## 
## $f.root
##            [,1]
## [1,] -2.220e-09
## [2,]  9.928e-09
## 
## $iter
## [1] 4
## 
## $estim.precis
## [1] 6.074e-09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After 4 iterations, we get that  &lt;img src=&#34;http://latex.codecogs.com/png.latex?\inline q_1&#34; alt=&#34;\inline q_1&#34; /&gt;  and  &lt;img src=&#34;http://latex.codecogs.com/png.latex?\inline q_2&#34; alt=&#34;\inline q_2&#34; /&gt; are equal to 0.84 and 0.69 respectively, which are the same values as in the book!&lt;/p&gt;

&lt;h3&gt;Suggestion by Reddit user, TheDrownedKraken&lt;/h3&gt;

&lt;p&gt;I posted this blog post on the rstats subbreddit on &lt;a href=&#34;http://www.reddit.com&#34;&gt;www.reddit.com&lt;/a&gt;. I got a very useful comment by reddit member TheDrownedKraken which suggested the following approach, which doesn&amp;#39;t need a new class to be build. I thank him for this. Here is his suggestion:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;generator &amp;lt;- function(eta, a) {
    e = -1/eta

    OptimCond &amp;lt;- function(q) {
        return(sum(q)^e + e * sum(q)^(e - 1) * q - diag(a) %*% q)
    }

    JacobiOptimCond &amp;lt;- function(q) {
        return((e * sum(q)^e) * array(1, c(2, 2)) + (e * sum(q)^(e - 1)) * diag(1, 
            2) + (e - 1) * e * sum(q)^(e - 2) * q * c(1, 1) - diag(a))
    }

    return(list(OptimCond = OptimCond, JacobiOptimCond = JacobiOptimCond))

}

f.s &amp;lt;- generator(eta = 1.6, a = c(0.6, 0.8))

multiroot(f = f.s$OptimCond, start = c(1, 1), maxiter = 100, jacfunc = f.s$JacobiOptimCond)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## $root
## [1] 0.8396 0.6888
## 
## $f.root
##            [,1]
## [1,] -2.220e-09
## [2,]  9.928e-09
## 
## $iter
## [1] 4
## 
## $estim.precis
## [1] 6.074e-09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using R as a Computer Algebra System with Ryacas</title>
      <link>/blog/2013-12-31-r-cas/</link>
      <pubDate>Tue, 31 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/blog/2013-12-31-r-cas/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;/head&gt;&lt;/p&gt;

&lt;p&gt;&lt;body&gt;
&lt;p&gt;R is used to perform statistical analysis and doesn&amp;#39;t focus on symbolic maths. But it is sometimes useful to let the computer derive a function for you (and have the analytic expression of said derivative), but maybe you don&amp;#39;t want to leave your comfy R shell. It is possible to turn R into a full-fledged computer algebra system. CASs are tools that perform symbolic operations, such as getting the expression of the derivative of a user-defined (and thus completely arbitrary) function. Popular CASs include the proprietary Mathematica and Maple. There exists a lot of CASs under a Free Software license, Maxima (based on the very old Macsyma), Yacas, Xcas&amp;hellip; In this post I will focus on Yacas and the &lt;code&gt;Ryacas&lt;/code&gt; libarary. There is also the possibility to use the &lt;code&gt;rSympy&lt;/code&gt; library that uses the &lt;code&gt;Sympy&lt;/code&gt; Python library, which has a lot more features than Yacas. However, depending on your operating system installation can be tricky as it also requires &lt;code&gt;rJava&lt;/code&gt; as a dependency. &lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Even though &lt;code&gt;Ryacas&lt;/code&gt; is quite nice to have, there are some issues though. For example, let&amp;#39;s say you want the first derivative of a certain function f. If you use &lt;code&gt;Ryacas&lt;/code&gt; to get it, the returned object won&amp;#39;t be a function. There is a way to &amp;ldquo;extract&amp;rdquo; the text from the returned object and make a function out of it. But there are still other issues; I&amp;#39;ll discuss them later.&lt;/p&gt;

&lt;h2&gt;Installation&lt;/h2&gt;

&lt;p&gt;Installation should be rather painless. On Linux you need to install Yacas first, which should be available in the major distros&amp;#39; repositories. Then you can install &lt;code&gt;Ryacas&lt;/code&gt; from within the R shell. On Windows, you need to run these three commands (don&amp;#39;t bother installing Yacas first):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;install.packages(&amp;#39;Ryacas&amp;#39;)
library(Ryacas)
yacasInstall()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can find more information on the &lt;a href=&#34;https://code.google.com/p/ryacas/#INSTALLATION&#34;&gt;project&amp;#39;s page&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Example session&lt;/h2&gt;

&lt;p&gt;First, you must load &lt;code&gt;Ryacas&lt;/code&gt; and define symbols that you will use in your functions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;require(&amp;quot;Ryacas&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Loading required package: Ryacas Loading required package: XML
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;x &amp;lt;- Sym(&amp;quot;x&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then define your fonctions:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;my_func &amp;lt;- function(x) {
    return(x/(x^2 + 3))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And you can get the derivative for instance:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;my_deriv &amp;lt;- yacas(deriv(my_func(x), x))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Starting Yacas!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you check the class of &lt;code&gt;my_deriv&lt;/code&gt;, you&amp;#39;ll see that it is of class &lt;code&gt;yacas&lt;/code&gt;, which is not very useful. Let&amp;#39;s «convert» it to a function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;my_deriv2 &amp;lt;- function(x) {
    eval(parse(text = my_deriv$YacasForm))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then evaluate it. A lot of different operations are possible. But there are some problems.&lt;/p&gt;

&lt;h2&gt;Issues with Ryacas&lt;/h2&gt;

&lt;p&gt;You can&amp;#39;t use elements of a vector as parameters of your function, i.e.:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;theta &amp;lt;- Sym(&amp;quot;theta&amp;quot;)
func &amp;lt;- function(x) {
    return(theta[1] * x + theta[2])
}
# Let&amp;#39;s integrate this
Func &amp;lt;- yacas(Integrate(func(x), x))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;returns &lt;code&gt;(x^2*theta)/2+NA*x;&lt;/code&gt; which is not quite what we want&amp;hellip;there is a workaround however. Define your functions like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;a &amp;lt;- Sym(&amp;quot;a&amp;quot;)
b &amp;lt;- Sym(&amp;quot;b&amp;quot;)
func2 &amp;lt;- function(x) {
    return(a * x + b)
}
# Let&amp;#39;s integrate this
Func2 &amp;lt;- yacas(Integrate(func2(x), x))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;we get the expected result: &lt;code&gt;(x^2*a)/2+b*x;&lt;/code&gt;. Now replace &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; by the thetas:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;Func2 &amp;lt;- gsub(&amp;quot;a&amp;quot;, &amp;quot;theta[1]&amp;quot;, Func2$YacasForm)
Func2 &amp;lt;- gsub(&amp;quot;b&amp;quot;, &amp;quot;theta[2]&amp;quot;, Func2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have what we want: &lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;Func2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;(x^2*theta[1])/2+theta[2]*x;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then copy-paste this result into a function.&lt;/p&gt;

&lt;p&gt;Another problem is if you use built-in functions that are different between R and Yacas. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;my_log &amp;lt;- function(x) {
    return(sin(log(2 + x)))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now try to differentiate it:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;dmy_log &amp;lt;- yacas(deriv(my_log(x), x))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;you get: &lt;code&gt;Cos(Ln(x+2))/(x+2);&lt;/code&gt;. The problem with this, is that R doesn&amp;#39;t recognize &lt;code&gt;Cos&lt;/code&gt; as the cosine (which is &lt;code&gt;cos&lt;/code&gt; in R) and the same goes for &lt;code&gt;Ln&lt;/code&gt;. These are valid Yacas functions, but that is not the case in R. So you&amp;#39;ll have to use &lt;code&gt;gsub&lt;/code&gt; to replace these functions and then copy paste the end result into a function.&lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While it has some flaws, &lt;code&gt;Ryacas&lt;/code&gt; can be quite useful if you need to derive or integrate complicated expressions that you then want to use in R. Using some of the tricks I showed here, you should be able to overcome some of its shortcomings. If installation of &lt;code&gt;rJava&lt;/code&gt; and thus &lt;code&gt;rSympy&lt;/code&gt; becomes easier, I&amp;#39;ll probably also do a short blog-post about it, as it has more features than &lt;code&gt;Ryacas&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Method of Simulated Moments with R</title>
      <link>/blog/2013-01-29-method-of-simulated-moments-with-r/</link>
      <pubDate>Wed, 11 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/blog/2013-01-29-method-of-simulated-moments-with-r/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;/head&gt;&lt;/p&gt;

&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;p&gt;This document details section &lt;em&gt;12.5.6. Unobserved Heterogeneity Example&lt;/em&gt;. The original source code giving the results from table 12.3 are available from the authors&amp;#39; site &lt;a href=&#34;http://cameron.econ.ucdavis.edu/mmabook/mmaprograms.html&#34;&gt;here&lt;/a&gt; and written for Stata. This is an attempt to translate the code to R.&lt;/p&gt;

&lt;p&gt;Consult the original source code if you want to read the authors&amp;#39; comments. If you want the R source code without all the commentaries, grab it here. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.&lt;/p&gt;

&lt;p&gt;The model is the same as the one described here, so I won&amp;#39;t go into details. The moment condition used is \( E[(y_i-\theta-u_i)]=0 \), so we can replace the expectation operator by the empirical mean:&lt;/p&gt;

&lt;p&gt;\[ \dfrac{1}{N} \sum_{i=1}^N(y_i - \theta - E[u_i])=0 \]&lt;/p&gt;

&lt;p&gt;Supposing that \( E[\overline{u}] \) is unknown, we can instead use the method of simulated moments for \( \theta \) defined by:&lt;/p&gt;

&lt;p&gt;\[ \dfrac{1}{N} \sum_{i=1}^N(y_i - \theta - \dfrac{1}{S} \sum_{s=1}^S u_i^s)=0 \]&lt;/p&gt;

&lt;h3&gt;Import the data&lt;/h3&gt;

&lt;p&gt;You can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn&amp;#39;t make mistakes I prefer importing their data directly from their website.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;data &amp;lt;- read.table(&amp;quot;http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc&amp;quot;)
u &amp;lt;- data[, 1]
e &amp;lt;- data[, 2]
y &amp;lt;- data[, 3]
numobs &amp;lt;- length(u)
simreps &amp;lt;- 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Simulation&lt;/h3&gt;

&lt;p&gt;In the code below, we simulate the equation defined above:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;usim &amp;lt;- -log(-log(runif(simreps)))
esim &amp;lt;- rnorm(simreps, 0, 1)

isim &amp;lt;- 0
while (isim &amp;lt; simreps) {

    usim = usim - log(-log(runif(simreps)))
    esim = esim + rnorm(simreps, 0, 1)

    isim = isim + 1

}

usimbar = usim/simreps
esimbar = esim/simreps

theta = y - usimbar - esimbar

theta_msm &amp;lt;- mean(theta)
approx_sterror &amp;lt;- sd(theta)/sqrt(simreps)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These steps yield the following results:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## Theta MSM= 1.188 Approximate Standard Error= 0.01676
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulated Maximum Likelihood with R</title>
      <link>/blog/2013-01-16-simulated-maximum-likelihood-with-r/</link>
      <pubDate>Wed, 11 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/blog/2013-01-16-simulated-maximum-likelihood-with-r/</guid>
      <description>&lt;p&gt;&lt;head&gt;
&lt;script src=&#34;https://polyfill.io/v3/polyfill.min.js?features=es6&#34;&gt;&lt;/script&gt;
&lt;script id=&#34;MathJax-script&#34; async src=&#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;/head&gt;&lt;/p&gt;

&lt;p&gt;&lt;body&gt;&lt;/p&gt;

&lt;p&gt;This document details section &lt;em&gt;12.4.5. Unobserved Heterogeneity 
Example&lt;/em&gt; from Cameron and Trivedi&#39;s book - MICROECONOMETRICS: Methods and 
Applications. The original source code giving the results from table 12.2 are 
available from the authors&amp;#39; site &lt;a 
href=&#34;http://cameron.econ.ucdavis.edu/mmabook/mmaprograms.html&#34;&gt;here&lt;/a&gt; and 
written for Stata. This is an attempt to translate the code to R. I&#39;d like to 
thank Reddit user &lt;a 
href=&#34;http://www.reddit.com/user/anonemouse2010&#34;&gt;anonemouse2010&lt;/a&gt; for his 
advice which helped me write the function.&lt;/p&gt;

&lt;p&gt;Consult the original source code if you want to read the authors&amp;#39; comments. If you want the R source code without all the commentaries, grab it &lt;a href=&#39;/assets/code/simulated_max_lik.R&#39;&gt;here&lt;/a&gt;. This is not guaranteed to work, nor to be correct. It could set your pet on fire and/or eat your first born. Use at your own risk. I may, or may not, expand this example. Corrections, constructive criticism are welcome.&lt;/p&gt;

&lt;p&gt;The model is \( y=\theta+u+\varepsilon \) where \( \theta \) is a scalar parameter equal to 1. \( u \) is extreme value type 1 (Gumbel distribution), \( \varepsilon \leadsto \mathbb{N}(0,1) \). For more details, consult the book.&lt;/p&gt;

&lt;h3&gt;Import the data&lt;/h3&gt;

&lt;p&gt;You can consult the original source code to see how the authors simulated the data. To get the same results, and verify that I didn&amp;#39;t make mistakes I prefer importing their data directly from their website.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;data &amp;lt;- read.table(&amp;quot;http://cameron.econ.ucdavis.edu/mmabook/mma12p2mslmsm.asc&amp;quot;)
u &amp;lt;- data[, 1]
e &amp;lt;- data[, 2]
y &amp;lt;- data[, 3]
numobs &amp;lt;- length(u)
simreps &amp;lt;- 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Simulation&lt;/h3&gt;

&lt;p&gt;In the code below, the following likelihood function:

$$\log{\hat{L}_N(\theta)} = \dfrac{1}{N} \sum_{i=1}^N\log{\big( \dfrac{1}{S}\sum_{s=1}^S \dfrac{1}{\sqrt{2\pi}} \exp \{ -(-y_i-\theta-u_i^s)^2/2 \}\big)}$$

which can be found on page 397 is programmed using the function &lt;code&gt;sapply&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;denssim &amp;lt;- function(theta) {
    loglik &amp;lt;- mean(sapply(y, function(y) log(mean((1/sqrt(2 * pi)) * exp(-(y - theta + log(-log(runif(simreps))))^2/2)))))
    return(-loglik)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This likelihood is then maximized:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;system.time(res &amp;lt;- optim(0.1, denssim, method = &amp;quot;BFGS&amp;quot;, control = list(maxit = simreps)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   21.98    0.08   22.09
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Convergence is achieved pretty rapidly, to &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## [1] 1.101
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which is close to the true value of the parameter 1 (which was used to generate the data). &lt;/p&gt;

&lt;p&gt;Let&amp;#39;s try again with another parameter value, for example \( \theta=2.5 \). We have to generate y again:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;y2 &amp;lt;- 2.5 + u + e
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and slightly modify the likelihood:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;denssim2 &amp;lt;- function(theta) {
    loglik &amp;lt;- mean(sapply(y2, function(y2) log(mean((1/sqrt(2 * pi)) * exp(-(y2 - 
        theta + log(-log(runif(simreps))))^2/2)))))
    return(-loglik)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which can then be maximized:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;system.time(res2 &amp;lt;- optim(0.1, denssim2, method = &amp;quot;BFGS&amp;quot;, control = list(maxit = simreps)))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   12.56    0.00   12.57
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The value that maximizes the likelihood is: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## [1] 2.713
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which is close to the true value of the parameter 2.5 (which was used to generate the data). &lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Nonlinear Gmm with R - Example with a logistic regression</title>
      <link>/blog/2013-11-07-gmm-with-rmd/</link>
      <pubDate>Thu, 07 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/blog/2013-11-07-gmm-with-rmd/</guid>
      <description>&lt;!-- MathJax scripts --&gt;

&lt;script type=&#34;text/javascript&#34; async
  src=&#34;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;&lt;/head&gt;&lt;/p&gt;

&lt;p&gt;&lt;body&gt;
&lt;p&gt;In this post, I will explain how you can use the R &lt;code&gt;gmm&lt;/code&gt; package to estimate a non-linear model, and more specifically a logit model. For my research, I have to estimate Euler equations using the Generalized Method of Moments. I contacted Pierre Chaussé, the creator of the &lt;code&gt;gmm&lt;/code&gt; library for help, since I was having some difficulties. I am very grateful for his help (without him, I&amp;#39;d still probably be trying to estimate my model!).&lt;/p&gt;&lt;/p&gt;

&lt;h3&gt;Theoretical background, motivation and data set&lt;/h3&gt;

&lt;p&gt;I will not dwell in the theory too much, because you can find everything you need &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_method_of_moments&#34;&gt;here&lt;/a&gt;. I think it&#39;s more interesting to try to understand why someone would use the Generalized Method of Moments instead of maximization of the log-likelihood. Well, in some cases, getting the log-likelihood can be quite complicated, as can be the case for arbitrary, non-linear models (for example if you want to estimate the parameters of a very non-linear utility function). Also, moment conditions can sometimes be readily available, so using GMM instead of MLE is trivial. And finally, GMM is... well, a very general method: every popular estimator can be obtained as a special case of the GMM estimator, which makes it quite useful.&lt;/p&gt;

&lt;p&gt;Another question that I think is important to answer is: why this post? Well, because that&#39;s exactly the kind of post I would have loved to have found 2 months ago, when I was beginning to work with the GMM. Most posts I found presented the &lt;code&gt;gmm&lt;/code&gt; package with very simple and trivial examples, which weren&#39;t very helpful. The example presented below is not very complicated per se, but much more closer to a real-world problem than most stuff that is out there. At least, I hope you will find it useful!&lt;/p&gt;

&lt;p&gt;For illustration purposes, I&amp;#39;ll use data from Marno Verbeek&amp;#39;s &lt;em&gt;A guide to modern Econometrics&lt;/em&gt;, used in the illustration on page 197. You can download the data from the book&amp;#39;s companion page &lt;a href=&#34;http://www.econ.kuleuven.ac.be/gme/&#34;&gt;here&lt;/a&gt; under the section &lt;em&gt;Data sets&lt;/em&gt; or from the &lt;code&gt;Ecdat&lt;/code&gt; package in R. I use the data set from Gretl though, as the dummy variables are numeric (instead of class &lt;code&gt;factor&lt;/code&gt;) which makes life easier when writing your own functions. You can get the data set &lt;a href=&#34;/assets/files/benefits.R&#34;&gt;here&lt;/a&gt;. &lt;/p&gt;

&lt;h3&gt;Implementation in R&lt;/h3&gt;

&lt;p&gt;I don&amp;#39;t estimate the exact same model, but only use a subset of the variables available in the data set. Keep in mind that this post is just for illustration purposes.&lt;/p&gt;

&lt;p&gt;First load the &lt;code&gt;gmm&lt;/code&gt; package and load the data set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;require(&amp;quot;gmm&amp;quot;)
data &amp;lt;- read.table(&amp;quot;path/to/data/benefits.R&amp;quot;, header = T)

attach(data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then estimate a logit model with the &lt;code&gt;glm()&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;native &amp;lt;- glm(y ~ age + age2 + dkids + dykids + head + male + married + rr +  rr2, family = binomial(link = &amp;quot;logit&amp;quot;), na.action = na.pass)

summary(native)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ age + age2 + dkids + dykids + head + male + 
##     married + rr + rr2, family = binomial(link = &amp;quot;logit&amp;quot;), na.action = na.pass)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.889  -1.379   0.788   0.896   1.237  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)   
## (Intercept) -1.00534    0.56330   -1.78   0.0743 . 
## age          0.04909    0.02300    2.13   0.0328 * 
## age2        -0.00308    0.00293   -1.05   0.2924   
## dkids       -0.10922    0.08374   -1.30   0.1921   
## dykids       0.20355    0.09490    2.14   0.0320 * 
## head        -0.21534    0.07941   -2.71   0.0067 **
## male        -0.05988    0.08456   -0.71   0.4788   
## married      0.23354    0.07656    3.05   0.0023 **
## rr           3.48590    1.81789    1.92   0.0552 . 
## rr2         -5.00129    2.27591   -2.20   0.0280 * 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 6086.1  on 4876  degrees of freedom
## Residual deviance: 5983.9  on 4867  degrees of freedom
## AIC: 6004
## 
## Number of Fisher Scoring iterations: 4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now comes the interesting part: how can you estimate such a non-linear model with the &lt;code&gt;gmm()&lt;/code&gt; function from the &lt;code&gt;gmm&lt;/code&gt; package? &lt;/p&gt;

&lt;p&gt;For every estimation with the Generalized Method of Moments, you will need valid moment conditions. It turns out that in the case of the logit model, this moment condition is quite simple:&lt;/p&gt;

&lt;p&gt;$$
E[X&amp;rsquo; * (Y-\Lambda(X&amp;rsquo;\theta))] = 0
$$&lt;/p&gt;

&lt;p&gt;where \( \Lambda() \) is the logistic function. Let&amp;#39;s translate this condition into code. First, we need the logistic function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;logistic &amp;lt;- function(theta, data) {
    return(1/(1 + exp(-data %*% theta)))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and let&amp;#39;s also define a new data frame, to make our life easier with the moment conditions (don&#39;t forget to add a column of ones to the matrix, hence the &lt;code&gt;1&lt;/code&gt; after &lt;code&gt;y&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;dat &amp;lt;- data.matrix(cbind(y, 1, age, age2, dkids, dykids, head, male, married, 
    rr, rr2))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and now the moment condition itself:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;moments &amp;lt;- function(theta, data) {
    y &amp;lt;- as.numeric(data[, 1])
    x &amp;lt;- data.matrix(data[, 2:11])
    m &amp;lt;- x * as.vector((y - logistic(theta, x)))
    return(cbind(m))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The moment condition(s) are given by a function which returns a matrix with as many columns as moment conditions (same number of columns as parameters for just-identified models).&lt;/p&gt;

&lt;p&gt;To use the &lt;code&gt;gmm()&lt;/code&gt; function to estimate our model, we need to specify some initial values to get the maximization routine going. One neat trick is simply to use the coefficients of a linear regression; I found it to work well in a lot of situations:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;init &amp;lt;- (lm(y ~ age + age2 + dkids + dykids + head + male + married + rr + rr2))$coefficients
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally, we have everything to use &lt;code&gt;gmm()&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;r&#34;&gt;my_gmm &amp;lt;- gmm(moments, x = dat, t0 = init, type = &amp;quot;iterative&amp;quot;, crit = 1e-25, wmatrix = &amp;quot;optimal&amp;quot;, method = &amp;quot;Nelder-Mead&amp;quot;, control = list(reltol = 1e-25, maxit = 20000))

summary(my_gmm)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
## Call:
## gmm(g = moments, x = dat, t0 = init, type = &amp;quot;iterative&amp;quot;, wmatrix = &amp;quot;optimal&amp;quot;, 
##     crit = 1e-25, method = &amp;quot;Nelder-Mead&amp;quot;, control = list(reltol = 1e-25, 
##         maxit = 20000))
## 
## 
## Method:  iterative 
## 
## Kernel:  Quadratic Spectral
## 
## Coefficients:
##              Estimate    Std. Error  t value     Pr(&amp;gt;|t|)  
## (Intercept)  -0.9090571   0.5751429  -1.5805761   0.1139750
## age           0.0394254   0.0231964   1.6996369   0.0891992
## age2         -0.0018805   0.0029500  -0.6374640   0.5238227
## dkids        -0.0994031   0.0842057  -1.1804799   0.2378094
## dykids        0.1923245   0.0950495   2.0234150   0.0430304
## head         -0.2067669   0.0801624  -2.5793498   0.0098987
## male         -0.0617586   0.0846334  -0.7297189   0.4655620
## married       0.2358055   0.0764071   3.0861736   0.0020275
## rr            3.7895781   1.8332559   2.0671300   0.0387219
## rr2          -5.2849002   2.2976075  -2.3001753   0.0214383
## 
## J-Test: degrees of freedom is 0 
##                 J-test               P-value            
## Test E(g)=0:    0.00099718345776501  *******            
## 
## #############
## Information related to the numerical optimization
## Convergence code =  10 
## Function eval. =  17767 
## Gradian eval. =  NA
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Please, notice the options &lt;code&gt;crit=1e-25,method=&amp;quot;Nelder-Mead&amp;quot;,control=list(reltol=1e-25,maxit=20000)&lt;/code&gt;: these options mean that the Nelder-Mead algorithm is used, and to specify further options to the Nelder-Mead algorithm, the &lt;code&gt;control&lt;/code&gt; option is used. This is very important, as Pierre Chaussé explained to me: non-linear optimization is an art, and most of the time the default options won&amp;#39;t cut it and will give you false results. To add insult to injury, the Generalized Method of Moments itself is very capricious and you will also have to play around with different initial values to get good results. As you can see, the Convergence code equals 10, which is a code specific to the Nelder-Mead method which indicates «degeneracy of the Nelder–Mead simplex.» . I&#39;m not sure if this is a bad thing though, but other methods can give you better results. I&#39;d suggest you try always different maximization routines with different starting values to see if your estimations are robust. Here, the results are very similar to what we obtained with the built-in function &lt;code&gt;glm()&lt;/code&gt; so we can stop here.&lt;/p&gt;

&lt;p&gt;Should you notice any error whatsoever, do not hesitate to tell me.&lt;/p&gt;

&lt;p&gt;&lt;/body&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Who am I?</title>
      <link>/about/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/about/</guid>
      <description>&lt;p&gt;My name is Bruno Rodrigues and hold a PhD in Economics from the
&lt;a href=&#34;http://www.unistra.fr&#34;&gt;University of Strasbourg&lt;/a&gt;.&lt;/p&gt;

&lt;div style=&#34;float: left;margin: 0px 80px 50px 0px&#34;&gt;
    &lt;img src=&#34;/img/profile.webp&#34; width=&#34;400&#34; height=&#34;400&#34;/&gt;
&lt;/div&gt;

&lt;p&gt;I&amp;rsquo;m currently employed as a statistician for the Ministry of Higher Education and Research in
Luxembourg. Before that I was senior data scientist and then manager in the data science team
at PwC Luxembourg, and before that I was a research assistant at &lt;a href=&#34;http://www.statistiques.public.lu/en/actors/statec/organisation/red/&#34;&gt;STATEC Research&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My hobbies are boxing, lifting weights, cycling, cooking and reading or listening to audiobooks,
which is more compatible with the life of a young father.
I started this blog to share my enthusiasm for statistics. My blog posts are reshared on
&lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;R-bloggers&lt;/a&gt; and &lt;a href=&#34;https://rweekly.org/&#34;&gt;RWeekly&lt;/a&gt;.
I also enjoy learning about the R programming language and sharing my knowledge.
That&amp;rsquo;s why I made this blog and also am writing an &lt;a href=&#34;https://b-rodrigues.github.io/modern_R/&#34;&gt;ebook&lt;/a&gt;.
I also have a &lt;a href=&#34;https://www.youtube.com/channel/UCTZXht1RTL2Duc3eU8MYGzQ&#34;&gt;youtube channel&lt;/a&gt;,
where I show some tips and tricks with R, or rant about stuff.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>