---
date: 2017-02-17T12:51:00+01:00
title: How to use jailbreakr
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/jailbreakr
    weight: 3
---

```{r, include=FALSE}
path_to_data <- "/home/cbrunos/Documents/blogdown_blog/static/assets/Investment_total_factors_nace2.xlsx"
```

## What is `jailbreakr`

The `jailbreakr` package is probably one of the most interesting packages I came across recently.
This package makes it possible to extract messy data from spreadsheets. What is meant by messy? I
am sure you already had to deal with spreadsheets that contained little tables inside a single
sheet for example. As far as I know, there is no simple way of extracting these tables without having
to fiddle around a lot. This is now over with `jailbreakr`. Well not entirely, because `jailbreakr`
is still in development, but it works well already. If you want to know more about the planned
features, you can watch the
following
[video](https://channel9.msdn.com/Events/useR-international-R-User-conference/useR2016/jailbreakr-Get-out-of-Excel-free) by
Jenny Bryan, one of the package's authors.

## Installation and data

You will have to install the package from Github, as it is not on CRAN
yet. [Here is the Github link](https://github.com/rsheets/jailbreakr). To install the package, just
run the following commands in an R console:

```{r, eval = FALSE}
devtools::install_github(c("hadley/xml2",
                           "rsheets/linen",
                           "rsheets/cellranger",
                           "rsheets/rexcel",
                           "rsheets/jailbreakr"))
```

If you get the following error:

```
devtools::install_github("hadley/xml2")
Downloading GitHub repo hadley/xml2@master
from URL https://api.github.com/repos/hadley/xml2/zipball/master
Error in system(full, intern = quiet, ignore.stderr = quiet, ...) : 
    error in running command
```
 and if you're on a GNU+Linux distribution try to run the following command:
 
```{r, eval = FALSE}
options(unzip = "internal")
```

and then run `github_install()` again.

As you can see, you need some other packages to make it work. Now we are going to get some data. We
are going to download some time series from the European Commission, data I had to deal with
recently. Download the data by clicking [here](http://ec.europa.eu/economy_finance/db_indicators/surveys/documents/series/nace2_ecfin_1701/investment_total_nsa_nace2.zip)
and look for the spreadsheet titled `Investment_total_factors_nace2.xlsx`. The data we are interested
in is on the second sheet, named `TOT`. You cannot import this sheet easily into R because there
are four tables on the same sheet. Let us use `jailbreakr` to get these tables out of the sheet and
into nice, tidy, data frames.

## `jailbreakr` to the rescue

The first step is to read the data in. For this, we are going to use the `rexcel` package, which is
also part of the `rsheets` organization on Github that was set up by Jenny Brian and Rich Fitzjohn,
the authors of these packages. `rexcel` imports the sheet you want but not in a way that is
immediately useful to you. It just gets the sheet into R, which makes it then possible to use
`jailbreakr`'s magic on it. First, let's import the packages we need:

```{r}
library("rexcel")
library("jailbreakr")
```

We need to check which sheet to import. There are two sheets, and we want to import the one called
`TOT`, the second one. But is it really the second one? I have noticed that sometimes, there are
hidden sheets which makes importing the one you want impossible. So first, let use use another
package, `readxl` and its function `excel_sheets()` to make sure we are extracting the sheet we
really need:

```{r}
sheets <- readxl::excel_sheets(path_to_data)

tot_sheet <- which(sheets == "TOT")

print(tot_sheet)
```
As you can see, the sheet we want is not the second, but the third! Let us import this sheet into R now 
(this might take more time than you think; on my computer it takes around 10 seconds):

```{r, eval = FALSE}
my_sheet <- rexcel_read(path_to_data, sheet = tot_sheet)
```

```{r, include = FALSE, cache = TRUE}
my_sheet <- rexcel_read(path_to_data, sheet = tot_sheet)
```

Now we can start using `jailbreakr`. The function `split_sheet()` is the one that splits the sheet
into little tables:

```{r}
tables <- split_sheet(my_sheet)
str(tables)
```

`tables` is actually a list containing `worksheet_view` objects. Take a look at the `dim`
attribute: you see the dimensions of the tables there. When I started using `jailbreakr` I was
stuck here. I was looking for the function that would extract the data frames and could not find
it. Then I watched the video and I understood what I had to do: a `worksheet_view` object has a
`values()` method that does the extraction for you. This is a bit unusual in R (it made me feel
like I was using Python); maybe in future versions this `values()` method will become a separate
function of its own in the package. What happens when we use `values()`?

```{r}
library("purrr")
list_of_data <-  map(tables, (function(x)(x$values())))
map(list_of_data, head)
```

We are getting really close to something useful! Now we can get the first table and do some basic
cleaning to have a tidy dataset:

```{r}

dataset1 <- list_of_data[[1]]

dataset1 <- dataset1[-c(1:3), ]
dataset1[dataset1 == ":"] <- NA
colnames(dataset1) <- c("country", seq(from = 1991, to = 2017))

head(dataset1)
```

Et voilà! We went from a messy spreadsheet to a tidy dataset in a matter of minutes. Even though
this package is still in early development and not all the features that are planned are available,
the basics are there and can save you a lot of pain!

<!--chapter:end:2017-02-17-how_to_use_jailbreakr.Rmd-->

---
date: 2017-03-08T12:00:00+01:00
title: "Lesser known dplyr tricks"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/lesserknowntricks
    weight: 2
---

```{r, include=FALSE}
library("dplyr")
data(mtcars)
```


In this blog post I share some lesser-known (at least I believe they are) tricks that use mainly functions from `dplyr`.

## Removing unneeded columns

Did you know that you can use `-` in front of a column name to remove it from a data frame?

```{r}
mtcars %>% 
    select(-disp) %>% 
    head()
```

## Re-ordering columns

Still using `select()`, it is easy te re-order columns in your data frame:

```{r}
mtcars %>% 
    select(cyl, disp, hp, everything()) %>% 
    head()
```

As its name implies `everything()` simply means all the other columns.

## Renaming columns with `rename()`

```{r}
mtcars <- rename(mtcars, spam_mpg = mpg)
mtcars <- rename(mtcars, spam_disp = disp)
mtcars <- rename(mtcars, spam_hp = hp)

head(mtcars)
```
      
## Selecting columns with a regexp

It is easy to select the columns that start with "spam" with some helper functions:

```{r}
mtcars %>% 
    select(contains("spam")) %>% 
    head()
```

take also a look at `starts_with()`, `ends_with()`, `contains()`, `matches()`, `num_range()`, `one_of()` and `everything()`.

## Create new columns with `mutate()` and `if_else()`

```{r}
mtcars %>% 
    mutate(vs_new = if_else(
        vs == 1, 
        "one", 
        "zero", 
        NA_character_)) %>% 
    head()
```

You might want to create a new variable conditionally on several values of another column:

```{r}
mtcars %>% 
    mutate(carb_new = case_when(.$carb == 1 ~ "one",
                                .$carb == 2 ~ "two",
                                .$carb == 4 ~ "four",
                                 TRUE ~ "other")) %>% 
    head(15)
```

Mind the `.$` before the variable `carb`. There is a [github issue](https://github.com/hadley/dplyr/issues/1965) 
about this, and it is already fixed in the development version of `dplyr`, which means that in the next version
of `dplyr`, `case_when()` will work as any other specialized `dplyr` function inside `mutate()`.

## Apply a function to certain columns only, by rows

```{r, include=FALSE}
mtcars %>%
    select(am, gear, carb) %>%
    purrrlyr::by_row(sum, .collate = "cols", .to = "sum_am_gear_carb") -> mtcars2
head(mtcars2)
```

```{r, eval=FALSE}
mtcars %>%
    select(am, gear, carb) %>%
    purrr::by_row(sum, .collate = "cols", .to = "sum_am_gear_carb") -> mtcars2
head(mtcars2)
```
For this, I had to use `purrr`'s `by_row()` function. You can then add this column to your original data frame:

```{r}
mtcars <- cbind(mtcars, "sum_am_gear_carb" = mtcars2$sum_am_gear_carb)
head(mtcars)
```

## Use `do()` to do any arbitrary operation

```{r}
mtcars %>% 
    group_by(cyl) %>% 
    do(models = lm(spam_mpg ~ drat + wt, data = .)) %>% 
    broom::tidy(models)
```

`do()` is useful when you want to use any R function (user defined functions work too!) with `dplyr` functions.
First I grouped the observations by `cyl` and then ran a linear model for each group. Then I converted the output
to a tidy data frame using `broom::tidy()`.

## Using `dplyr` functions inside your own functions

```{r}
extract_vars <- function(data, some_string){
    
  data %>%
    select_(lazyeval::interp(~contains(some_string))) -> data
    
  return(data)
}

extract_vars(mtcars, "spam")
```

About this last point, you can read more about it [here](http://www.brodrigues.co/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/). 

Hope you liked this small list of tricks!


<!--chapter:end:2017-02-17-lesser_known_tricks.Rmd-->

---
date: 2017-03-24T12:00:00+01:00
title: "Lesser known purrr tricks"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/lesserknownpurrr
    weight: 2
---

```{r, include=FALSE}
library("purrr")
set.seed(1)
mat1 <- matrix(rnorm(10), nrow = 2)
set.seed(2)
mat2 <- matrix(rnorm(10), nrow = 2)
set.seed(3)
mat3 <- matrix(rnorm(10), nrow = 2)
```

`purrr` is a package that extends R's functional programming capabilities. It brings a lot of new stuff to
the table and in this post I show you some of the most useful (at least to me) functions included in `purrr`.

## Getting rid of loops with `map()`

```{r}
library(purrr)

numbers <- list(11, 12, 13, 14)

map_dbl(numbers, sqrt)
```

You might wonder why this might be preferred to a for loop? It's a lot less verbose, and you do not need to
initialise any kind of structure to hold the result. If you google "create empty list in R" you will see that
this is very common. However, with the `map()` family of functions, there is no need for an initial structure.
`map_dbl()` returns an atomic list of real numbers, but if you use `map()` you will get a list back. Try them all out!

## Map conditionally

#### map_if()

```{r}
# Create a helper function that returns TRUE if a number is even
is_even <- function(x){
  !as.logical(x %% 2)
}

map_if(numbers, is_even, sqrt)
```

#### map_at()

```{r}
map_at(numbers, c(1,3), sqrt)
```

`map_if()` and `map_at()` have a further argument than `map()`; in the case of `map_if()`, a predicate function (
a function that returns `TRUE` or `FALSE`) and a vector of positions for `map_at()`. This allows you to map your
function only when certain conditions are met, which is also something that a lot of people google for.

## Map a function with multiple arguments

```{r}
numbers2 <- list(1, 2, 3, 4)

map2(numbers, numbers2, `+`)
```

You can map two lists to a function which takes two arguments using `map_2()`. You can even map an arbitrary number
of lists to any function using `pmap()`.

By the way, try this in: ``` `+`(1,3)``` and see what happens.

## Don't stop execution of your function if something goes wrong

```{r}
possible_sqrt <- possibly(sqrt, otherwise = NA_real_)

numbers_with_error <- list(1, 2, 3, "spam", 4)

map(numbers_with_error, possible_sqrt)
```

Another very common issue is to keep running your loop even when something goes wrong. In most cases the loop simply stops
at the error, but you would like it to continue and see where it failed. Try to google "skip error in a loop"
or some variation of it and you'll see that a lot of people really just want that.
This is possible by combining `map()` and `possibly()`. Most solutions involve the use of
`tryCatch()` which I personally do not find very easy to use.

## Don't stop execution of your function if something goes wrong and capture the error

```{r}
safe_sqrt <- safely(sqrt, otherwise = NA_real_)

map(numbers_with_error, safe_sqrt)
```

`safely()` is very similar to `possibly()` but it returns a list of lists. An element is thus a list of the result
and the accompagnying error message. If there is no error, the error component is `NULL` if there is an error, it
returns the error message.

## Transpose a list

```{r}
safe_result_list <- map(numbers_with_error, safe_sqrt)

transpose(safe_result_list)
```

Here we transposed the above list. This means that we still have a list of lists, but where the first list holds
all the results (which you can then access with `safe_result_list$result`) and the second list holds all the errors
(which you can access with `safe_result_list$error`). This can be quite useful!

## Apply a function to a lower depth of a list

```{r}
transposed_list <- transpose(safe_result_list)

transposed_list %>%
    at_depth(2, is_null)
```

Sometimes working with lists of lists can be tricky, especially when we want to apply a function to the sub-lists. This
is easily done with `at_depth()`!

## Set names of list elements

```{r}
name_element <- c("sqrt()", "ok?")

set_names(transposed_list, name_element)
```

## Reduce a list to a single value

```{r}
reduce(numbers, `*`)
```

`reduce()` applies the function `*` iteratively to the list of numbers. There's also `accumulate()`:

```{r}
accumulate(numbers, `*`)
```

which keeps the intermediary results.

This function is very general, and you can reduce anything:

Matrices:
```{r, eval = FALSE}
mat1 <- matrix(rnorm(10), nrow = 2)
mat2 <- matrix(rnorm(10), nrow = 2)
mat3 <- matrix(rnorm(10), nrow = 2)
```

```{r}
list_mat <- list(mat1, mat2, mat3)

reduce(list_mat, `+`)
```

even data frames:

```{r}
df1 <- as.data.frame(mat1)
df2 <- as.data.frame(mat2)
df3 <- as.data.frame(mat3)

list_df <- list(df1, df2, df3)

reduce(list_df, dplyr::full_join)
```

Hope you enjoyed this list of useful functions! If you enjoy the content of my blog, you can follow me on [twitter](https://www.twitter.com/brodriguesco).

<!--chapter:end:2017-03-24-lesser_known_purrr.Rmd-->

---
date: 2017-03-27T09:23:56+02:00
title: "Introducing brotools"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/intro_brotools
    weight: 2
---

I'm happy to announce my first R package, called `brotools`. This is a package that contains
functions that are specific to my needs but that you might find also useful. I blogged about some
of these functions, so if you follow my blog you might already be familiar with some of them. It is 
not on CRAN and might very well never be. The code is hosted on [bitbucket](https://bitbucket.org/b-rodrigues/brotools)
and you can install the package with 

```{r, eval=FALSE}
devtools::install_bitbucket("b-rodrigues/brotools")
```

Hope you'll find the `brotools` useful!

<!--chapter:end:2017-03-27-introducing_brotools.Rmd-->

---
date: 2017-03-29T06:45:48+02:00
title: "Make ggplot2 purrr"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/ggplot2_purrr
    weight: 2
---

```{r setup, include=FALSE}
library(ggplot2)
library(ggthemes)
library(dplyr)
library(tidyr)
library(purrr)
library(pwt9)
```

*Update*: I've included another way of saving a separate plot by group in this article, as pointed out
by [`@monitus`](https://twitter.com/monitus/status/849033025631297536). Actually, this is the preferred
solution; using `dplyr::do()` is deprecated, according to Hadley Wickham [himself](https://twitter.com/hadleywickham/status/719542847045636096).

I'll be honest: the title is a bit misleading. I will not use `purrr` that much in this blog post.
Actually, I will use one single `purrr` function, at the very end. I use `dplyr` much more.
However *Make ggplot2 purrr* sounds better than *Make ggplot dplyr* or whatever the verb for `dplyr` would be.

Also, this blog post was inspired by a stackoverflow question and in particular one of the
[answers](http://stackoverflow.com/a/29035145/1298051). So I don't bring anything new to the table,
but I found this stackoverflow answer so useful and so underrated (only 16 upvotes as I'm writing
this!) that I wanted to write something about it.

Basically the idea of this blog post is to show how to create graphs using `ggplot2`, but by
grouping by a factor variable beforehand. To illustrate this idea, let's use the data from the [Penn World
Tables 9.0](http://www.rug.nl/ggdc/productivity/pwt/). The easiest way to get this data is to
install the package called `pwt9` with:

```{r, eval=FALSE}
install.packages("pwt9")
```

and then load the data with:

```{r, eval=FALSE}
data("pwt9.0")
```

Now, let's load the needed packages. I am also using `ggthemes` which makes themeing your ggplots
very easy. I'll be making [Tufte](https://en.wikipedia.org/wiki/Edward_Tufte)-style plots.

```{r, eval=FALSE}
library(ggplot2)
library(ggthemes)
library(dplyr)
library(tidyr)
library(purrr)
library(pwt9)
```

First let's select a list of countries:

```{r}
country_list <- c("France", "Germany", "United States of America", "Luxembourg", "Switzerland", "Greece")

small_pwt <- pwt9.0 %>%
  filter(country %in% country_list)
```

Let's us also order the countries in the data frame as I have written them in `country_list`:

```{r}
small_pwt <- small_pwt %>%
  mutate(country = factor(country, levels = country_list, ordered = TRUE))
```

You might be wondering why this is important. At the end of the article, we are going to save the
plots to disk. If we do not re-order the countries inside the data frame as in `country_list`, the
name of the files will not correspond to the correct plots!

*Update*: While this can still be interesting to know, especially if you want to order the bars
of a barplot made with `ggplot2`, I included a suggestion by [`@expersso`](https://twitter.com/expersso/status/846986357792739328)
that does not require your data to be ordered!

Now when you want to plot the same variable by countries, say `avh` (*Average annual hours worked by
persons engaged*), the usual way to do this is with one of `facet_wrap()` or `facet_grid()`:

```{r}
ggplot(data = small_pwt) + theme_tufte() +
  geom_line(aes(y = avh, x = year)) +
  facet_wrap(~country)
```

```{r}
ggplot(data = small_pwt) + theme_tufte() +
  geom_line(aes(y = avh, x = year)) +
  facet_grid(country~.)
```

As you can see, for this particular example, `facet_grid()` is not very useful, but do notice its
argument, `country~.`, which is different from `facet_wrap()`'s argument. This way, I get the graphs
stacked horizontally. If I had used `facet_grid(~country)` the graphs would be side by side and completely
unreadable.

Now, let's go to the meat of this post: what if you would like to have one single graph for each
country? You'd probably think of using `dplyr::group_by()` to form the groups and then the graphs. This
is the way to go, but you also have to use `dplyr::do()`. This is because as far as I understand,
`ggplot2` is not `dplyr`-aware, and using an arbitrary function with groups is only possible with
`dplyr::do()`.

*Update*: As explained in the intro above, I also added the solution that uses `tidyr::nest()`:

```{r, eval = FALSE}
# Ancient, deprecated way of doing this
plots <- small_pwt %>%
  group_by(country) %>%
  do(plot = ggplot(data = .) + theme_tufte() +
       geom_line(aes(y = avh, x = year)) +
       ggtitle(unique(.$country)) +
       ylab("Year") +
       xlab("Average annual hours worked by persons engaged"))
```

And this is the approach that uses `tidyr::nest()`:

```{r}
# Preferred approach
plots <- small_pwt %>%
  group_by(country) %>%
  nest() %>%
  mutate(plot = map2(data, country, ~ggplot(data = .x) + theme_tufte() +
       geom_line(aes(y = avh, x = year)) +
       ggtitle(.y) +
       ylab("Year") +
       xlab("Average annual hours worked by persons engaged")))
```

If you know `dplyr` at least a little bit, the above lines should be easy for you to understand.
But notice how we get the title of the graphs, with `ggtitle(unique(.$country))`, which was actually
the point of the stackoverflow question.

*Update:* The modern version uses `tidyr::nest()`. Its documentation tells us:

*There are many possible ways one could choose to nest columns inside a data frame.
`nest()` creates a list of data frames containing all the nested variables: this seems to be the most useful form in practice.*
Let's take a closer look at what it does exactly:

```{r}
small_pwt %>%
  group_by(country) %>%
  nest() %>%
  head()
```

This is why I love lists in R; we get a `tibble` where each element of the column `data` is itself a `tibble`.
We can now apply any function that we know works on lists.

What might be surprising though, is the object that is created by this code. Let's take a look at
`plots`:

```{r}
print(plots)
```

As `dplyr::do()`'s documentation tells us, the return values get stored inside a list. And this is
exactly what we get back; a list of plots! Lists are a very flexible and useful class, and you cannot
spell *list* without `purrr` (at least not when you're a ne`R`d).

Here are the final lines that use `purrr::map2()` to save all these plots at once inside your working directory:

*Update*: I have changed the code below which does not require your data frame to be ordered according
to the variable `country_list`.

```{r, eval=FALSE}
# file_names <- paste0(country_list, ".pdf")

map2(paste0(plots$country, ".pdf"), plots$plot, ggsave)
```

As I said before, if you do not re-order the countries inside the data frame, the names of the files
and the plots will not match. Try running all the code without re-ordering, you'll see!

I hope you found this post useful. You can follow me on [twitter](https://www.twitter.com/brodriguesco) for
blog updates.

*Update*: Many thanks to the readers of this article and for their useful suggestions. I love the R
community; everyday I learn something new and useful!

<!--chapter:end:2017-03-29-make-ggplot2-purrr.Rmd-->

---
date: 2017-07-02
title: "Lesser known dplyr 0.7* tricks"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/lesserknowndplyr07tricks
    weight: 2
---

This blog post is an update to an older [one](http://www.brodrigues.co/blog/2017-02-17-lesser_known_tricks/)
I wrote in March.
In the post from March, `dplyr` was at version 0.50, but since then a major update introduced some
changes that make some of the tips in that post obsolete. So here I revisit the blog post from March
by using `dplyr` 0.70.

## Create new columns with `mutate()` and `case_when()`

The basic things such as selecting columns, renaming them, filtering, etc did not change with this new
version. What did change however is creating new columns using `case_when()`.
First, load `dplyr` and the `mtcars` dataset:

```{r, include=FALSE}
library("dplyr")
data(mtcars)
```

```{r, eval=FALSE}
library("dplyr")
data(mtcars)
```

This was how it was done in version 0.50 (notice the '.$' symbol before the variable 'carb'):

```{r}
mtcars %>%
    mutate(carb_new = case_when(.$carb == 1 ~ "one",
                                .$carb == 2 ~ "two",
                                .$carb == 4 ~ "four",
                                 TRUE ~ "other")) %>%
    head(5)
```

This has been simplified to:

```{r}
mtcars %>%
    mutate(carb_new = case_when(carb == 1 ~ "one",
                                carb == 2 ~ "two",
                                carb == 4 ~ "four",
                                TRUE ~ "other")) %>%
    head(5)
```

No need for `.$` anymore.

## Apply a function to certain columns only, by rows, with `purrrlyr`

`dplyr` wasn't the only package to get an overhaul, `purrr` also got the same treatment.

In the past, I applied a function to certains columns like this:

```{r, eval=FALSE}
mtcars %>%
    select(am, gear, carb) %>%
    purrr::by_row(sum, .collate = "cols", .to = "sum_am_gear_carb") -> mtcars2
head(mtcars2)
```

Now, `by_row()` does not exist in `purrr` anymore, but instead a new package called `purrrlyr`
was introduced with functions that don't really fit inside `purrr` nor `dplyr`:

```{r}
mtcars %>%
    select(am, gear, carb) %>%
    purrrlyr::by_row(sum, .collate = "cols", .to = "sum_am_gear_carb") -> mtcars2
head(mtcars2)
```

Think of `purrrlyr` as `purrr`s and `dplyr`s love child.

## Using `dplyr` functions inside your own functions, or what is `tidyeval`

Programming with `dplyr` has been simplified a lot. Before version `0.70`, one needed to use
`dplyr` in conjuction with `lazyeval` to use `dplyr` functions inside one's own fuctions. It was
not always very easy, especially if you mixed columns and values inside your functions. Here's the
example from the March blog post:


```{r, eval=FALSE}
extract_vars <- function(data, some_string){

  data %>%
    select_(lazyeval::interp(~contains(some_string))) -> data

  return(data)
}

extract_vars(mtcars, "spam")
```

More examples are available in [this other blog post](http://www.brodrigues.co/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/).

I will revisit them now with `dplyr`'s new `tidyeval` syntax. I'd recommend you read the *Tidy evaluation*
vignette [here](https://cran.r-project.org/web/packages/rlang/vignettes/tidy-evaluation.html). This vignette
is part of the `rlang` package, which gets used under the hood by `dplyr` for all your programming needs.
Here is the function I called `simpleFunction()`, written with the old `dplyr` syntax:

```{r}
simpleFunction <- function(dataset, col_name){
  dataset %>%
    group_by_(col_name) %>%
    summarise(mean_mpg = mean(mpg)) -> dataset
  return(dataset)
}


simpleFunction(mtcars, "cyl")
```

With the new synax, it must be rewritten a little bit:

```{r}
simpleFunction <- function(dataset, col_name){
  col_name <- enquo(col_name)
  dataset %>%
    group_by(!!col_name) %>%
    summarise(mean_mpg = mean(mpg)) -> dataset
  return(dataset)
}


simpleFunction(mtcars, cyl)
```

What has changed? Forget the underscore versions of the usual functions such as `select_()`,
`group_by_()`, etc. Now, you must quote the column name using `enquo()` (or just `quo()` if working
interactively, outside a function), which returns a **quosure**. This **quosure** can then be
evaluated using `!!` in front of the quosure and inside the usual `dplyr` functions.

Let's look at another example:

```{r}
simpleFunction <- function(dataset, col_name, value){
  filter_criteria <- lazyeval::interp(~y == x, .values=list(y = as.name(col_name), x = value))
  dataset %>%
    filter_(filter_criteria) %>%
    summarise(mean_cyl = mean(cyl)) -> dataset
  return(dataset)
}


simpleFunction(mtcars, "am", 1)
```

As you can see, it's a bit more complicated, as you needed to use `lazyeval::interp()` to make it work.
With the improved `dplyr`, here's how it's done:

```{r}
simpleFunction <- function(dataset, col_name, value){
  col_name <- enquo(col_name)
  dataset %>%
    filter((!!col_name) == value) %>%
    summarise(mean_cyl = mean(cyl)) -> dataset
  return(dataset)
}


simpleFunction(mtcars, am, 1)
```

Much, much easier! There is something that you must pay attention to though. Notice that I've written:

```{r, eval=FALSE}
filter((!!col_name) == value)
```

and not:

```{r, eval=FALSE}
filter(!!col_name == value)
```

I have enclosed `!!col_name` inside parentheses. I struggled with this, but thanks to help
from [\@dmi3k](https://twitter.com/dmi3k/status/880374506291953664) and
[\@_lionelhenry](https://twitter.com/_lionelhenry/status/880380691078361090) I was able to understand
what was happening (isn't the #rstats community on twitter great?).

One last thing: let's make this function a bit more general. I hard-coded the variable `cyl` inside the
body of the function, but maybe you'd like the mean of another variable? Easy:

```{r}
simpleFunction <- function(dataset, group_col, mean_col, value){
  group_col <- enquo(group_col)
  mean_col <- enquo(mean_col)
  dataset %>%
    filter((!!group_col) == value) %>%
    summarise(mean((!!mean_col))) -> dataset
  return(dataset)
}


simpleFunction(mtcars, am, cyl, 1)
```

*«That's very nice Bruno, but `mean((cyl))` in the output looks ugly as sin»* you might think, and you'd be
right. It is possible to set the name of the column in the output using `:=` instead of `=`:

```{r}
simpleFunction <- function(dataset, group_col, mean_col, value){
  group_col <- enquo(group_col)
  mean_col <- enquo(mean_col)
  mean_name <- paste0("mean_", mean_col)[2]
  dataset %>%
    filter((!!group_col) == value) %>%
    summarise(!!mean_name := mean((!!mean_col))) -> dataset
  return(dataset)
}


simpleFunction(mtcars, am, cyl, 1)
```
To get the name of the column I added this line:

```{r, eval=FALSE}
mean_name <- paste0("mean_", mean_col)[2]
```

To see what it does, try the following inside an R interpreter (remember to us `quo()` instead of `enquo()`
outside functions!):

```{R}
paste0("mean_", quo(cyl))
```

`enquo()` quotes the input, and with `paste0()` it gets converted to a string that can be used as a column
name. However, the `~` is in the way and the output of `paste0()` is a vector of two strings: the correct
name is contained in the second element, hence the `[2]`. There might be a more elegant way of doing that,
but for now this has been working well for me.

That was it folks! I do recommend you read the *Programming with dplyr* vignette
[here](https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html) as well as other blog posts,
such as the one recommended to me by [\@dmi3k](https://twitter.com/dmi3k)
[here](http://www.win-vector.com/blog/2017/06/non-standard-evaluation-and-function-composition-in-r/).

Have fun with `dplyr 0.70`!

<!--chapter:end:2017-06-19-dplyr-0-70-tutorial.Rmd-->

---
date: 2017-07-27
title: "tidyr::spread() and dplyr::rename_at() in action"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/spread_rename_at
    weight: 1
---

I was recently confronted to a situation that required going from a long dataset to a wide dataset,
but with a small twist: there were two datasets, which I had to merge into one. You might wonder
what kinda crappy twist that is, right? Well, let's take a look at the data:

```{r, include=FALSE}
library("dplyr")
library("tidyr")
data1 <- readr::read_csv("test_data1.csv")
data2 <- readr::read_csv("test_data2.csv")
```

```{r}
data1; data2
```

As explained in [Hadley (2014)](http://vita.had.co.nz/papers/tidy-data.html), this is how you should keep your data... But for a particular
purpose, I had to transform these datasets. What I was asked to do was to merge these into a single
wide data frame. Doing this for one dataset is easy:

```{r}
data1 %>%
  spread(variable_1, value)
```

But because `data1` and `data2` have the same levels for `variable_1` and `variable_2`, this would not
work. So the solution I found online, in this [SO thread](https://stackoverflow.com/questions/43578723/conditional-replacement-of-column-name-in-tibble-using-dplyr) was to use `tidyr::spread()` with
`dplyr::rename_at()` like this:

```{r}
data1 <- data1 %>%
  spread(variable_1, value) %>%
  rename_at(vars(-country, -date), funs(paste0("variable1:", .)))

glimpse(data1)
```

```{r}
data2 <- data2 %>%
  spread(variable_2, value) %>%
  rename_at(vars(-country, -date), funs(paste0("variable2:", .)))

glimpse(data2)
```

`rename_at()` needs variables which you pass to `vars()`, a helper function to select variables, and
a function that will do the renaming, passed to `funs()`. The function I use is simply `paste0()`,
which pastes a string, for example "variable1:" with the name of the columns, given by the single '.',
a dummy argument. Now these datasets can be merged:

```{r}
data1 %>%
  full_join(data2) %>%
  glimpse()
```

Hope this post helps you understand the difference between long and wide datasets better, as well
as `dplyr::rename_at()`!

<!--chapter:end:2017-07-27-spread_rename_at.Rmd-->

---
date: 2017-08-27
title: "Why I find tidyeval useful"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/why_tidyeval
    weight: 1
---

First thing's first: maybe you shouldn't care about `tidyeval`. Maybe you don't need it. If you
exclusively work interactively, I don't think that learning about `tidyeval` is important. I can
only speak for me, and explain to you why I personally find `tidyeval` useful.

I wanted to write  this blog post after reading this
[twitter thread](https://twitter.com/dataandme/status/901429535266267136)
and specifically [this question](https://twitter.com/Kwarizmi/status/901457435948236801).

[Mara Averick](https://twitter.com/dataandme) then wrote
[this blogpost](http://maraaverick.rbind.io/2017/08/tidyeval-resource-roundup/) linking to 6 other blog
posts that give some `tidyeval` examples. Reading them, plus the
[Programming with dplyr](http://dplyr.tidyverse.org/articles/programming.html) vignette should help you
get started with `tidyeval`.

But maybe now you know how to use it, but not why and when you should use it... Basically, whenever
you want to write a function that looks something like this:

```{r, eval = FALSE}
my_function(my_data, one_column_inside_data)
```

is when you want to use the power of `tidyeval`.

I work at [STATEC](http://www.statistiques.public.lu/en/index.html),
Luxembourg's national institute of statistics. I work on all kinds of different projects, and when
data gets updated (for example because a new round of data collection for some survey finished),
I run my own scripts on the fresh data to make the data nice and tidy for analysis. Because surveys
get updated, sometimes column names change a little bit, and this can cause some issues.

Very recently, a dataset I work with got updated. Data collection was finished, so I
just loaded my hombrewed package written for this project, changed the path from last year's script
to this year's fresh data path, ran the code, and watched as the folders got populated with new
`ggplot2` graphs and LaTeX tables with descriptive statistics and regression
results. This is then used to generate this year's report. However, by looking at the graphs, I
noticed something weird; some graphs were showing some very strange patterns. It turns out that one
column got its name changed, and also one of its values got changed too.

Last year, this column, let's call it `spam`, had values `1` for `good` and `0` for `bad`.
This year the column is called `Spam` and the values are `1` and `2`. When I found out that this was
the source of the problem, I just had to change the arguments of my functions from

```{r, eval = FALSE}
generate_spam_plot(dataset = data2016, column = spam, value = 1)
generate_spam_plot(dataset = data2016, column = spam, value = 0)
```

to

```{r, eval=FALSE}
generate_spam_plot(dataset = data2017, column = Spam, value = 1)
generate_spam_plot(dataset = data2017, column = Spam, value = 2)
```

without needing to change anything else. This is why I use `tidyeval`; without it, writing a
function such as `genereta_spam_plot` would not be easy. It would be possible, but not easy.

If you want to know more about `tidyeval` and working programmatically with R, I shamelessly
invite you to read a book I've been working on: https://b-rodrigues.github.io/fput/
It's still a WIP, but maybe you'll find it useful. I plan on finishing it by the end of the year,
but there's already some content to keep you busy!

<!--chapter:end:2017-08-27-why_tidyeval.Rmd-->

---
date: 2017-10-26
title: "Easy peasy STATA-like marginal effects with R"
tags: [R]
menu:
  main:
    parent: Blog
    identifier: /blog/margins_r
    weight: 1
---

Model interpretation is essential in the social sciences. If one wants to know the effect of
variable `x` on the dependent variable `y`, marginal effects are an easy way to get the answer.
STATA includes a `margin` command that has been ported by [Thomas J. Leeper](http://thomasleeper.com/)
of the London School of Economics and Political Science. 
You can find the source code of the package 
[on github](https://github.com/leeper/margins). In this short blog post, I demo some of the 
functionality of `margins`.

First, let's load some packages:

```{r}

```{r, include=FALSE}
library(ggplot2)
library(tibble)
library(broom)
library(margins)
library(Ecdat)
```

```{r, eval=FALSE}
library(ggplot2)
library(tibble)
library(broom)
library(margins)
library(Ecdat)
```

As an example, we are going to use the `Participation` data from the `Ecdat` package:

```{r}
data(Participation)
```

```{r, eval=FALSE}
?Participation
```


```
Labor Force Participation

Description

a cross-section

number of observations : 872

observation : individuals

country : Switzerland

Usage

data(Participation)
Format

A dataframe containing :

lfp
labour force participation ?

lnnlinc
the log of nonlabour income

age
age in years divided by 10

educ
years of formal education

nyc
the number of young children (younger than 7)

noc
number of older children

foreign
foreigner ?

Source

Gerfin, Michael (1996) “Parametric and semiparametric estimation of the binary response”, Journal of Applied Econometrics, 11(3), 321-340.

References

Davidson, R. and James G. MacKinnon (2004) Econometric Theory and Methods, New York, Oxford University Press, http://www.econ.queensu.ca/ETM/, chapter 11.

Journal of Applied Econometrics data archive : http://qed.econ.queensu.ca/jae/.
```

The variable of interest is `lfp`: whether the individual participates in the labour force. To know
which variables are relevant in the decision to participate in the labour force, one could estimate
a logit model, using `glm()`.

```{r}
logit_participation = glm(lfp ~ ., data = Participation, family = "binomial")

```

Now that we ran the regression, we can take a look at the results. I like to use `broom::tidy()` 
to look at the results of regressions, as `tidy()` returns a nice
`data.frame`, but you could use `summary()` if you're only interested in reading the output:

```{r}
tidy(logit_participation)
```

From the results above, one can only interpret the sign of the coefficients. To know how much a
variable influences the labour force participation, one has to use `margins()`:

```{r}
effects_logit_participation = margins(logit_participation) 

print(effects_logit_participation)
```

Using `summary()` on the object returned by `margins()` provides more details:

```{r}
summary(effects_logit_participation)
```

And it is also possible to plot the effects with base graphics:

```{r}
plot(effects_logit_participation)
```

This uses the basic R plotting capabilities, which is useful because it is a simple call to the
function `plot()` but if you've been using `ggplot2` and want this graph to have the same look as
the others made with `ggplot2` you first need to save the summary in a variable.
Let's overwrite this `effects_logit_participation` variable with its summary:


```{r}
effects_logit_participation = summary(effects_logit_participation)
```

And now it is possible to use `ggplot2` to create the same plot:

```{r}
ggplot(data = effects_logit_participation) +
  geom_point(aes(factor, AME)) +
  geom_errorbar(aes(x = factor, ymin = lower, ymax = upper)) +
  geom_hline(yintercept = 0) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45))
```


So an infinitesimal increase, in say, non-labour income (`lnnlinc`) of 0.001 is associated with a
decrease of the probability of labour force participation by 0.001*17 percentage points.

You can also extract the marginal effects of a single variable, with `dydx()`:

```{r}
head(dydx(Participation, logit_participation, "lnnlinc"))
```

Which makes it possible to extract the effects for a list of individuals that you can create yourself:

```{r}
my_subjects = tribble(
    ~lfp,  ~lnnlinc, ~age, ~educ, ~nyc, ~noc, ~foreign,
    "yes",   10.780,  7.0,     4,    1,    1,    "yes",
     "no",     1.30,  9.0,     1,    4,    1,    "yes"
)

dydx(my_subjects, logit_participation, "lnnlinc")
```

I used the `tribble()` function from the `tibble` package to create this test data set, row by 
row. Then, using `dydx()`, I get the marginal effect of variable `lnnlinc` for these two individuals.
No doubt that this package will be a huge help convincing more social scientists to try out R and
make a potential transition from STATA easier.

<!--chapter:end:2017-10-26-margins_r.Rmd-->

